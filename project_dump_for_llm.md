# Parsed codebase for the project: sefaria


## Directory Structure
- sefaria/
- sefaria/local_settings_coolify.py (833 bytes)
- sefaria/conftest.py (194 bytes)
- sefaria/clean.py (3963 bytes)
- sefaria/pytest.ini (610 bytes)
- sefaria/sheets.py (43963 bytes)
- sefaria/.pytest_cache/
- sefaria/.pytest_cache/CACHEDIR.TAG (191 bytes)
- sefaria/.pytest_cache/README.md (302 bytes)
- sefaria/.pytest_cache/v/
- sefaria/.pytest_cache/v/cache/
- sefaria/.pytest_cache/v/cache/nodeids (5074 bytes)
- sefaria/.pytest_cache/v/cache/lastfailed (806 bytes)
- sefaria/.pytest_cache/v/cache/stepwise (2 bytes)
- sefaria/spacy_function_registry.py (1053 bytes)
- sefaria/sefaria_tasks_interace/
- sefaria/sefaria_tasks_interace/history_change.py (333 bytes)
- sefaria/constants/
- sefaria/constants/model.py (1158 bytes)
- sefaria/recommendation_engine.py (17291 bytes)
- sefaria/sitemap.py (6699 bytes)
- sefaria/tests/
- sefaria/tests/conftest.py (203 bytes)
- sefaria/tests/texts_test.py (1153 bytes)
- sefaria/tests/activity_test.py (947 bytes)
- sefaria/tests/summaries_test.py (8043 bytes)
- sefaria/tests/__init__.py (0 bytes)
- sefaria/tests/links_test.py (968 bytes)
- sefaria/tests/counts_test.py (176 bytes)
- sefaria/tests/test_tracker.py (35 bytes)
- sefaria/tests/search.py (1567 bytes)
- sefaria/tests/recommendation_test.py (1308 bytes)
- sefaria/celery_setup/
- sefaria/celery_setup/config.py (648 bytes)
- sefaria/celery_setup/generate_config.py (2743 bytes)
- sefaria/celery_setup/app.py (243 bytes)
- sefaria/__init__.py (0 bytes)
- sefaria/datatype/
- sefaria/datatype/jagged_array.py (30854 bytes)
- sefaria/datatype/tests/
- sefaria/datatype/tests/jagged_array_test.py (12798 bytes)
- sefaria/datatype/__init__.py (25 bytes)
- sefaria/utils/
- sefaria/utils/user.py (4989 bytes)
- sefaria/utils/talmud.py (1092 bytes)
- sefaria/utils/log.py (2198 bytes)
- sefaria/utils/testing_utils.py (2153 bytes)
- sefaria/utils/util.py (20297 bytes)
- sefaria/utils/tests/
- sefaria/utils/tests/calendars_test.py (1298 bytes)
- sefaria/utils/tests/util_test.py (2901 bytes)
- sefaria/utils/tests/hebrew_test.py (4940 bytes)
- sefaria/utils/tests/time_test.py (6 bytes)
- sefaria/utils/__init__.py (25 bytes)
- sefaria/utils/hebrew.py (19883 bytes)
- sefaria/utils/calendars.py (17987 bytes)
- sefaria/export.py (25681 bytes)
- sefaria/google_storage_manager.py (3793 bytes)
- sefaria/local_settings.py (12422 bytes)
- sefaria/forms.py (5995 bytes)
- sefaria/system/
- sefaria/system/logging.py (1046 bytes)
- sefaria/system/caches.py (7215 bytes)
- sefaria/system/validators.py (1311 bytes)
- sefaria/system/database.py (8407 bytes)
- sefaria/system/serializers.py (494 bytes)
- sefaria/system/tests/
- sefaria/system/tests/test_database.py (1192 bytes)
- sefaria/system/tests/test_decorators.py (3416 bytes)
- sefaria/system/tests/test_varnish.py (912 bytes)
- sefaria/system/cache.py (5737 bytes)
- sefaria/system/__init__.py (0 bytes)
- sefaria/system/cloudflare.py (3464 bytes)
- sefaria/system/varnish/
- sefaria/system/varnish/wrapper.py (7047 bytes)
- sefaria/system/varnish/__init__.py (0 bytes)
- sefaria/system/varnish/common.py (1504 bytes)
- sefaria/system/varnish/thin_wrapper.py (1189 bytes)
- sefaria/system/multiserver/
- sefaria/system/multiserver/coordinator.py (5692 bytes)
- sefaria/system/multiserver/monitor.py (4101 bytes)
- sefaria/system/multiserver/__init__.py (0 bytes)
- sefaria/system/multiserver/messaging.py (1544 bytes)
- sefaria/system/context_processors.py (7274 bytes)
- sefaria/system/exceptions.py (3433 bytes)
- sefaria/system/testing.py (108 bytes)
- sefaria/system/middleware.py (10671 bytes)
- sefaria/system/decorators.py (5949 bytes)
- sefaria/stats.py (13252 bytes)
- sefaria/model/
- sefaria/model/abstract.py (25223 bytes)
- sefaria/model/queue.py (885 bytes)
- sefaria/model/autospell.py (29951 bytes)
- sefaria/model/link.py (23970 bytes)
- sefaria/model/guide.py (6105 bytes)
- sefaria/model/timeperiod.py (13484 bytes)
- sefaria/model/story.py (1910 bytes)
- sefaria/model/place.py (4075 bytes)
- sefaria/model/manuscript.py (9152 bytes)
- sefaria/model/media.py (2008 bytes)
- sefaria/model/marked_up_text_chunk.py (3169 bytes)
- sefaria/model/portal.py (4830 bytes)
- sefaria/model/passage.py (1387 bytes)
- sefaria/model/webpage.py (27715 bytes)
- sefaria/model/linker/
- sefaria/model/linker/named_entity_resolver.py (5178 bytes)
- sefaria/model/linker/tests/
- sefaria/model/linker/tests/named_entity_resolver_tests.py (703 bytes)
- sefaria/model/linker/tests/test_linker_paragraphs.py (1024 bytes)
- sefaria/model/linker/tests/resolved_ref.py (3497 bytes)
- sefaria/model/linker/tests/linker_test.py (37381 bytes)
- sefaria/model/linker/tests/ne_span_test.py (3515 bytes)
- sefaria/model/linker/tests/linker_linker_test.py (0 bytes)
- sefaria/model/linker/tests/__init__.py (0 bytes)
- sefaria/model/linker/tests/category_matcher_test.py (1724 bytes)
- sefaria/model/linker/tests/resolved_category_test.py (1179 bytes)
- sefaria/model/linker/tests/non_unique_term_test.py (825 bytes)
- sefaria/model/linker/tests/linker_test_utils.py (4857 bytes)
- sefaria/model/linker/ne_span.py (3883 bytes)
- sefaria/model/linker/match_template.py (9606 bytes)
- sefaria/model/linker/linker_entity_recognizer.py (13166 bytes)
- sefaria/model/linker/has_match_template.py (788 bytes)
- sefaria/model/linker/resolved_ref_refiner.py (9296 bytes)
- sefaria/model/linker/named_entity_recognizer.py (4251 bytes)
- sefaria/model/linker/linker.py (10533 bytes)
- sefaria/model/linker/ref_resolver.py (40260 bytes)
- sefaria/model/linker/ref_part.py (16299 bytes)
- sefaria/model/linker/referenceable_book_node.py (19159 bytes)
- sefaria/model/linker/category_resolver.py (1457 bytes)
- sefaria/model/linker/resolved_ref_refiner_factory.py (3219 bytes)
- sefaria/model/garden.py (18717 bytes)
- sefaria/model/trend.py (25827 bytes)
- sefaria/model/notification.py (13637 bytes)
- sefaria/model/tests/
- sefaria/model/tests/index_offsets_by_depth_tests.py (4091 bytes)
- sefaria/model/tests/index_test.py (1216 bytes)
- sefaria/model/tests/virtual_text_tests.py (1102 bytes)
- sefaria/model/tests/note_test.py (552 bytes)
- sefaria/model/tests/link_test.py (6401 bytes)
- sefaria/model/tests/terms_test.py (7938 bytes)
- sefaria/model/tests/marked_up_text_chunk.py (5710 bytes)
- sefaria/model/tests/schema_test.py (12000 bytes)
- sefaria/model/tests/story_test.py (54 bytes)
- sefaria/model/tests/autospell_test.py (2273 bytes)
- sefaria/model/tests/abstract_test.py (7176 bytes)
- sefaria/model/tests/lock_test.py (821 bytes)
- sefaria/model/tests/user_history_test.py (1621 bytes)
- sefaria/model/tests/text_test.py (31194 bytes)
- sefaria/model/tests/portal_test.py (15558 bytes)
- sefaria/model/tests/__init__.py (0 bytes)
- sefaria/model/tests/ref_test.py (57046 bytes)
- sefaria/model/tests/ref_catching_test.py (4236 bytes)
- sefaria/model/tests/library_test.py (27795 bytes)
- sefaria/model/tests/chunk_test.py (17920 bytes)
- sefaria/model/tests/lexicon_tests.py (4228 bytes)
- sefaria/model/tests/index_schema_test.py (37677 bytes)
- sefaria/model/tests/he_ref_test.py (14550 bytes)
- sefaria/model/tests/webpage_test.py (6007 bytes)
- sefaria/model/tests/category_test.py (10549 bytes)
- sefaria/model/tests/manuscript_test.py (5942 bytes)
- sefaria/model/tests/topic_test.py (12565 bytes)
- sefaria/model/tests/vstate_test.py (1220 bytes)
- sefaria/model/tests/node_test.py (12355 bytes)
- sefaria/model/tests/collection_test.py (1073 bytes)
- sefaria/model/version_state.py (19413 bytes)
- sefaria/model/__init__.py (2827 bytes)
- sefaria/model/text_request_adapter.py (12352 bytes)
- sefaria/model/lexicon.py (22977 bytes)
- sefaria/model/following.py (4218 bytes)
- sefaria/model/blocking.py (1319 bytes)
- sefaria/model/text.py (277500 bytes)
- sefaria/model/lock.py (1689 bytes)
- sefaria/model/topic.py (53908 bytes)
- sefaria/model/collection.py (16034 bytes)
- sefaria/model/layer.py (2625 bytes)
- sefaria/model/audio.py (1910 bytes)
- sefaria/model/note.py (1762 bytes)
- sefaria/model/ref_data.py (2352 bytes)
- sefaria/model/category.py (22280 bytes)
- sefaria/model/dependencies.py (7875 bytes)
- sefaria/model/schema.py (105572 bytes)
- sefaria/model/user_profile.py (35798 bytes)
- sefaria/model/history.py (6944 bytes)
- sefaria/local_settings_example.py (11457 bytes)
- sefaria/settings.py (11297 bytes)
- sefaria/site/
- sefaria/site/__init__.py (0 bytes)
- sefaria/site/site_settings.py (158 bytes)
- sefaria/site/urls.py (161 bytes)
- sefaria/search.py (28470 bytes)
- sefaria/tracker.py (10454 bytes)
- sefaria/django_cache/
- sefaria/django_cache/83695201f865d21bbd5db3cedaaef331.djcache (15 bytes)
- sefaria/django_cache/d881eb1c1507162aeeb4faf3657d25fb.djcache (15 bytes)
- sefaria/django_cache/0d358aca2952a9665b3a56f9278152f1.djcache (15 bytes)
- sefaria/django_cache/e622be3bbef6debf24c40e00dad1b6ce.djcache (15 bytes)
- sefaria/django_cache/0b1a9dc95cb3a70c48d092d69b9c9a8f.djcache (15 bytes)
- sefaria/django_cache/305b92d80db44227dcdf36af831a853a.djcache (15 bytes)
- sefaria/django_cache/7ed7448b888c6e04767484ad1de1bd87.djcache (15 bytes)
- sefaria/django_cache/2c56f94a394f128917e0ba424ee584f2.djcache (15 bytes)
- sefaria/django_cache/21ba67101433eaaea3eb026a84dc5191.djcache (15 bytes)
- sefaria/django_cache/f6168f1f26e457d1a48c2ee176ae9ebf.djcache (15 bytes)
- sefaria/django_cache/8fc22ab9d0a1ac8bfb099a1b7abd3b29.djcache (15 bytes)
- sefaria/django_cache/38b39a3b430bd9b4c7c5b7c12ad3334e.djcache (15 bytes)
- sefaria/django_cache/b2707d4d95f42be54b00193f8abead08.djcache (15 bytes)
- sefaria/django_cache/fbd13ec814111cedd0d1e1f104385a41.djcache (15 bytes)
- sefaria/django_cache/616ab890d3e29ddd2650528697cf62a4.djcache (15 bytes)
- sefaria/django_cache/752b5c26357457aaf190a7d628bc1e92.djcache (15 bytes)
- sefaria/django_cache/eb3fb91d20716377feb15a1e04676576.djcache (15 bytes)
- sefaria/django_cache/c270c32e832335eb14f4250973a9139e.djcache (15 bytes)
- sefaria/django_cache/bce5dd05acbc3b400147ee1c9f598dca.djcache (15 bytes)
- sefaria/django_cache/d2d483527e0ffbb5141af7397f506bae.djcache (15 bytes)
- sefaria/django_cache/1aac8b941e09f9e4d7321c092099b356.djcache (15 bytes)
- sefaria/django_cache/ceb521617019dd41d2afeff595f1c7a7.djcache (15 bytes)
- sefaria/django_cache/c52dc22594a8ca4d1a97e3ba406d9caa.djcache (15 bytes)
- sefaria/django_cache/2ed18b5292c8236d36fa0f1a99b5d58d.djcache (15 bytes)
- sefaria/django_cache/289149b6c76cb6a4e42c1ce6bf612857.djcache (15 bytes)
- sefaria/django_cache/12da11633a5e1831e193fc268144ec04.djcache (15 bytes)
- sefaria/django_cache/39b9a7db75659e73190e2781c9563190.djcache (15 bytes)
- sefaria/django_cache/db4ca7642be68f669dee04e6cdac9ca6.djcache (15 bytes)
- sefaria/django_cache/fc7da76462a60f57f5b28484638759c5.djcache (15 bytes)
- sefaria/django_cache/f89ef728a3571bf55e6ba777e7b79a76.djcache (15 bytes)
- sefaria/django_cache/7a4d864d34d73aae2f28c0778b1f5e1d.djcache (15 bytes)
- sefaria/django_cache/764ab871b10316305c642f70b74620fe.djcache (15 bytes)
- sefaria/django_cache/825503a435a8c3c1ea8d2df8e2d0710a.djcache (15 bytes)
- sefaria/django_cache/d5a465aa1bbeeadd713296376126d591.djcache (15 bytes)
- sefaria/django_cache/9ef469561003a773688eba451b18cd6e.djcache (15 bytes)
- sefaria/django_cache/4941457d521fc968ebe4c0038663db42.djcache (15 bytes)
- sefaria/django_cache/d98f0627079d1a6a00a9ad4ea4980472.djcache (15 bytes)
- sefaria/django_cache/8804d8374c7d15ae44e6470a1e21ccee.djcache (15 bytes)
- sefaria/django_cache/e3e4baf1ffae89881104150d527f76d2.djcache (15 bytes)
- sefaria/django_cache/37a9fd57c14ac4ea762e6034177cbad5.djcache (15 bytes)
- sefaria/django_cache/72c638384d74c369c8192ce373af2a10.djcache (15 bytes)
- sefaria/django_cache/d1baa6f1912acf99f869fe93075787b4.djcache (15 bytes)
- sefaria/django_cache/c3bb5c3a46fd17efe7acf17637a99061.djcache (15 bytes)
- sefaria/django_cache/d14a169e7c9f3d25502ffe6ab1f810cc.djcache (15 bytes)
- sefaria/django_cache/9300691e443aa0dc0e4ef6bb1b52a77c.djcache (15 bytes)
- sefaria/django_cache/02610440d4a982aed74a0fc30c331a61.djcache (15 bytes)
- sefaria/django_cache/26d8dd90ed611522eafa264debf057f3.djcache (15 bytes)
- sefaria/django_cache/1089ecdca55e3a33153cd442bf850914.djcache (15 bytes)
- sefaria/django_cache/fb5ba610854c6c793bdfb1c712c2677b.djcache (15 bytes)
- sefaria/django_cache/8b2977ec4b6fbbe9b45f5010f434528d.djcache (15 bytes)
- sefaria/django_cache/409cdb63d4e9d79b9c2f65ae2f99b5e0.djcache (15 bytes)
- sefaria/django_cache/776e2a42e831634da51d6c12cef914e0.djcache (15 bytes)
- sefaria/pagesheetrank.py (14910 bytes)
- sefaria/settings_utils.py (1088 bytes)
- sefaria/urls.py (27008 bytes)
- sefaria/helper/
- sefaria/helper/link.py (25916 bytes)
- sefaria/helper/llm/
- sefaria/helper/llm/tasks.py (1099 bytes)
- sefaria/helper/llm/tests/
- sefaria/helper/llm/tests/topic_prompt_test.py (3422 bytes)
- sefaria/helper/llm/__init__.py (0 bytes)
- sefaria/helper/llm/topic_prompt.py (7550 bytes)
- sefaria/helper/linker/
- sefaria/helper/linker/tasks.py (1694 bytes)
- sefaria/helper/linker/__init__.py (0 bytes)
- sefaria/helper/linker/linker.py (10689 bytes)
- sefaria/helper/marked_up_text_chunk_generator.py (2914 bytes)
- sefaria/helper/legacy_ref.py (6409 bytes)
- sefaria/helper/crm/
- sefaria/helper/crm/crm_mediator.py (2871 bytes)
- sefaria/helper/crm/nationbuilder.py (5527 bytes)
- sefaria/helper/crm/crm_connection_manager.py (1854 bytes)
- sefaria/helper/crm/salesforce.py (6515 bytes)
- sefaria/helper/crm/dummy_crm.py (1010 bytes)
- sefaria/helper/crm/tests/
- sefaria/helper/crm/tests/crm_mediator_test.py (6376 bytes)
- sefaria/helper/crm/tests/crm_connection_manager_test.py (515 bytes)
- sefaria/helper/crm/__init__.py (0 bytes)
- sefaria/helper/crm/crm_info_store.py (2338 bytes)
- sefaria/helper/crm/crm_factory.py (725 bytes)
- sefaria/helper/community_page.py (12824 bytes)
- sefaria/helper/tests/
- sefaria/helper/tests/normalization_tests.py (6923 bytes)
- sefaria/helper/tests/schema_test.py (16531 bytes)
- sefaria/helper/tests/text_test.py (969 bytes)
- sefaria/helper/tests/linker_test.py (11992 bytes)
- sefaria/helper/tests/legacy_ref_test.py (6308 bytes)
- sefaria/helper/tests/search_test.py (4496 bytes)
- sefaria/helper/tests/topic_test.py (10447 bytes)
- sefaria/helper/tests/auto_linking_test.py (23049 bytes)
- sefaria/helper/__init__.py (0 bytes)
- sefaria/helper/trend_manager.py (5359 bytes)
- sefaria/helper/file.py (973 bytes)
- sefaria/helper/texts/
- sefaria/helper/texts/tasks.py (3446 bytes)
- sefaria/helper/text.py (34315 bytes)
- sefaria/helper/topic.py (63967 bytes)
- sefaria/helper/search.py (5579 bytes)
- sefaria/helper/slack/
- sefaria/helper/slack/send_message.py (591 bytes)
- sefaria/helper/slack/__init__.py (0 bytes)
- sefaria/helper/normalization.py (25032 bytes)
- sefaria/helper/linker_index_converter.py (21640 bytes)
- sefaria/helper/descriptions.py (12787 bytes)
- sefaria/helper/category.py (9112 bytes)
- sefaria/helper/schema.py (42212 bytes)
- sefaria/local_settings_ci.py (6314 bytes)
- sefaria/client/
- sefaria/client/wrapper.py (16494 bytes)
- sefaria/client/util.py (1996 bytes)
- sefaria/client/__init__.py (0 bytes)
- sefaria/gauth/
- sefaria/gauth/__init__.py (0 bytes)
- sefaria/gauth/tests.py (383 bytes)
- sefaria/gauth/views.py (3043 bytes)
- sefaria/gauth/decorators.py (2703 bytes)
- sefaria/profiling.py (283 bytes)
- sefaria/views.py (57077 bytes)
- sefaria/image_generator.py (5971 bytes)
- sefaria/wsgi.py (1137 bytes)
- sefaria/history.py (13538 bytes)
- sefaria/decorators.py (1081 bytes)

## Summary
- Total files: 282
- Total directories: 32
- Total text file size (including ignored): 2248.84 KB
- Total tokens: 562354
- Analyzed text content size: 2246.73 KB

Top largest non-ignored files:
- sefaria/model/text.py (271.00 kB)
- sefaria/model/schema.py (103.10 kB)
- sefaria/helper/topic.py (62.47 kB)
- sefaria/views.py (55.74 kB)
- sefaria/model/tests/ref_test.py (55.71 kB)
- sefaria/model/topic.py (52.64 kB)
- sefaria/sheets.py (42.93 kB)
- sefaria/helper/schema.py (41.22 kB)
- sefaria/model/linker/ref_resolver.py (39.32 kB)
- sefaria/model/tests/index_schema_test.py (36.79 kB)

Top largest non-ignored directories:
- sefaria/model (1284.05 kB)
- sefaria/helper (396.90 kB)
- sefaria/model/tests (303.64 kB)
- sefaria/model/linker (187.50 kB)
- sefaria/system (80.03 kB)
- sefaria/helper/tests (78.82 kB)
- sefaria/utils (76.03 kB)
- sefaria/model/linker/tests (53.42 kB)
- sefaria/datatype (42.68 kB)
- sefaria/helper/crm (27.20 kB)


## Ignore summary:
During the analysis, some files were ignored:
- No of files ignored during parsing: 143
- Patterns used to ignore files: {'*.swp', '*.egg-info', '.idea', 'venv', 'Thumbs.db', 'dist', '.vscode', '*.tmp', '.git', '.venv', '__pycache__', 'build', '*.log', '*.bak', '*.pyc', '*.dll', 'bower_components', '*.so', '*.pyd', '*.dylib', '.gitignore', '*.pyo', '.DS_Store', '.svn', '.hg', 'node_modules', 'env'}

## Files:
### sefaria/local_settings_coolify.py

```
import os

SILENCED_SYSTEM_CHECKS = ['captcha.recaptcha_test_key_error']
ALLOWED_HOSTS = ['*']

MONGO_HOST = os.getenv("MONGO_HOST", "localhost")
MONGO_PORT = int(os.getenv("MONGO_PORT", 27017))
SEFARIA_DB = os.getenv('MONGO_DB_NAME')
SEFARIA_DB_USER = os.getenv('MONGO_DB_USER', '')
SEFARIA_DB_PASSWORD = os.getenv('MONGO_DB_PASSWORD', '')

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': os.getenv('POSTGRES_DB', 'sefaria_auth'),
        'USER': os.getenv('POSTGRES_USER', 'admin'),
        'PASSWORD': os.getenv('POSTGRES_PASSWORD', 'admin'),
        'HOST': os.getenv('POSTGRES_HOST', 'localhost'),
        'PORT': os.getenv('POSTGRES_PORT', '5432'),
    }
}

ADMIN_PATH = 'admin'

WEBHOOK_USERNAME = os.getenv("WEBHOOK_USERNAME")
WEBHOOK_PASSWORD = os.getenv("WEBHOOK_PASSWORD")
```

### sefaria/conftest.py

```
def pytest_configure(config):
    import sys
    import django
    sys._called_from_test = True
    django.setup()


def pytest_unconfigure(config):
    import sys
    del sys._called_from_test

```

### sefaria/clean.py

```
"""
Small utilities for fixing problems that occur in the DB.
"""
from copy import deepcopy

import sefaria.model as model
from sefaria.system.database import db
from sefaria.utils.util import rtrim_jagged_string_array
from sefaria.system.exceptions import BookNameError


def remove_refs_with_false():
    """
    Removes any links and history records about links that contain False
    as one of the refs.
    """
    model.LinkSet({"refs": False}).delete()
    model.HistorySet({"new.refs": False}).delete()


"""
Detect any links that contain Refs we can't understand.
"""
def broken_links(tref=None, auto_links = False, manual_links = False, delete_links = False, check_text_exists=False):
    links = model.LinkSet(model.Ref(tref)) if tref else model.LinkSet()
    broken_links_list = []
    for link in links:
        errors = [0,0,0,0]
        try:
            rf1 = model.Ref(link.refs[0])
            errors[0] = 1
            if check_text_exists and rf1.is_empty():
                raise Exception("no text at this Ref")
            errors[1] = 1
            rf2 = model.Ref(link.refs[1])
            errors[2] = 1
            if check_text_exists and rf2.is_empty():
                raise Exception("no text at this Ref")
            errors[3] = 1
        except:
            if link.auto:
                if auto_links is False:
                    continue
            else:
                if manual_links is False:
                    continue
            link_type = "auto - {}".format(link.generated_by) if link.auto else "manual"
            error_code = sum(errors)
            if error_code == 0:
                error_msg = "Ref 1 is bad"
            elif error_code == 1:
                error_msg = "Ref 1 has no text in the system"
            elif error_code == 2:
                error_msg = "Ref 2 is bad"
            elif error_code == 3:
                error_msg = "Ref 2 has no text in the system"

            broken_links_list.append("{}\t{}\t{}".format(link.refs, link_type, error_msg))
            print(broken_links_list[-1])
            if delete_links:
                link.delete()
    return broken_links_list



def remove_bad_links():
    """
    Remove any links that contain Refs we can't understand.
    """
    broken_links(True, True, True)


def remove_old_counts():
    """
    Deletes counts documents which no longer correspond to a text or category.
    """
    # If there are counts documents save in the DB with invalid titles,
    # instantiation of the Count will cause a BookNameError.
    # But in this code instantiation happens in the line 'for count in counts'
    # How do we catch that? Additionally, we need access to the bad title after
    # The error has occurred. How would we get that? Reverting to direct DB call for now.
    counts = db.vstate.find({}, {"title": 1})
    for count in counts:
        if count.get("title", None):
            print("Checking " + count["title"])
            try:
                i = model.library.get_index(count["title"])
                if model.VersionSet({"title": i.title}).count() == 0:
                    print("Old count for Commentary with no content: %s" % count["title"])
                    db.vstate.delete_one({"_id": count["_id"]})                    
            except BookNameError:
                print("Old count: %s" % count["title"])
                db.vstate.delete_one({"_id": count["_id"]})


def remove_trailing_empty_segments():
    """
    Removes empty segments from the end of any text section.
    """
    texts = model.VersionSet()
    for text in texts:
        if not model.Ref.is_ref(text.title):
            continue # Ignore text versions we don't understand
        new_text = rtrim_jagged_string_array(deepcopy(text.chapter))
        if new_text != text.chapter:
            print(text.title + " CHANGED")
            text.chapter = new_text
            text.save()
            model.VersionState(text.title).refresh()
```

### sefaria/pytest.ini

```
[pytest]
markers =
    deep: marks tests as slow
    failing: marks tests as known no passing
DJANGO_SETTINGS_MODULE = sefaria.settings

python_files =
    sefaria/tests/*_test.py
    sefaria/system/tests/*_test.py
    sefaria/model/tests/*_test.py
    sefaria/helper/tests/*_test.py
    sefaria/datatype/tests/*_test.py

;;;;;; Test Suite Notes
; sefaria/tests -- 14/14 tests passing, 1m20s
; sefaria/system/tests -- 6/6 tests passing, 0.04s
; sefaria/model/tests RUNTIME ERROR Skipping these for now
; sefaria/helper/tests -- 11/15 tests passing, 7m45s
; sefaria/datatype/tests -- 24/25 tests passing, 0.14s

```

### sefaria/sheets.py

```
# -*- coding: utf-8 -*-
"""
sheets.py - backend core for Sefaria Source sheets

Writes to MongoDB Collection: sheets
"""
from sefaria.client.util import jsonResponse
import sys
import hashlib
import urllib.request, urllib.parse, urllib.error
import structlog
import regex
import dateutil.parser
import bleach
from datetime import datetime, timedelta
from functools import wraps
from bson.son import SON
from collections import defaultdict
from pymongo.errors import DuplicateKeyError
import uuid

import sefaria.model as model
import sefaria.model.abstract as abstract
from sefaria.system.database import db
from sefaria.model.notification import Notification, NotificationSet
from sefaria.model.following import FollowersSet
from sefaria.model.user_profile import UserProfile, annotate_user_list, public_user_data, user_link
from sefaria.model.collection import Collection, CollectionSet
from sefaria.model.topic import TopicSet, Topic, RefTopicLink, RefTopicLinkSet
from sefaria.utils.util import strip_tags, string_overlap, titlecase
from sefaria.utils.hebrew import has_hebrew, is_all_hebrew
from sefaria.system.exceptions import InputError, DuplicateRecordError
from sefaria.system.cache import django_cache
from .history import record_sheet_publication, delete_sheet_publication
from .settings import SEARCH_INDEX_ON_SAVE
from . import search
from sefaria.google_storage_manager import GoogleStorageManager
import re
from django.http import Http404

logger = structlog.get_logger(__name__)

if not hasattr(sys, '_doc_build'):
	from django.contrib.auth.models import User
from django.contrib.humanize.templatetags.humanize import naturaltime

import structlog
logger = structlog.get_logger(__name__)


def get_sheet(id=None):
	"""
	Returns the source sheet with id.
	"""
	if id is None:
		return {"error": "No sheet id given."}
	s = db.sheets.find_one({"id": int(id)})
	if not s:
		return {"error": "Couldn't find sheet with id: %s" % (id)}
	s["topics"] = add_langs_to_topics(s.get("topics", []))
	s["_id"] = str(s["_id"])
	collections = CollectionSet({"sheets": id, "listed": True})
	s["collections"] = [{"name": collection.name, "slug": collection.slug} for collection in collections]
	return s


def get_sheet_metadata(id = None):
	"""Returns only metadata on the sheet document"""
	assert id
	s = db.sheets.find_one({"id": int(id)}, {"title": 1, "owner": 1, "summary": 1, "ownerImageUrl": 1, "via": 1})
	return s


def get_sheet_listing_data(id):
	"""Returns metadata on sheet document plus data about its author"""
	s = get_sheet_metadata(id)
	del s["_id"]
	s["title"] = strip_tags(s["title"]).replace("\n", " ")
	profile = public_user_data(s["owner"])
	s.update({
		"ownerName": profile["name"],
		"ownerImageUrl": profile["imageUrl"],
		"ownerProfileUrl": profile["profileUrl"],
		"ownerOrganization": profile["organization"],
	})
	return s


def get_sheet_metadata_bulk(id_list, public=True):
	query = {"id": {"$in": id_list}}
	if public:
		query['status'] = 'public'
	return db.sheets.find(query, {"id": 1, "title": 1, "owner": 1, "summary": 1, "ownerImageUrl": 1, "via": 1})


def get_sheet_node(sheet_id=None, node_id=None):
	if sheet_id is None:
		return {"error": "No sheet id given."}
	if node_id is None:
		return {"error": "No node id given."}
	s = db.sheets.find_one({
		"id": int(sheet_id),
		"sources.node": int(node_id)
	}, {
		"sources.$": 1,
		"_id": 0
	})

	if not s:
		return {"error": "Couldn't find node with sheet id: %s and node id: %s" % (sheet_id, node_id)}
	return s["sources"][0]


def get_sheet_for_panel(id=None):
	sheet = get_sheet(id)
	if "spam_sheet_quarantine" in sheet and sheet["spam_sheet_quarantine"]:
		raise Http404
	if "error" in sheet and sheet["error"] != "Sheet updated.":
		return sheet
	if "assigner_id" in sheet:
		asignerData = public_user_data(sheet["assigner_id"])
		sheet["assignerName"]  = asignerData["name"]
	if "viaOwner" in sheet:
		viaOwnerData = public_user_data(sheet["viaOwner"])
		sheet["viaOwnerName"]  = viaOwnerData["name"]
	ownerData = public_user_data(sheet["owner"])
	sheet["ownerName"]  = ownerData["name"]
	sheet["ownerProfileUrl"] = public_user_data(sheet["owner"])["profileUrl"]
	sheet["ownerImageUrl"] = public_user_data(sheet["owner"])["imageUrl"]
	sheet["sources"] = annotate_user_links(sheet["sources"])
	sheet["topics"] = add_langs_to_topics(sheet.get("topics", []))
	sheet["sheetNotice"] = present_sheet_notice(sheet.get("is_moderated", None))
	if "displayedCollection" in sheet:
		collection = Collection().load({"slug": sheet["displayedCollection"]})
		if collection:
			sheet["collectionImage"] = getattr(collection, "imageUrl", None)
			sheet["collectionName"] = collection.name
		else:
			del sheet["displayedCollection"]
	return sheet


def user_sheets(user_id, sort_by="date", limit=0, skip=0, private=True):
	query = {"owner": int(user_id)}
	if not private:
		query["status"] = "public"
	if sort_by == "date":
		sort = [["dateModified", -1]]
	elif sort_by == "views":
		sort = [["views", -1]]
	else:
		sort = None

	sheets = sheet_list(query=query, sort=sort, limit=limit, skip=skip)

	if private:
		sheets = annotate_user_collections(sheets, user_id)
	else:
		sheets = annotate_displayed_collections(sheets)

	response = {
		"sheets": sheets
	}
	return response


def public_sheets(sort=[["datePublished", -1]], limit=50, skip=0, lang=None, filtered=False):
	if filtered:
		query = {"status": "public", "sources.ref": {"$exists": True}}
	else:
		query = {"status": "public"}
	if lang:
		query["sheetLanguage"] = lang
	response = {
		"sheets": sheet_list(query=query, sort=sort, limit=limit, skip=skip)
	}
	return response


def sheet_list(query=None, sort=None, skip=0, limit=None):
	"""
	Returns a list of sheets with only fields needed for displaying a list.
	"""
	projection = {
		"id": 1,
		"title": 1,
		"summary": 1,
		"status": 1,
		"owner": 1,
		"views": 1,
		"dateModified": 1,
		"dateCreated": 1,
		"datePublished": 1,
		"topics": 1,
		"displayedCollection": 1,
	}
	if not query:
		return []
	sort = sort if sort else [["dateModified", -1]]
	sheets = db.sheets.find(query, projection).sort(sort).skip(skip)
	if limit:
		sheets = sheets.limit(limit)

	return [sheet_to_dict(s) for s in sheets]


def sheet_to_dict(sheet):
	"""
	Returns a JSON serializable dictionary of Mongo document `sheet`.
	Annotates sheet with user profile info that is useful to client.
	"""
	profile = public_user_data(sheet["owner"])
	sheet_dict = {
		"id": sheet["id"],
		"title": strip_tags(sheet["title"]) if "title" in sheet else "Untitled Sheet",
		"summary": sheet.get("summary", None),
		"status": sheet["status"],
		"author": sheet["owner"],
		"ownerName": profile["name"],
		"ownerImageUrl": profile["imageUrl"],
		"ownerProfileUrl": profile["profileUrl"],
		"ownerOrganization": profile["organization"],
		"sheetUrl": "/sheets/" + str(sheet["id"]),
		"views": sheet["views"],
		"displayedCollection": sheet.get("displayedCollection", None),
		"modified": dateutil.parser.parse(sheet["dateModified"]).strftime("%m/%d/%Y"),
		"created": sheet.get("dateCreated", None),
		"published": sheet.get("datePublished", None),
		"topics": add_langs_to_topics(sheet.get("topics", [])),
		"tags": [t['asTyped'] for t in sheet.get("topics", [])],  # for backwards compatibility with mobile
		"options": sheet["options"] if "options" in sheet else [],
	}
	return sheet_dict



def add_sheet_to_collection(sheet_id, collection, is_sheet_owner, override_displayedCollection=False):
    sheet = db.sheets.find_one({"id": sheet_id})
    if not sheet:
        raise Exception("Sheet not found")
    if sheet["id"] not in collection.sheets:
        collection.sheets.append(sheet["id"])
        # If a sheet's owner adds it to a collection, and the sheet is not highlighted
        # in another collection, set it to highlight this collection.
        if is_sheet_owner and (not sheet.get("displayedCollection", None) or override_displayedCollection):
            sheet["displayedCollection"] = collection.slug
            db.sheets.replace_one({"_id": sheet["_id"]}, sheet, upsert=True)


def change_sheet_owner(sheet_id, new_owner_id):
    sheet = db.sheets.find_one({"id": sheet_id})
    if not sheet:
        raise Exception("Sheet not found")
    sheet["owner"] = new_owner_id
    # The following info should not be stored -- delete it so it doesn't cause issues
    if "ownerImageUrl" in sheet:
        del sheet["ownerImageUrl"]
    if "ownerProfileUrl" in sheet:
        del sheet["ownerProfileUrl"]
    if "ownerOrganization" in sheet:
        del sheet["ownerOrganization"]
    db.sheets.replace_one({"_id": sheet["_id"]}, sheet, upsert=True)

def annotate_user_collections(sheets, user_id):
	"""
	Adds a `collections` field to each sheet in `sheets` which includes the collections
	that `user_id` has put that sheet in.
	"""
	sheet_ids = [sheet["id"] for sheet in sheets]
	user_collections = CollectionSet({"sheets": {"$in": sheet_ids}})
	for sheet in sheets:
		sheet["collections"] = []
		for collection in user_collections:
			if sheet["id"] in collection.sheets:
				sheet["collections"].append({"name": collection.name, "slug": collection.slug})

	return sheets


def annotate_displayed_collections(sheets):
	"""
	Adds `displayedCollectionName` field to each sheet in `sheets` that has `displayedCollection`.
	"""
	slugs = list(set([sheet["displayedCollection"] for sheet in sheets if sheet.get("displayedCollection", None)]))
	if len(slugs) == 0:
		return sheets
	displayed_collections = CollectionSet({"slug": {"$in": slugs}})
	for sheet in sheets:
		if not sheet.get("displayedCollection", None):
			continue
		for collection in displayed_collections:
			if sheet["displayedCollection"] == collection.slug:
				sheet["displayedCollectionName"] = collection.name

	return sheets


def annotate_user_links(sources):
	"""
	Search a sheet for any addedBy fields (containg a UID) and add corresponding user links.
	"""
	for source in sources:
		if "addedBy" in source:
			source["userLink"] = user_link(source["addedBy"])
	return sources


def user_tags(uid):
	"""
	Returns a list of tags that `uid` has, ordered by tag order in user profile (if existing)
	"""
	user_tags = sheet_topics_counts({"owner": uid})
	user_tags = order_tags_for_user(user_tags, uid)
	return user_tags


def sheet_topics_counts(query, sort_by="count"):
	"""
	Returns topics ordered by count for sheets matching `query`.
	"""
	if sort_by == "count":
		sort_query = SON([("count", -1), ("_id", -1)])
	elif sort_by == "alpha":
		sort_query = SON([("_id", 1)])
	else:
		return []

	topics = db.sheets.aggregate([
		{"$match": query},
		{"$unwind": "$topics"},
		{"$group": {"_id": "$topics.slug", "count": {"$sum": 1}, "asTyped": {"$first": "$topics.asTyped"}}},
		{"$sort": sort_query},
		{"$project": {"_id": 0, "slug": "$_id", "count": "$count", "asTyped": "$asTyped"}}], cursor={})
	return add_langs_to_topics(list(topics))


def order_tags_for_user(tag_counts, uid):
	"""
	Returns of list of tag/count dicts order according to user's preference,
	Adds empty tags if any appear in user's sort list but not in tags passed in
	"""
	profile   = UserProfile(id=uid)
	tag_order = getattr(profile, "tag_order", None)
	if tag_order:
		empty_tags = tag_order[:]
		tags = [tag_count["slug"] for tag_count in tag_counts]
		empty_tags = [tag for tag in tag_order if tag not in tags]

		for tag in empty_tags:
			tag_counts.append({"tag": tag, "count": 0})
		try:
			tag_counts = sorted(tag_counts, key=lambda x: tag_order.index(x["tag"]))
		except:
			pass

	return tag_counts

@django_cache(timeout=6 * 60 * 60)
def trending_topics(days=7, ntags=14):
	"""
	Returns a list of trending topics plus sheet count and author count modified in the last `days`.
	"""
	cutoff = datetime.now() - timedelta(days=days)
	query = {
		"status": "public",
		"dateModified": {"$gt": cutoff.isoformat()},
		"viaOwner": {"$exists": 0},
		"assignment_id": {"$exists": 0}
	}

	topics = db.sheets.aggregate([
			{"$match": query},
			{"$unwind": "$topics"},
			{"$group": {"_id": "$topics.slug", "sheet_count": {"$sum": 1}, "authors": {"$addToSet": "$owner"}}},
			{"$project": {"_id": 0, "slug": "$_id", "sheet_count": "$sheet_count", "authors": "$authors"}}], cursor={})

	topics_list = list(topics)
	results = add_langs_to_topics([{
		"slug": topic['slug'],
		"count": topic['sheet_count'],
		"author_count": len(topic['authors']),
	} for topic in filter(lambda x: len(x["authors"]) > 1, topics_list)], use_as_typed=False, backwards_compat_lang_fields={'en': 'tag', 'he': 'he_tag'})
	results = sorted(results, key=lambda x: -x["author_count"])


	# For testing purposes: if nothing is trennding in specified number of days,
	# (because local data is stale) look at a bigger window
	# ------
	# Updated to return an empty array on 7/29/21 b/c it was causing a recursion error due to stale data on sandboxes
	# or local and for folks who only had the public dump.
	# -----------
	if len(results) == 0:
		return[]
		#return trending_topics(days=180, ntags=ntags)

	return results[:ntags]


def rebuild_sheet_nodes(sheet):
	def find_next_unused_node(node_number, used_nodes):
		while True:
			node_number += 1
			if node_number not in used_nodes:
				return node_number

	try:
		sheet_id = sheet["id"]
	except KeyError:  # this will occur on new sheets, as we won't know the id until the sheet is succesfully saved
		sheet_id = 'New Sheet'
	next_node, checked_sources, nodes_used = 0, [], set()

	for source in sheet.get("sources", []):
		if "node" not in source:
			print("adding nodes to sheet {}".format(sheet_id))
			next_node = find_next_unused_node(next_node, nodes_used)
			source["node"] = next_node

		elif source["node"] is None:
			print("found null node in sheet {}".format(sheet_id))
			next_node = find_next_unused_node(next_node, nodes_used)
			source["node"] = next_node
			nodes_used.add(next_node)

		elif source["node"] in nodes_used:
			print("found repeating node in sheet " + str(sheet_id))
			next_node = find_next_unused_node(next_node, nodes_used)
			source["node"] = next_node

		nodes_used.add(source["node"])

		if "ref" in source and "text" not in source:
			print("adding sources to sheet {}".format(sheet_id))
			source["text"] = {}

			try:
				oref = model.Ref(source["ref"])
				tc_eng = model.TextChunk(oref, "en")
				tc_heb = model.TextChunk(oref, "he")
				if tc_eng:
					source["text"]["en"] = tc_eng.ja().flatten_to_string()
				if tc_heb:
					source["text"]["he"] = tc_heb.ja().flatten_to_string()

			except:
				print("error on {} on sheet {}".format(source["ref"], sheet_id))
				continue

		checked_sources.append(source)

	sheet["sources"] = checked_sources
	sheet["nextNode"] = find_next_unused_node(next_node, nodes_used)
	return sheet


def save_sheet(sheet, user_id, search_override=False, rebuild_nodes=False):
	from pathlib import Path
	"""
	Saves sheet to the db, with user_id as owner.
	"""
	def next_sheet_id():
		last_id = db.sheets.find().sort([['id', -1]]).limit(1)
		if len(list(last_id.clone())):
			sheet_id = last_id.next()["id"] + 1
		else:
			sheet_id = 1
		return sheet_id

	sheet["dateModified"] = datetime.now().isoformat()
	status_changed = False
	if "id" in sheet:
		new_sheet = False
		existing = db.sheets.find_one({"id": sheet["id"]})

		if sheet["lastModified"] != existing["dateModified"]:
			# Don't allow saving if the sheet has been modified since the time
			# that the user last received an update
			existing["error"] = "Sheet updated."
			existing["rebuild"] = True
			return existing
		del sheet["lastModified"]
		if sheet["status"] != existing["status"]:
			status_changed = True

		old_topics = existing.get("topics", [])
		topics_diff = topic_list_diff(old_topics, sheet.get("topics", []))

		old_media_urls = set([source["media"] for source in existing.get("sources") if "media" in source])
		if len(old_media_urls) > 0:
			new_media_urls = set([source["media"] for source in sheet.get("sources") if "media" in source])
			if len(old_media_urls) != len(new_media_urls):
				deleted_media = set(old_media_urls).difference(new_media_urls)
				print(deleted_media)
				for url in deleted_media:
					if url.startswith(GoogleStorageManager.BASE_URL):
						GoogleStorageManager.delete_filename((re.findall(r"/([^/]+)$", url)[0]), GoogleStorageManager.UGC_SHEET_BUCKET)

		# Protected fields -- can't be set from outside
		sheet["views"] = existing["views"]
		sheet["owner"] = existing["owner"]
		sheet["likes"] = existing["likes"] if "likes" in existing else []
		sheet["dateCreated"] = existing["dateCreated"]
		if "datePublished" in existing:
			sheet["datePublished"] = existing["datePublished"]
		if "noindex" in existing:
			sheet["noindex"] = existing["noindex"]

		# make sure sheets never get saved with an "error: field to the db...
		# Not entirely sure why the error "Sheet updated." sneaks into the db sometimes.
		if "error" in sheet:
			del sheet["error"]
		if "error" in existing:
			del existing["error"]

		existing.update(sheet)
		sheet = existing

	else:
		new_sheet = True
		sheet["dateCreated"] = datetime.now().isoformat()
		if "status" not in sheet:
			sheet["status"] = "unlisted"
		sheet["owner"] = user_id
		sheet["views"] = 1

		old_topics = []
		topics_diff = topic_list_diff(old_topics, sheet.get("topics", []))

		# ensure that sheet sources have nodes (primarily for sheets posted via API)
		# and ensure that images from copied sheets hosted on google cloud get duplicated as well
		nextNode = sheet.get("nextNode", 1)
		sheet["nextNode"] = nextNode
		checked_sources = []
		for source in sheet["sources"]:
			if "node" not in source:
				source["node"] = nextNode
				nextNode += 1
			if "media" in source and source["media"].startswith(GoogleStorageManager.BASE_URL):
				old_file = (re.findall(r"/([^/]+)$", source["media"])[0])
				source_path = source['media']
				path_suffix = Path(source_path).suffix.strip(".")
				to_file = f"{user_id}-{uuid.uuid1()}.{path_suffix}"
				bucket_name = GoogleStorageManager.UGC_SHEET_BUCKET
				duped_image_url = GoogleStorageManager.duplicate_file(old_file, to_file, bucket_name)
				source["media"] = duped_image_url
			checked_sources.append(source)
		sheet["sources"] = checked_sources

	if status_changed and not new_sheet:
		if sheet["status"] == "public" and "datePublished" not in sheet:
			# PUBLISH
			sheet["datePublished"] = datetime.now().isoformat()
			record_sheet_publication(sheet["id"], user_id)  # record history
			broadcast_sheet_publication(user_id, sheet["id"])
		if sheet["status"] != "public":
			# UNPUBLISH
			if SEARCH_INDEX_ON_SAVE and not search_override:
				es_index_name = search.get_new_and_current_index_names("sheet")['current']
				search.delete_sheet(es_index_name, sheet['id'])

			delete_sheet_publication(sheet["id"], user_id)  # remove history

			NotificationSet({"type": "sheet publish",
								"uid": user_id,
								"content.publisher_id": user_id,
								"content.sheet_id": sheet["id"]
							}).delete()

	sheet["includedRefs"] = refs_in_sources(sheet.get("sources", []))
	sheet["expandedRefs"] = model.Ref.expand_refs(sheet["includedRefs"])
	sheet["sheetLanguage"] = get_sheet_language(sheet)

	if rebuild_nodes:
		sheet = rebuild_sheet_nodes(sheet)

	if new_sheet:
		# mongo enforces a unique sheet id, get a new id until a unique one has been found
		while True:
			try:
				sheet["id"] = next_sheet_id()
				db.sheets.insert_one(sheet)
				break
			except DuplicateKeyError:
				pass

	else:
		db.sheets.find_one_and_replace({"id": sheet["id"]}, sheet)

	if len(topics_diff["added"]) or len(topics_diff["removed"]):
		update_sheet_topics(sheet["id"], sheet.get("topics", []), old_topics)
		sheet = db.sheets.find_one({"id": sheet["id"]})

	if status_changed and sheet["status"] == "public":
		# Publish, update sheet topic links as though all are new - add links for all
		update_sheet_topic_links(sheet["id"], sheet["topics"], [])
	elif status_changed and sheet["status"] != "public":
		# Unpublish, update sheet topic links as though there are now none - remove links for all
		update_sheet_topic_links(sheet["id"], [], old_topics)


	if sheet["status"] == "public" and SEARCH_INDEX_ON_SAVE and not search_override:
		try:
			index_name = search.get_new_and_current_index_names("sheet")['current']
			search.index_sheet(index_name, sheet["id"])
		except:
			logger.error("Failed index on " + str(sheet["id"]))

	return sheet


def is_valid_source(source):
	if not ("ref" in source or "outsideText" in source or "outsideBiText" in source or "comment" in source or "media" in source):
		return False
	return True


def bleach_text(text):
	ok_sheet_tags = ['blockquote', 'p', 'a', 'ul', 'ol', 'nl', 'li', 'b', 'i', 'strong', 'em', 'small', 'big', 'span', 'strike',
			'hr', 'br', 'div', 'table', 'thead', 'caption', 'tbody', 'tr', 'th', 'td', 'pre', 'sup', 'u', 'h1']

	ok_sheet_attrs = {'a': [ 'href', 'name', 'target', 'data-ref' ],'img': [ 'src' ], 'p': ['style'], 'span': ['style'], 'div': ['style'], 'td': ['colspan'],"*": ["class"]}

	ok_sheet_styles = ['color', 'background-color', 'text-align']

	return bleach.clean(text, tags=ok_sheet_tags, attributes=ok_sheet_attrs, styles=ok_sheet_styles, strip=True)


def clean_source(source):
	if "ref" in source:
		source["text"]["he"] = bleach_text(source["text"]["he"])
		source["text"]["en"] = bleach_text(source["text"]["en"])

	elif "outsideText" in source:
		source["outsideText"] = bleach_text(source["outsideText"])

	elif "comment" in source:
		source["comment"] = bleach_text(source["comment"])

	elif "outsideBiText" in source:
		source["outsideBiText"]["he"] = bleach_text(source["outsideBiText"]["he"])
		source["outsideBiText"]["en"] = bleach_text(source["outsideBiText"]["en"])

	return source


def get_sheet_language(sheet):
	"""
	Returns the language we believe `sheet` to be written in,
	based on the language of its title.
	"""
	title = strip_tags(sheet.get("title", "")).replace("(Copy)", "").replace("\n", " ")
	return "hebrew" if is_all_hebrew(title) else "english"


def test():
	ss = db.sheets.find({}, sort=[["_id", -1]], limit=10000)

	for s in ss:
		lang = get_sheet_language(s)
		if lang == "some hebrew":
			print("{}\thttps://www.sefaria.org/sheets/{}".format(strip_tags(s["title"]).replace("\n", ""), s["id"]))



def add_source_to_sheet(id, source, note=None):
	"""
	Add source to sheet 'id'.
	Source is a dictionary that includes one of the following:
		'ref' (indicating a source)
		'outsideText' (indicating a single language outside text)
		'outsideBiText' (indicating a bilingual outside text)
		'comment' (indicating a comment)
		'media' (indicating a media object)
	if string `note` is present, add it as a coment immediately after the source.
		pass
	"""
	if not is_valid_source(source):
		return {"error": "Malformed source could not be added to sheet"}
	sheet = db.sheets.find_one({"id": id})
	if not sheet:
		return {"error": "No sheet with id %s." % (id)}
	sheet["dateModified"] = datetime.now().isoformat()
	nextNode = sheet.get("nextNode", 1)
	source["node"] = nextNode
	sheet["nextNode"] = nextNode + 1
	sheet["sources"].append(source)
	if note:
		sheet["sources"].append({"outsideText": note, "options": {"indented": "indented-1"}})
	db.sheets.replace_one({"_id": sheet["_id"]}, sheet, upsert=True)
	return {"status": "ok", "id": id, "source": source}


def add_ref_to_sheet(id, ref, request):
	"""
	Add source 'ref' to sheet 'id'.
	"""
	sheet = db.sheets.find_one({"id": id})
	if not sheet:
		return {"error": "No sheet with id %s." % (id)}
	if(sheet["owner"] != request.user.id):
		return jsonResponse({"error": "user can only add refs to their own sheet"})
	sheet["dateModified"] = datetime.now().isoformat()
	sheet["sources"].append({"ref": ref})
	db.sheets.replace_one({"_id": sheet["_id"]}, sheet, upsert=True)
	return {"status": "ok", "id": id, "ref": ref}

def refs_in_sources(sources, refine_refs=False):
	"""
	Returns a list of refs found in sources.
	"""
	refs = []
	for source in sources:
		if "ref" in source:
			ref = source["ref"]
			if refine_refs:
				text = source.get("text", {}).get("he", None)
				ref  = refine_ref_by_text(ref, text) if text else source["ref"]
			refs.append(ref)
	return refs


def refine_ref_by_text(ref, text):
	"""
	Returns a ref (string) which refines 'ref' (string) by comparing 'text' (string),
	to the hebrew text stored in the Library.
	"""
	try:
		oref   = model.Ref(ref).section_ref()
	except:
		return ref
	needle = strip_tags(text).strip().replace("\n", "")
	hay    = model.TextChunk(oref, lang="he").text

	start, end = None, None
	for n in range(len(hay)):
		if not isinstance(hay[n], str):
			# TODO handle this case
			# happens with spanning ref like "Shabbat 3a-3b"
			return ref

		if needle in hay[n]:
			start, end = n+1, n+1
			break

		if not start and string_overlap(hay[n], needle):
			start = n+1
		elif string_overlap(needle, hay[n]):
			end = n+1
			break

	if start and end:
		if start == end:
			refined = "%s:%d" % (oref.normal(), start)
		else:
			refined = "%s:%d-%d" % (oref.normal(), start, end)
		ref = refined

	return ref


def update_included_refs(query=None, hours=None, refine_refs=False):
	"""
	Rebuild included_refs index on sheets matching `query` or sheets
	that have been modified in the last `hours`.
	"""
	if hours:
		cutoff = datetime.now() - timedelta(hours=hours)
		query = { "dateModified": { "$gt": cutoff.isoformat() } }

	if query is None:
		print("Specify either a query or number of recent hours to update.")
		return

	sheets = db.sheets.find(query)

	for sheet in sheets:
		sources = sheet.get("sources", [])
		refs = refs_in_sources(sources, refine_refs=refine_refs)
		db.sheets.update({"_id": sheet["_id"]}, {"$set": {"includedRefs": refs, "expandedRefs": model.Ref.expand_refs(refs)}})


def get_top_sheets(limit=3):
	"""
	Returns 'top' sheets according to some magic heuristic.
	Currently: return the most recently active sheets with more than 100 views.
	"""
	query = {"status": "public", "views": {"$gte": 100}}
	return sheet_list(query=query, limit=limit)


def get_sheets_for_ref(tref, uid=None, in_collection=None):
	"""
	Returns a list of sheets that include ref,
	formating as need for the Client Sidebar.
	If `uid` is present return user sheets, otherwise return public sheets.
	If `in_collection` (list of slugs) is present, only return sheets in one of the listed collections.
	"""
	oref = model.Ref(tref)
	# perform initial search with context to catch ranges that include a segment ref
	segment_refs = [r.normal() for r in oref.all_segment_refs()]
	query = {"expandedRefs": {"$in": segment_refs}}
	if uid:
		query["owner"] = uid
	else:
		query["status"] = "public"
	if in_collection:
		collections = CollectionSet({"slug": {"$in": in_collection}})
		sheets_list = [collection.sheets for collection in collections]
		sheets_ids = [sheet for sublist in sheets_list for sheet in sublist]
		query["id"] = {"$in": sheets_ids}

	sheetsObj = db.sheets.find(query,
		{"id": 1, "title": 1, "owner": 1, "viaOwner":1, "via":1, "dateCreated": 1, "includedRefs": 1, "expandedRefs": 1, "views": 1, "topics": 1, "status": 1, "summary":1, "attribution":1, "assigner_id":1, "likes":1, "displayedCollection":1, "options":1}).sort([["views", -1]])
	sheetsObj.hint("expandedRefs_1")
	sheets = [s for s in sheetsObj]
	user_ids = list({s["owner"] for s in sheets})
	django_user_profiles = User.objects.filter(id__in=user_ids).values('email','first_name','last_name','id')
	user_profiles = {item['id']: item for item in django_user_profiles}
	mongo_user_profiles = list(db.profiles.find({"id": {"$in": user_ids}},{"id":1,"slug":1,"profile_pic_url_small":1}))
	mongo_user_profiles = {item['id']: item for item in mongo_user_profiles}
	for profile in user_profiles:
		try:
			user_profiles[profile]["slug"] = mongo_user_profiles[profile]["slug"]
		except:
			user_profiles[profile]["slug"] = "/"

		try:
			user_profiles[profile]["profile_pic_url_small"] = mongo_user_profiles[profile].get("profile_pic_url_small", '')
		except:
			user_profiles[profile]["profile_pic_url_small"] = ""

	results = []
	for sheet in sheets:
		anchor_ref_list, anchor_ref_expanded_list = oref.get_all_anchor_refs(segment_refs, sheet.get("includedRefs", []), sheet.get("expandedRefs", []))
		ownerData = user_profiles.get(sheet["owner"], {'first_name': 'Ploni', 'last_name': 'Almoni', 'email': 'test@sefaria.org', 'slug': 'Ploni-Almoni', 'id': None, 'profile_pic_url_small': ''})

		if "assigner_id" in sheet:
			asignerData = public_user_data(sheet["assigner_id"])
			sheet["assignerName"] = asignerData["name"]
			sheet["assignerProfileUrl"] = asignerData["profileUrl"]
		if "viaOwner" in sheet:
			viaOwnerData = public_user_data(sheet["viaOwner"])
			sheet["viaOwnerName"] = viaOwnerData["name"]
			sheet["viaOwnerProfileUrl"] = viaOwnerData["profileUrl"]

		if "displayedCollection" in sheet:
			collection = Collection().load({"slug": sheet["displayedCollection"]})
			sheet["collectionTOC"] = getattr(collection, "toc", None)
		topics = add_langs_to_topics(sheet.get("topics", []))
		for anchor_ref, anchor_ref_expanded in zip(anchor_ref_list, anchor_ref_expanded_list):
			sheet_data = {
				"owner":           sheet["owner"],
				"_id":             str(sheet["_id"]),
				"id":              str(sheet["id"]),
				"public":          sheet["status"] == "public",
				"title":           strip_tags(sheet["title"]),
				"sheetUrl":        "/sheets/" + str(sheet["id"]),
				"anchorRef":       anchor_ref.normal(),
				"anchorRefExpanded": [r.normal() for r in anchor_ref_expanded],
				"options": 		   sheet["options"],
				"collectionTOC":   sheet.get("collectionTOC", None),
				"ownerName":       ownerData["first_name"]+" "+ownerData["last_name"],
				"via":			   sheet.get("via", None),
				"viaOwnerName":	   sheet.get("viaOwnerName", None),
				"assignerName":	   sheet.get("assignerName", None),
				"viaOwnerProfileUrl":	   sheet.get("viaOwnerProfileUrl", None),
				"assignerProfileUrl":	   sheet.get("assignerProfileUrl", None),
				"ownerProfileUrl": "/profile/" + ownerData["slug"],
				"ownerImageUrl":   ownerData.get('profile_pic_url_small',''),
				"status":          sheet["status"],
				"views":           sheet["views"],
				"topics":          topics,
				"likes":           sheet.get("likes", []),
				"summary":         sheet.get("summary", None),
				"attribution":     sheet.get("attribution", None),
				"is_featured":     sheet.get("is_featured", False),
				"category":        "Sheets", # ditto
				"type":            "sheet", # ditto
			}

			results.append(sheet_data)
	return results


def topic_list_diff(old, new):
	"""
	Returns a dictionary with fields `removed` and `added` that describes the differences
	in topics (slug, titles pairs) between lists `old` and `new`.
	"""
	old_set = set([(t["asTyped"], t.get("slug", None)) for t in old])
	new_set = set([(t["asTyped"], t.get("slug", None)) for t in new])

	return {
		"removed": list(old_set - new_set),
		"added":   list(new_set - old_set),
	}


def update_sheet_topics(sheet_id, topics, old_topics):
	"""
	Sets the topic list for `sheet_id` to those listed in list `topics`,
	containing fields `asTyped` and `slug`.
	Performs some normalization of `asTyped` and creates new topic objects for new topics.
	"""
	normalized_slug_title_pairs = set()

	for topic in topics:
	# Dedupe, normalize titles, create/choose topics for any missing slugs
		title = normalize_new_topic_title(topic["asTyped"])
		if "slug" not in topic:
			match = choose_existing_topic_for_title(title)
			if match:
				topic["slug"] = match.slug
			else:
				new_topic = create_topic_from_title(title)
				topic["slug"] = new_topic.slug
		normalized_slug_title_pairs.add((title, topic["slug"]))

	normalized_topics = [{"asTyped": pair[0], "slug": pair[1]} for pair in normalized_slug_title_pairs]

	db.sheets.update_one({"id": sheet_id}, {"$set": {"topics": normalized_topics}})

	update_sheet_topic_links(sheet_id, normalized_topics, old_topics)

	return {"status": "ok"}


def normalize_new_topic_title(title):
	ALLOWED_HASHTAGS = ("#MeToo")
	if title not in ALLOWED_HASHTAGS:
		title = title.replace("#","")
	# replace | with - b/c | is a reserved char for search sheet queries when filtering on tags
	title = titlecase(title).replace('|','-')
	return title


def choose_existing_topic_for_title(title):
	"""
	Returns the best existing topic to match with `title` or None if none matches.
	"""
	existing_topics = TopicSet.load_by_title(title)
	if existing_topics.count() == 0:
		return None

	from functools import cmp_to_key

	def is_title_primary(title, topic):
		all_primary_titles = [topic.get_primary_title(lang) for lang in topic.title_group.langs]
		return title in all_primary_titles

	def compare(t1, t2):
		if is_title_primary(title, t1) == is_title_primary(title, t2):
			# If both or neither match primary title, prefer greater number of sources
			return getattr(t1, "numSources", 0) - getattr(t2, "numSources", 0)
		else:
		 	# Prefer matches to primary title
		 	return 1 if is_title_primary(title, t1) else -1

	return max(list(existing_topics), key=cmp_to_key(compare))


def update_sheet_topic_links(sheet_id, new_topics, old_topics):
	"""
	Adds and removes sheet topic links per differences in old and new topics list.
	Only adds link for public sheets.
	"""
	topic_diff = topic_list_diff(old_topics, new_topics)

	for removed in topic_diff["removed"]:
		#print("removing {}".format(removed[1]))
		RefTopicLinkSet({
			"class": "refTopic",
			"toTopic": removed[1],
			"expandedRefs": "Sheet {}".format(sheet_id),
			"linkType": "about",
			"is_sheet": True,
			"dataSource": "sefaria-users"
		}, hint="expandedRefs_1").delete()

	status = db.sheets.find_one({"id": sheet_id}, {"status": 1}).get("status", "unlisted")
	if status != "public":
		return

	for added in topic_diff["added"]:
		#print("adding {}".format(added[1]))
		attrs = {
			"class": "refTopic",
			"toTopic": added[1],
			"ref": "Sheet {}".format(sheet_id),
			"expandedRefs": ["Sheet {}".format(sheet_id)],
			"linkType": "about",
			"is_sheet": True,
			"dataSource": "sefaria-users"
		}
		tl = RefTopicLink(attrs)
		try:
			tl.save()
		except DuplicateRecordError:
			pass

def create_topic_from_title(title):
	topic = Topic({
		"slug": Topic.normalize_slug(title),
		"titles": [{
			"text": title,
			"lang": "he" if has_hebrew(title) else "en",
		"primary": True,
		}]
	})
	topic.save()
	return topic


def add_langs_to_topics(topic_list: list, use_as_typed=True, backwards_compat_lang_fields: dict = None) -> list:
	"""
	adds primary en and he to each topic in topic_list and returns new topic_list
	:param list topic_list: list of topics where each item is dict of form {'slug': required, 'asTyped': optional}
	:param dict backwards_compat_lang_fields: of shape {'en': str, 'he': str}. Defines lang fields for backwards compatibility. If None, ignore.
	:param bool use_as_typed:
	"""
	new_topic_list = []
	from sefaria.model import library
	topic_map = library.get_topic_mapping()
	if len(topic_list) > 0:
		for topic in topic_list:
			# Fall back on `asTyped` if no data is in mapping yet. If neither `asTyped` nor mapping data is availble fail safe by reconstructing a title from a slug (HACK currently affecting trending topics if a new topic isn't in cache yet)
			default_title = topic['asTyped'] if use_as_typed else topic['slug'].replace("-", " ").title()
			topic_titles = topic_map.get(topic['slug'], {"en": default_title, "he": default_title})
			new_topic = topic.copy()
			tag_lang = 'en'
			if use_as_typed:
				tag_lang = 'he' if has_hebrew(new_topic['asTyped']) else 'en'
				new_topic[tag_lang] = new_topic['asTyped']
			if not use_as_typed or tag_lang == 'en':
				new_topic['he'] = topic_titles["he"]
			if not use_as_typed or tag_lang == 'he':
				new_topic['en'] = topic_titles["en"]

			if backwards_compat_lang_fields is not None:
				for lang in ('en', 'he'):
					new_topic[backwards_compat_lang_fields[lang]] = new_topic[lang]
			new_topic_list += [new_topic]

	return new_topic_list


def get_last_updated_time(sheet_id):
	"""
	Returns a timestamp of the last modified date for sheet_id.
	"""
	sheet = db.sheets.find_one({"id": sheet_id}, {"dateModified": 1})

	if not sheet:
		return None

	return sheet["dateModified"]


@django_cache(timeout=(60 * 60))
def public_tag_list(sort_by="alpha"):
	"""
	Returns a list of all public tags, sorted either alphabetically ("alpha") or by popularity ("count")
	"""
	seen_titles = set()
	results = []
	from sefaria.helper.topic import get_all_topics
	all_tags = get_all_topics()
	lang = "he" if sort_by == "alpha-hebrew" else "en"
	for tag in all_tags:
		title = tag.get_primary_title(lang)
		if title in seen_titles:
			continue
		seen_titles.add(title)
		results.append({"tag": title, "count": getattr(tag, 'numSources', 0)})

	sort_keys =  {
		"alpha": lambda x: x["tag"],
		"count": lambda x: -x["count"],
		"alpha-hebrew": lambda x: x["tag"] if len(x["tag"]) and x["tag"][0] in "0123456789" else "" + x["tag"],
	}
	results = sorted(results, key=sort_keys[sort_by])

	return results


def get_sheets_by_topic(topic, public=True, proj=None, limit=0, page=0):
	"""
	Returns all sheets tagged with 'topic'
	"""
	# try to normalize for backwards compatibility
	from sefaria.model.abstract import SluggedAbstractMongoRecord
	topic = SluggedAbstractMongoRecord.normalize_slug(topic)
	query = {"topics.slug": topic} if topic else {"topics": {"$exists": 0}}

	if public:
		query["status"] = "public"

	sheets = db.sheets.find(query, proj).sort([["views", -1]]).limit(limit).skip(page * limit)
	return sheets


def add_visual_data(sheet_id, visualNodes, zoom):
	"""
	Adds visual layout data to db
	"""
	db.sheets.update({"id": sheet_id},{"$unset": { "visualNodes": "", "zoom": "" } })
	db.sheets.update({"id": sheet_id},{"$push": {"visualNodes": {"$each": visualNodes},"zoom" : zoom}})


def add_like_to_sheet(sheet_id, uid):
	"""
	Add uid as a liker of sheet_id.
	"""
	db.sheets.update({"id": sheet_id}, {"$addToSet": {"likes": uid}})
	sheet = get_sheet(sheet_id)

	notification = Notification({"uid": sheet["owner"]})
	notification.make_sheet_like(liker_id=uid, sheet_id=sheet_id)
	notification.save()


def remove_like_from_sheet(sheet_id, uid):
	"""
	Remove uid as a liker of sheet_id.
	"""
	db.sheets.update({"id": sheet_id}, {"$pull": {"likes": uid}})


def likers_list_for_sheet(sheet_id):
	"""
	Returns a list of people who like sheet_id, including their names and profile links.
	"""
	sheet = get_sheet(sheet_id)
	likes = sheet.get("likes", [])
	return(annotate_user_list(likes))


def broadcast_sheet_publication(publisher_id, sheet_id):
	"""
	Notify everyone who follows publisher_id about sheet_id's publication
	"""
	#todo: work on batch creation / save pattern
	followers = FollowersSet(publisher_id)
	for follower in followers.uids:
		n = Notification({"uid": follower})
		n.make_sheet_publish(publisher_id=publisher_id, sheet_id=sheet_id)
		n.save()


def make_sheet_from_text(text, sources=None, uid=1, generatedBy=None, title=None, segment_level=False):
	"""
	Creates a source sheet owned by 'uid' that includes all of 'text'.
	'sources' is a list of strings naming commentators or texts to include.
	"""
	oref  = model.Ref(text)
	sheet = {
		"title": title if title else oref.normal() if not sources else oref.normal() + " with " + ", ".join([s.replace(" on " + text, "") for s in sources]),
		"sources": [],
		"status": 0,
		"options": {"numbered": 0, "divineNames": "noSub"},
		"generatedBy": generatedBy or "make_sheet_from_text",
		"promptedToPublish": datetime.now().isoformat(),
	}

	i     = oref.index
	leafs = i.nodes.get_leaf_nodes()
	for leaf in leafs:
		refs = []
		if leaf.first_section_ref() != leaf.last_section_ref():
			leaf_spanning_ref = leaf.first_section_ref().to(leaf.last_section_ref())
			assert isinstance(leaf_spanning_ref, model.Ref)
			if segment_level:
				refs += [ref for ref in leaf_spanning_ref.all_segment_refs() if oref.contains(ref)]
			else:  # section level
				refs += [ref for ref in leaf_spanning_ref.split_spanning_ref() if oref.contains(ref)]
		else:
			refs.append(leaf.ref())

		for ref in refs:
			ref_dict = { "ref": ref.normal() }
			sheet["sources"].append(ref_dict)

	return save_sheet(sheet, uid)



class Sheet(abstract.AbstractMongoRecord):
	# This is here as an alternative interface - it's not yet used, generally.

	# Warning: this class doesn't implement all of the saving logic in save_sheet()
	# In current form should only be used for reading or for changes that are known to be
	# safe and without need of side effects.
	#
	# Warning: there are fields on some individual sheet documents that aren't enumerated here,
	# trying to load a document with an attribute not listed here will cause an error.

	collection = 'sheets'

	required_attrs = [
		"title",
		"sources",
		"status",
		"options",
		"dateCreated",
		"dateModified",
		"owner",
		"id"
	]
	optional_attrs = [
		"is_featured",  # boolean - show this sheet, unsolicited.
		"includedRefs",
		"expandedRefs",
		"views",
		"nextNode",
		"tags",
		"topics",
		"promptedToPublish",
		"attribution",
		"datePublished",
		"lastModified",
		"via",
		"viaOwner",
		"assignment_id",
		"assigner_id",
		"likes",
		"group",
		"displayedCollection",
		"spam_sheet_quarantine",
		"generatedBy",
		"zoom",
		"visualNodes",
		"highlighterTags",
		"summary",
        "reviewed",
        "sheetLanguage",
        "ownerImageUrl",   # TODO this shouldn't be stored on sheets, but it is for many
        "ownerProfileUrl", # TODO this shouldn't be stored on sheets, but it is for many
	]

	def _sanitize(self):
		pass

	def is_hebrew(self):
		"""Returns True if this sheet appears to be in Hebrew according to its title"""
		import regex
		title = strip_tags(self.title)
		# Consider a sheet Hebrew if its title contains Hebrew character but no English characters
		return has_hebrew(title) and not regex.search("[a-z|A-Z]", title)


class SheetSet(abstract.AbstractMongoSet):
	recordClass = Sheet


def change_tag(old_tag, new_tag_or_list):
	# new_tag_or_list can be either a string or a list of strings
	# if a list of strings, then old_tag is replaced with all of the tags in the list

	new_tag_list = [new_tag_or_list] if isinstance(new_tag_or_list, str) else new_tag_or_list

	for sheet in SheetSet({"tags": old_tag}):
		sheet.tags = [tag for tag in sheet.tags if tag != old_tag] + new_tag_list
		sheet.save()

def get_sheet_categorization_info(find_without, skip_ids=[]):
	"""
	Returns a pseudorandom sheetId for categorization along with all existing categories
	:param find_without: the field that must contain no elements for the sheet to be returned
	:param skip_ids: sheets to skip in this session:
	"""
	if find_without == "topics":
		sheet = db.sheets.aggregate([
		{"$match": {"topics": {"$in": [None, []] }, "id": {"$nin": skip_ids}, "noTags": {"$in": [None, False]}, "status": "public"}},
		{"$sample": {"size": 1}}]).next()
	else: #categories
		sheet = db.sheets.aggregate([
		{"$match": {"categories": {"$in": [None, []] }, "sources.outsideText": {"$exists": True}, "id": {"$nin": skip_ids}, "noTags": {"$in": [None, False]}, "status": "public"}},
		{"$sample": {"size": 1}}]).next()
	categories_all = list(filter(lambda x: x != None, db.sheets.distinct("categories"))) # this is slow; maybe add index or ...?
	categorize_props = {
		"doesNotContain": find_without,
		"sheetId": sheet['id'],
		"allCategories": categories_all
	}
	return categorize_props


def update_sheet_tags_categories(body, uid):
	update_sheet_topics(body['sheetId'], body["tags"], [])
	time = datetime.now().isoformat()
	noTags = time if body.get("noTags", False) else False
	db.sheets.update_one({"id": body['sheetId']}, {"$set": {"categories": body['categories'], "noTags": noTags}, "$push": {"moderators": {"uid": uid, "time": time}}})


def present_sheet_notice(is_moderated):
	"""This method is here in case one day we will want to differentiate based on other logic on moderation"""
	return is_moderated

```

### sefaria/.pytest_cache/CACHEDIR.TAG

```
Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html

```

### sefaria/.pytest_cache/README.md

```
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.

```

### sefaria/.pytest_cache/v/cache/nodeids

```
[
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[Berakhot 2a-It says in the Talmud, \"Don't steal\" which implies it's bad to steal.-en-expected_trefs0-expected_pretty_texts0]",
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[Gilyon HaShas on Berakhot 25b:1-\\u05e8\\u05e9\"\\u05d9 \\u05ea\\u05de\\u05d5\\u05e8\\u05d4 \\u05db\\u05d7 \\u05e2\"\\u05d1 \\u05d3\"\\u05d4 \\u05e0\\u05e2\\u05d1\\u05d3 \\u05e9\\u05d4\\u05d5\\u05d0 \\u05de\\u05d5\\u05ea\\u05e8. \\u05d6\\u05d4 \\u05e8\\u05e9\"\\u05d9 \\u05de\\u05d0\\u05d5\\u05d3 \\u05d9\\u05e4\\u05d4.-he-expected_trefs6-expected_pretty_texts6]",
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[None-It says in the Torah, \"Don't steal\" which implies it's bad to steal.-en-expected_trefs1-expected_pretty_texts1]",
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[None-See Genesis 1:1. It says in the Torah, \"Don't steal\". It also says in 1:3 \"Let there be light\".-en-expected_trefs7-expected_pretty_texts7]",
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[None-\\u05d2\\u05de' \\u05d1\\u05de\\u05d4 \\u05de\\u05d7\\u05e0\\u05db\\u05d9\\u05df. \\u05e2\\u05d9' \\u05de\\u05e0\\u05d7\\u05d5\\u05ea \\u05d3\\u05e3 \\u05e2\\u05d7 \\u05e2\"\\u05d0 \\u05ea\\u05d5\\u05e1' \\u05d3\"\\u05d4 \\u05d0\\u05d7\\u05ea:-he-expected_trefs4-expected_pretty_texts4]",
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[None-\\u05d2\\u05de' \\u05e9\\u05de\\u05d6\\u05d5\\u05e0\\u05d5\\u05ea\\u05df \\u05e2\\u05dc\\u05d9\\u05da. \\u05e2\\u05d9\\u05d9\\u05df \\u05d1\\u05d9\\u05e6\\u05d4 (\\u05d3\\u05e3 \\u05d8\\u05d5 \\u05e2\"\\u05d1 \\u05e8\\u05e9\"\\u05d9 \\u05d3\"\\u05d4 \\u05e9\\u05de\\u05d0 \\u05d9\\u05e4\\u05e9\\u05e2:)-he-expected_trefs2-expected_pretty_texts2]",
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[None-\\u05e9\\u05dd \\u05d0\\u05dc\\u05d0 \\u05d1\\u05d9\\u05ea\\u05da \\u05dc\"\\u05dc. \\u05e2' \\u05de\\u05e0\\u05d7\\u05d5\\u05ea \\u05de\\u05d3 \\u05e2\"\\u05d0 \\u05ea\\u05d3\"\\u05d4 \\u05d8\\u05dc\\u05d9\\u05ea:-he-expected_trefs3-expected_pretty_texts3]",
  "model/linker/tests/linker_test.py::test_full_pipeline_ref_resolver[None-cf. Ex. 9:6,12:8-en-expected_trefs5-expected_pretty_texts5]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[None-node_b0-Zohar, Introduction 1-Zohar, Volume I, Introduction 1b-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a0-node_b0-None-None-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a0-node_b0-None-Zohar, Volume I, Introduction 1b-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a0-node_b0-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a0-node_b0-Zohar, Volume I-Zohar, Volume I, Introduction 1b-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a1-None-None-other_ref1-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a1-node_b1-Genesis 1-Genesis 1:2-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a1-node_b1-None-other_ref1-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a1-node_b1-self_ref1-None-is_contained1]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a2-node_b2-Genesis 2-Genesis 1:2-False]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a3-node_b3-None-Sefer HaChinukh, 2-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a4-node_b4-Sefer HaChinukh, 2-None-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a5-node_b5-Sefer HaChinukh, 4-None-False]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a6-node_b6-None-Sefer HaChinukh, 3-False]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a7-node_b7-None-Zohar, Volume I, Introduction 1b-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a8-node_b8-None-Zohar, Introduction 2:4-4:12-True]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a8-node_b8-Zohar, Volume I, Introduction 1b-None-False]",
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a9-node_b9-Zohar, Volume I, Introduction 1b-None-False]",
  "model/linker/tests/referenceable_book_node_test.py::test_draft",
  "model/tests/marked_up_text_chunk.py::TestMarkedUpTextChunk::test_empty_spans",
  "model/tests/marked_up_text_chunk.py::TestMarkedUpTextChunk::test_incorrect_text_span",
  "model/tests/marked_up_text_chunk.py::TestMarkedUpTextChunk::test_inserted_records_match_input",
  "model/tests/marked_up_text_chunk.py::TestMarkedUpTextChunk::test_primary_key_uniqueness",
  "model/tests/marked_up_text_chunk.py::TestMarkedUpTextChunk::test_validation_failure",
  "model/tests/marked_up_text_chunk.py::test_marked_up_text_chunk_insertion"
]
```

### sefaria/.pytest_cache/v/cache/lastfailed

```
{
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a0-node_b0-True]": true,
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a1-node_b1-self_ref1-None-is_contained1]": true,
  "model/linker/tests/referenceable_book_node_test.py": true,
  "model/linker/tests/referenceable_book_node_test.py::test_contains[None-node_b0-Zohar, Introduction 1-Zohar, Volume I, Introduction 1b-True]": true,
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a0-node_b0-Zohar, Volume I-Zohar, Volume I, Introduction 1b-True]": true,
  "model/linker/tests/referenceable_book_node_test.py::test_contains[node_a0-node_b0-None-Zohar, Volume I, Introduction 1b-True]": true,
  "model/tests/marked_up_text_chunk.py::test_marked_up_text_chunk_insertion": true
}
```

### sefaria/.pytest_cache/v/cache/stepwise

```
[]
```

### sefaria/spacy_function_registry.py

```
import re
try:
    import spacy
    from spacy.tokenizer import Tokenizer
except ImportError:
    spacy = Tokenizer = None


def inner_punct_tokenizer_factory():
    def inner_punct_tokenizer(nlp):
        # infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)
        infix_re = re.compile(r'''[.,?!:;`"'~\-/()<>]''')
        prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)
        suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)

        return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                         suffix_search=suffix_re.search,
                         infix_finditer=infix_re.finditer,
                         token_match=None)
    return inner_punct_tokenizer


if spacy:
    spacy.registry.tokenizers("inner_punct_tokenizer")(inner_punct_tokenizer_factory)


def get_spacy_tokenizer():
    """
    language agnostic spacy tokenizer that uses inner punctuation
    @return:
    """
    nlp = spacy.blank("en")
    return inner_punct_tokenizer_factory()(nlp)

```

### sefaria/sefaria_tasks_interace/history_change.py

```
from dataclasses import dataclass

@dataclass
class AbstractHistoryChange:
    uid: int
    method: str  # ("API" or "Site")

@dataclass
class LinkChange(AbstractHistoryChange):
    raw_link: dict

@dataclass
class VersionChange(AbstractHistoryChange):
    raw_version: dict
    patch: bool
    skip_links: bool
    count_after: int

```

### sefaria/constants/model.py

```
ALLOWED_TAGS_IN_ABSTRACT_TEXT_RECORD = ("i", "b", "br", "u", "strong", "em", "big", "small", "img", "sup", "sub", "span", "a")
ALLOWED_ATTRS_IN_ABSTRACT_TEXT_RECORD = {
    'sup': ['class'],
    'span': ['class', 'dir'],
    # There are three uses of i tags.
    # footnotes: uses content internal to <i> tag.
    # commentary placement: uses 'data-commentator', 'data-order', 'data-label'
    # structure placement (e.g. page transitions): uses 'data-overlay', 'data-value'
    'i': ['data-overlay', 'data-value', 'data-commentator', 'data-order', 'class', 'data-label', 'dir'],
    'img': ['src', 'alt'],
    'a': ['dir', 'class', 'href', 'data-ref', "data-ven", "data-vhe", 'data-scroll-link'],
}

LANGUAGE_CODES = {
    #maps ISO language codes to their nother language (i.e. jrb to Arabic rather than Judeo-Arabic)
    "ar": "arabic",
    "de": "german",
    "en": "english",
    "eo": "esperanto",
    "es": "spanish",
    "fa": "persian",
    "fi": "finnish",
    "fr": "french",
    "he": "hebrew",
    "it": "italian",
    "lad": "ladino",
    "pl": "polish",
    "pt": "portuguese",
    "ru": "russian",
    "yi": "yiddish",
    "jrb": "arabic",
}

```

### sefaria/recommendation_engine.py

```
import django
from functools import reduce
django.setup()
from collections import defaultdict
from sefaria.model import *
from sefaria.client.wrapper import get_links
from sefaria.system.database import db
from sefaria.system.exceptions import InputError
from sefaria.model.schema import DictionaryEntryNode

# TODO dont double count source and its commentary (might be costly)
# TODO do better job of double author
# TODO maybe distance penalty also??

DIRECT_LINK_SCORE = 2.0
COMMENTARY_LINK_SCORE = 0.7
SHEET_REF_SCORE = 1.0
INCLUDED_REF_MAX = 50
REF_RANGE_MAX = 30


class RecommendationSource:

    def __init__(self, source, anchor_ref):
        self.source = source
        self.anchor_ref = anchor_ref

    def __str__(self):
        return "RecommendationSource: {}, {}".format(self.source, self.anchor_ref.normal())

    def __repr__(self):
        return "{}({}, {})".format(self.__class__.__name__, self.source, self.anchor_ref).encode('utf-8')

    def __eq__(self, other):
        return self.__hash__() == other.__hash__()

    def __hash__(self):
        return hash(self.source + self.anchor_ref.normal())

    def to_dict(self):
        return {
            "source": self.source,
            "anchor_ref": self.anchor_ref.normal()
        }


class Recommendation:

    def __init__(self, oref=None, relevance=0.0, score=None, novelty=None, sources=None):
        self.ref = oref
        self.relevance = relevance
        self.novelty = novelty
        self._score = score
        self.sources = sources if sources is not None else []

    def __add__(self, other):
        new_ref = self.ref if self.ref is not None else other.ref
        return Recommendation(new_ref, relevance=self.relevance + other.relevance, novelty=self.novelty, sources=self.sources + other.sources)

    def __iadd__(self, other):
        self.ref = self.ref if self.ref is not None else other.ref
        self.relevance += other.relevance
        self.sources += other.sources
        return self

    def __str__(self):
        return "Recommendation: {}, {}, {}".format(self.ref.normal(), self.score, self.sources)

    def __repr__(self):
        return "{}({}, score={}, sources={})".format(self.__class__.__name__, self.ref, self.score, self.sources).encode('utf-8')

    def to_dict(self):
        return {
            "ref": self.ref.normal(),
            "score": self.score,
            "sources": [s.to_dict() for s in self.sources],
        }

    @property
    def score(self):
        if self._score is None:
            ref_data = RefData().load({"ref": self.ref.normal()})
            self.novelty = ref_data.inverse_pagesheetrank() if ref_data is not None else 1.0
            self._score = self.relevance * self.novelty
        return self._score

    def sources_interesting(self):
        # make sure either source has more than 2 sheets or direct linkss
        filt = [x for x in self.sources if (x.source.startswith("Sheet ") or x.source == "direct")]
        return len(filt) >= 2


class RecommendationEngine:

    def __init__(self, tref, top=10, exclude_direct_commentary=True, limit_to_direct_link=False, cluster_max_dist=5):
        """
        returns an object with recommendations for `tref`
        :param str tref: Ref represented as a string
        :param int top: maximum number of recommendations to generate. you may still get less recommendations if `tref` does not have many links/sheets.
        :param bool exclude_direct_commentary: True if you don't want recommendations to direct commentaries of `tref`. Default is True
        :param bool limit_to_direct_link: True if you only want recommendations from the list of direct links to `tref`
        :param int cluster_max_dist: max distance between recommendations beyond which the recommendations will not be merged into one ranged ref

        """
        self.ref = Ref(tref)
        self.top = top
        self.exclude_direct_commentary = exclude_direct_commentary
        self.limit_to_direct_link = limit_to_direct_link
        self.cluster_max_dist = cluster_max_dist
        self.commentary_link_ref_set = set()
        self.direct_link_ref_set = set()
        self.recommendations = []

        self.get_all_possible_recs()
        self.reduce_recs()
        self.filter_recs()
        self.choose_top(top*5)
        self.cluster_recs()
        self.choose_top(top)

    def get_all_possible_recs(self):
        sheet_recs = self.get_recs_thru_sheets(self.ref)
        link_recs, commentary_link_ref_set, direct_link_ref_set = self.get_recs_thru_links(self.ref)
        self.commentary_link_ref_set = commentary_link_ref_set
        self.direct_link_ref_set = direct_link_ref_set

        self.recommendations = sheet_recs + link_recs
        return self

    def reduce_recs(self):
        d = defaultdict(Recommendation)
        for temp_rec in self.recommendations:
            d[temp_rec.ref.normal()] += temp_rec
            pass
        self.recommendations = list(d.values())
        return self

    def filter_recs(self):
        def filterer(rec):
            tref = rec.ref.normal()
            if self.exclude_direct_commentary and tref in self.commentary_link_ref_set:
                return False
            if self.limit_to_direct_link and tref not in self.direct_link_ref_set:
                return False
            if not rec.sources_interesting():
                return False
            return True
        self.recommendations = list(filter(filterer, self.recommendations))
        return self

    def choose_top(self, top):
        self.recommendations.sort(key=lambda rec: rec.score, reverse=True)
        self.recommendations = self.recommendations[:top]
        return self

    def cluster_recs(self):
        def combine_cluster(cluster):
            if len(cluster) == 1:
                return cluster[0]["data"]
            else:
                scores = [item["data"].score for item in cluster]
                sources = reduce(lambda a, b: a + b["data"].sources, cluster, [])

                if cluster[0]["data"].ref.primary_category in ("Tanakh", "Talmud"):
                    # only combine clusters for Tanakh and Talmud
                    ranged_ref = cluster[0]["data"].ref.to(cluster[-1]["data"].ref)
                    return Recommendation(ranged_ref, score=max(scores), sources=list(set(sources)))
                else:
                    argmax = max(list(range(len(scores))), key=lambda i: scores[i])  # see here for this semi-readable hack for argmax() https://towardsdatascience.com/there-is-no-argmax-function-for-python-list-cd0659b05e49
                    return cluster[argmax]["data"]

        ref_list = [rec.ref for rec in self.recommendations]
        clusters = self.cluster_close_refs(ref_list, self.recommendations, self.cluster_max_dist)
        self.recommendations = list(map(combine_cluster, clusters))
        return self

    @staticmethod
    def get_recs_thru_links(oref):
        '''
        Given a ref, returns items connected to central ref through links - direct links and links through commentaries.
        :param oref:
        :return: Twos things:
                    list of `Recommendation`s
                    [tref, tref] - all of the refs in the above set that are direct commentaries of original tref
        '''

        direct_links = set()
        section_ref_list = [r.section_ref() for r in oref.split_spanning_ref()]
        range_set = {r.normal() for r in oref.all_segment_refs()}
        for section_ref in section_ref_list:
            section_ref = oref.section_ref()
            commentary_links = []
            commentary_author_set = set()
            # set is used b/c sometimes there are duplicate links
            temp_direct_links = set()
            initial_links = get_links(section_ref.normal(), with_text=False)
            filtered_links = [l for l in initial_links if len(range_set & {r.normal() for r in Ref(l['anchorRef']).range_list()}) > 0]
            direct_links |= {(l['ref'], l['category'] in ('Commentary', 'Modern Commentary'), Ref(l['anchorRef'])) for l in filtered_links}
        for link_tref, is_comment, anchor_ref in direct_links:
            # Steinsaltz is hard-coded to have same connections as Talmud which will double count Talmud connections
            if is_comment and not link_tref.startswith("Steinsaltz on "):
                link_oref = Ref(link_tref)
                author = getattr(link_oref.index, "collective_title", None)
                temp_commentary_links, _, _, _ = RecommendationEngine.normalize_related_refs([x["ref"] for x in get_links(link_tref, with_text=False)], None, COMMENTARY_LINK_SCORE)
                for commentary_link in temp_commentary_links:
                    if author is not None and (commentary_link, author) in commentary_author_set:
                        # don't add same ref twice from same author
                        continue
                    commentary_author_set.add((commentary_link, author))
                    commentary_links += [Recommendation(Ref(commentary_link), relevance=COMMENTARY_LINK_SCORE, sources=[RecommendationSource(link_tref, anchor_ref)])]
        other_data = [(x[1], x[2]) for x in direct_links]
        direct_links, _, other_data, focus_ref_subref = RecommendationEngine.normalize_related_refs([x[0] for x in direct_links], None, DIRECT_LINK_SCORE, other_data=other_data)
        direct_ref_set = set(direct_links)
        is_comment_list, anchor_ref_list = list(zip(*other_data))
        final_rex = [Recommendation(Ref(x), relevance=DIRECT_LINK_SCORE, sources=[RecommendationSource('direct', anchor_ref)]) for x, anchor_ref in zip(direct_links, anchor_ref_list)] + commentary_links
        commentary_ref_set = set([x[0] for x in [x for x in zip(direct_links, is_comment_list) if x[1]]])
        return final_rex, commentary_ref_set, direct_ref_set

    @staticmethod
    def is_interesting_sheet(sheet):
        included_refs = sheet.get("includedRefs", [])
        if len(included_refs) > INCLUDED_REF_MAX:
            # this guy has waaaay too much to talk about. probably not interesting
            return False
        uninteresting_sheet_titles = ["New Source Sheet", "Untitled Source Sheet", "Untitled"]
        if sheet.get("title", "") in uninteresting_sheet_titles:
            # didn't care enough to change default title. wow
            return False
        return True

    @staticmethod
    def get_recs_thru_sheets(oref):
        '''

        :param oref:
        :return: list of tuples, each one with (tref, score, way of connection[str])
        '''
        sheets = []
        section_ref_list = [r.section_ref() for r in oref.split_spanning_ref()]
        range_set = {r.normal() for r in oref.all_segment_refs()}
        for section_ref in section_ref_list:
            regex_list = section_ref.regex(as_list=True)
            ref_clauses = [{"includedRefs": {"$regex": r}} for r in regex_list]
            query = {"status": "public", "$or": ref_clauses, "viaOwner": {"$exists": 0}, "assignment_id": {"$exists": 0}}
            sheets_cursor = db.sheets.find(query, {"includedRefs": 1, "owner": 1, "id": 1, "tags": 1, "title": 1})
            sheets += [s for s in sheets_cursor if RecommendationEngine.is_interesting_sheet(s)]
        included_ref_dict = {}
        for sheet in sheets:
            temp_included, focus_range_factor, _, focus_ref_subref = RecommendationEngine.normalize_related_refs(sheet.get("includedRefs", []), range_set, SHEET_REF_SCORE, check_has_ref=True, count_steinsaltz=True)
            if focus_range_factor == 0:
                continue
            ref_owner_keys = [(r, sheet["owner"]) for r in temp_included]
            for k in ref_owner_keys:
                if (k in included_ref_dict and included_ref_dict[k]["score"] < focus_range_factor) or k not in included_ref_dict:
                    included_ref_dict[k] = {"score": focus_range_factor, "source": "Sheet " + str(sheet["id"]), "anchor_ref": focus_ref_subref}

        return [Recommendation(Ref(r), relevance=d["score"], sources=[RecommendationSource(d["source"], d["anchor_ref"])]) for (r, _), d in list(included_ref_dict.items())]

    @staticmethod
    def cluster_close_refs(ref_list, data_list, dist_threshold):
        '''

        :param ref_list: list of orefs
        :param data_list: list of data to associate w/ refs (same length as ref_list)
        :param dist_threshold: max distance where you want two refs clustered
        :return: List of lists where each internal list is a cluster of segment refs
        '''

        clusters = []
        item_list = sorted(zip(ref_list, data_list), key=lambda x: x[0].order_id())
        last_cluster = None
        for temp_oref, temp_data in item_list:
            new_cluster_item = {"ref": temp_oref, "data": temp_data}
            if last_cluster is None or (not (-1 < last_cluster[-1]["ref"].distance(temp_oref) <= dist_threshold)):
                last_cluster = [new_cluster_item]
                clusters += [last_cluster]
            else:
                last_cluster.append(new_cluster_item)
        return clusters

    @staticmethod
    def includes_section(oref):
        """
        makes sure oref is not a range which makes up at least one entire section
        :param oref:
        :return:
        """
        if oref.is_section_level():
            return True
        if isinstance(oref.index_node, JaggedArrayNode) and oref.index.schema.get("depth", 0) == 2:
            # doesn't work for dictionary entries and not relevant anyway
            range_set = {r.normal() for r in oref.all_segment_refs()}
            section_range_set = {r.normal() for r in Ref(next(iter(range_set))).section_ref().all_segment_refs()}
            if len(range_set & section_range_set) == len(section_range_set):
                # ref is simply a full section. user didn't bother trimming it down
                return True
        return False

    @staticmethod
    def normalize_related_refs(related_refs, focus_ref_set, base_score, check_has_ref=False, other_data=None, count_steinsaltz=False):
        '''

        :param related_refs:
        :param focus_ref_set:
        :param base_score:
        :param check_has_ref:
        :param other_data:
        :param count_steinsaltz:
        :return:
        '''
        # make sure oref is in includedRefs but don't actually add those to the final includedRefs
        focus_ref_subset = None
        focus_range_factor = 0.0  # multiplicative factor based on how big a range the focus_ref is in
        final_refs = []
        other_data = [None]*len(related_refs) if other_data is None else other_data
        final_other_data = None if other_data is None else []
        for temp_tref, other_data_item in zip(related_refs, other_data):
            try:
                temp_oref = Ref(temp_tref)
            except InputError:
                continue

            if isinstance(temp_oref.index_node, DictionaryEntryNode):
                # dictionary entries aren't interesting
                continue
            if RecommendationEngine.includes_section(temp_oref):
                continue
            if temp_oref.is_range():
                temp_range_set = {subref.normal() for subref in temp_oref.range_list()}
                in_common_set = set() if focus_ref_set is None else temp_range_set & focus_ref_set
                if len(in_common_set) > 0:
                    temp_focus_range_factor = (len(in_common_set) * base_score)/len(temp_range_set) if len(temp_range_set) < REF_RANGE_MAX else 0.0
                    if temp_focus_range_factor > focus_range_factor:
                        focus_range_factor = temp_focus_range_factor
                    if focus_ref_subset is None or len(in_common_set) < len(focus_ref_subset):
                        focus_ref_subset = in_common_set
                    continue
                final_refs += list(temp_range_set)
                final_other_data += [other_data_item] * len(temp_range_set)

            else:
                if focus_ref_set is not None and temp_oref.normal() in focus_ref_set:
                    focus_range_factor = base_score
                    focus_ref_subset = {temp_oref.normal()}
                    continue
                final_refs += [temp_tref]
                final_other_data += [other_data_item]
        if count_steinsaltz:
            # transform mentions of steinsaltz to talmud
            final_refs = [x.replace("Steinsaltz on ", "") for x in final_refs]
        else:
            # throw out steinsaltz
            filter_result = [x for x in zip(final_refs, final_other_data) if "Steinsaltz on " not in x[0]]
            final_refs, final_other_data = reduce(lambda a, b: [a[0]+[b[0]], a[1]+[b[1]]], filter_result, [[], []])

        focus_ref_subref = None
        if focus_ref_subset is not None:
            focus_ref_subref_list = sorted([Ref(x) for x in list(focus_ref_subset)], key=lambda x: x.order_id())
            focus_ref_subref = focus_ref_subref_list[0].to(focus_ref_subref_list[-1])
        if focus_ref_subset is not None or not check_has_ref:
            return final_refs, focus_range_factor, final_other_data, focus_ref_subref
        return [], focus_range_factor, final_other_data, focus_ref_subref

```

### sefaria/sitemap.py

```
"""
sitemap.py - generate sitemaps of all available texts for search engines.

Outputs sitemaps and sitemapindex to the first entry of STATICFILES_DIRS by default, a custom directory can be supplied.
"""
import os, errno
from datetime import datetime

from sefaria.model import *
from sefaria.system.database import db
from .settings import STATICFILES_DIRS, STATIC_URL


def chunks(l, n):
    """
    Yield successive n-sized chunks from l.
    """
    for i in range(0, len(l), n):
        yield l[i:i + n]


class SefariaSiteMapGenerator(object):

    hostnames = {
        'org': {'interfaceLang': 'en', 'hostname':'https://www.sefaria.org'},
        'org.il': {'interfaceLang': 'he', 'hostname':'https://www.sefaria.org.il'},
    }
    static_urls = [
        "",
        "/explore",
        "/texts",
        "/visualizations",
        "/activity",
        "/educators",
        "/donate",
        "/supporters",
        "/mobile",
        "/app",
        "/daf-yomi",
        "/linker",
        "/jobs",
        "/help",
        "/metrics",
        "/sheets",
        "/collections",
        "/login",
        "/register",
        "/terms",
        "/testimonials",
        "/privacy-policy",
        "/updates",
        "/people",
        "/people/Talmud",
        "/william-davidson-talmud",
        "/nash-bravmann-collection",
    ]
    sitemaps = []

    def __init__(self, hostSuffix='org', output_directory=STATICFILES_DIRS[0]):
        if hostSuffix in SefariaSiteMapGenerator.hostnames:
            self._interfaceLang = SefariaSiteMapGenerator.hostnames.get(hostSuffix).get("interfaceLang")
            self._hostname = SefariaSiteMapGenerator.hostnames.get(hostSuffix).get("hostname")
            self.output_directory = output_directory
            path = self.output_directory + "sitemaps/" + self._interfaceLang
            if not os.path.exists(path):
                os.makedirs(path)
        else:
            raise KeyError("Illegal hostname for SiteMapGenerator")

    def generate_texts_sitemaps(self):
        """
        Create sitemap for each text section for which content is available.
        Returns the number of files written (each sitemap can have only 50k URLs)
        """
        refs = library.ref_list() # All refs at section level
        
        segment_level_categories = ("Tanakh", "Mishnah")
        for cat in segment_level_categories:
            books = library.get_indexes_in_corpus(cat, full_records=True)
            for book in books:
                refs += book.all_segment_refs()

        urls = [self._hostname + "/" + oref.url() for oref in refs]

        maps = list(chunks(urls, 40000))

        for n in range(len(maps)):
            self.write_urls(maps[n], "texts-sitemap%d.xml" % n)

        return len(maps)

    def generate_texts_toc_sitemap(self):
        """
        Creates a sitemap for each text table of contents page.
        """
        titles = library.get_toc_tree().flatten()
        urls = [self._hostname + "/" + Ref(title).url() for title in titles]
        self.write_urls(urls, "text-toc-sitemap.xml")

    def generate_categories_sitemap(self):
        """
        Creates sitemap for each category page.
        """
        toc = library.get_toc()
        def cat_paths(toc):
            paths = []
            for t in toc:
                cat = t.get("category", None)
                if cat:
                    cat = cat.replace(" ", "%20")
                    paths.append(cat)
                    try:
                        subpaths = cat_paths(t["contents"])
                    except KeyError:
                        continue
                    paths = paths + [cat + "/" + sp for sp in subpaths]
            return paths
        paths = cat_paths(toc)
        urls = [self._hostname + "/texts/" + p for p in paths]
        self.write_urls(urls, "categories-sitemap.xml")

    def generate_sheets_sitemap(self):
        """
        Creates a sitemap for each public source sheet.
        """
        query = {"status": "public", "noindex": {"$ne": True}}
        public = db.sheets.find(query).distinct("id")
        urls = [self._hostname + "/sheets/" + str(id) for id in public]
        self.write_urls(urls, "sheets-sitemap.xml")

    def generate_topics_sitemap(self):
        """
        Creates a sitemap for each topic that has at least one source or source sheet.
        """
        topics = TopicSet()
        topics = [topic for topic in topics if topic.should_display()]
        urls = [self._hostname + "/topics/" + topic.slug for topic in topics]
        self.write_urls(urls, "topics-sitemap.xml")

    def generate_static_sitemap(self):
        """
        Creates a sitemap of static content listed above.
        """
        self.write_urls([self._hostname + "/" + url for url in self.static_urls], "static-sitemap.xml")

    def write_urls(self, urls, filename):
        """
        Writes the list of `urls` to `filename`.
        """
        def escape_url(url):
            return url.replace("&", "&amp;")

        content = ""
        for url in urls:
            content += ("<url><loc>{}</loc></url>\n".format(escape_url(url)))

        xml = ('<?xml version="1.0" encoding="UTF-8"?>'
               '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
               '{}'
               '</urlset>'.format(content))

        out = self.output_directory + "sitemaps/" + self._interfaceLang + "/" + filename
        f = open(out, 'w')
        f.write(xml)
        f.close()
        self.sitemaps.append(filename)

    def generate_sitemap_index(self):
        now = datetime.now().strftime("%Y-%m-%d")
        xml = ""
        for m in self.sitemaps:
            xml += ('<sitemap>'
                    '<loc>{}{}sitemaps/{}/{}</loc>'
                    '<lastmod>{}</lastmod>'
                    '</sitemap>'.format(self._hostname, STATIC_URL, self._interfaceLang, m, now))

        sitemapindex = ('<?xml version="1.0" encoding="UTF-8"?>'
            '<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
            '{}'
            '</sitemapindex>'.format(xml))

        out = self.output_directory + "sitemaps/" + self._interfaceLang + "/sitemapindex.xml"
        f = open(out, 'w')
        f.write(sitemapindex)
        f.close()

    def generate_sitemaps(self):
        """
        Creates all sitemap files then creates and index file for all.
        """
        self.generate_static_sitemap()
        self.generate_sheets_sitemap()
        self.generate_texts_toc_sitemap()
        self.generate_categories_sitemap()
        self.generate_texts_sitemaps()
        self.generate_topics_sitemap()

        self.generate_sitemap_index()

```

### sefaria/tests/conftest.py

```
import sys, os

# Make sure that the application source directory (this directory's parent) is
# on sys.path.

here = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, here)

```

### sefaria/tests/texts_test.py

```
"""
Tests of texts.py (and things recently factored out. :)
"""
import pytest
from helper.text import rename_category

import sefaria.model.text as tm
from sefaria.model.link import get_book_link_collection

@pytest.mark.deep
def test_rename_category():

    old = "Rishonim"
    new = "Rishon'im"

    assert not tm.IndexSet({"categories": new}).count()
    c = tm.IndexSet({"categories": old}).count()
    assert c
    rename_category(old, new)
    assert not tm.IndexSet({"categories": old}).count()
    assert tm.IndexSet({"categories": new}).count()
    rename_category(new, old)
    assert c == tm.IndexSet({"categories": old}).count()
    assert not tm.IndexSet({"categories": new}).count()


def test_get_commentary_texts_list():
    l = tm.library.get_dependant_indices()
    assert "Ba'al HaTurim on Genesis" in l
    assert 'Bartenura on Mishnah Eduyot' in l
    assert 'Tosafot on Pesachim' in l


def test_get_text_categories():
    l = tm.library.get_text_categories()
    assert 'Torah' in l
    assert 'Talmud' in l


def test_get_book_link_collection():
    res = get_book_link_collection("Shabbat", "Tanakh")
    assert len(res) > 650

```

### sefaria/tests/activity_test.py

```
# -*- coding: utf-8 -*-
import pytest

from .. import history


def setup_module(module): 
	global activity_a, activity_b, activity_c, activity_d
	activity_a = {
		"ref": "Job 2:2",
		"rev_type": "edit text",
		"user": 1,
		"version": "Test Version",
		"language": "he",
	}
	activity_b = {
		"ref": "Job 2:3",
		"rev_type": "edit text",
		"user": 1,
		"version": "Test Version",
		"language": "he",
	}

	activity_c = {
		"ref": "Job 2:4",
		"rev_type": "edit text",
		"user": 1,
		"version": "Test Version",
		"language": "he",
	}

	activity_d = {
		"ref": "Job 3:2",
		"rev_type": "edit text",
		"user": 1,
		"version": "Test Version",
		"language": "he",
	}



class Test_collapse_activity():

	def test_catch_collapse(self):
		collapsed = history.collapse_activity([activity_a, activity_b])
		assert len(collapsed) == 1

	def test_no_collapse(self):
		collapsed = history.collapse_activity([activity_a, activity_d])
		assert len(collapsed) == 2
```

### sefaria/tests/summaries_test.py

```
# -*- coding: utf-8 -*-

import pytest
from sefaria.system.exceptions import BookNameError
from sefaria.utils.testing_utils import *

#create, update, delete, change categories
# test that old title goes away on index title change (regular + commentary)
# test that no commentator is added
# no wandering commentaries


""" SOME SETUP """

text_titles = model.IndexSet({}).distinct('title')
model.library.rebuild_toc()


""" THE TESTS """

class Test_Toc(object):

    @classmethod
    def setup_class(cls):
        model.library.rebuild_toc()

    @classmethod
    def teardown_class(cls):
        titles = ["New Toc Title Test", "New Toc Test", "Another New Toc Test", "Harchev Davar on Joshua", "Bob is your Uncle"]
        for title in titles:
            model.IndexSet({"title": title}).delete()
            model.VersionSet({"title": title}).delete()

    def test_toc_integrity(self):
        self.recur_toc_integrity(model.library.get_toc())

    def recur_toc_integrity(self, toc, depth=0):
        for toc_elem in toc:
            if 'category' in toc_elem and 'contents' in toc_elem:
                #verify proper category node (including that it doesnt have a title attr)
                self.verify_category_node_integrity(toc_elem)
                self.recur_toc_integrity(toc_elem['contents'], depth+1)
            elif toc_elem.get('isCollection', False):
                #verify collection leaf integrity
                self.verify_collection_node_integrity(toc_elem)
            elif 'title' in toc_elem:
                #verify text leaf integrity
                self.verify_text_node_integrity(toc_elem)

    def verify_category_node_integrity(self, node):
        # search toc doesn't have 'enComplete' or 'heComplete' empty categories don't have 'contents'
        try:
            assert set(node.keys()) <= {'category', 'heCategory', 'enDesc', 'heDesc', 'enShortDesc', 'heShortDesc', 'contents', 'enComplete', 'heComplete', 'order', "isPrimary","searchRoot"}
            if getattr(node, 'contents', None):
                assert {'category', 'heCategory', 'contents'} <= set(node.keys())
                assert isinstance(node['contents'], list)
            else:
                assert {'category', 'heCategory'} <= set(node.keys())
            assert isinstance(node['category'], str)
            assert isinstance(node['heCategory'], str)

        except AssertionError as e:
            print("Bad category:")
            print(node)
            raise

    def verify_text_node_integrity(self, node):
        global text_titles
        expected_keys = {'title', 'heTitle'}
        assert set(node.keys()) >= expected_keys
        assert (node['title'] in text_titles), node['title']
        assert 'category' not in node
        #do we need to assert that the title is not equal to any category name?

    def verify_collection_node_integrity(self, node):
        expected_keys = set(('name', 'slug', 'title', 'heTitle'))
        assert set(node.keys()) >= expected_keys
        assert 'category' not in node  

    @pytest.mark.deep
    def test_new_index_title_change(self):
        new_index = model.Index({
            "title": "New Toc Title Test",
            "heTitle": "",
            "titleVariants": [],
            "sectionNames": ["Chapter", "Paragraph"],
            "categories": ["Philosophy"]
        })
        verify_existence_across_tocs(new_index.title, None)
        new_index.save()
        verify_existence_across_tocs(new_index.title, expected_toc_location=new_index.categories)
        # title change
        old_title = new_index.title
        new_title = "Bob is your Uncle"
        new_index.title = new_title
        new_index.save()
        verify_existence_across_tocs(old_title, None)
        verify_existence_across_tocs(new_title, expected_toc_location=new_index.categories)
        new_index.delete()
        verify_existence_across_tocs(new_title, None)
        verify_existence_across_tocs(old_title, None)


    def test_index_add_delete(self):
        #test that the index
        new_index = model.Index({
            "title": "New Toc Test",
            "heTitle": "",
            "titleVariants": [],
            "sectionNames": ["Chapter", "Paragraph"],
            "categories": ["Jewish Thought"]
        })
        verify_existence_across_tocs(new_index.title, None)
        new_index.save()
        verify_existence_across_tocs(new_index.title, expected_toc_location=new_index.categories)
        new_index.delete()
        verify_existence_across_tocs(new_index.title, None)

        """
        # Adding Indexes to non-existent categories doesn't work anymore.
        new_other_index = model.Index({
            "title": "Another New Toc Test",
            "heTitle": u"",
            "titleVariants": [],
            "sectionNames": ["Chapter", "Paragraph"],
            "categories": ["Law"]
        })
        verify_existence_across_tocs(new_other_index.title, None)
        new_other_index.save()
        verify_existence_across_tocs(new_other_index.title, expected_toc_location=['Other'] + new_other_index.categories)
        new_other_index.delete()
        verify_existence_across_tocs(new_other_index.title, None)
        """

        new_commentary_index = model.Index({
            "title": "Harchev Davar on Joshua",
            "heTitle": "   ",
            "dependence": "Commentary",
            "base_text_titles": ["Joshua"],
            "collective_title": "Harchev Davar",
            "sectionNames": ["Chapter", "Paragraph", "Comment"],
            "categories": ["Tanakh", "Acharonim on Tanakh", "Harchev Davar"]
        })
        verify_existence_across_tocs(new_commentary_index.title, None)
        new_commentary_index.save()
        verify_title_existence_in_toc(new_commentary_index.title, expected_toc_location=new_commentary_index.categories, toc=model.library.get_toc())
        new_commentary_index.delete()
        verify_existence_across_tocs(new_commentary_index.title, None)

    def test_index_attr_change(self):
        indx = model.Index().load({"title": "Or HaChaim on Genesis"})
        verify_title_existence_in_toc(indx.title, expected_toc_location=["Tanakh", "Acharonim on Tanakh", "Or HaChaim", "Torah"], toc=model.library.get_toc())
        indx.nodes.add_title("Or HaChaim HaKadosh", "en")
        indx.save()
        verify_title_existence_in_toc(indx.title, expected_toc_location=["Tanakh", "Acharonim on Tanakh", "Or HaChaim", "Torah"])


        indx2 = model.Index().load({"title": "Kuzari"})
        verify_existence_across_tocs(indx2.title, expected_toc_location=indx2.categories)
        indx2.nodes.add_title("Kuzari Test", "en")
        indx2.save()
        verify_existence_across_tocs(indx2.title, expected_toc_location=indx2.categories)

    def test_text_change(self):
        pass

    @pytest.mark.deep
    def test_index_title_change(self):
        try:
            i = model.library.get_index("The Likutei Moharan")
            if i:
                i.delete()
        except BookNameError:
            pass

        old_title = 'Likutei Moharan'
        new_title = 'The Likutei Moharan'
        toc_location = ['Chasidut', 'Breslov']
        old_toc_path = get_all_toc_locations(old_title, model.library.get_toc())[0]
        assert toc_path_to_string(old_toc_path) == toc_path_to_string(toc_location)
        i = model.Index().load({"title": old_title})
        i.title = new_title
        i.save()
        #old title not there anymore
        verify_existence_across_tocs(old_title, None)
        #new one in it's place
        verify_existence_across_tocs(new_title, expected_toc_location=old_toc_path)
        #do testing: make sure new title is in the old place in the toc and that the old title is removed
        i.title = old_title
        i.save()
        #old title not there anymore
        verify_existence_across_tocs(new_title, None)
        #new one in it's place
        verify_existence_across_tocs(old_title, expected_toc_location=old_toc_path)


```

### sefaria/tests/__init__.py

```

```

### sefaria/tests/links_test.py

```
# -*- coding: utf-8 -*-
import pytest

from sefaria.client.wrapper import get_links
from sefaria.model import *

def setup_module(module): 
    pass


class Test_get_links():

    def test_get_links_on_range(self):
        r3 = [l["ref"] + l["type"] for l in get_links("Exodus 2:3")]
        r4 = [l["ref"] + l["type"]  for l in get_links("Exodus 2:4")]
        r34 = [l["ref"] + l["type"]  for l in get_links("Exodus 2:3-4")]

        # All links in first segment present in range
        assert all([r in r34 for r in r3])
        # All links in second segment present in range
        assert all([r in r34 for r in r4])
        # No links in range absent from segments
        assert all(r in r3 or r in r4 for r in r34)


class Test_links_from_get_text():

    def test_links_from_padded_ref(self):
        t1 = TextFamily(Ref("Exodus ")).contents()
        t2 = TextFamily(Ref("Exodus 1")).contents()

        assert len(t1["commentary"]) == len(t2["commentary"])
```

### sefaria/tests/counts_test.py

```
import pytest
from sefaria.model.link import get_link_counts


@pytest.mark.deep
def test_get_link_counts():
    a = get_link_counts("Tanakh", "Bavli")
    assert len(a) > 970

```

### sefaria/tests/test_tracker.py

```
import sefaria.tracker as tracker


```

### sefaria/tests/search.py

```
import pytest
import re
from sefaria.search import TextIndexer as TI
from sefaria.model import *
from sefaria.utils.hebrew import strip_cantillation

def test_make_text_index_document():
    oref = Ref('Genesis 1')
    tref = oref.normal()
    he_ref = oref.he_normal()
    vtitle = "Tanach with Ta'amei Hamikra"
    version = [v for v in oref.versionset() if v.versionTitle == vtitle][0]
    lang = version.language
    priority = version.priority
    content = TextChunk(oref, lang, vtitle=vtitle).ja().flatten_to_string()
    index = TI.curr_index = oref.index
    categories = index.categories
    heVtitle = version.versionTitleInHebrew

    TI.best_time_period = index.best_time_period()
    comp_date = int(TI.best_time_period.start)

    doc = TI.make_text_index_document(tref, he_ref, vtitle, lang, priority, content, categories, heVtitle)

    ref_data = RefData().load({"ref": tref})
    pagesheetrank = ref_data.pagesheetrank if ref_data is not None else RefData.DEFAULT_PAGESHEETRANK
    content = TI.modify_text_in_doc(content)
    assert doc == {
        "ref": tref,
        "heRef": he_ref,
        "version": vtitle,
        "lang": lang,
        "version_priority": priority,
        "titleVariants": oref.index_node.all_tree_titles("en"),
        "categories": categories,
        "order": oref.order_id(),
        "path": "/".join(categories + [index.title]),
        "pagesheetrank": pagesheetrank,
        "comp_date": comp_date,
        "exact": content,
        "naive_lemmatizer": content,
        'hebrew_version_title': heVtitle,

    }



```

### sefaria/tests/recommendation_test.py

```
from sefaria.model import *
from sefaria.recommendation_engine import RecommendationEngine

class TestClustering:

    def test_simple(self):
        trefs = ['Genesis 1:1', 'Genesis 1:2', 'Genesis 1:4']
        refs = [Ref(tref) for tref in trefs]
        clusters = RecommendationEngine.cluster_close_refs(refs, [None]*len(refs), dist_threshold=2)
        assert len(clusters) == 1

    def test_two_clusters(self):
        trefs = ['Genesis 1:1', 'Genesis 1:2', 'Genesis 1:5', 'Genesis 1:7']
        refs = [Ref(tref) for tref in trefs]
        clusters = RecommendationEngine.cluster_close_refs(refs, [None]*len(refs), dist_threshold=2)
        assert len(clusters) == 2
        assert clusters[0][0]['ref'].normal() == 'Genesis 1:1'
        assert clusters[1][0]['ref'].normal() == 'Genesis 1:5'

    def test_out_of_order(self):
        trefs = ['Genesis 1:5', 'Genesis 1:1', 'Exodus 1:1', 'Genesis 1:2', 'Genesis 1:7', 'Exodus 1:3']
        refs = [Ref(tref) for tref in trefs]
        clusters = RecommendationEngine.cluster_close_refs(refs, [None]*len(refs), dist_threshold=2)
        assert len(clusters) == 3
        assert clusters[0][0]['ref'].normal() == 'Genesis 1:1'
        assert clusters[1][0]['ref'].normal() == 'Genesis 1:5'
        assert clusters[2][0]['ref'].normal() == 'Exodus 1:1'

```

### sefaria/celery_setup/config.py

```
from sefaria.settings import (REDIS_URL, REDIS_PASSWORD, REDIS_PORT, CELERY_REDIS_BROKER_DB_NUM,
                             CELERY_REDIS_RESULT_BACKEND_DB_NUM, SENTINEL_HEADLESS_URL, SENTINEL_PASSWORD,
                             SENTINEL_TRANSPORT_OPTS)
from sefaria.celery_setup.generate_config import generate_config, SentinelConfig, RedisConfig


def generate_config_from_env():
    return generate_config(
        RedisConfig(REDIS_URL, REDIS_PASSWORD, REDIS_PORT, CELERY_REDIS_BROKER_DB_NUM, CELERY_REDIS_RESULT_BACKEND_DB_NUM),
        SentinelConfig(SENTINEL_HEADLESS_URL, SENTINEL_PASSWORD, REDIS_PORT, SENTINEL_TRANSPORT_OPTS)
    )



```

### sefaria/celery_setup/generate_config.py

```
"""
NOTE: This file is a direct copy of the same file in the LLM repo
This file is required for any new service that wants to configure celery
We should consider releasing this as a pip module but it's not clear where that would live at this point
"""
import re
import dns.resolver
from dataclasses import dataclass


@dataclass
class SentinelConfig:
    url: str
    password: str
    port: str
    transport_opts: dict

    def is_configured(self) -> bool:
        """
        Return True if this config has the data it needs to connect to Sentinel
        :return:
        """
        return bool(self.url)


@dataclass
class RedisConfig:
    url: str
    password: str
    port: str
    broker_db_num: str
    result_backend_db_num: str


def add_db_num_to_url(url, port, db_num):
    return url.replace(f':{port}', f':{port}/{db_num}')


def add_password_to_url(url, password):
    if not password:
        return url
    return re.sub(r'((?:redis|sentinel)://)', fr'\1:{password}@', url)


def generate_config(redis_config: RedisConfig, sentinel_config: SentinelConfig = None) -> dict:
    """
    :param redis_config: required, whether connecting to redis or redis sentinel, the redis config is required.
    :param sentinel_config: optional, only pass if connecting to redis sentinel
    """
    if sentinel_config is not None and sentinel_config.is_configured():
        redisdns = dns.resolver.resolve(sentinel_config.url, 'A')
        addressstring = []
        for res in redisdns.response.answer:
            for item in res.items:
                curr_redis_url = f"sentinel://{item.to_text()}:{sentinel_config.port}"
                curr_redis_url = add_password_to_url(curr_redis_url, redis_config.password)
                addressstring.append(curr_redis_url)
        joined_address = ";".join(addressstring)
        merged_transport_opts = {
            **sentinel_config.transport_opts,
            "sentinel_kwargs": {"password": sentinel_config.password}
        }

        return {
            "broker_url": add_db_num_to_url(joined_address, sentinel_config.port, redis_config.broker_db_num),
            "result_backend": add_db_num_to_url(joined_address, sentinel_config.port, redis_config.result_backend_db_num),
            "result_backend_transport_options": merged_transport_opts,
            "broker_transport_options": merged_transport_opts,
        }
    else:
        redis_url = add_password_to_url(f"{redis_config.url}:{redis_config.port}", redis_config.password)
        return {
            "broker_url": add_db_num_to_url(redis_url, redis_config.port, redis_config.broker_db_num),
            "result_backend": add_db_num_to_url(redis_url, redis_config.port, redis_config.result_backend_db_num),
        }

```

### sefaria/celery_setup/app.py

```
from celery import Celery
from sefaria.celery_setup.config import generate_config_from_env

app = Celery('sefaria')
app.conf.update(**generate_config_from_env())
app.autodiscover_tasks(packages=['sefaria.helper.llm', 'sefaria.helper.linker'])

```

### sefaria/__init__.py

```

```

### sefaria/datatype/jagged_array.py

```
"""
jagged_array.py: a sparse array of arrays

"""

# WARNING! instanciation creates a *reference* to the passed array.
# This is fine for analysis, but for modification, may modify the original array

# All methods that modify self._store need to be aware of this
# Potentially problematic methods marked with '#warning, writes!'

import re
from functools import reduce
from itertools import zip_longest
import structlog
logger = structlog.get_logger(__name__)


class JaggedArray(object):

    def __init__(self, ja=None):
        if ja is None:
            ja = []
        self._store = ja  # do not modify _store from outside the object.  See above.
        self.e_count = None
        self._depth = None

    #Intention is to call this when the contents of the JA change, so that counts don't get stale
    def _reinit(self):
        self.e_count = None
        self._depth = None

    def array(self):
        return self._store

    def is_first(self, indexes1, indexes2):
        """

        :param indexes1: list of 0 based indexes for digging len(indexes) levels into the array
        :param indexes2: ditto
        :return: True if indexes1 is before indexes2. If equal, False
        """

        #pad with 0s so their len == _depth
        N = self.get_depth()
        if len(indexes1) <= N:
            indexes1 += [0] * (N - len(indexes1))
        else:
            raise IndexError

        if len(indexes2) <= N:
            indexes2 += [0] * (N - len(indexes2))
        else:
            raise IndexError

        first_diff_index = 0
        for i in range(N):
            if indexes1[i] != indexes2[i]:
                first_diff_index = i
                break

        return indexes1[first_diff_index] < indexes2[first_diff_index]

    def distance(self, indexes1, indexes2):
        """
        :param indexes1: list of 0 based indexes for digging len(indexes) levels into the array
        :param indexes2: ditto
        :return: the distance, measured in array elements, between indexes1 and indexes2
        """

        if indexes1 == indexes2:
            return 0

        # make sure indexes1 represents earliest index
        if self.is_first(indexes2,indexes1):
            indexes1, indexes2 = (indexes2, indexes1)

        # pad with 0s so their len == _depth
        N = self.get_depth()
        if len(indexes1) <= N:
            indexes1 += [0] * (N - len(indexes1))
        else:
            raise IndexError

        if len(indexes2) <= N:
            indexes2 += [0] * (N - len(indexes2))
        else:
            raise IndexError

        first_diff_index = 0
        for i in range(N):
            if indexes1[i] != indexes2[i]:
                first_diff_index = i
                break


        if first_diff_index == N-1:
            #base case
            if self.sub_array_length(indexes1[:-1]) == 0:
                # empty section
                return 0
            return abs(indexes1[-1] - indexes2[-1])
        else:
            #recurse
            distance = 0
            temp_start_index = indexes1[:]
            for i in range(indexes1[first_diff_index],indexes2[first_diff_index]+1):
                is_zero_len_section = False

                if indexes2[first_diff_index] == i:
                    temp_end_index = indexes2[:]
                else:
                    temp_end_index = temp_start_index[:]
                    # max out all indexes greater than first_diff_index

                    temp_subarray_indexes = indexes1[:first_diff_index+1]
                    temp_subarray_indexes[first_diff_index] = i
                    for j in range(first_diff_index+1,N):
                        temp_subarray_len = self.sub_array_length(temp_subarray_indexes)
                        if temp_subarray_len == 0 or temp_subarray_len is None:  # it's None when you try to index past list end
                            is_zero_len_section = True
                            break

                        temp_end_index[j] = temp_subarray_len - 1
                        temp_subarray_indexes += [temp_end_index[j]]

                if not is_zero_len_section:
                    distance += self.distance(temp_start_index,temp_end_index) + 1  # + 1 to include the current seg
                temp_start_index[first_diff_index] = i + 1
                # set all indexes greater than first_diff_index to zero because you've moved on to the next section
                for j in range(first_diff_index+1,N):
                    temp_start_index[j] = 0

            return distance - 1  # - 1 to not include the first seg in the sequence

    def shape(self, _cur=None):
        """
        Returns a List one level shallower than this one, whose values are the length of the lowest level arrays of this jagged array.
        So:
            For depth 1, returns an Integer - length
            For depth 2, returns a List of chapter lengths
            For depth 3, returns a List of list of chapter lengths
        :return: List
        """

        # If the values of the array are integers, return an integer
        # If the values of the list are lists, recur

        if _cur is None:
            _cur = self._store

        if len(_cur) and isinstance(_cur[0], list):
            return [self.shape(e) for e in _cur]
        else:
            return len(_cur)

    def sub_array_length(self, indexes=None, until_last_nonempty=False):
        """
        :param indexes:  a list of 0 based indexes, for digging len(indexes) levels into the array
        :param until_last_nonempty_section: True if you want to return the length of the last nonempty (super-section, section, segment)
        :return: The length of the array at the provided index
            If indexes are beyond end of book, return None   # Is this best?
        """
        if indexes is None:
            indexes = []
        a = self._store
        if len(indexes) == 0 and not until_last_nonempty:
            return len(a)
        for i in range(0, len(indexes)):
            if indexes[i] > len(a) - 1:
                return None
            a = a[indexes[i]]
        try:
            if until_last_nonempty and len(a) > 0 and type(a[-1]) == list:  # and not at end of `a`
                curr_result = len(a)
                while self.sub_array_length(indexes + [curr_result - 1]) == 0 and curr_result > 0:
                    curr_result -= 1
                result = curr_result
            else:
                result = len(a)
        except TypeError as e:
            result = 0
        return result

    def next_index(self, starting_points=None):
        """
        Return the next populated address in a JA
        :param starting_points: An array indicating starting address in the JA
        """
        return self._dfs_traverse(self._store, starting_points)

    def prev_index(self, starting_points=None):
        """
        Return the previous populated address in a JA
        :param starting_points: An array indicating starting address in the JA
        """
        return self._dfs_traverse(self._store, starting_points, False)

    def is_full(self, _cur=None):
        if _cur is None:
            return self.is_full(_cur=self._store)
        if isinstance(_cur, list):
            if not len(_cur):
                return False
            for a in _cur:
                if not self.is_full(a):
                    return False
        else:
            if not _cur:
                return False
        return True

    def is_empty(self, _cur=None) -> bool:
        if _cur is None:
            return self.is_empty(_cur=self._store)
        if isinstance(_cur, list):
            if not len(_cur):
                return True
            return all([self.is_empty(a) for a in _cur])
        else:
            return not bool(_cur)

    def sections(self, _cur=None):
        """
        List of valid indexes in this object, to depth one up from bottom
        :param _cur: list of indexes
        :return:
        """
        if _cur is None:
            _cur = []
        if self.get_depth() - 1 <= len(_cur):
            return [_cur]
        return reduce(lambda a, b: a + self.sections(b), [_cur + [i] for i in range(self.sub_array_length(_cur))], [])

    def non_empty_sections(self):
        return [s for s in self.sections() if not self.subarray(s).is_empty()]

    def element_count(self) -> int:
        if self.e_count is None:
            self.e_count = self._ecnt(self._store)
        return self.e_count if self.e_count else 0

    def _ecnt(self, jta) -> int:
        if isinstance(jta, list):
            return sum([self._ecnt(i) for i in jta])
        else:
            return 1

    @staticmethod
    def _dfs_traverse(counts_map, starting_points=None, forward=True, depth=0):
        """
        Private function to recusrsively iterate through the counts doc to find the next available section
        :param counts_map: the counts doc map of available texts
        :param forward: if to move forward or backwards
        :param starting_points: the indices from which to start looking.
        :param depth: tracking parameter for recursion.
        :return: the indices where the next section is at.
        """
        if starting_points is None:
            starting_points = []

        #at the lowest level, we will have either strings or ints indicating text existence or not.
        if isinstance(counts_map, (int, str)):
            return bool(counts_map)

        #otherwise iterate through the sections
        else:
            #doesn't matter if we are out of bounds (slicing returns empty arrays for illegal indices)
            if forward:
                #we have been told where to start looking
                if depth < len(starting_points):
                    begin_index = starting_points[depth]
                    #this is in case we come back to this depth, then we want to start from 0 becasue the start point only matters for the
                    #array element we were in to begin with
                    starting_points[depth] = 0
                else:
                    begin_index = 0
                #we are going in order, so we want the next element (we also want to preserve the original indices)
                #TODO: this is a bit of wasted memory allocation, but have not yet found a better way
                section_to_traverse = enumerate(counts_map[begin_index:], begin_index)
            else:
                if depth < len(starting_points):
                    #we want to include the element we are on when going backwards.
                    begin_index = starting_points[depth] + 1 if starting_points[depth] is not None else None
                    #this will make the slice go to the end.
                    starting_points[depth] = None
                else:
                    begin_index = None
                #we are going in reverse, so we want everything up to the current element.
                #this weird hack will preserve the original numeric indices and allow reverse iterating
                section_to_traverse = reversed(list(enumerate(counts_map[:begin_index])))

            for n, j in section_to_traverse:
                result = JaggedArray._dfs_traverse(j, starting_points, forward, depth+1)
                if result:
                    #if we have a result, add the index location to a list that will eventually map to this section.
                    indices = [n] + result if isinstance(result, list) else [n]
                    return indices
            return False

    def mask(self, __curr=None):
        """
        Returns a new jagged array which corresponds in shape to this jagged array,
        with each terminal element populated with 1 or 0
        if a truthy value is present in each position - 1, if not 0.
        :return JaggedIntArray:
        """
        if __curr is None:  # On simple call, return object.
            return JaggedIntArray(self.mask(self._store))
        if isinstance(__curr, list):  # on recursed calls, return array
            return [self.mask(c) for c in __curr]
        else:
            return 0 if not __curr else 1

    def zero_mask(self):
        """
        Returns a jagged array of identical shape to 'array'
        with all elements replaced by 0.
        """
        return self.constant_mask(0)

    def constant_mask(self, constant=None, __curr=None):
        if __curr is None:  # On simple call, return object.
            return JaggedIntArray(self.constant_mask(constant, self._store))
        if isinstance(__curr, list):
            return [self.constant_mask(constant, c) for c in __curr]
        else:
            return constant

    def get_depth(self):
        if not self._depth:
            self._depth = self.depth()
        return self._depth

    def depth(self, _cur=None, deep=False) -> int:
        """
        returns 1 for [n], 2 for [[n],[p]], etc.
        Special case returns zero for an empty array []
        :parm x - a list
        :param deep - whether or not to count a level when not all elements in
        that level are lists.
        e.g. [[], ""] has a list depth of 1 with depth=False, 2 with depth=True
        """

        if _cur is None:
            if not self._store:
                return 0
            return self.depth(_cur=self._store, deep=deep)
        if not isinstance(_cur, list):
            return 0
        elif len(_cur) > 0 and (deep or all([isinstance(y, list) for y in _cur])):
            return 1 + max([self.depth(y, deep=deep) for y in _cur])
        else:
            return 1

    # derived from TextChunk.trim_text
    def subarray_with_ref(self, ref):
        start = [i - 1 for i in ref.sections]
        end = [i - 1 for i in ref.toSections]
        return self.subarray(start, end)

    # derived from TextChunk.trim_text
    def subarray(self, start_indexes, end_indexes=None):
        """
        Trims a JA to the specifications of start_indexes and end_indexes
        This works on simple Refs and range refs of unlimited depth and complexity.
        :param start_indexes: List of zero-based indexes
        :param end_indexes: List of zero-based indexes
        :return: List|String depending on depth of Ref
        """
        if not end_indexes:
            end_indexes = start_indexes

        assert len(start_indexes) == len(end_indexes)
        if len(start_indexes) > self.get_depth():
            return self.__class__([])

        range_index = len(start_indexes)

        for i in range(0, len(start_indexes)):
            if start_indexes[i] != end_indexes[i]:
                range_index = i
                break
        sub = self._store[:]
        if not start_indexes:
            pass
        else:
            for i in range(0, len(start_indexes)):
                if range_index > i:  # Either not range, or range begins later.  Return simple value.
                    if isinstance(sub, list) and len(sub) > start_indexes[i]:
                        sub = sub[start_indexes[i]]
                    else:
                        return self.__class__([])
                elif range_index == i:  # Range begins here
                    start = start_indexes[i]
                    end = end_indexes[i] + 1
                    sub = sub[start:end]
                else:  # range_index < i, range continues here
                    begin = end = sub
                    for _ in range(range_index, i - 1):
                        begin = begin[0]
                        end = end[-1]
                    begin[0] = begin[0][start_indexes[i]:]
                    end[-1] = end[-1][:end_indexes[i] + 1]
        return self.__class__(sub)

    def resize(self, factor):
        """
        Return a resized jagged array for 'text' either up or down by int 'factor'.
        Size up if factor is positive, down if negative.
        Size up or down the number of times per factor's size.
        E.g., up twice for '2', down twice for '-2'.
        """
        if factor > 0:
            for i in range(factor):
                self._upsize()
        elif factor < 0:
            for i in range(abs(factor)):
                self._downsize()
        self._reinit()
        return self

    def normalize(self, terminal_depth=None, _cur=None, depth=1):
        """
        :param terminal_depth: The desired depth before which everything should be arrays
        :return: Bool if there were any actual modifications made or not.
        Normalizes the array so on any given depth, there are either arrays (incl empty) or primitives, not both.
        e.g. [[], ""] becomes [[], []]
        """
        normalized = False
        if not terminal_depth:
            terminal_depth = self.depth(deep=True)
        if _cur is None:
            if not self._store:
                return normalized
            return self.normalize(terminal_depth=terminal_depth, _cur=self._store)
        if depth < terminal_depth:
            for i,elem in enumerate(_cur):
                if not isinstance(_cur[i], list):
                    if isinstance(_cur[i], str) and not len(_cur[i].strip()):
                        _cur[i] = []
                    else:
                        for _ in range(depth, terminal_depth):
                            _cur[i] = [_cur[i]]
                    normalized = True
                else:
                    res = self.normalize(terminal_depth=terminal_depth, _cur=_cur[i], depth=depth+1)
                    normalized = normalized or res
        return normalized

    # todo: move to JaggedTextArray?
    def _upsize(self, _cur=None):
        """
        Returns a jagged array for text which restructures the content of text
        to include one additional level of structure.
        ["One", "Two", "Three"] -> [["One"], ["Two"], ["Three"]]
        """
        if _cur is None:
            self._store = self._upsize(_cur=self._store)
            return self

        new_text = []
        for segment in _cur:
            if isinstance(segment, str):
                new_text.append([segment])
            elif isinstance(segment, list):
                new_text.append(self._upsize(segment))
        return new_text

    # todo: move to JaggedTextArray?
    def _downsize(self, _cur=None):
        """
        Returns a jagged array for text which restructures the content of text
        to include one less level of structure.
        Existing segments are concatenated with " "
        [["One1", "One2"], ["Two1", "Two2"], ["Three1", "Three2"]] - >["One1 One2", "Two1 Two2", "Three1 Three2"]
        """
        if _cur is None:
            self._store = self._downsize(_cur=self._store)
            return self

        if len(_cur) == 0:
            return ""

        new_text = []
        for segment in _cur:
            # Assumes segments are of uniform type, either all strings or all lists
            if isinstance(segment, str):
                return " ".join(_cur)
            elif isinstance(segment, list):
                new_text.append(self._downsize(segment))
        # Return which was filled in, defaulted to [] if both are empty
        return new_text

    def get_element(self, indx_list):
        sa = reduce(lambda a, i: a[i],
                    indx_list[:-1],
                    self._store
        )
        return sa[indx_list[-1]]

    # warning, writes!
    def set_element(self, indx_list, value, pad=None):
        '''
        Set element at position specified by indx_list to value.
        If JA is not big enough, pad with [] at higher levels, and value of pad variable at last level.
        :param indx_list:
        :param value:
        :param pad:
        :return:
        '''
        def pad_and_walk(arry, indx):
            if len(arry) <= indx:
                for _ in range(len(arry), indx + 1):
                    arry += [[]]
            return arry[indx]

        sa = reduce(pad_and_walk, #lambda a, i: a[i],
                    indx_list[:-1],
                    self._store
        )
        if len(sa) <= indx_list[-1]:
            sa += [pad] * (indx_list[-1] - len(sa) + 1)

        sa[indx_list[-1]] = value
        return self

    def flatten_to_array(self, _cur=None):
        if _cur is None:
            if not isinstance(self._store, list):
                return [self._store]
            return self.flatten_to_array(_cur=self._store)

        flat = []
        for el in _cur:
            if isinstance(el, list):
                flat += self.flatten_to_array(el)
            else:
                flat += [el]
        return flat

    def flatten_to_array_with_indices(self, _cur=None):
        if _cur is None:
            if not isinstance(self._store, list):
                return [[[], self._store]]
            return self.flatten_to_array_with_indices(_cur=self._store)

        flat = []
        for i, el in enumerate(_cur):
            if isinstance(el, list):
                sub_flat = self.flatten_to_array_with_indices(_cur=el)
                for item in sub_flat:
                    item[0] = [i+1] + item[0]
                    flat += [item]
            else:
                flat += [[[i+1], el]]
        return flat

    def last_index(self, depth):
        """
        Return indicies of the last populated element of this JaggedArray.
        :param depth: Return indicies only to this depth
        :return: Array of 0 based indexes
        """
        if depth > self.get_depth():
            depth = self.get_depth()
        res = []
        next = self
        for _ in range(depth):
            res += [len(next.array()) - 1]
            next = next.subarray(res[-1:])
            if next.array() == []:
                # For sparse texts that end before the array ends
                return self.prev_index(res)
        return res

    @staticmethod
    def get_offset_sections(relative_sections, start_sections):
        """
        Gets absolute section (according to some outside context, e.g. textchunk or version) indices given `relative_sections`
        :param relative_sections: array(int). sections into current jagged array
        :param start_sections: array(int). absolute sections from outside context. usually textchunk or version
        """
        if start_sections is None:
            # relative_sections are actually absolute in this case
            sections = relative_sections
        else:
            # relative_sections is only as deep as ja. however, top-level ja could be deeper
            # use start_sections as a starting point and then update with relative_sections to get absolute section indexes
            sections = start_sections[:]
            for rel_section_index, abs_section_index in enumerate(range(len(sections)-len(relative_sections), len(sections))):
                sections[abs_section_index] = relative_sections[rel_section_index]
                if rel_section_index == 0 or relative_sections[0] == 0:
                    # first section should always be offset by start_sections. later sections should only be offset if first section is 0
                    sections[abs_section_index] += start_sections[abs_section_index]
        return sections

    def __eq__(self, other):
        return self._store == other._store

    def __len__(self):
        return self.sub_array_length()

    def __repr__(self):
        return f"{self.__class__.__name__}({self._store})"

    def length(self):
        return self.__len__()


class JaggedTextArray(JaggedArray):

    def __init__(self, ja=None):
        JaggedArray.__init__(self, ja)
        self.w_count = None
        self.c_count = None

    def _reinit(self):
        super(JaggedTextArray, self)._reinit()
        self.w_count = None
        self.c_count = None

    def verse_count(self) -> int:
        return self.element_count()

    def word_count(self) -> int:
        """ return word count in this JTA """
        if self.w_count is None:
            self.w_count = self._wcnt(self._store)
        return self.w_count if self.w_count else 0

    def _wcnt(self, jta) -> int:
        """ Returns the number of words in an undecorated jagged array """
        if isinstance(jta, str):
            return len(re.split(r"[\s\u05be]+", jta.strip()))
        elif isinstance(jta, list):
            return sum([self._wcnt(i) for i in jta])
        else:
            return 0

    def char_count(self) -> int:
        """ return character count in this JTA """
        if self.c_count is None:
            self.c_count = self._ccnt(self._store)
        return self.c_count if self.c_count else 0

    def _ccnt(self, jta) -> int:
        """ Returns the number of characters in an undecorated jagged array """
        if isinstance(jta, str):
            return len(jta)
        elif isinstance(jta, list):
            return sum([self._ccnt(i) for i in jta])
        else:
            return 0

    def modify_by_function(self, func, start_sections=None, _cur=None, _curSections=None):
        """
        Returns the jagged array but with each terminal string processed by func
        Func should accept two parameters: 1) text of current segment 2) zero-indexed indices of segment
        :param start_sections: array(int), optional param. Sections passed to `func` will be offset by `start_sections`, if passed
        """
        if _cur is None:
            _cur = self._store
        if isinstance(_cur, str):
            _curSections = _curSections or [0]
            return func(_cur, self.get_offset_sections(_curSections, start_sections))
        elif isinstance(_cur, list):
            _curSections = _curSections or []
            return [self.modify_by_function(func, start_sections, temp_curr, _curSections + [i]) for i, temp_curr in enumerate(_cur)]

    def flatten_to_array(self, _cur=None):
        # Flatten deep jagged array to flat array

        if _cur is None:
            if isinstance(self._store, str):
                return [self._store]
            return self.flatten_to_array(_cur=self._store)

        flat = []
        for el in _cur:
            if isinstance(el, list):
                flat += self.flatten_to_array(el)
            else:
                flat += [str(el)]
        return flat

    def flatten_to_string(self, joiner=" "):
        return joiner.join(self.flatten_to_array())

    # warning, writes!
    def trim_ending_whitespace(self):
        """
        Removes ending whitespace items from jagged array.
        These include empty string, None or items that are entirely whitespace.
        Performs process recursively on nested lists
        @return: list
        """
        self._store = self._trim_ending_whitespace_recursive(self._store)
        return self

    def _trim_ending_whitespace_recursive(self, curr_ja: list) -> list:
        if not isinstance(curr_ja, list):  # shouldn't get here
            return curr_ja

        # recursive step
        curr_ja = [self._trim_ending_whitespace_recursive(item) if isinstance(item, list) else item for item in curr_ja]
        return self._trim_ending_whitespace_list_of_strs(curr_ja)

    @staticmethod
    def _trim_ending_whitespace_list_of_strs(curr_ja: list) -> list:
        """
        Removes ending whitespace items from _cur. See docs for `trim_ending_whitespace()` for details.
        Doesn't recurse on any nested lists.
        @param curr_ja: list with items that are either lists, strs or None
        @return: list
        """
        final_index = len(curr_ja) - 1
        for item in reversed(curr_ja):
            if isinstance(item, list) or (isinstance(item, str) and len(item.strip()) > 0):
                break
            final_index -= 1
        del curr_ja[final_index+1:]
        return curr_ja

    def overlaps(self, other=None, _self_cur=None, _other_cur=None) -> bool:
        """
        Returns True if self and other contain one or more positions where both are non empty.
        Runs recursively.
        """
        if other:
            return self.overlaps(_self_cur=self._store, _other_cur=other._store)
        if isinstance(_self_cur, list) and isinstance(_other_cur, list):
            for i in range(min(len(_self_cur), len(_other_cur))):
                if self.overlaps(_self_cur=_self_cur[i], _other_cur=_other_cur[i]):
                    return True
        if isinstance(_self_cur, str) and isinstance(_other_cur, str):
            if _self_cur and _other_cur:
                return True
        return False


class JaggedIntArray(JaggedArray):
    def add(self, other):
        return self.__add__(other)

    def __add__(self, other):
        """
        :return JaggedIntArray:
        """
        assert isinstance(other, JaggedIntArray)
        return JaggedIntArray(self._add(self._store, other._store))

    @staticmethod
    def _add(a, b):
        """
        Returns a multi-dimensional array which sums each position of
        two multidimensional arrays of ints. Missing elements are given 0 value.
        [[1, 2], [3, 4]] + [[2,3], [4]] = [[3, 5], [7, 4]]
        """
        # Treat None as 0
        if a is None:
            return JaggedIntArray._add(0, b)
        if b is None:
            return JaggedIntArray._add(a, 0)

        # If one value is an int while the other is a list,
        # Treat the int as an empty list.
        # Needed e.g, when a whole chapter is missing appears as 0
        if isinstance(a, int) and isinstance(b, list):
            return JaggedIntArray._add([],b)
        if isinstance(b, int) and isinstance(a, list):
            return JaggedIntArray._add(a,[])

        # If both are ints, return the sum
        if isinstance(a, int) and isinstance(b, int):
            return a + b
        # If both are lists, recur on each pair of values
        # map results in None value when element not present
        if isinstance(a, list) and isinstance(b, list):
            return [JaggedIntArray._add(a2, b2) for a2, b2 in zip_longest(a, b)]

        raise Exception("JaggedIntArray._add() reached a condition it shouldn't have reached")

    def depth_sum(self, depth):
        return self._depth_sum(self._store, depth)

    @staticmethod
    def _depth_sum(curr, depth):
        """
        Sum the counts of a text at given depth to get the total number of a given kind of section
        E.g, for counts on all of Job, depth 0 counts chapters, depth 1 counts verses
        """
        if depth == 0:
            if isinstance(curr, int):
                return min(curr, 1)
            else:
                sum = 0
                for i in range(len(curr)):
                    sum += min(JaggedIntArray._depth_sum(curr[i], 0), 1)
                return sum
        else:
            sum = 0
            for i in range(len(curr)):
                sum += JaggedIntArray._depth_sum(curr[i], depth - 1)
            return sum

```

### sefaria/datatype/tests/jagged_array_test.py

```
# -*- coding: utf-8 -*-

import sefaria.datatype.jagged_array as ja
import pytest


def setup_module(module):
    global twoby, threeby, threeby_empty_section, two_by_mask
    twoby = [
                ["Line 1:1", "This is the first second", "First third"],
                ["Chapter 2, Verse 1", "2:2", "2:3"],
                ["Third first", "Third second", "Third third"]
    ]
    two_by_mask = [
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]
    ]
    threeby = [
        [
            ["Part 1 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ],
        [
            ["Part 2 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ],
        [
            ["Part 3 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ],
    ]
    threeby_empty_section = [
        [
            ["Part 1 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ],
        [
            ["Part 2 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ],
        [
            [],
            []
        ],
        [
            ["Part 3 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ],
    ]

class Test_Jagged_Array(object):

    def test_ja_normalize(self):
        input_ja = ["a",[],["","a", ["c"]],["",""],["b"]]
        output_ja = [[["a"]],[],[[],["a"], ["c"]],[[],[]],[["b"]]]
        jaobj = ja.JaggedArray(input_ja)
        jaobj.normalize()
        assert jaobj.array() == output_ja

    def test_last_index(self):
        assert ja.JaggedIntArray([
            [[1,3],[4,5],[7]],
            [[1,2,3],[2,2],[8,8,8]],
            [[0],[1],[2,3,4],[7,7,7,7,7]]
        ]).last_index(3) == [2, 3, 4]
        assert ja.JaggedIntArray([
            [[1,3],[4,5],[7]],
            [[1,2,3],[2,2],[8,8,8]],
            [[0],[1],[2,3,4],[7,7,7,7,7],[],[]]
        ]).last_index(3) == [2, 3, 4]


class Test_Jagged_Int_Array(object):
    def test_sum(self):
        x = ja.JaggedIntArray([[1, 2], [3, 4]]) + ja.JaggedIntArray([[2, 3], [4]])
        assert x.array() == [[3, 5], [7, 4]]

class Test_Jagged_Text_Array(object):
    def test_until_last_nonempty(self):
        sparse_ja = ja.JaggedTextArray([["", "", ""], ["", "foo", "", "bar", ""], ["", "", ""],[]])
        assert sparse_ja.sub_array_length([],until_last_nonempty=True) == 3

    def test_count_words(self):
        assert ja.JaggedTextArray(twoby).word_count() == 21
        assert ja.JaggedTextArray(threeby).word_count() == 69


    def test_count_chars(self):
        assert ja.JaggedTextArray(twoby).char_count() == 101
        assert ja.JaggedTextArray(threeby).char_count() == 324

    def test_verse_count(self):
        assert ja.JaggedTextArray(twoby).verse_count() == 9
        assert ja.JaggedTextArray(threeby).verse_count() == 27

    def test_equality(self):
        assert ja.JaggedTextArray(twoby) == ja.JaggedTextArray(twoby)
        assert ja.JaggedTextArray(threeby) == ja.JaggedTextArray(threeby)
        assert ja.JaggedTextArray(twoby) != ja.JaggedTextArray(threeby)


    def test_distance(self):
        jia = ja.JaggedTextArray(threeby)
        jia_empty = ja.JaggedTextArray(threeby_empty_section)
        assert jia.distance([0],[0,0,2]) == 2 #check if padding correctly
        assert jia.distance([0],[0,2]) == 6 #padding for both inputs
        assert jia.distance([0,0,1],[2,2,2]) == 25 #recursive distance
        assert jia_empty.distance([0,0,1], [3,2,2])  == 25
        assert jia_empty.distance([0,0,1], [2,1,3]) == 17
    def test_subarray(self):
        assert ja.JaggedTextArray(threeby).subarray([0],[0]) == ja.JaggedTextArray([
            ["Part 1 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ])
        assert ja.JaggedTextArray(threeby).subarray([1],[1]) == ja.JaggedTextArray([
            ["Part 2 Line 1:1", "This is the first second", "First third"],
            ["Chapter 2, Verse 1", "2:2", "2:3"],
            ["Third first", "Third second", "Third third"]
        ])
        assert ja.JaggedTextArray(threeby).subarray([1,1,1],[1,2,1]) == ja.JaggedTextArray([
            ["2:2", "2:3"],
            ["Third first", "Third second"]
        ])

        assert ja.JaggedTextArray(threeby).subarray([1, 1, 1], [1, 1, 2]) == ja.JaggedTextArray(
            ["2:2", "2:3"],
        )

    def test_set_element(self):
        j = ja.JaggedTextArray(twoby).set_element([1,1], "Foobar")
        assert j.get_element([1, 1]) == "Foobar"
        assert j.array() == [
                ["Line 1:1", "This is the first second", "First third"],
                ["Chapter 2, Verse 1", "Foobar", "2:3"],
                ["Third first", "Third second", "Third third"]
        ]
        j = ja.JaggedTextArray(twoby).set_element([1], ["Foobar", "Flan", "Bob"])
        assert j.get_element([1]) == ["Foobar", "Flan", "Bob"]
        assert j.array() == [
                ["Line 1:1", "This is the first second", "First third"],
                ["Foobar", "Flan", "Bob"],
                ["Third first", "Third second", "Third third"]
        ]
        j = ja.JaggedTextArray()
        assert j.set_element([2, 3], "Foo").array() == [
            [],
            [],
            [None, None, None, "Foo"]
        ]

    def test_mask(self):
        assert ja.JaggedTextArray(twoby).mask() == ja.JaggedIntArray(two_by_mask)
        assert ja.JaggedTextArray(
            [
                ["a",[],[],["",""],["b"]],
                ["a",[],["","a"],["",""],["b"]]
            ]
        ).mask() == ja.JaggedIntArray(
            [
                [1,[],[],[0,0],[1]],
                [1,[],[0,1],[0,0],[1]]
            ]
        )

        assert ja.JaggedTextArray(
            [
                ["a",[],[],["",""],["b"]],
                ["a",[],["","a"],["",""],["b"]]
            ]
        ).zero_mask() == ja.JaggedIntArray(
            [
                [0,[],[],[0,0],[0]],
                [0,[],[0,0],[0,0],[0]]
            ]
        )

        assert ja.JaggedTextArray(
            [
                ["a",[],[],["",""],["b"]],
                ["a",[],["","a"],["",""],["b"]]
            ]
        ).constant_mask(None) == ja.JaggedIntArray(
            [
                [None,[],[],[None,None],[None]],
                [None,[],[None,None],[None,None],[None]]
            ]
        )

    def test_is_full(self):
        assert ja.JaggedTextArray(twoby).is_full()
        assert ja.JaggedTextArray(threeby).is_full()
        assert not ja.JaggedTextArray([]).is_full()
        assert not ja.JaggedTextArray([[]]).is_full()
        assert not ja.JaggedTextArray([[""]]).is_full()
        assert not ja.JaggedTextArray([["a","b","c",""]]).is_full()
        assert not ja.JaggedTextArray([["a","b","c",[""]]]).is_full()

    def test_is_empty(self):
        assert not ja.JaggedTextArray(twoby).is_empty()
        assert not ja.JaggedTextArray(threeby).is_empty()
        assert ja.JaggedTextArray([]).is_empty()
        assert ja.JaggedTextArray([[]]).is_empty()
        assert ja.JaggedTextArray([[""]]).is_empty()
        assert not ja.JaggedTextArray([["a","b","c",""]]).is_empty()
        assert not ja.JaggedTextArray([["a","b","c",[""]]]).is_empty()

    def test_sections(self):
        assert ja.JaggedTextArray(twoby).sections() == [[0],[1],[2]]
        assert ja.JaggedTextArray(threeby).sections() == [[0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]

    def test_shape(self):
        assert ja.JaggedTextArray(twoby).shape() == [3,3,3]
        assert ja.JaggedTextArray(threeby).shape() == [[3, 3, 3],[3, 3, 3],[3, 3, 3]]
        assert ja.JaggedTextArray(["a","b","c"]).shape() == 3

    def test_trim_ending_whitespace(self):
        depth_two = [["a", "b"], ["a"], ["d"]]
        depth_two_with_space = [["a", "b", ""], ["a", ""], ["d"]]
        depth_three = [[["a"], []], [["a", "", "b"], ["", "a", "b", "c"]]]
        depth_three_with_space = [[["a", ""], [""]], [["a", "", "b"], ["", "a", "b", "c"]]]
        # do no harm
        assert ja.JaggedTextArray(depth_two).trim_ending_whitespace() == ja.JaggedTextArray(depth_two)
        assert ja.JaggedTextArray(depth_three).trim_ending_whitespace() == ja.JaggedTextArray(depth_three)

        # trim
        assert ja.JaggedTextArray(["a","b","c","",""]).trim_ending_whitespace() == ja.JaggedTextArray(["a","b","c"])
        assert ja.JaggedTextArray(["",None,"\t\n ","",""]).trim_ending_whitespace() == ja.JaggedTextArray([])
        assert ja.JaggedTextArray(["", ["a"]]).trim_ending_whitespace() == ja.JaggedTextArray(["", ["a"]])
        assert ja.JaggedTextArray([[""], "a"]).trim_ending_whitespace() == ja.JaggedTextArray([[], "a"])
        assert ja.JaggedTextArray(depth_two_with_space).trim_ending_whitespace() == ja.JaggedTextArray(depth_two)
        assert ja.JaggedTextArray(depth_three_with_space).trim_ending_whitespace() == ja.JaggedTextArray(depth_three)

    def test_overlap(self):
        a = ja.JaggedTextArray([["","b",""],["d","","f"],["","h",""]])
        b = ja.JaggedTextArray([["","","c"],["","e",""],["g","",""]])
        c = ja.JaggedTextArray([["","",""],["","q",""],["","",""]])
        assert not a.overlaps(b)
        assert not a.overlaps(c)
        assert b.overlaps(c)

    def test_resize(self):
        assert ja.JaggedTextArray(twoby).resize(1).resize(-1) == ja.JaggedTextArray(twoby)

    def test_resize_with_empty_string(self):
        a = ["Foo","Bar","","Quux"]
        assert ja.JaggedTextArray(a).resize(1).resize(-1) == ja.JaggedTextArray(a)

        # A bug had left [] alone during downsize.
        b = [["Foo"],["Bar"],[],["Quux"]]
        c = [["Foo"],["Bar"],[""],["Quux"]]

        jb = ja.JaggedTextArray(b).resize(-1)
        jc = ja.JaggedTextArray(c).resize(-1)
        assert jb == jc, "{} != {}".format(jb.array(), jc.array())

    def test_flatten_to_array(self):
        assert ja.JaggedTextArray(threeby).flatten_to_array() == [
            "Part 1 Line 1:1", "This is the first second", "First third",
            "Chapter 2, Verse 1", "2:2", "2:3",
            "Third first", "Third second", "Third third",
            "Part 2 Line 1:1", "This is the first second", "First third",
            "Chapter 2, Verse 1", "2:2", "2:3",
            "Third first", "Third second", "Third third",
            "Part 3 Line 1:1", "This is the first second", "First third",
            "Chapter 2, Verse 1", "2:2", "2:3",
            "Third first", "Third second", "Third third"
        ]

    def test_flatten_to_string(self):
        assert ja.JaggedTextArray("Test").flatten_to_string() == "Test"
        assert ja.JaggedTextArray(["Test", "More", "Test"]).flatten_to_string() == "Test More Test"

    def test_next_prev(self):
        sparse_ja = ja.JaggedTextArray([["","",""],["","foo","","bar",""],["","",""]])
        assert sparse_ja.next_index([0,0]) == [1, 1]
        assert sparse_ja.next_index([]) == [1, 1]
        assert sparse_ja.next_index() == [1, 1]

        assert sparse_ja.prev_index([]) == [1, 3]
        assert sparse_ja.prev_index() == [1, 3]


class Test_Depth_0(object):
    def test_depth_0(self):
        j = ja.JaggedTextArray("Fee Fi Fo Fum")
        assert j._store == "Fee Fi Fo Fum"
        assert j.is_full()
        assert not j.is_empty()
        assert j.verse_count() == 1
        assert j.mask() == ja.JaggedIntArray(1)
        assert j.flatten_to_array() == ["Fee Fi Fo Fum"]


class Test_Modify_by_Func():

    def test_modify_by_func(self):
        j = ja.JaggedTextArray(threeby_empty_section)
        self.modifier_input = []
        j.modify_by_function(self.modifier)
        assert self.modifier_input[0][1] == [0,0,0]
        assert self.modifier_input[-1][1] == [3,2,2]
        assert self.modifier_input[-1][0] == threeby_empty_section[3][2][2]

        self.modifier_input = []
        j.modify_by_function(self.modifier, start_sections=[1,2,3,4,5])
        assert self.modifier_input[0][1] == [1,2,3,4,5]
        assert self.modifier_input[-1][1] == [1,2,6,2,2]

    def modifier(self, s, sections):
        self.modifier_input += [(s, sections)]
        return s


```

### sefaria/datatype/__init__.py

```
__author__ = 'levisrael'

```

### sefaria/utils/user.py

```
"""
user.py - helper functions related to users

Uses MongoDB collections: apikeys, sheets, notes, profiles, notifications
"""
from django.contrib.auth.models import User
import structlog

import sefaria.model as model
from sefaria.system.database import db
from sefaria.helper.crm.crm_mediator import CrmMediator

logger = structlog.get_logger(__name__)


def delete_user_account(uid, confirm=True):
    """ Deletes the account of `uid` as well as all owned data
    Returns True if user is successfully deleted from Mongo & User DB
    """
    user = model.UserProfile(id=uid)
    if confirm:
        print("Are you sure you want to delete the account of '%s' (%s)?" % (user.full_name, user.email))
        if input("Type 'DELETE' to confirm: ") != "DELETE":
            print("Canceled.")
            return

    try:
        crm_mediator = CrmMediator()
        if not crm_mediator.mark_for_review_in_crm(profile=user):
            logger.error("Failed to mark user for review in CRM")
    except Exception as e:
        logger.error("Failed to mark user for review in CRM")

    # Delete user's reading history
    user.delete_user_history(exclude_saved=False, exclude_last_place=False)
    # Delete Sheets
    db.sheets.delete_many({"owner": uid})
    # Delete Notes
    db.notes.delete_many({"owner": uid})
    # Delete Notifcations
    db.notifications.delete_many({"uid": uid})
    # Delete Following Relationships
    db.following.delete_many({"follower": uid})
    db.following.delete_many({"followee": uid})
    # Delete Sheet Likes
    db.sheets.update_many({"likes": uid}, { "$pull": { "likes": uid } })
    # Delete Profile
    db.profiles.delete_one({"id": uid})
    # Delete User Object
    user = User.objects.get(id=uid)
    user.delete()
    
    # History is left for posterity, but will no longer be tied to a user profile

    print("User %d deleted." % uid)
    return True


def merge_user_accounts(from_uid, into_uid, fill_in_profile_data=True, override_profile_data=False):
    """ Moves all content of `from_uid` into `into_uid` then deletes `from_uid`"""
    from_user = model.UserProfile(id=from_uid)
    if not from_user._id:
        print("Source user %d does not have a profile." % from_uid)
        return

    into_user = model.UserProfile(id=into_uid)
    if not into_user._id:
        print("Destination user %d does not have a profile." % into_uid)
        return

    print("Are you sure you want to merge the account of '%s' (%s) into the account of of '%s' (%s)" % (from_user.full_name, from_user.email, into_user.full_name, into_user.email))
    if input("Type 'MERGE' to confirm: ") != "MERGE":
        print("Canceled.")
        return

    # Move user reading history
    db.user_history.update_many({"uid": from_uid}, {"$set": {"uid": into_uid}})
    # Move group admins
    db.groups.update({"admins": from_uid}, {"$set": {"admins.$": into_uid}})
    # Move group members
    db.groups.update({"members": from_uid}, {"$set": {"members.$": into_uid}})
    # Move Sheets
    db.sheets.update_many({"owner": from_uid}, {"$set": {"owner": into_uid}})
    # Move Notes
    db.notes.update_many({"owner": from_uid}, {"$set": {"owner": into_uid}})
    # Move Notifcations
    db.notifications.update_many({"uid": from_uid}, {"$set": {"uid": into_uid}})
    # Move Following Relationships
    db.following.update_many({"follower": from_uid}, {"$set": {"follower": into_uid}})
    db.following.update_many({"followee": from_uid}, {"$set": {"followee": into_uid}})
    # Delete Sheet Likes
    db.sheets.update_many({"likes": from_uid}, { "$addToSet": { "likes": into_uid } })
    db.sheets.update_many({"likes": from_uid}, { "$pull": { "likes": from_uid } })
    # Move History
    db.history.update_many({"user": from_uid}, {"$set": {"user": into_uid}})

    print("Content from %s moved into %s's account." % (from_user.email, into_user.email))
    if override_profile_data:
        into_user.update(from_user.to_mongo_dict())
        into_user.save()
    elif fill_in_profile_data:
        into_user.update_empty(from_user.to_mongo_dict())
        into_user.save()

    delete_user_account(from_uid, confirm=False)


def generate_api_key(uid):
    """ Save a new random API key for `uid` """
    user = model.UserProfile(id=uid)
    if not user._id:
        print("User %d does not exist." % uid)
        return

    import base64, hashlib, random
    key = base64.b64encode(hashlib.sha256(bytes(str(random.getrandbits(256)), encoding='utf-8')).digest(),
                           random.choice([b'rA', b'aZ', b'gQ', b'hH', b'hG', b'aR', b'DD'])).rstrip(b'==').decode('utf-8')
    db.apikeys.delete_many({"uid": uid})
    db.apikeys.insert_one({"uid": uid, "key": key})

    print("API Key for %s (uid: %d, email: %s): %s" % (user.full_name, uid, user.email, key))


def reset_all_api_keys():
    """ Updates all existing API keys with new random values """
    keys = db.apikeys.find()
    for key in keys:
        generate_api_key(key["uid"])



```

### sefaria/utils/talmud.py

```
from sefaria.utils.hebrew import encode_hebrew_numeral, encode_small_hebrew_numeral


#Overlapping with AddressTalmud.toString()
def section_to_daf(section, lang="en"):
    """
    Transforms a section number to its corresponding daf string,
    in English or in Hebrew.
    """
    section += 1
    daf = section // 2

    if lang == "en":
        if section > daf * 2:
            daf = "{}b".format(daf)
        else:
            daf = "{}a".format(daf)

    elif lang == "he":
        if section > daf * 2:
            daf = "{}{}".format(sanitize(encode_small_hebrew_numeral(daf), False) if daf < 1200 else encode_hebrew_numeral(daf, punctuation=False), ':')
        else:
            daf = "{}{}".format(sanitize(encode_small_hebrew_numeral(daf), False) if daf < 1200 else encode_hebrew_numeral(daf, punctuation=False), '.')

    return daf


def daf_to_section(daf):
    """
    Transforms a daf string (e.g., '4b') to its corresponding stored section number.
    """
    amud = daf[-1]
    daf = int(daf[:-1])
    section = daf * 2
    if amud == "a": section -= 1
    return section

```

### sefaria/utils/log.py

```
# -*- coding: utf-8 -*-

"""
Deprecated as we move to structured logging

import logging
from django.conf import settings
import requests, json, traceback
from requests.exceptions import ConnectionError



class CategoryFilter(logging.Filter):
    def __init__(self, categories=None):
        self.categories = categories if isinstance(categories, list) else [categories]

    def filter(self, record):
        if self.categories is None:
            return True
        if record:
            pass


class ErrorTypeFilter(logging.Filter):
    def __init__(self, error_types, exclude= True):
        self.error_types = error_types
        self.exclude = exclude

    def filter(self, record):
        #print record.exc_info[0].__name__
        if 'Favicon.ico' in record.msg: #ignore and filter out super annoying error about favicon.
            return False
        if not record.exc_info:
            retval = True if self.exclude else False
        else:
            if self.exclude:
                retval = all(record.exc_info[0].__name__ != err_type for err_type in self.error_types)
            else:
                retval = any(record.exc_info[0].__name__ == err_type for err_type in self.error_types)
                #get rid of the stack trace?
                record.exc_info = None
        return retval


class SlackLogHandler(logging.Handler):
    def __init__(self, logging_url="", channel="@slackbot", stack_trace=False):
        logging.Handler.__init__(self)
        self.logging_url = logging_url
        self.channel = channel
        self.stack_trace = stack_trace

    def emit(self, record):
        message = '%s' % (self.formatter.format(record))
        if self.stack_trace:
            if record.exc_info:
                message += '\n'.join(traceback.format_exception(*record.exc_info))
        slack_payload = {
            "text": message,
            "username": "errorbot",
            "icon_emoji": ":scream:",
            "channel": self.channel
        }
        try:
            requests.post(self.logging_url, data=json.dumps(slack_payload))
        except ConnectionError:
            pass  # basa. but slack posting failures should not crash a script

"""

```

### sefaria/utils/testing_utils.py

```
import sefaria.model as model


""" SOME UTILS """

def get_all_toc_locations(title, toc):
    """
    Finds ALL occurrences of a text title in the toc. Recursively looks through the ToC to find the category paths
    of the given title
    :param title: the title to look for.
    :return: a list of lists of categories leading to the title or an empty array if not found.
    """
    results = []
    for toc_elem in toc:
        #base element, a text- check if title matches.
        if 'title' in toc_elem:
            if toc_elem['title'] == title:
                #if there is a match, append to this recursion's list of results.
                results.append(True)
        #category
        elif 'category' in toc_elem and 'contents' in toc_elem:
            #first go down the tree
            sub_result = get_all_toc_locations(title, toc_elem['contents'])
            #add the current category name to any already-found results (since at this point we are on our way up from the recursion.
            if sub_result:
                for path in sub_result:
                    new_path = [toc_elem['category']] + path if isinstance(path, list) else [toc_elem['category']]
                    results.append(new_path)
    return results


def get_lang_keys():
    return {'he', 'en'}


def toc_path_to_string(toc_path):
    return ",".join(toc_path)


def verify_title_existence_in_toc(title, expected_toc_location=None, toc=None):
    if toc is None:
        toc = model.library.get_toc()
    locations_in_toc = get_all_toc_locations(title, toc)
    #a title should always be in the toc no more than once. 0 if we do not expect to find it there.
    num_appearances_in_toc = 1 if expected_toc_location is not None else 0
    assert len(locations_in_toc) == num_appearances_in_toc, "title appears %d times" % len(locations_in_toc)
    if expected_toc_location:
        assert toc_path_to_string(expected_toc_location) == toc_path_to_string(locations_in_toc[0]), locations_in_toc


def verify_existence_across_tocs(title, expected_toc_location=None):
    verify_title_existence_in_toc(title, expected_toc_location, toc=model.library.get_toc())

```

### sefaria/utils/util.py

```
# -*- coding: utf-8 -*-
"""
Miscellaneous functions for Sefaria.
"""
from datetime import datetime
from html.parser import HTMLParser
import re
from functools import wraps
from itertools import zip_longest
from sefaria.constants.model import ALLOWED_TAGS_IN_ABSTRACT_TEXT_RECORD

"""
Time utils
"""

epoch = datetime.utcfromtimestamp(0)

def epoch_time(since=None):
    if since is None:
        since = datetime.utcnow()
    # define total_seconds which exists in Python3
    total_seconds = lambda delta: int(delta.days * 86400 + delta.seconds + delta.microseconds / 1e6)
    return total_seconds(since - epoch)

def td_format(td_object):
    """
    Turn a timedelta object into a nicely formatted string.
    """
    seconds = int(td_object.total_seconds())
    periods = [
            ('year',        60*60*24*365),
            ('month',       60*60*24*30),
            ('day',         60*60*24),
            ('hour',        60*60),
            ('minute',      60),
            ('second',      1)
            ]

    strings=[]
    for period_name,period_seconds in periods:
            if seconds > period_seconds:
                    period_value , seconds = divmod(seconds,period_seconds)
                    if period_value == 1:
                            strings.append("%s %s" % (period_value, period_name))
                    else:
                            strings.append("%s %ss" % (period_value, period_name))

    return ", ".join(strings)

def get_hebrew_date(dt_obj:datetime) -> tuple:
    """

    :param dt_obj : datetime object
    :return: en date and he date for Hebrew date
    """
    from convertdate import hebrew
    months = [
        ("Nisan", ""),
        ("Iyar", ""),
        ("Sivan", ""),
        ("Tammuz", ""),
        ("Av", ""),
        ("Elul", ""),
        ("Tishrei", ""),
        ("Cheshvan", ""),
        ("Kislev", ""),
        ("Tevet", ""),
        ("Shevat", ""),
        ("Adar", ""),
        ("Adar II", " "),
    ]
    y, m, d = hebrew.from_gregorian(dt_obj.year, dt_obj.month, dt_obj.day)
    en = "{} {}, {}".format(months[m-1][0], d, y)
    he = "{} {}, {}".format(months[m-1][1], d, y)
    return en, he


'''
Data structure utils - lists
'''

# also at JaggedArray.depth().  Still needed?
def list_depth(x, deep=False):
    """
    returns 1 for [], 2 for [[]], etc.
    :parm x - a list
    :param deep - whether or not to count a level when not all elements in
    that level are lists.
    e.g. [[], ""] has a list depth of 1 with depth=False, 2 with depth=True
    """
    if isinstance(x, int):
        return 0
    elif len(x) > 0 and (deep or all([isinstance(y, list) for y in x])):
        return 1 + max([list_depth(y, deep=deep) for y in x])
    else:
        return 1

"""
# Create a list that from the results of the function chunks:
    names = ['Genesis', 'Exodus', 'Leviticus', 'Numbers','Deuteronomy','Joshua', 'Judges', 'Samuel', 'Kings','Isaiah', 'Jeremiah', 'Ezekiel','Hosea']
    list(list_chunks(names, 5))
    >>>[['Genesis', 'Exodus', 'Leviticus', 'Numbers','Deuteronomy'],
       ['Joshua', 'Judges', 'Samuel', 'Kings','Isaiah'],
       ['Jeremiah', 'Ezekiel','Hosea']]
       credit: https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
"""

def list_chunks(l, n):
    # For item i in a range that is a length of l,
    for i in range(0, len(l), n):
        # Create an index range for l of n items:
        yield l[i:i+n]

def union(a, b):
    """ return the union of two lists """
    return list(set(a) | set(b))

'''
Data structure utils - dicts
'''

def traverse_dict_tree(dict_tree: dict, key_list: list):
    """
    For a list [a, b, c] return dict_tree[a][b][c]
    :param dict_tree: a list of nested dict objects
    :param key_list: list of keys
    :return:
    """
    current_node = dict_tree
    for key in key_list:
        current_node = current_node[key]
    return current_node

def deep_update(dict1, dict2):
    """
    Merges dict2 into dict1. Will recursively merge as deep as necessary. returns merged dict
    @param dict1:
    @param dict2:
    @return: merged dict
    """
    from collections.abc import Mapping

    for k, v in dict2.items():
        if isinstance(v, Mapping):
            dict1[k] = deep_update(dict1.get(k, {}), v)
        else:
            dict1[k] = v
    return dict1

'''
Data structure utils - jagged arrays
'''

# Moving to JaggedArray.flattenToArray()
def flatten_jagged_array(jagged):
    """
    Returns a 1D list of each terminal element in a jagged array.
    """
    flat = []
    for el in jagged:
        if isinstance(el, list):
            flat = flat + flatten_jagged_array(el)
        else:
            flat.append(el)

    return flat

def is_text_empty(text):
    """
    Returns true if a jagged array 'test' is emtpy or contains
    only "" or 0.
    """
    text = flatten_jagged_array(text)

    text = [t if t != 0 else "" for t in text]
    return not len("".join(text))

def rtrim_jagged_string_array(ja):
    """
    Returns a jagged string array corresponding to ja with any
    trailing Falsey values in any subsection removed.
    """
    if not isinstance(ja, list):
        return ja
    while len(ja) and not ja[-1]:
        ja.pop() # Remove any trailing Falsey values ("", 0, False)
    return [rtrim_jagged_string_array(j) for j in ja]

def text_preview(en, he):
    """
    Returns a jagged array terminating in dicts like {'he': '', 'en': ''} which offers preview
    text merging what's available in jagged string arrays 'en' and 'he'.
    """
    n_chars = 80
    en = [en] if isinstance(en, str) else [""] if en == [] or not isinstance(en, list) else en
    he = [he] if isinstance(he, str) else [""] if he == [] or not isinstance(he, list) else he

    def preview(section):
        """Returns a preview string for list section"""
        section =[s for s in section if isinstance(s, str)]
        section = " ".join(map(str, section))
        return strip_tags(section[:n_chars]).strip()

    if not any(isinstance(x, list) for x in en + he):
        return {'en': preview(en), 'he': preview(he)}
    else:
        zipped = zip_longest(en, he)
        return [text_preview(x[0], x[1]) for x in zipped]


'''
file utils
'''

#checks if a file is in directory
def in_directory(file, directory):
    import os.path
    # make both absolute
    directory = os.path.join(os.path.realpath(directory), '')
    file = os.path.realpath(file)
    if not os.path.exists(directory) or not os.path.isdir(directory):
        return False
    if not os.path.exists(file):
        return False
    # return true, if the common prefix of both is equal to directory
    # e.g. /a/b/c/d.rst and directory is /a/b, the common prefix is /a/b
    return os.path.commonprefix([file, directory]) == directory


def get_directory_content(dirname, modified_after=False):
    import os
    import os.path
    filenames = []
    for path, subdirs, files in os.walk(dirname):
        for name in files:
            filepath = os.path.join(path, name)
            if modified_after is False or os.path.getmtime(filepath) > modified_after:
                filenames.append(filepath)
    return filenames



'''
text utils
'''

def string_overlap(text1, text2):
    """
    Returns the number of characters that the end of text1 overlaps the beginning of text2.
    https://neil.fraser.name/news/2010/11/04/
    """
    # Cache the text lengths to prevent multiple calls.
    text1_length = len(text1)
    text2_length = len(text2)
    # Eliminate the null case.
    if text1_length == 0 or text2_length == 0:
        return 0
    # Truncate the longer string.
    if text1_length > text2_length:
        text1 = text1[-text2_length:]
    elif text1_length < text2_length:
        text2 = text2[:text1_length]
    # Quick check for the worst case.
    if text1 == text2:
        return min(text1_length, text2_length)

    # Start by looking for a single character match
    # and increase length until no match is found.
    best = 0
    length = 1
    while True:
        pattern = text1[-length:]
        found = text2.find(pattern)
        if found == -1:
            return best
        length += found
        if text1[-length:] == text2[:length]:
            best = length
            length += 1


def replace_using_regex(regex, query, old, new, endline=None):
    """
    This is an enhancement of str.replace(). It will only call str.replace if the regex has
    been found, thus allowing replacement of tags that may serve multiple or ambiguous functions.
    Should there be a need, an endline parameter can be added which will be appended to the end of
    the string
    :param regex: A regular expression. Will be compiled locally.
    :param query: The input string to be examined.
    :param old: The text to be replaced.
    :param new: The text that will be inserted instead of 'old'.
    :param endline: An optional argument that can be appended to the end of the string.
    :return: A new string with 'old' replaced by 'new'.
    """

    # compile regex and search
    reg = re.compile(regex)
    result = re.search(reg, query)
    if result:

        # get all instances of match
        matches = re.findall(reg, query)
        for match in matches:
            temp = match.replace(old, new)
            query = query.replace(match, temp)
        if endline is not None:
            query.replace('\n', endline + '\n')
    return query


def count_by_regex(some_file, regex):
    """
    After OCR, text files are returned with many tags, the meaning of which may not be clear or ambiguous.
    Even if the meaning of each tag is known it can be useful to know how many times each tag appears, as
    errors may have arisen during the scanning and OCR. By using a regular expression to search, entire
    documents can be scanned quickly and efficiently.

    :param some_file: A file to be scanned.
    :param regex: The regex to be used
    :return: A dictionary where the keys are all the strings that match the regex and the values are the
    number of times each one appears.
    """

    # instantiate a dictionary to hold results
    result = {}

    # compile regex
    reg = re.compile(regex)

    # loop through file
    for line in some_file:

        # search for regex
        found = re.findall(reg, line)

        # count instances found
        for item in found:
            if item not in result:
                result[item] = 1
            else:
                result[item] += 1
    return result


def titlecase(text):
    """
    This function is based on some Perl code by: John Gruber http://daringfireball.net/ 10 May 2008
    & a Python version by Stuart Colville http://muffinresearch.co.uk
    under the terms of the MIT license: http://www.opensource.org/licenses/mit-license.php

    changes all words to Title Caps, and attempts to be clever about not capitalizing SMALL words
    like a/an/the in the input. The list of "SMALL words" comes from the NYTimes Manual of Style,
    plus 'vs' and 'v'.

    Sentences that are all caps are left alone.

    Words with capitalized letters in the middle (e.g. Tu B'Shvat, iTunes, etc) are left alone as well.
    """

    SMALL = r'a|an|and|as|at|but|by|en|for|if|in|of|on|or|the|to|v\.?|via|vs\.?'
    PUNCT = r"""!"#$%&'()*+,\-./:;?@[\\\]_`{|}~"""
    SMALL_WORDS = re.compile(r'^(%s)$' % SMALL, re.I)
    INLINE_PERIOD = re.compile(r'[a-z][.][a-z]', re.I)
    UC_ELSEWHERE = re.compile(r'[%s]*?[a-zA-Z]+[A-Z]+?' % PUNCT)
    CAPFIRST = re.compile(r"^[%s]*?([A-Za-z])" % PUNCT)
    SMALL_FIRST = re.compile(r'^([%s]*)(%s)\b' % (PUNCT, SMALL), re.I)
    SMALL_LAST = re.compile(r'\b(%s)[%s]?$' % (SMALL, PUNCT), re.I)
    SUBPHRASE = re.compile(r'([:.;?!\-\][ ])(%s)' % SMALL)
    APOS_SECOND = re.compile(r"^[dol]{1}[']{1}[a-z]+(?:['s]{2})?$", re.I)
    ALL_CAPS = re.compile(r'^[A-Z\s\d%s]+$' % PUNCT)
    UC_INITIALS = re.compile(r"^(?:[A-Z]{1}\.{1}|[A-Z]{1}\.{1}[A-Z]{1})+$")
    MAC_MC = re.compile(r"^([Mm]c|MC)(\w.+)")

    lines = re.split('[\r\n]+', text)
    processed = []
    for line in lines:
        all_caps = ALL_CAPS.match(line)
        words = re.split('[\t ]', line)
        tc_line = []
        for word in words:

            if all_caps:
                if UC_INITIALS.match(word):
                    tc_line.append(word)
                    continue

            if APOS_SECOND.match(word):
                if len(word[0]) == 1 and word[0] not in 'aeiouAEIOU':
                    word = word[0].lower() + word[1] + word[2].upper() + word[3:]
                else:
                    word = word[0].upper() + word[1] + word[2].upper() + word[3:]
                tc_line.append(word)
                continue

            match = MAC_MC.match(word)
            if match:
                tc_line.append("%s%s" % (match.group(1).capitalize(),
                                         titlecase(match.group(2))))
                continue

            if INLINE_PERIOD.search(word) or (not all_caps and UC_ELSEWHERE.match(word)):
                tc_line.append(word)
                continue
            if SMALL_WORDS.match(word):
                tc_line.append(word.lower())
                continue

            if "/" in word and "//" not in word:
                slashed = [titlecase(t) for t in word.split('/')]
                tc_line.append("/".join(slashed))
                continue

            if '-' in word:
                hyphenated = [titlecase(t) for t in word.split('-')]
                tc_line.append("-".join(hyphenated))
                continue

            # Just a normal word that needs to be capitalized
            tc_line.append(CAPFIRST.sub(lambda m: m.group(0).upper(), word))

        result = " ".join(tc_line)

        result = SMALL_FIRST.sub(lambda m: '%s%s' % (
            m.group(1),
            m.group(2).capitalize()
        ), result)

        result = SMALL_LAST.sub(lambda m: m.group(0).capitalize(), result)

        result = SUBPHRASE.sub(lambda m: '%s%s' % (
            m.group(1),
            m.group(2).capitalize()
        ), result)

        processed.append(result)

    return "\n".join(processed)

def wrap_chars_with_overlaps(s, chars_to_wrap, get_wrapped_text, return_chars_to_wrap=False):
    chars_to_wrap.sort(key=lambda x: (x[0],x[0]-x[1]))
    for i, (start, end, metadata) in enumerate(chars_to_wrap):
        wrapped_text, start_added, end_added = get_wrapped_text(s[start:end], metadata)
        s = s[:start] + wrapped_text + s[end:]
        chars_to_wrap[i] = (start, end + start_added + end_added, metadata)
        for j, (start2, end2, metadata2) in enumerate(chars_to_wrap[i+1:]):
            if start2 >= end:
                start2 += end_added
            start2 += start_added
            if end2 > end:
                end2 += end_added
            end2 += start_added
            chars_to_wrap[i+j+1] = (start2, end2, metadata2)
    if return_chars_to_wrap:
        return s, chars_to_wrap
    return s

def find_all_html_elements_indices(input_string: str) -> dict:
    tags = ALLOWED_TAGS_IN_ABSTRACT_TEXT_RECORD
    tags_regex = f'(?:{"|".join(tags)})'
    html_element_indices = {}
    for m in re.finditer(f'</?{tags_regex}(?: [^>]*)?>', input_string):
        html_element_indices[m.end()-1] = m.start()
    return html_element_indices

def truncate_string(string, min_length, max_length):
    if len(string) <= max_length:
        return string
    html_element_indices = find_all_html_elements_indices(string)
    next_html_closing_tag_index = string.find('>', max_length)
    for break_char in ".;, ":
        pos = next_html_closing_tag_index if next_html_closing_tag_index != -1 else max_length
        while min_length <= pos:
            while pos in html_element_indices:
                pos = html_element_indices[pos] - 1
            if string[pos] == break_char and pos <= max_length:
                return string[:pos] + ""
            pos -= 1
    return string

'''
strip utils
'''

class MLStripper(HTMLParser):
    def __init__(self):
        super().__init__()

        self.reset()
        self.strict = False
        self.convert_charrefs = True
        self.fed = []

    def handle_data(self, d):
        self.fed.append(d)

    def get_data(self):
        return ' '.join(self.fed)


def strip_tags(html, remove_new_lines=False):
    """
    Returns the text of html with tags stripped.
    Customized to insert a space between adjacent tags after stripping.
    """
    html = html or ""
    s = MLStripper()
    s.feed(html)
    stripped = s.get_data().strip()
    if remove_new_lines:
        stripped = re.sub(r"\n+", " ", stripped)
    return stripped

'''
language code utils
'''

def short_to_long_lang_code(code):
    if code in ("bi", "he-en", "en-he"):
        code = "bilingual"
    elif code in ('he', 'he-il'):
        code = 'hebrew'
    elif code in ("en"):
        code = "english"
    return code


def get_lang_codes_for_territory(territory_code, min_pop_perc=0.2, official_status=False):
    """
    Wrapper for babel.languages.get_territory_language_info
    Documentation here: https://github.com/python-babel/babel/blob/master/babel/languages.py#L45 (strange that this function isn't documented on their official site)

    :param territory_code: two letter territory ISO code. If doesn't match anything babel recognizes, returns empty array
    :param min_pop_perc: min population percentage of language usage in territory. stats are likely only mildly accurate but good enough
    :param official_status: the status of the language in the territory. I think this can be 'official', 'de_facto_official', None, 'official_regional'. False means return all.

    returns array of ISO lang codes
    """
    from babel import languages
    lang_dict = languages.get_territory_language_info(territory_code)
    langs = [lang_code for lang_code, _ in filter(lambda x: x[1]['population_percent'] >= (min_pop_perc * 100) and (
                (official_status == False) or x[1]['official_status'] == official_status), lang_dict.items())]
    return langs


'''
subclass utils
'''

def get_all_subclasses(cls):
    subclasses = set()
    work = [cls]
    while work:
        parent = work.pop()
        for child in parent.__subclasses__():
            if child not in subclasses:
                subclasses.add(child)
                work.append(child)
    return subclasses


def get_all_subclass_attribute(cls, attr):
    subclasses = get_all_subclasses(cls)
    attr_vals = []
    for s in subclasses:
        attr_val = getattr(s, attr, None)
        if attr_val:
            attr_vals.append(attr_val)
    return attr_vals


'''
other utils
'''

def get_size(obj, seen=None):
    """Recursively finds size of objects in bytes"""
    import sys
    import inspect
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    # Important mark as seen *before* entering recursion to gracefully handle
    # self-referential objects
    seen.add(obj_id)
    if hasattr(obj, '__dict__'):
        for cls in obj.__class__.__mro__:
            if '__dict__' in cls.__dict__:
                d = cls.__dict__['__dict__']
                if inspect.isgetsetdescriptor(d) or inspect.ismemberdescriptor(d):
                    size += get_size(obj.__dict__, seen)
                break
    if isinstance(obj, dict):
        size += sum((get_size(v, seen) for v in list(obj.values())))
        size += sum((get_size(k, seen) for k in list(obj.keys())))
    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):
        size += sum((get_size(i, seen) for i in obj))
    return size

def graceful_exception(logger=None, logLevel="exception", return_value=[], exception_type=Exception):
    def argumented_decorator(func):
        @wraps(func)
        def decorated_function(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except exception_type as e:
                if logger:
                    logger.exception(str(e)) if logLevel == "exception" else logger.warning(str(e))
            return return_value
        return decorated_function
    return argumented_decorator

```

### sefaria/utils/tests/calendars_test.py

```
# -*- coding: utf-8 -*-

import sefaria.utils.calendars as c
from datetime import date
from sefaria.system.database import db


def setup_module(module): 
	db.dafyomi.insert_many([
		{
			'date': '1/1/1100',
			'daf': 'Berakhot 2'
		},
		{
			'date': '1/2/1100',
			'daf': 'Berakhot 3',
			'displayValue': {
				'en': 'Funny Name!',
				'he': ' '
			}
		}
	])


def teardown_module():
	db.dafyomi.delete_one({'date': '1/1/1100'})
	db.dafyomi.delete_one({'date': '1/2/1100'})


class Test_daf_yomi():

	def test_simple(self):
		df = c.daf_yomi(date(1100, 1, 1))[0]
		keys = ['title', 'displayValue', 'url', 'ref', 'order', 'category']
		assert all(k in df for k in keys)
		assert df['title']['en'] == 'Daf Yomi'
		assert df['title']['he'] == ' '
		assert df['displayValue']['en'] == 'Berakhot 2'
		assert df['displayValue']['he'] == ' '
		assert df['ref'] == 'Berakhot 2'
		assert df['url'] == 'Berakhot.2'
		assert df['order'] == 3
		assert df['category'] == 'Talmud'

	def test_special_display(self):
		df = c.daf_yomi(date(1100, 1, 2))[0]
		assert df['ref'] == 'Berakhot 3'
		assert df['displayValue']['en'] == 'Funny Name!'
		assert df['displayValue']['he'] == ' '
		assert df['url'] == 'Berakhot.3'

class Test_this_weeks_parasha():
	pass
	#c.this_weeks_parasha()
```

### sefaria/utils/tests/util_test.py

```
from sefaria.utils.util import find_all_html_elements_indices, truncate_string

class TestFindAllHtmlElementsIndices:

    def test_empty_input(self):
        input_string = ""
        expected_output = {}
        assert find_all_html_elements_indices(input_string) == expected_output

    def test_no_html_elements(self):
        input_string = "This is a test string without any HTML elements."
        expected_output = {}
        assert find_all_html_elements_indices(input_string) == expected_output

    def test_single_html_element(self):
        input_string = "<b>This is a paragraph.</b>"
        expected_output = {2: 0, 26: 23}
        assert find_all_html_elements_indices(input_string) == expected_output

    def test_multiple_html_elements(self):
        input_string = '<a href="sefaria data-ref="sefaria">This is a <b>test</b> string with <i>HTML</i> elements.</a>'
        expected_output = {35: 0, 48: 46, 56: 53, 72: 70, 80: 77, 94: 91}
        assert find_all_html_elements_indices(input_string) == expected_output


class TestTruncateString:

    def test_short_string(self):
        string = "This is a short string."
        min_length = 10
        max_length = 25
        expected_output = "This is a short string."
        assert truncate_string(string, min_length, max_length) == expected_output

    def test_long_string_without_break_chars(self):
        string = "This is a long string without any break characters."
        min_length = 10
        max_length = 20
        expected_output = "This is a long"
        assert truncate_string(string, min_length, max_length) == expected_output

    def test_long_string_with_break_chars(self):
        string = "This is a long string, which has multiple break characters, like .,;."
        min_length = 10
        max_length = 25
        expected_output = "This is a long string"
        assert truncate_string(string, min_length, max_length) == expected_output

    def test_long_string_with_html_elements(self):
        string = '<b>This is a long string with <sub class="footnote">HTML</sup> attributes.</b>'
        min_length = 10
        max_length = 35
        expected_output = "<b>This is a long string with"
        assert truncate_string(string, min_length, max_length) == expected_output

    def test_string_length_equals_max(self):
        string = 'string with length of 24'
        min_length = 10
        max_length = 24
        expected_output = "string with length of 24"
        assert truncate_string(string, min_length, max_length) == expected_output

    def test_long_string_with_html_closing_tag_after_max_length(self):
        string = 'This is a long string aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa <i>a</i>'
        min_length = 10
        max_length = 22
        expected_output = "This is a long string"
        assert truncate_string(string, min_length, max_length) == expected_output

```

### sefaria/utils/tests/hebrew_test.py

```
# -*- coding: utf-8 -*-

from sefaria.utils import hebrew as h
import pytest

def setup_module(module):
    global e
    global d
    e = h.encode_hebrew_numeral
    d = h.decode_hebrew_numeral


class TestSanityCheck():

    def test_can_encode_without_errors(self):
        for x in range(1, 5000):
            _ = h.encode_hebrew_numeral(x)

    def test_encodes_and_decodes_correctly(self):
        for x in range(1, 5000):
            if x in (2000, 3000, 4000, 5000):
                # known ambiguity with single thousands above 1000
                pass
            else:
                assert x == h.decode_hebrew_numeral(h.encode_hebrew_numeral(x))


class TestSpecificInOutTests():

    def test_some_basic_encoding_tests(self):
        assert '' == e(300)
        assert '' == e(33, True)
        assert '' == e(5764)
        assert '' == e(1000005)

    def test_special_cases_tests(self):
        assert '' == e(275)
        assert '' == e(270)
        assert '' == e(272)

        assert '' == e(15)
        assert '' == e(16)

    def test_encoding_without_punctuation(self):
        assert '' == e(35, False)
        assert '' == e(42, False)
        assert '' == e(129, False)


    def test_some_basic_decoding_tests(self):
        assert d('') == 1
        assert d('') == 1100
        assert d('') == 515
        assert d('') == 5764
        assert d("'") == 4

    def test_undefined_conventions(self):
        assert '' == e(15000)
        assert '' == e(16000)


class TestFunctionTests(object):

    def test_break_int_magnitudes(self):
        assert h.break_int_magnitudes(15000) == [10000, 5000, 0, 0, 0]


class TestNikkudUtils():

    def test_strip_nikkud(self):
        assert h.strip_nikkud(' ') == ' '
        assert h.strip_nikkud("     ") == "     "


class TestIsHebrewFuncs:
    """
    Tests the various functions that check for the amount of Hebrew in a string
    """
    def test_has_hebrew(self):
        assert h.has_hebrew("")
        assert h.has_hebrew(" world")
        assert not h.has_hebrew("hello world")

    def test_is_all_hebrew(self):
        assert h.is_all_hebrew("")
        assert not h.is_all_hebrew(" world")

    def test_is_mostly_hebrew(self):
        assert h.is_mostly_hebrew(" ")
        assert not h.is_mostly_hebrew(" world")  # exactly one less than half
        assert not h.is_mostly_hebrew(" world")  # exactly half
        assert h.is_mostly_hebrew(" word")  # exactly one more than half
        assert not h.is_mostly_hebrew("word ", len_to_check=4)
        assert h.is_mostly_hebrew("", len_to_check=4)


class TestGematria():
    def test_simple_gematria(self):
        assert h.gematria("") == 204
        assert h.gematria(" ") == 204 + 249
        assert h.gematria("") == 1000 + 450 + 45

    def test_final_letters(self):
        # Assumption is that final letters are counted as simple
        assert h.gematria("") == 280

    def test_with_nikkud(self):
        assert h.gematria(' ') == 501 + 261

    def test_punctuation(self):
        assert h.gematria("") == h.gematria("[]{}()?!..,,    - -")


@pytest.mark.parametrize(('hebrew', 'other_hebrew', 'should_match'), [
    ("  ", "  ", True),    # same
    ("  ", " ", True),         # same other shorter
    (" ", "  ", False),        # same other longer
    ("  ", " ", True),          # same other abbrev
    (" ", "  ", True),          # same orig abbrev
    ("  ", "  ", False),  # same other longer abbrev
    ("        ", "     ", True),    # multiple abbrevs
    ("        ", "     ", False),    # diff multiple abbrevs
])
def test_hebrew_starts_with(hebrew, other_hebrew, should_match):
    assert h.hebrew_starts_with(hebrew, other_hebrew) == should_match


@pytest.mark.parametrize(('abbr', 'unabbr', 'match'), [
    ("", "  ", "  "),
    pytest.param("", "  ", "  ", marks=pytest.mark.xfail(reason="cannot handle non-sofit letters in abbrev matching sofit in unabbr")),
    ("", "   ", "   "),
    ("", "   ", "   "),
    ("", "   ", None),
    ("", "   ", " "),
])
def test_get_abbr(abbr, unabbr, match):
    unabbr_words = unabbr.split()
    test_match = h.get_abbr(abbr, unabbr_words)
    if match is not None:
        match = match.split()
    assert test_match == match

```

### sefaria/utils/tests/time_test.py

```
pass


```

### sefaria/utils/__init__.py

```
__author__ = 'levisrael'

```

### sefaria/utils/hebrew.py

```
# -*- coding: utf-8 -*-
"""
hebrew.py - functions relating to reading and generating Hebrew numerals.

Issues:
   Numbers like 1 million are ambiguous
   Number like 2000 is ambiguous
   Okay to construct 15/16 and then make tet-vav/etc?
"""
import dataclasses
import re
import regex
import math
from typing import List
import itertools
from sefaria.system.decorators import memoized
import structlog
logger = structlog.get_logger(__name__)


### Change to all caps for constants
GERESH = "\u05F3"
GERSHAYIM = "\u05F4"
ALPHABET_22 = ""
FINAL_LETTERS = ""
ALPHABET_27 = ALPHABET_22 + FINAL_LETTERS

H2E_KEYBOARD_MAP = {"/": "q", "": "w", "": "e", "": "r", "": "t", "": "y", "": "u", "": "i", "": "o", "": "p", "": "a", "": "s", "": "d", "": "f", "": "g", "": "h", "": "j", "": "k", "": "l", "": ";", ",": "'", "": "z", "": "x", "": "c", "": "v", "": "b", "": "n", "": "m", "": ",", "": ".", ".": "/"}
E2H_KEYBOARD_MAP = {"'": ',', ',': '\u05ea', '.': '\u05e5', '/': '.', ';': '\u05e3', 'A': '\u05e9', 'B': '\u05e0', 'C': '\u05d1', 'D': '\u05d2', 'E': '\u05e7', 'F': '\u05db', 'G': '\u05e2', 'H': '\u05d9', 'I': '\u05df', 'J': '\u05d7', 'K': '\u05dc', 'L': '\u05da', 'M': '\u05e6', 'N': '\u05de', 'O': '\u05dd', 'P': '\u05e4', 'Q': '/', 'R': '\u05e8', 'S': '\u05d3', 'T': '\u05d0', 'U': '\u05d5', 'V': '\u05d4', 'W': '\u05f3', 'X': '\u05e1', 'Y': '\u05d8', 'Z': '\u05d6', 'a': '\u05e9', 'b': '\u05e0', 'c': '\u05d1', 'd': '\u05d2', 'e': '\u05e7', 'f': '\u05db', 'g': '\u05e2', 'h': '\u05d9', 'i': '\u05df', 'j': '\u05d7', 'k': '\u05dc', 'l': '\u05da', 'm': '\u05e6', 'n': '\u05de', 'o': '\u05dd', 'p': '\u05e4', 'q': '/', 'r': '\u05e8', 's': '\u05d3', 't': '\u05d0', 'u': '\u05d5', 'v': '\u05d4', 'w': '\u05f3', 'x': '\u05e1', 'y': '\u05d8', 'z': '\u05d6'}
KEYBOARD_SWAP_MAP = {"/": "q", "": "w", "": "e", "": "r", "": "t", "": "y", "": "u", "": "i", "": "o", "": "p", "": "a", "": "s", "": "d", "": "f", "": "g", "": "h", "": "j", "": "k", "": "l", "": ";", ",": "'", "": "z", "": "x", "": "c", "": "v", "": "b", "": "n", "": "m", "": ",", "": ".", ".": "/",
					"'": ',', ',': '\u05ea', '.': '\u05e5', '/': '.', ';': '\u05e3', 'A': '\u05e9', 'B': '\u05e0', 'C': '\u05d1', 'D': '\u05d2', 'E': '\u05e7', 'F': '\u05db', 'G': '\u05e2', 'H': '\u05d9', 'I': '\u05df', 'J': '\u05d7', 'K': '\u05dc', 'L': '\u05da', 'M': '\u05e6', 'N': '\u05de', 'O': '\u05dd', 'P': '\u05e4', 'Q': '/', 'R': '\u05e8', 'S': '\u05d3', 'T': '\u05d0', 'U': '\u05d5', 'V': '\u05d4', 'W': '\u05f3', 'X': '\u05e1', 'Y': '\u05d8', 'Z': '\u05d6', 'a': '\u05e9', 'b': '\u05e0', 'c': '\u05d1', 'd': '\u05d2', 'e': '\u05e7', 'f': '\u05db', 'g': '\u05e2', 'h': '\u05d9', 'i': '\u05df', 'j': '\u05d7', 'k': '\u05dc', 'l': '\u05da', 'm': '\u05e6', 'n': '\u05de', 'o': '\u05dd', 'p': '\u05e4', 'q': '/', 'r': '\u05e8', 's': '\u05d3', 't': '\u05d0', 'u': '\u05d5', 'v': '\u05d4', 'w': '\u05f3', 'x': '\u05e1', 'y': '\u05d8', 'z': '\u05d6'}


@memoized
def heb_to_int(unicode_char):
	"""Converts a single Hebrew unicode character into its Hebrew numerical equivalent."""

	hebrew_numerals = {
		"\u05D0": 1,
		"\u05D1": 2,
		"\u05D2": 3,
		"\u05D3": 4,
		"\u05D4": 5,
		"\u05D5": 6,
		"\u05D6": 7,
		"\u05D7": 8,
		"\u05D8": 9,
		"\u05D9": 10,
		"\u05DB": 20,
		"\u05DC": 30,
		"\u05DE": 40,
		"\u05E0": 50,
		"\u05E1": 60,
		"\u05E2": 70,
		"\u05E4": 80,
		"\u05E6": 90,
		"\u05E7": 100,
		"\u05E8": 200,
		"\u05E9": 300,
		"\u05EA": 400,  	# u"\u05F3": "'", # Hebrew geresh  # u"\u05F4": '"', # Hebrew gershayim  # u"'":	   "'",
		"\u05DA": 20,		# khaf sofit
		"\u05DD": 40,		# mem sofit
		"\u05DF": 50, 		# nun sofit
		"\u05E3": 80, 		# peh sofit
		"\u05E5": 90, 		# tzadi sofit
	}

	if unicode_char not in list(hebrew_numerals.keys()):
		raise KeyError("Invalid Hebrew numeral character {}".format(unicode_char))

	else:
		return hebrew_numerals[unicode_char]


def split_thousands(n, littleendian=True):
	"""
	Takes a string representing a Hebrew numeral, returns a tuple of the component thousands
	places.  Requires a geresh (apostrophe or '\\u05F3') to indicate thousands.
	Ignores single geresh at end for numbers < 10.

	Default returns the smallest thousands group first in the tuple (little-endian).  Can be changed
	to big-endian by setting littleendian=False.
	"""

	# Ignore geresh on digit < 10, if present
	if n[-1] == GERESH or n[-1] == "'" or n[-1] == "\u2018" or n[-1] == "\u2019":
		n = n[:-1]

	#assume that two single quotes in a row should be a double quote. '' -> "
	n = n.replace(GERESH, "'").replace("''", "\"")

	ret = n.split("'")
	if littleendian:
		return reversed(ret)
	else:
		return ret


def heb_string_to_int(n):
	'''
	Takes a single thousands block of Hebrew characters, and returns the integer value of
	that set of characters, ignoring thousands order of magnitude.

	>>> heb_string_to_int(u'\\u05ea\\u05e9\\u05e1\\u05d3') # = u''
	764
	'''

	n = re.sub('[\u05F4"\u201c\u201d\u200e]', '', n)  # remove gershayim, double quote, fancy double quote, LTR Control Character
	return sum(map(heb_to_int, n))

@memoized
def decode_hebrew_numeral(numeral: str):
	"""
	Takes any string representing a Hebrew numeral and returns it integer value.

	>>> decode_hebrew_numeral(u'')
	5764
	"""

	t = list(map(heb_string_to_int, split_thousands(numeral)))  # split and convert to numbers
	t = [pow(10, 3 * E_num[0]) * E_num[1] for E_num in enumerate(t)]  # take care of thousands and add
	return sum(t)


########## ENCODING #############

def chunks(l, n):
	"""
	Yield successive n-sized chunks from l.
	"""
	for i in range(0, len(l), n):
		yield l[i:i + n]

@memoized
def int_to_heb(integer):
	"""
	Converts an integer that can be expressed by a single Hebrew character (1..9, 10..90, 100.400)
	and returns the Hebrew character that represents that integer.

	Also accepts values divisible by 100 from 500 to 1100.

	>> int_to_heb(10)          #This fails as a doctest.  The yud isn't seen as u'\\u05d9'
	
	>> int_to_heb(800)          #TavTav is not seen as u'\\u05ea\\u05ea'
	
	"""

	hebrew_numerals = {
		0: "",
		1: "\u05D0",
		2: "\u05D1",
		3: "\u05D2",
		4: "\u05D3",
		5: "\u05D4",
		6: "\u05D5",
		7: "\u05D6",
		8: "\u05D7",
		9: "\u05D8",
		10: "\u05D9",
		15: "\u05D8\u05D5",  # Will not be hit when used with break_int_magnitudes
		16: "\u05D8\u05D6",  # Will not be hit when used with break_int_magnitudes
		20: "\u05DB",
		30: "\u05DC",
		40: "\u05DE",
		50: "\u05E0",
		60: "\u05E1",
		70: "\u05E2",
		80: "\u05E4",
		90: "\u05E6",
		100: "\u05E7",
		200: "\u05E8",
		300: "\u05E9",
		400: "\u05EA",
	}

	# Fill in hebrew_numeral mappings up to 1100
	for num in range(500, 1200, 100):
		hebrew_numerals[num] = hebrew_numerals[400] * (num // 400) + hebrew_numerals[num % 400]

	if integer > 1100:
		raise KeyError("Asked to convert individual integer {} above 1100; too large.".format(integer))

	else:
		return hebrew_numerals[integer]


def break_int_magnitudes(n, start=None):
	"""break_int_magnitudes(n, start=None)

	Accepts an integer and an optional integer (multiple of 10) for at what order of
	magnitude to start breaking apart the integer.  If no option "start" is provided,
	function will determine the size of the input integer and start that the largest order
	of magnitude.

	Returns a big-endian list of the various orders of magnitude, by 10s, broken apart.

	>>> break_int_magnitudes(1129, 100)
	[1100, 20, 9]

	>>> break_int_magnitudes(2130)
	[2000, 100, 30, 0]

	>>> break_int_magnitudes(15000)
	[10000, 5000, 0, 0, 0]
	"""

	if type(n) is not int:
		raise TypeError("Argument 'n' must be int, {} provided.".format(type(n)))

	# if n == 0:
	# 	return [0]

	# Set a default for 'start' if none specified
	if start is not None:
		if not (start % 10 == 0 or start == 1):
			raise TypeError("Argument 'start' must be 1 or divisible by 10, {} provided.".format(start))
	else:
		start = 10 ** int(math.log10(n))

	if start == 1:
		return [n]
	else:
		return [n // start * start] + break_int_magnitudes(n - n // start * start, start=start // 10)


@memoized
def sanitize(input_string, punctuation=True):
	"""sanitize(input_string, punctuation=True)

	Takes a Hebrew number input string and applies appropriate formatting and changes.  This function
	includes any special cases, like 15 and 16.

	Optional addition of gershayim or geresh at end where appropriate with "punctuation" arg.
	Thousands geresh will be added regardless from previous functions.

	Note that high numbers may appear oddly due to lack of convention.  For example,
	the sanitized version of 15000 will appear as .

	"""

	# deal with 15 and 16
	# Should we support numbers like 15,000?  Would that look like tet-vav-geresh?

	# if input_string[-2:] in (encode_small_hebrew_numeral(15), encode_small_hebrew_numeral(16)):
	# 	input_string = input_string[:-2] + int_to_heb(heb_string_to_int(input_string[-2:]))

	# This takes care of all instances of 15/16, even in the thousands

	replacement_pairs = (
		('\u05d9\u05d4', '\u05d8\u05d5'),  #15
		('\u05d9\u05d5', '\u05d8\u05d6'),  #16
		('\u05e8\u05e2\u05d4', '\u05e2\u05e8\u05d4'),  #275
		('\u05e8\u05e2\u05d1', '\u05e2\u05e8\u05d1'),  #272
		('\u05e8\u05e2', '\u05e2\u05e8'),  #270
	)

	for wrong, right in replacement_pairs:
		input_string = re.sub(wrong, right, input_string)

	if punctuation:
		# add gershayim at end
		if len(input_string) > 1:
			if GERESH not in input_string[-2:]:
				input_string = input_string[:-1] + GERSHAYIM + input_string[-1:]
		else:
			# or, add single geresh at end
			input_string += GERESH

	return input_string


def decompose_presentation_forms(orig_char):
	decomp_map = {'': '\u05d9\u05b4',
		' ' : '\u05bf',
		'': '\u05f2\u05b7',
		'': '\u05e2',
		'': '\u05d0',
		'': '\u05d3',
		'': '\u05d4',
		'': '\u05db',
		'': '\u05dc',
		'': '\u05dd',
		'': '\u05e8',
		'': '\u05ea',
		'': '\u05e9\u05c1',
		'': '\u05e9\u05c2',
		'': '\u05e9\u05bc\u05c1',
		'': '\u05e9\u05bc\u05c2',
		'': '\u05d0\u05b7',
		'': '\u05d0\u05b8',
		'': '\u05d0\u05bc',
		'': '\u05d1\u05bc',
		'': '\u05d2\u05bc',
		'': '\u05d3\u05bc',
		'': '\u05d4\u05bc',
		'': '\u05d5\u05bc',
		'': '\u05d6\u05bc',
		'': '\u05d8\u05bc',
		'': '\u05d9\u05bc',
		'': '\u05da\u05bc',
		'': '\u05db\u05bc',
		'': '\u05dc\u05bc',
		'': '\u05de\u05bc',
		'': '\u05e0\u05bc',
		'': '\u05e1\u05bc',
		'': '\u05e3\u05bc',
		'': '\u05e4\u05bc',
		'': '\u05e6\u05bc',
		'': '\u05e7\u05bc',
		'': '\u05e8\u05bc',
		'': '\u05e9\u05bc',
		'': '\u05ea\u05bc',
		'': '\u05d5\u05b9',
		'': '\u05d1\u05bf',
		'': '\u05db\u05bf',
		'': '\u05e4\u05bf',
		'': '\u05d0\u05dc'
	}
	# if isinstance(orig_char, str): #needs to be unicode
	#	orig_char = str(orig_char, 'utf-8')
	return decomp_map.get(orig_char, '')

presentation_re = re.compile(r"[\uFB1D-\uFB4F]")


def decompose_presentation_forms_in_str(orig_str):
	return presentation_re.sub(lambda match: decompose_presentation_forms(match.group()), orig_str)


def normalize_final_letters(orig_char):

	decomp_map = {
		"\u05DA": "\u05DB",		# khaf sofit
		"\u05DD": "\u05DE",		# mem sofit
		"\u05DF": "\u05E0", 		# nun sofit
		"\u05E3": "\u05E4", 		# peh sofit
		"\u05E5": "\u05E6", 		# tzadi sofit
	}

	# if isinstance(orig_char, str): #needs to be unicode
	#	orig_char = str(orig_char, 'utf-8')
	return decomp_map.get(orig_char, '')

final_letter_re = re.compile("[" + FINAL_LETTERS + "]")


def normalize_final_letters_in_str(orig_str):
	return final_letter_re.sub(lambda match: normalize_final_letters(match.group()), orig_str)


def swap_keyboards_for_letter(orig_char):
	# if isinstance(orig_char, str):  # needs to be unicode
	#	orig_char = str(orig_char, 'utf-8')
	return KEYBOARD_SWAP_MAP.get(orig_char, orig_char)


def swap_keyboards_for_string(orig_str):
	return re.sub(r".", lambda match: swap_keyboards_for_letter(match.group()), orig_str)

@memoized
def encode_small_hebrew_numeral(n):
	"""
	Takes an integer under 1200 and returns a string encoding it as a Hebrew numeral.
	"""

	if n >= 1200:
		raise ValueError("Tried to encode small numeral >= 1200.")
	else:
		return ''.join(map(int_to_heb, break_int_magnitudes(n, 100)))

@memoized
def encode_hebrew_numeral(n, punctuation=True):
	"""encode_hebrew_numeral(n, punctuation=True)

	Takes an integer and returns a string encoding it as a Hebrew numeral.
	Optional "punctuation" argument adds gershayim between last two characters
	or final geresh.

	Under 1200, will use taf-taf-shin, etc.
	Above 1200, will use aleph + geresh for thousands.

	This function is not intended for numbers 1,000,000 or more, as there is not currently
	an established convention and there can be ambiguity.  This can be the same for numbers like
	2000 (which would be displayed as bet-geresh) and should instead possibly use words, like "bet elef."
	"""

	if n < 1200:
		ret = encode_small_hebrew_numeral(n)
	else:

		# Break into magnitudes, then break into thousands buckets, big-endian
		ret = list(chunks(list(reversed(break_int_magnitudes(n))), 3))

		# Eliminate the orders of magnitude in preparation for being encoded
		ret = [int(sum(x_y[1]) * pow(10, -3 * x_y[0])) for x_y in enumerate(ret)]

		# encode and join together, separating thousands with geresh
		ret = GERESH.join(map(encode_small_hebrew_numeral, reversed(ret)))

	ret = sanitize(ret, punctuation)

	return ret


@memoized
def encode_hebrew_daf(daf):
	"""
	Turns a daf string ("21a") to a hebrew daf string (".")
	"""
	daf, amud = daf[:-1], daf[-1]
	amud_mark = {"a": ".", "b": ":"}[amud]
	return encode_hebrew_numeral(int(daf), punctuation=False) + amud_mark


def strip_nikkud(rawString):
	return regex.sub(r"[\u0591-\u05C7]", "", rawString)


#todo: rewrite to handle edge case of hebrew words in english texts, and latin characters in Hebrew text

any_hebrew = regex.compile(r"\p{Hebrew}")
any_english = regex.compile(r"[a-zA-Z]")


def has_hebrew(s):
	return any_hebrew.search(s)


def is_all_hebrew(s):
	return any_hebrew.search(s) and not any_english.search(s)


def is_mostly_hebrew(s: str, len_to_check: int = 300):
	"""
	Check if input string is majority Hebrew
	@param s: Input string to check
	@param len_to_check: Maximum number of characters to check. Use `None` to check the whole string.
	@return: Returns True if text is majority Hebrew
	"""
	s = regex.sub(r"(?:[0-9.,'\"?!;:\-=@\#$%^&*()/<>]|\s)", "", s)  # remove punctuation/spaces/numbers
	s = s[:len_to_check]
	he_count = len(any_hebrew.findall(s))
	return he_count > len(s)/2


def strip_cantillation(text, strip_vowels=False):
	if strip_vowels:
		strip_regex = re.compile(r"[\u0591-\u05bd\u05bf-\u05c5\u05c7]", re.UNICODE)
	else:
		strip_regex = re.compile(r"[\u0591-\u05af\u05bd\u05bf\u05c0\u05c4\u05c5]", re.UNICODE)
	return strip_regex.sub('', text)


def has_cantillation(text, detect_vowels=False):
	if detect_vowels:
		rgx = re.compile(r"[\u0591-\u05bd\u05bf-\u05c5\u05c7]", re.UNICODE)
	else:
		rgx = re.compile(r"[\u0591-\u05af\u05bd\u05bf\u05c0\u05c4\u05c5]", re.UNICODE)
	return bool(rgx.search(text))


def gematria(string):
	"""Returns the gematria of `str`, ignore any characters in string that have now gematria (like spaces)"""
	total = 0
	for letter in string:
		try:
			total += heb_to_int(letter)
		except:
			pass
	return total


def hebrew_plural(s):
	"""
	Hebrew friendly plurals
	"""
	known = {
		"Daf":      "Dappim",
		"Mitzvah":  "Mitzvot",
		"Negative Mitzvah": "Negative Mitzvot",
		"Positive Mitzvah": "Positive Mitzvot",
		"Mitsva":   "Mitzvot",
		"Mesechet": "Mesechtot",
		"Perek":    "Perokim",
		"Siman":    "Simanim",
		"Seif":     "Seifim",
		"Se'if":    "Se'ifim",
		"Seif Katan": "Seifim Katanim",
		"Mishnah":  "Mishnayot",
		"Mishna":   "Mishnayot",
		"Chelek":   "Chelekim",
		"Parasha":  "Parshiot",
		"Parsha":   "Parshiot",
		"Pasuk":    "Psukim",
		"Midrash":  "Midrashim",
		"Teshuva":  "Teshuvot",
		"Aliyah":   "Aliyot",
		"Tikun":    "Tikunim",
	}

	return known[s] if s in known else str(s) + "s"


def hebrew_term(s):
	from sefaria.model import library
	from sefaria.system.exceptions import BookNameError

	if has_hebrew(s):
		return s

	term = library.get_simple_term_mapping().get(s)
	if term:
		return term["he"]
	else:
		try:
			# If s is a text title, look for a stored Hebrew title
			i = library.get_index(s)
			return i.get_title("he")
		except BookNameError:
			return ''


def hebrew_parasha_name(value):
	"""
	Returns a Hebrew ref for the english ref passed in.
	"""
	from sefaria.model import Term, library
	if not value:
		return ""
	if "-" in value:
		if value == "Lech-Lecha":
			return hebrew_parasha_name(value.replace("-", " "))
		else:
			names = value.split("-")
			return ("-").join(map(hebrew_parasha_name, names))
	else:
		try:
			parasha = library.get_simple_term_mapping().get(value)["he"]
		except Exception as e:
			logger.error(str(e))
			parasha = value
		return parasha


########
# Hebrew Abbrev Matching
########

def get_he_key(key):
	return 'he' + key[0].upper() + key[1:]  # birthPlace => heBirthPlace

def get_abbr(abbr: str, unabbr: List[str], match=lambda x, y: x.startswith(y), lang='he'):
	abbr = re.sub('[^-]', '', abbr) if lang == 'he' else re.sub('[^a-z]', '', abbr)
	indexes = [[index for index, letter in enumerate(abbr) if word[0] == letter] for w, word in enumerate(unabbr)]
	first_empty_ind = None
	for i in range(len(indexes)):
		if len(indexes[i]) == 0:
			if i == 0: return None  # nothing matched
			first_empty_ind = i
			break
	indexes = indexes[:first_empty_ind]
	choices = itertools.product(*indexes)
	for choice in choices:
		if choice[0] != 0: continue
		longest_desc_choice = []
		for i, j in zip(choice, choice[1:] + (None,)):
			longest_desc_choice += [i]
			if j is None or i >= j: break
		choice = longest_desc_choice
		temp_unabbr = unabbr[:len(choice)]
		choice += [None]
		if all(match(temp_unabbr[n], abbr[choice[n]:choice[n + 1]]) for n in range(len(temp_unabbr))):
			return temp_unabbr
	return None


@dataclasses.dataclass
class Abbrev:
	abbr: str
	unabbr: List[str]
	iabbr: int
	unabbr_slice: slice


def is_abbr(word):
	return re.search(r'[^"]["][^"]', word) is not None


def get_all_abbrs(abbr_words, unabbr_words) -> List[Abbrev]:
	"""
	Get all abbreviations in `abbr_words`
	"""
	abbrevs = []
	for iabbr, abbr in enumerate(abbr_words):
		if not is_abbr(abbr): continue
		for iother in range(len(unabbr_words)):
			unabbr = get_abbr(abbr, unabbr_words[iother:])
			if unabbr:
				unabbr_slice = slice(iother, iother+len(unabbr))
				abbrevs += [Abbrev(abbr, unabbr, iabbr, unabbr_slice)]
	return abbrevs


def hebrew_starts_with(he: str, other_he: str) -> False:
	"""
	does `he` start with `other_he`?
	includes possible abbreviation expansion
	TODO. in future may include fuzzy matching
	"""
	he_words = he.split()
	other_words = other_he.split()
	he_abbrs = get_all_abbrs(he_words, other_words)
	other_abbrs = get_all_abbrs(other_words, he_words)

	ihe, iother = 0, 0
	iabbr, i_other_abbr = 0, 0
	while ihe < len(he_words) and iother < len(other_words):
		if iabbr < len(he_abbrs) and he_abbrs[iabbr].iabbr == ihe:
			ihe += 1
			iother += len(he_abbrs[iabbr].unabbr)
			iabbr += 1
			continue
		if i_other_abbr < len(other_abbrs) and other_abbrs[i_other_abbr].iabbr == iother:
			ihe += len(other_abbrs[i_other_abbr].unabbr)
			iother += 1
			i_other_abbr += 1
			continue
		if other_words[iother] != he_words[ihe]:
			break
		ihe += 1
		iother += 1
	return iother == len(other_words)


########
# Hebrew Abbrev Matching END
########

PREFIXES = {
	'', '', '', '', '', '', '',   # single letter
	'', '', '', '', '', '',  # double letter
}  # careful of Ayin prefix...


def get_prefixless_inds(st: str) -> List[int]:
	"""
	get possible indices of start of string `st` with prefixes stripped
	"""
	starti_list = []
	for prefix in PREFIXES:
		if not st.startswith(prefix): continue
		starti_list += [len(prefix)]
	return starti_list

```

### sefaria/utils/calendars.py

```
# -*- coding: utf-8 -*-
"""
ar.py - functions for looking up information relating texts to dates.

Uses MongoDB collections: dafyomi, parshiot
"""
import datetime
import p929
import re
import urllib
from django.utils import timezone

import sefaria.model as model
from sefaria.system.database import db
from sefaria.utils.util import graceful_exception
from sefaria.utils.hebrew import encode_hebrew_numeral, hebrew_parasha_name
from sefaria.site.site_settings import SITE_SETTINGS
from sefaria.model.schema import Term

import structlog
logger = structlog.get_logger(__name__)


"""
Calendar items:
calendar title
hebrew calendar title
display value
hebrew display value
ref
"""

@graceful_exception(logger=logger, logLevel="warning", return_value=[])
def daily_929(datetime_obj):
    #datetime should just be a date, like datetime.today()
    p = p929.Perek(datetime_obj.date())
    rf = model.Ref("{} {}".format(p.book_name, p.book_chapter))
    display_en = "{} ({})".format(rf.normal(), p.number)
    display_he = "{} ({})".format(rf.he_normal(), p.number)
    return [{
        'title' : {'en':'929', 'he': '929'},
        'displayValue': {'en':display_en, 'he': display_he},
        'url': rf.url(),
        'ref': rf.normal(),
        'order': 4,
        'category': rf.index.get_primary_category()
    }]


@graceful_exception(logger=logger, return_value=[])
def daf_yomi(datetime_obj):
    """
    Returns the daf yomi for date
    """
    date_str = datetime_obj.strftime(" %m/ %d/%Y").replace(" 0", "").replace(" ", "")
    daf = db.dafyomi.find_one({"date": date_str})
    daf_en, daf_he = None, None
    if 'displayValue' in daf:
        daf_en, daf_he = daf['displayValue'].get('en', None), daf['displayValue'].get('he', None)
    daf_str = [daf["daf"]] if isinstance(daf["daf"], str) else daf["daf"]
    daf_yomi = []
    for d in daf_str:
        rf = model.Ref(d)

        daf_yomi.append({
            'title': {'en': 'Daf Yomi', 'he': ' '},
            'displayValue': {'en': daf_en if daf_en else rf.normal(), 'he': daf_he if daf_he else rf.he_normal()},
            'url': rf.url(),
            'ref': rf.normal(),
            'order': 3,
            'category': rf.index.get_primary_category()
        })
    return daf_yomi


@graceful_exception(logger=logger, return_value=[])
def daily_mishnayot(datetime_obj):
    mishnah_items = []
    datetime_obj = datetime.datetime(datetime_obj.year,datetime_obj.month,datetime_obj.day)
    daily_mishnahs = db.daily_mishnayot.find({"date": {"$eq": datetime_obj}}).sort([("date", 1)])
    for dm in daily_mishnahs:
        rf = model.Ref(dm["ref"])
        mishnah_items.append({
        'title': {'en': 'Daily Mishnah', 'he': ' '},
        'displayValue': {'en': rf.normal(), 'he': rf.he_normal()},
        'url': rf.url(),
        'ref': rf.normal(),
        'order': 5,
        'category': rf.index.get_primary_category()
    })
    return mishnah_items


@graceful_exception(logger=logger, return_value=[])
def daily_rambam(datetime_obj):
    datetime_obj = datetime.datetime(datetime_obj.year,datetime_obj.month,datetime_obj.day)
    daily_rambam = db.daily_rambam.find_one({"date": {"$eq": datetime_obj}})
    rf = model.Ref(daily_rambam["ref"])
    display_value_en = rf.normal().replace("Mishneh Torah, ","")
    display_value_he = rf.he_normal().replace(" , ", "")
    return [{
        'title': {'en': 'Daily Rambam', 'he': '" '},
        'displayValue': {'en': display_value_en, 'he': display_value_he},
        'url': rf.url(),
        'ref': rf.normal(),
        'order': 6,
        'category': rf.index.get_primary_category()
    }]

@graceful_exception(logger=logger, return_value=[])
def arukh_hashulchan(datetime_obj):
    items = []
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    database_obj = db.arukh_hashulchan.find_one({"date": {"$eq": datetime_obj}})
    if not database_obj:
        return []
    rf = database_obj["refs"]
    rf = model.Ref(rf)
    display_en = rf.normal()
    display_he = rf.he_normal()
    items.append({
        "title": {"en": "Arukh HaShulchan Yomi", "he": '  '},
        "displayValue": {"en": display_en.replace("Arukh HaShulchan, ", ""), "he": display_he.replace(" , ", "")},
        "url": rf.url(),
        "ref": rf.normal(),
        "order": 10,
        "category": rf.index.get_primary_category()
    })
    return items

@graceful_exception(logger=logger, return_value=[])
def daily_rambam_three(datetime_obj):
    rambam_items = []
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    database_obj = db.daily_rambam_three.find_one({"date": {"$eq": datetime_obj}})
    if not database_obj:
        return []
    for rf in database_obj["refs"]:
        rf = model.Ref(rf)
        display_en = rf.normal().replace("Mishneh Torah, ", "")
        display_he = rf.he_normal().replace(" , ", "")
        rambam_items.append({
            "title": {"en": "Daily Rambam (3 Chapters)", "he": '"  {}'.format("(3 )")},
            "displayValue": {"en": display_en, "he": display_he},
            "url": rf.url(),
            "ref": rf.normal(),
            "order": 7,
            "category": rf.index.get_primary_category()
        })
    return rambam_items


@graceful_exception(logger=logger, return_value=[])
def tanakh_yomi(datetime_obj):
    tanakh_items = []
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    database_obj = db.tanakh_yomi.find_one({"date": {"$eq": datetime_obj}})
    if not database_obj:
        return []
    rf = database_obj["ref"]
    rf = model.Ref(rf)
    display_en = database_obj["displayValue"]
    display_he = database_obj["heDisplayValue"]
    tanakh_items.append({
        "title": {"en": "Tanakh Yomi", "he": '" '},
        "displayValue": {"en": display_en, "he": display_he},
        "url": rf.url(),
        "ref": rf.normal(),
        "order": 11,
        "category": rf.index.get_primary_category()
    })
    return tanakh_items


@graceful_exception(logger=logger, return_value=[])
def tikkunei_yomi(datetime_obj):
    tikkunei_items = []
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    database_obj = db.daily_tikkunei_zohar.find_one({"date": {"$eq": datetime_obj}})
    if not database_obj:
        return []
    rf = database_obj["ref"]
    rf = model.Ref(rf)
    display_en = database_obj["displayValue"]
    display_he = database_obj["heDisplayValue"]
    tikkunei_items.append({
        "title": {"en": "Zohar for Elul", "he": '   '},
        "displayValue": {"en": display_en, "he": display_he},
        "url": rf.url(),
        "ref": rf.normal(),
        "order": 13,
        "category": rf.index.get_primary_category()
    })
    return tikkunei_items

@graceful_exception(logger=logger, return_value=[])
def daf_weekly(datetime_obj):
    """
    :param datetime.datetime datetime_obj:
    :return:
    """
    """
    Weekday values in datetime start on Monday, i.e.
    Monday = 0, Tuesday = 1, Wednesday = 2,... Sunday = 6 
    We want to start on Sunday (a new daf is started every Sunday):
    Sunday = 0, Monday = 1, Tuesday = 2,...
    """
    cur_weekday = (datetime_obj.weekday() + 1) % 7
    sunday_obj = datetime_obj - datetime.timedelta(cur_weekday)
    sunday_obj = datetime.datetime(sunday_obj.year, sunday_obj.month, sunday_obj.day)

    daf = db.daf_weekly.find_one({"date": {"$eq": sunday_obj}})
    daf_str = [daf["daf"]] if isinstance(daf["daf"], str) else daf["daf"]
    daf_weekly_list = []

    for d in daf_str:
        rf = model.Ref(d)
        daf_weekly_list.append({
            "title": {"en": "Daf a Week", "he": " "},
            "displayValue": {"en": rf.normal(), "he": rf.he_normal()},
            "url": rf.url(),
            "ref": rf.normal(),
            "order": 8,
            "category": rf.index.get_primary_category()
        })
    return daf_weekly_list


@graceful_exception(logger=logger, return_value=[])
def halakhah_yomit(datetime_obj):
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    db_obj = db.halakhah_yomit.find_one({"date": {"$eq": datetime_obj}})
    rf = model.Ref(db_obj["ref"])
    display_en = rf.normal().replace("Shulchan Arkuh, ", "")
    display_he = rf.he_normal()
    halakha = {
        "title": {"en": "Halakhah Yomit", "he": " "},
        "displayValue": {"en": display_en, "he": display_he},
        "url": rf.url(),
        "ref": rf.normal(),
        "order": 9,
        "category": rf.index.get_primary_category()
    }
    return [halakha]


def make_haftarah_by_custom_response_from_calendar_entry(db_haftara, custom, add_custom_to_display):
    shorthands = {
        "ashkenazi" : {"en": 'A', "he": ''},
        "sephardi": {"en": 'S', "he": ''},
        "edot hamizrach": {"en": 'EM', "he": ''}
    }
    haftarah_objs = []
    for h in db_haftara:
        rf = model.Ref(h)
        haftara = {
            'title': {'en': 'Haftarah', 'he': ''},
            'displayValue': {'en': rf.normal(), 'he': rf.he_normal()},
            'url': rf.url(),
            'ref': rf.normal(),
            'order': 2,
            'category': rf.index.get_primary_category(),
        }
        if add_custom_to_display:
            for lang in haftara['title']:
                haftara['title'][lang] = '{} ({})'.format(haftara['title'][lang], shorthands[custom][lang])
        haftarah_objs.append(haftara)
    return haftarah_objs


def make_haftarah_response_from_calendar_entry(db_parasha, custom=None):
    haftarah_objs = []
    if isinstance(db_parasha["haftara"], list): #backwards compatability
        haftarah_objs += make_haftarah_by_custom_response_from_calendar_entry(db_parasha["haftara"], "ashkenazi", False)
    elif len(list(db_parasha["haftara"].keys())) == 1 or not len(db_parasha["haftara"].get(custom, [])):
        haftarah_objs += make_haftarah_by_custom_response_from_calendar_entry(db_parasha["haftara"].get("ashkenazi"), "ashkenazi", False)
    elif custom:
        haftarah_objs += make_haftarah_by_custom_response_from_calendar_entry(db_parasha["haftara"].get(custom, []), custom, True)
    else:
        for key in db_parasha["haftara"]:
            haftarah_objs += make_haftarah_by_custom_response_from_calendar_entry(db_parasha["haftara"].get(key), key, True)
    return haftarah_objs


def make_parashah_response_from_calendar_entry(db_parasha, include_topic_slug=False):
    rf = model.Ref(db_parasha["ref"])

    parasha_topic = model.Topic().load({"parasha": db_parasha['parasha']})
    p_en = p_he = ""
    if parasha_topic:
        p_en = parasha_topic.description["en"]
        p_he = parasha_topic.description["he"]
    parasha_description = {"en": p_en, "he": p_he}

    parasha = {
        'title': {'en': 'Parashat Hashavua', 'he': ' '},
        'displayValue': {'en': db_parasha["parasha"], 'he': hebrew_parasha_name(db_parasha["parasha"])},
        'url': rf.url(),
        'ref': rf.normal(),
        'heRef': rf.he_normal(),
        'order': 1,
        'category': rf.index.get_primary_category(),
        'extraDetails': {'aliyot': db_parasha["aliyot"]},
        'description': parasha_description
    }
    if include_topic_slug and parasha_topic:
        parasha['topic'] = parasha_topic.slug   # include the slug so that the Sheets Homepage has access to the parasha's topic link
    return [parasha]


def aliyah_ref(parasha_db, aliyah):
    assert 1 <= aliyah <= 7
    return model.Ref(parasha_db["aliyot"][aliyah - 1])


def get_parasha(datetime_obj, diaspora=True, parasha=None):
    """
    Returns the upcoming Parasha for datetime.
    """
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    query = {"date": {"$gte": datetime_obj}, "diaspora": {'$in': [diaspora, None]}}
    if parasha is not None:
        # regex search for potential double parasha. there can be dash before or after name
        query["parasha"] = re.compile('(?:(?<=^)|(?<=-)){}(?=-|$)'.format(parasha))
    p = db.parshiot.find(query, limit=1).sort([("date", 1)])
    p = next(p)
    return p


def get_todays_parasha(diaspora=True):
    return get_parasha(timezone.localtime(timezone.now()), diaspora=diaspora)


@graceful_exception(logger=logger, return_value=[])
def parashat_hashavua_and_haftara(datetime_obj, diaspora=True, custom=None, parasha=None, ret_type='list'):
    db_parasha = get_parasha(datetime_obj, diaspora=diaspora, parasha=parasha)
    parasha_item = make_parashah_response_from_calendar_entry(db_parasha)
    haftarah_item = make_haftarah_response_from_calendar_entry(db_parasha, custom)
    if ret_type not in {'list', 'dict'}:
        raise Exception('ret_type parameter must be either "list" or "dict')
    if ret_type == 'list':
        parasha_items = parasha_item + haftarah_item
    elif ret_type == 'dict':
        from sefaria.utils.util import get_hebrew_date
        he_date_in_english, he_date_in_hebrew = get_hebrew_date(db_parasha.get('date', None))
        parasha_items = {
            'parasha': parasha_item[0],
            'haftarah': haftarah_item,
            'date': db_parasha.get('date', None),
            'he_date': {
                "en": he_date_in_english,
                "he": he_date_in_hebrew
            }
        }
    return parasha_items


@graceful_exception(logger=logger, return_value=[])
def hok_leyisrael(datetime_obj, diaspora=True):

    def get_hok_parasha(datetime_obj, diaspora=diaspora):
        parasha = get_parasha(datetime_obj, diaspora=diaspora)['parasha']
        parasha = parasha.replace('Lech-Lecha', 'Lech Lecha')
        parasha = parasha.split('-')[0]
        if parasha == 'Shmini Atzeret':
            parasha = "V'Zot HaBerachah"
        parasha_term = Term().load({'category': 'Torah Portions', 'titles': {'$elemMatch': {'text': parasha}}})
        if not parasha_term:
            parasha_term = get_hok_parasha(datetime_obj + datetime.timedelta(7), diaspora=diaspora)
        return parasha_term

    parasha_term = get_hok_parasha(datetime_obj, diaspora=diaspora)
    parasha_he = parasha_term.get_primary_title('he')
    return [{
        "title": {"en": "Chok LeYisrael", "he": ' '},
        "displayValue": {"en": parasha_term.name, "he": parasha_he},
        "url": f'collections/-?tag={urllib.parse.quote(parasha_term.name)}',
        "order": 12,
        "category": 'Tanakh'
    }]


@graceful_exception(logger=logger, return_value=[])
def tanya_yomi(datetime_obj):
    tanya_items = []
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    database_obj = db.tanya_yomi.find_one({"date": {"$eq": datetime_obj}})
    if not database_obj:
        return []
    rf = database_obj["ref"]
    rf = model.Ref(rf)
    display_en = database_obj["displayValue"]
    display_he = database_obj["heDisplayValue"]
    tanya_items.append({
        "title": {"en": "Tanya Yomi", "he": ' '},
        "displayValue": {"en": display_en, "he": display_he},
        "url": rf.url(),
        "ref": rf.normal(),
        "order": 15,
        "category": rf.index.get_primary_category()
    })
    return tanya_items


@graceful_exception(logger=logger, return_value=[])
def yerushalmi_yomi(datetime_obj):
    yerushalmi_items = []
    datetime_obj = datetime.datetime(datetime_obj.year, datetime_obj.month, datetime_obj.day)
    database_obj = db.yerushalmi_yomi.find_one({"date": {"$eq": datetime_obj}})
    if not database_obj:
        return []
    rf = database_obj["ref"]
    rf = model.Ref(rf)
    display_en = database_obj["displayValue"]
    display_he = database_obj["heDisplayValue"]
    yerushalmi_items.append({
        "title": {"en": "Yerushalmi Yomi", "he": ' '},
        "displayValue": {"en": display_en, "he": display_he},
        "url": rf.url(),
        "ref": rf.normal(),
        "order": 16,
        "category": rf.index.get_primary_category()
    })
    return yerushalmi_items


def get_all_calendar_items(datetime_obj, diaspora=True, custom="sephardi"):
    if not SITE_SETTINGS["TORAH_SPECIFIC"]:
        return []
    cal_items  = []
    cal_items += parashat_hashavua_and_haftara(datetime_obj, diaspora=diaspora, custom=custom)
    cal_items += daf_yomi(datetime_obj)
    cal_items += daily_929(datetime_obj)
    cal_items += daily_mishnayot(datetime_obj)
    cal_items += daily_rambam(datetime_obj)
    cal_items += daily_rambam_three(datetime_obj)
    cal_items += daf_weekly(datetime_obj)
    cal_items += halakhah_yomit(datetime_obj)
    cal_items += arukh_hashulchan(datetime_obj)
    cal_items += tanakh_yomi(datetime_obj)
    cal_items += tikkunei_yomi(datetime_obj)
    cal_items += hok_leyisrael(datetime_obj, diaspora=diaspora)
    cal_items += tanya_yomi(datetime_obj)
    cal_items += yerushalmi_yomi(datetime_obj)
    cal_items = [item for item in cal_items if item]
    return cal_items


def get_keyed_calendar_items(diaspora=True, custom=None):
    cal_items = get_todays_calendar_items(diaspora=diaspora, custom=custom)
    cal_dict = {}
    for cal_item in cal_items:
        cal_dict[cal_item["title"]["en"]] = cal_item
    return cal_dict


def get_todays_calendar_items(diaspora=True, custom=None):
    return get_all_calendar_items(timezone.localtime(timezone.now()), diaspora=diaspora, custom=custom)



def verify_parashah_topics_exist():
    """Checks that topics exist corresponding to every entry in the parshiot collection"""
    missing = False
    parshiot = db.parshiot.distinct("parasha")
    for parashah in parshiot:
        if not model.Topic().load({"parasha": parashah}):
            missing = True
            print("Missing parashah topic for: {}".format(parashah))

    if not missing:
        print("All parashahs in the parshiot collection have corresponding topics.")

```

### sefaria/export.py

```
"""
export.py - functions for exporting texts to various text formats.

Exports to the directory specified in SEFARIA_EXPORT_PATH.
"""
import sys
import os
import io
import unicodecsv as csv
import re
import json
from shutil import rmtree
from random import random
from pprint import pprint
from datetime import datetime
from collections import Counter
from copy import deepcopy
import django
django.setup()
from sefaria.model import *
from sefaria.model.text import AbstractIndex
from sefaria.utils.talmud import section_to_daf
from sefaria.system.exceptions import InputError
from .settings import SEFARIA_EXPORT_PATH
from sefaria.system.database import db
from sefaria.tracker import modify_bulk_text
from collections import defaultdict

lang_codes = {
    "he": "Hebrew",
    "en": "English"
}


def log_error(msg):
    msg = '{}\n'.format(msg)
    log_error.all_errors.append(msg)
    sys.stderr.write(msg)
log_error.all_errors = []


def print_errors():
    if len(log_error.all_errors):
        sys.stderr.write('\n___ERRORS___\n')
    for i, error in enumerate(log_error.all_errors):
        sys.stderr.write('{}. {}'.format(i, error))

def make_path(doc, format, extension=None):
    """
    Returns the full path and file name for exporting 'doc' in 'format'.
    """
    if doc["categories"][0] not in library.get_top_categories():
        doc["categories"].insert(0, "Other")
    path = "%s/%s/%s/%s/%s/%s.%s" % (SEFARIA_EXPORT_PATH,
                                            format,
                                            "/".join(doc["categories"]),
                                            doc["title"],
                                            lang_codes[doc["language"]],
                                            remove_illegal_file_chars(doc["versionTitle"]),
                                            extension or format)
    return path


def remove_illegal_file_chars(filename_str):
    p = re.compile('[/:()<>"|?*]|(\\\)')
    new_filename = p.sub('', filename_str)
    return new_filename


def make_json(doc):
    """
    Returns JSON of 'doc' with export settings.
    """
    if "original_text" in doc:
        doc = {k: v for k, v in doc.items() if k != "original_text"}
    return json.dumps(doc, indent=4, ensure_ascii=False)


def make_text(doc, strip_html=False):
    """
    Export doc into a simple text format.

    if complex, go through nodes depth first,
    at each node, output name of node
    if node is leaf, run flatten on it

    """
    # We have a strange beast here - a merged content tree.  Loading it into a synthetic version.
    chapter = doc.get("original_text", doc["text"])
    version = Version({"chapter": chapter})

    index = library.get_index(doc["title"])
    versionSource = doc["versionSource"] or ""
    text = "\n".join([doc["title"], doc.get("heTitle", ""), doc["versionTitle"], versionSource])

    if "versions" in doc:
        if not len(doc["versions"]):
            return None # Occurs when text versions don't actually have content
        text += "\nThis file contains merged sections from the following text versions:"
        for v in doc["versions"]:
            text += "\n-%s\n-%s" % (v[0], v[1])

    def make_node(node, depth, **kwargs):
        if not node.children:
            content = "\n\n%s\n\n" % node.primary_title(doc["language"])
            cnode = version.content_node(node)
            if strip_html:
                cnode = version.remove_html_and_make_presentable(cnode)
            content += flatten(cnode, node.sectionNames, node.addressTypes)
            return content
        else:
            return "\n\n%s" % node.primary_title(doc["language"])

    def flatten(text, sectionNames, addressTypes):
        text = text or ""
        if len(addressTypes) == 1:
            text = [t if t else "" for t in text]
            # Bandaid for mismatch between text structure, join recursively if text
            # elements are lists instead of strings.
            return "\n".join([t if isinstance(t, str) else "\n".join(t) for t in text])
        flat = ""
        for i in range(len(text)):
            section = section_to_daf(i + 1) if addressTypes[0] == "Talmud" else str(i + 1)
            flat += "\n\n%s %s\n\n%s" % (sectionNames[0], section, flatten(text[i], sectionNames[1:], addressTypes[1:]))

        return flat

    text += index.nodes.traverse_to_string(make_node)


    return text


def make_cltk_full(doc):
    index = library.get_index(doc["title"])
    chapter = doc.get("original_text", doc["text"])
    version = Version({"chapter": chapter})
    sec_name_count = {}
    def make_node(node,depth,**kwargs):
        content = {
            "title": node.primary_title("en")
        }
        if not node.children:
            content["content"] = version.content_node(node)
            content["section_names"] = node.sectionNames
        return content

    def traverse_to_cltk(old_js, **kwargs):
        new_js = {}
        if not "nodes" in old_js:
            content_list = []
            if "content" in old_js:
                #Beginning of jagged array
                content_list = old_js["content"]
                section_names = tuple(old_js["section_names"])
                try:
                    sec_name_count[section_names] += 1
                except KeyError:
                    sec_name_count[section_names] = 1
            elif type(old_js) == list:
                #Traversing jagged array
                content_list = old_js
            for i,content in enumerate(content_list):
                if type(content) == list:
                    temp = traverse_to_cltk(content,**kwargs)
                    if len(list(temp.keys())) > 0:
                        new_js[str(i)] = temp
                elif content != "":
                    new_js[str(i)] = content

        else:
            for i,childJs in enumerate(old_js["nodes"]):
                currNode = old_js["nodes"][i]
                new_js[str(i) + "_" + currNode["title"]] = traverse_to_cltk(currNode,**kwargs)

        return new_js

    temp_doc = index.nodes.traverse_to_json(make_node)
    cltk_doc = {"text":traverse_to_cltk(temp_doc)}
    best_sec_names = []
    best_count = 0
    for sec_names in sec_name_count:
        if sec_name_count[sec_names] > best_count:
            best_count = sec_name_count[sec_names]
            best_sec_names = sec_names

    cltk_doc["meta"] = '-'.join(best_sec_names)
    cltk_doc["work"] = doc["title"]
    return json.dumps(cltk_doc, indent=4, ensure_ascii=False)


def make_cltk_flat(doc):
    index = library.get_index(doc["title"])
    chapter = doc.get("original_text", doc["text"])
    version = Version({"chapter": chapter})
    sec_name_count = {}
    def make_node(node,depth,**kwargs):
        content = {
            "title": node.primary_title("en")
        }
        if not node.children:
            content["content"] = version.content_node(node)
            content["section_names"] = node.sectionNames
        return content

    def traverse_to_cltk(old_js,title="",section_names=None, **kwargs):
        new_js = {}
        title_begin = title if title == "" else "{}, ".format(title)
        if not "nodes" in old_js:
            content_list = []
            if "content" in old_js:
                #Beginning of jagged array
                content_list = old_js["content"]
                section_names = old_js["section_names"]
                try:
                    sec_name_count[tuple(section_names)] += 1
                except KeyError:
                    sec_name_count[tuple(section_names)] = 1
            elif type(old_js) == list:
                #Traversing jagged array
                content_list = old_js
                section_names = section_names[1:]

            for i,content in enumerate(content_list):
                curr_section = "" if len(section_names) == 0 else "_{}".format(section_names[0])
                temp_title = "{}{}{}".format(title_begin,str(i),curr_section)
                if type(content) == list:
                    new_js.update(traverse_to_cltk(content,temp_title,section_names,**kwargs))
                elif content != "":
                    new_js[temp_title] = content

        else:
            for i,childJs in enumerate(old_js["nodes"]):
                curr_node = old_js["nodes"][i]
                new_js.update(traverse_to_cltk(curr_node,"{}{}_{}".format(title_begin,str(i),curr_node["title"]),**kwargs))

        return new_js

    temp_doc = index.nodes.traverse_to_json(make_node)
    cltk_doc = {"text":traverse_to_cltk(temp_doc)}
    best_sec_names = []
    best_count = 0
    for sec_names in sec_name_count:
        if sec_name_count[sec_names] > best_count:
            best_count = sec_name_count[sec_names]
            best_sec_names = sec_names

    cltk_doc["meta"] = '-'.join(best_sec_names)
    cltk_doc["work"] = doc["title"]
    return json.dumps(cltk_doc, indent=4, ensure_ascii=False)
"""
List of export formats, consisting of a name and function.
The name is used as a top level directory and file suffix, unless there are three elements.
With 3 elements, the first is the top level and the third is the file suffix
The function takes a document and returns the text to output.
"""
export_formats = (
    ('json', make_json),
    ('txt', make_text),
    ('cltk-full',make_cltk_full,'json'), #cltk format with fully nested structure
    ('cltk-flat',make_cltk_flat,'json')  #cltk format, flattened
)


def clear_exports():
    """
    Deletes all files from any export directory listed in export_formats.
    """
    for format in export_formats:
        if os.path.exists(SEFARIA_EXPORT_PATH + "/" + format[0]):
            rmtree(SEFARIA_EXPORT_PATH + "/" + format[0])
    if os.path.exists(SEFARIA_EXPORT_PATH + "/schemas"):
        rmtree(SEFARIA_EXPORT_PATH + "/schemas")
    if os.path.exists(SEFARIA_EXPORT_PATH + "/links"):
        rmtree(SEFARIA_EXPORT_PATH + "/links")

def write_text_doc_to_disk(doc=None):
    """
    Writes document to disk according to all formats in export_formats
    """
    assert doc is not None
    for format in export_formats:
        out = format[1](doc)
        if not out:
            print("Skipping %s - no content" % doc["title"])
            return
        path = make_path(doc, format[0], extension=format[2] if len(format) == 3 else None)
        if not os.path.exists(os.path.dirname(path)):
            os.makedirs(os.path.dirname(path))
        try:
            with open(path, "w") as f:
                f.write(out)
        except IOError as e:
            log_error('failed to write to disk: {}'.format(str(e)))

def prepare_text_for_export(text):
    """
    Exports 'text' (a document from the texts collection, or virtual merged document)
    by preparing it as a export document and passing to 'write_text_doc_to_disk'.
    """
    try:
        print(text["title"])
    except KeyError as e:
        log_error('text does\'t contain "title": {}'.format(str(text)))
        return

    try:
        index = library.get_index(text["title"])
    except Exception as e:
        print("Skipping %s - %s" % (text["title"], str(e)))
        return
    if any([n.is_virtual for n in index.nodes.get_leaf_nodes()]):  #skip virtual nodes
        return

    text["heTitle"] = index.nodes.primary_title("he")
    text["categories"] = index.categories

    text["text"] = text.get("text", None) or text.get("chapter", "")

    if index.is_complex():
        text["original_text"] = deepcopy(text["text"])

        def min_node_props(node, depth, **kwargs):
            js = {"heTitle": node.primary_title("he"),
                  "enTitle": node.primary_title("en"),
                  "key": node.key}

            return js

        def key2title(text_node, schema_node):
            for temp_schema_node in schema_node:
                new_key = temp_schema_node["enTitle"]
                try:
                    text_node[new_key] = text_node.pop(temp_schema_node["key"])
                except KeyError:
                    text_node[new_key] = ""

                del temp_schema_node["key"]
                if "nodes" in temp_schema_node:
                    key2title(text_node[new_key], temp_schema_node["nodes"])

        text["schema"] = index.nodes.traverse_to_json(min_node_props)
        key2title(text["text"], text["schema"]["nodes"])

    else:
        text["sectionNames"] = index.schema["sectionNames"]

    if "_id" in text:
        del text["_id"]
        del text["chapter"]

    return text


def text_is_copyright(text):
    return "license" in text and (type(text['license']) is str or type(text['license']) is str) \
           and text["license"].startswith("Copyright")


def export_texts():
    """
    Step through every text in the texts collection and export it with each format
    listed in export_formats.
    """
    clear_exports()

    texts = db.texts.find()

    for text in texts:
        if text_is_copyright(text):
            # Don't export copyrighted texts.
            continue

        prepped_text = prepare_text_for_export(text)
        if prepped_text:
            write_text_doc_to_disk(prepped_text)


def prepare_merged_text_for_export(title, lang=None):
    """
    Exports a "merged" version of title, including the maximal text we have available
    in a single document.
    """

    assert lang is not None

    doc = {
        "title": title,
        "language": lang,
        "versionTitle": "merged",
        "versionSource": "https://www.sefaria.org/%s" % title.replace(" ", "_"),
    }
    text_docs = db.texts.find({"title": title, "language": lang}).sort([["priority", -1], ["_id", 1]])

    # Exclude copyrighted docs from merging
    text_docs = [text for text in text_docs if not text_is_copyright(text)]
    print("%d versions in %s" % (len(text_docs), lang))

    if len(text_docs) == 0:
        return
    elif len(text_docs) == 1:
        text_doc = text_docs[0]
        doc["text"] = text_doc["chapter"]  # TODO: sort complex according to Index
        doc["versions"] = [(text_doc["versionTitle"], text_doc["versionSource"])]
    else:
        index = library.get_index(title)
        sourceset = set()

        def merge_visitor(node, *texts, **kwargs):
            merged, merged_sources = merge_texts(texts, kwargs.get("sources"))
            sourceset.update(merged_sources)
            return merged

        merged = index.nodes.visit_content(merge_visitor,
                                   *[text["chapter"] for text in text_docs],
                                   sources=[(text["versionTitle"], text["versionSource"]) for text in text_docs]
                                   )

        merged_sources = list(sourceset)

        doc.update({
            "text": merged,
            "versions": merged_sources,
        })

    return prepare_text_for_export(doc)


def export_all_merged():
    """
    Iterate through all index records and exports a merged text for each.
    """
    texts = db.texts.find().distinct("title")

    for title in texts:
        try:
            Ref(title)
        except:
            continue

        print(title)
        if not title:
            log_error('None title in texts')
            continue
        for lang in ("he", "en"):
            prepped_text = prepare_merged_text_for_export(title, lang=lang)
            if prepped_text:
                write_text_doc_to_disk(prepped_text)

def export_schemas():
    print('exporting schemas...')
    path = SEFARIA_EXPORT_PATH + "/schemas/"
    if not os.path.exists(path):
        os.makedirs(path)
    for i in library.all_index_records():
        title = i.title.replace(" ", "_")

        with open(path + title + ".json", "w") as f:
            try:
                f.write(make_json(i.contents()))

            except InputError as e:
                print("InputError: %s" % e)
                with open(SEFARIA_EXPORT_PATH + "/errors.log", "a") as error_log:
                    error_log.write("%s - InputError: %s\n" % (datetime.now(), e))
            except Exception as e:
                log_error('schemas error on {}: {}'.format(title, str(e)))


def export_toc():
    """
    Exports the TOC to a JSON file.
    """
    toc = library.get_toc()
    with open(SEFARIA_EXPORT_PATH + "/table_of_contents.json", "w") as f:
        f.write(make_json(toc))

def export_links():
    """
    Creates a single CSV file containing all links known to Sefaria.
    """
    print("Exporting links...")
    links_by_book = Counter()
    links_by_book_without_commentary = Counter()

    links = db.links.find().sort([["refs.0", 1]])
    path = SEFARIA_EXPORT_PATH + "/links/"
    if not os.path.exists(os.path.dirname(path)):
        os.makedirs(os.path.dirname(path))

    link_file_number = 0
    links = db.links.find().sort([["refs.0", 1]])
    new_links_file_size = 300000
    for i, link in enumerate(links):
        if i % new_links_file_size == 0:
            filename = '{}links{}.csv'.format(path, link_file_number)
            try:
                csvfile.close()
            except:
                pass
            csvfile = open(filename, 'wb')
            writer = csv.writer(csvfile)
            writer.writerow([
                    "Citation 1",
                    "Citation 2",
                    "Conection Type",
                    "Text 1",
                    "Text 2",
                    "Category 1",
                    "Category 2",
            ])
            link_file_number += 1

        try:
            oref1 = Ref(link["refs"][0])
            oref2 = Ref(link["refs"][1])
        except InputError:
            continue

        writer.writerow([
            link["refs"][0],
            link["refs"][1],
            link["type"],
            oref1.book,
            oref2.book,
            oref1.index.categories[0],
            oref2.index.categories[0],
        ])

        book_link = tuple(sorted([oref1.index.title, oref2.index.title]))
        links_by_book[book_link] += 1
        if link["type"] not in ("commentary", "Commentary", "targum", "Targum"):
            links_by_book_without_commentary[book_link] += 1

    def write_aggregate_file(counter, filename):
        with open(SEFARIA_EXPORT_PATH + "/links/%s" % filename, 'wb') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([
                "Text 1",
                "Text 2",
                "Link Count",
            ])
            for link in counter.most_common():
                writer.writerow([
                    link[0][0],
                    link[0][1],
                    link[1],
                ])

    write_aggregate_file(links_by_book, "links_by_book.csv")
    write_aggregate_file(links_by_book_without_commentary, "links_by_book_without_commentary.csv")


def export_topic_graph():
    print("Exporting topic graph...")
    counts = Counter()
    sheets = db.sheets.find()
    topics = db.sheets.distinct("topics")
    for topic in topics:
        sheets = db.sheets.find({"topics.asTyped": topic["asTyped"]})
        for sheet in sheets:
            for topic2 in sheet["topics"]:
                if topic["asTyped"] != topic2["asTyped"]:
                    counts[tuple(sorted([topic["asTyped"], topic2["asTyped"]]))] += 0.5

    path = SEFARIA_EXPORT_PATH + "/misc/"
    if not os.path.exists(path):
        os.makedirs(path)
    with open(path + "topic_graph.csv", 'wb') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([
            "Topic 1",
            "Topic 2",
            "Co-occurrence Count",
        ])
        for link in counts.most_common():
            writer.writerow([
                link[0][0],
                link[0][1],
                link[1],
            ])


def make_export_log():
    """
    Exports a file that logs the last export time.
    """
    with open(SEFARIA_EXPORT_PATH + "/last_export.txt", "w") as f:
        f.write(datetime.now().isoformat())


def export_all():
    """
    Export all texts, merged texts, links, schemas, toc, links & export log.
    """
    clear_exports()
    export_texts()
    export_all_merged()
    export_links()
    export_schemas()
    export_toc()
    export_topic_graph()
    make_export_log()
    print_errors()



# CSV Version import export format:
#
# Column 1: References
# Columns 2-n: Versions

# Row 1: Index title (will repeat?)
# Row 2: Version Title
# Row 3: Version Language
# Row 4: Version Source


def export_version_csv(index, version_list):
    assert isinstance(index, AbstractIndex)
    assert isinstance(version_list, list) or isinstance(version_list, VersionSet)
    assert all(isinstance(v, Version) for v in version_list)

    csv.field_size_limit(sys.maxsize)

    output = io.BytesIO()
    writer = csv.writer(output)

    # write header data
    writer.writerow(["Index Title"] + [index.title for _ in version_list])
    writer.writerow(["Version Title"] + [v.versionTitle for v in version_list])
    writer.writerow(["Language"] + [v.language for v in version_list])
    writer.writerow(["Version Source"] + [v.versionSource for v in version_list])
    writer.writerow(["Version Notes"] + [getattr(v, "versionNotes", "") for v in version_list])

    section_refs = index.all_section_refs()

    for section_ref in section_refs:
        segment_refs = section_ref.all_subrefs()
        seg_vers = {}

        # set blank array for version data
        for ref in segment_refs:
            seg_vers[ref.normal()] = []

        # populate each version
        for version in version_list:
            section = section_ref.text(vtitle=version.versionTitle, lang=version.language).text
            for ref in segment_refs:
                if ref.sections[-1] > len(section):
                    seg_vers[ref.normal()] += [""]
                else:
                    seg_vers[ref.normal()] += [section[ref.sections[-1] - 1]]

        # write lines for each section
        for ref in segment_refs:
            writer.writerow([ref.normal()] + seg_vers[ref.normal()])

    return output.getvalue()


def export_merged_csv(index, lang=None):
    assert isinstance(index, Index)
    assert lang in ["en", "he"]

    csv.field_size_limit(sys.maxsize)

    output = io.BytesIO()
    writer = csv.writer(output)

    # write header data
    writer.writerow(["Index Title"] + [index.title])
    writer.writerow(["Version Title"] + ["merged"])
    writer.writerow(["Language"] + [lang])
    writer.writerow(["Version Source"] + ["-"])
    writer.writerow(["Version Notes"] + ["-"])

    section_refs = index.all_section_refs()

    for section_ref in section_refs:
        segment_refs = section_ref.all_subrefs()
        seg_vers = {}

        # set blank array for version data
        for ref in segment_refs:
            seg_vers[ref.normal()] = []

        # populate each version
        section = section_ref.text(lang=lang, exclude_copyrighted=True).text
        for ref in segment_refs:
            if ref.sections[-1] > len(section):
                seg_vers[ref.normal()] += [""]
            else:
                seg_vers[ref.normal()] += [section[ref.sections[-1] - 1]]

        # write lines for each section
        for ref in segment_refs:
            writer.writerow([ref.normal()] + seg_vers[ref.normal()])

    return output.getvalue()


def import_versions_from_stream(csv_stream, columns, user_id):
    csv.field_size_limit(sys.maxsize)
    reader = csv.reader(csv_stream)
    rows = [row for row in reader]
    return _import_versions_from_csv(rows, columns, user_id)


def import_versions_from_file(csv_filename, columns):
    """
    Import the versions in the columns listed in `columns`
    :param columns: zero-based list of column numbers with a new version in them
    :return:
    """
    csv.field_size_limit(sys.maxsize)
    with open(csv_filename, 'rb') as csvfile:
        reader = csv.reader(csvfile)
        rows = [row for row in reader]
    return _import_versions_from_csv(rows, columns)


def _import_versions_from_csv(rows, columns, user_id):

    multi = str(rows[0][0]).strip().lower() == "version title"
    jobs = []  # (idx_title, vt, lang, src, notes, text_map)

    if multi:
        col = columns[0]
        vt, lang, src, notes = [rows[i][col] for i in range(4)]
        book_maps = defaultdict(dict)
        for r in rows[4:]:
             if not r or len(r) <= col:
                 continue
             ref, txt = r[0], r[col]
             book_maps[Ref(ref).index.title][ref] = txt
        for idx_title, text_map in book_maps.items():
            jobs.append((idx_title, vt, lang, src, notes, text_map))
    else:
         col = columns[-1]
         idx_title, vt, lang, src, notes = [rows[i][col] for i in range(5)]
         text_map = {r[0]: r[col] for r in rows[5:] if len(r) > col}
         jobs.append((idx_title, vt, lang, src, notes, text_map))

    for idx_title, vt, lang, src, notes, text_map in jobs:
        index = Index().load({'title': idx_title})
        if not index:
            raise InputError(f'No book with primary title "{idx_title}"')
        index_node = index.nodes

        v = Version().load({
            "title": idx_title,
            "versionTitle": vt,
            "language": lang
        })
        action = "edit"
        if v is None:
            action = "add"
            v = Version({
                "chapter": index_node.create_skeleton(),
                "title": idx_title,
                "versionTitle": vt,
                "language": lang,
                "versionSource": src,
                "versionNotes": notes,
            }).save()

        modify_bulk_text(user_id, v, text_map, type=action)
```

### sefaria/google_storage_manager.py

```
from .settings import GOOGLE_APPLICATION_CREDENTIALS_FILEPATH
from google.cloud import storage
import re
from io import BytesIO
from sefaria.site.site_settings import SITE_SETTINGS

class GoogleStorageManager(object):

    """
    Wrapper class for interacting with Google Cloud storage via Google's API classes.
    Please note that several Google exceptions (mostly subclasses of google.cloud.exceptions.GoogleAPICallError)
    or Python Exceptions if used incorrectly may be raised and that calling functions should handle them.
    https://googleapis.dev/python/google-api-core/latest/exceptions.html#google.api_core.exceptions.GoogleAPIError
    https://googleapis.dev/python/storage/latest/client.html
    https://googleapis.dev/python/storage/latest/buckets.html
    """

    COLLECTIONS_BUCKET = SITE_SETTINGS["COLLECTIONS_BUCKET"]
    PROFILES_BUCKET = SITE_SETTINGS["PROFILES_BUCKET"]
    UGC_SHEET_BUCKET = SITE_SETTINGS["UGC_BUCKET"]
    TOPICS_BUCKET = SITE_SETTINGS["TOPICS_BUCKET"]

    BASE_URL = "https://storage.googleapis.com"

    @classmethod
    def get_bucket(cls, bucket_name):
        if getattr(cls, 'client', None) is None:
            # for local development, change below line to cls.client = storage.Client(project="production-deployment")
            cls.client = storage.Client.from_service_account_json(GOOGLE_APPLICATION_CREDENTIALS_FILEPATH)
        bucket = cls.client.get_bucket(bucket_name)
        return bucket

    @classmethod
    def upload_file(cls, from_file, to_filename, bucket_name, old_filename=None):
        """
        Used to upload a file to google cloud
        :param from_file: either full path to file to upload or file-like object
        :param to_filename: filename to save in cloud. should not include folders
        :param bucket_name: name of the bucket to save the file
        """
        bucket = cls.get_bucket(bucket_name)
        if old_filename is not None:
            cls.delete_filename(old_filename, bucket_name)
        blob = bucket.blob(to_filename)
        if isinstance(from_file, str):
            blob.upload_from_filename(from_file)
        else:
            # assume file-like object
            blob.upload_from_file(from_file)
        return cls.get_url(to_filename, bucket_name)

    @classmethod
    def duplicate_file(cls, from_file, to_filename, bucket_name):
        bucket = cls.get_bucket(bucket_name)
        source_blob = bucket.blob(from_file)
        blob_copy = bucket.copy_blob(source_blob, bucket, to_filename)
        return cls.get_url(to_filename, bucket_name)


    @classmethod
    def delete_filename(cls, filename, bucket_name):
        bucket = cls.get_bucket(bucket_name)
        blob = bucket.blob(filename)
        if blob.exists():
            blob.delete()

    @classmethod
    def file_exists(cls, filename, bucket_name):
        bucket = cls.get_bucket(bucket_name)
        blob = bucket.blob(filename)
        return blob.exists()

    @classmethod
    def get_filename(cls, filename, bucket_name):
        """
        Downloads `filename` and returns a file-like object with the data
        @param filename: name of file in `bucket_name`
        @param bucket_name: name of bucket
        @return: file-like object with the data
        """
        bucket = cls.get_bucket(bucket_name)
        blob = bucket.blob(filename)
        in_memory_file = BytesIO()
        blob.download_to_file(in_memory_file)
        in_memory_file.seek(0)
        return in_memory_file

    @classmethod
    def get_url(cls, filename, bucket_name):
        return "{}/{}/{}".format(cls.BASE_URL, bucket_name, filename)

    @classmethod
    def get_filename_from_url(cls, old_file_url):
        return re.findall(r"/([^/]+)$", old_file_url)[0] if old_file_url.startswith(cls.BASE_URL) else None
```

### sefaria/local_settings.py

```
# An example of settings needed in a local_settings.py file.
# copy this file to sefaria/local_settings.py and provide local info to run.
from datetime import timedelta
import sys
import structlog
import sefaria.system.logging as sefaria_logging
import os

# These are things you need to change!

################
# YOU ONLY NEED TO CHANGE "NAME" TO THE PATH OF YOUR SQLITE DATA FILE
# If the db.sqlite file does not exist, simply list a path where it can be created.
# You can set the path to /path/to/Sefaria-Project/db.sqlite, since we git-ignore all sqlite files
# (you do not need to create the empty db.sqlite file, as Django will handle that later)
# ########################################
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.
        'NAME': '/Users/yon/projects/Sefaria-Project/db.sqlite', # Path to where you would like the database to be created including a file name, or path to an existing database file if using sqlite3.
        'USER': '',                      # Not used with sqlite3.
        'PASSWORD': '',                  # Not used with sqlite3.
        'HOST': '',                      # Set to empty string for localhost. Not used with sqlite3.
        'PORT': '',                      # Set to empty string for default. Not used with sqlite3.
    }
}
SILENCED_SYSTEM_CHECKS = ['captcha.recaptcha_test_key_error']

"""
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'name of db table here',
        'USER': 'name of db user here',
        'PASSWORD': 'password here',
        'HOST': '127.0.0.1',
        'PORT': '',
    }
}"""

# Map domain to an interface language that the domain should be pinned to.
# Leave as {} to prevent language pinning, in which case one domain can serve either Hebrew or English
DOMAIN_LANGUAGES = {
    "http://hebrew.example.org": "hebrew",
    "http://english.example.org": "english",
}


################ These are things you can change! ###########################################################################
#SILENCED_SYSTEM_CHECKS = ['captcha.recaptcha_test_key_error']

ALLOWED_HOSTS = ["localhost", "127.0.0.1","0.0.0.0"]

ADMINS = (
     ('Your Name', 'you@example.com'),
)
ADMIN_PATH = 'admin' #This will be the path to the admin site, locally it can also be 'admin'

PINNED_IPCOUNTRY = "IL" #change if you want parashat hashavua to be diaspora.

MONGO_REPLICASET_NAME = None # If the below is a list, this should be set to something other than None.
# This can be either a string of one mongo host server or a list of `host:port` string pairs. So either e.g "localhost" of ["localhost:27017","localhost2:27017" ]
MONGO_HOST = "localhost"
MONGO_PORT = 27017 # Not used if the above is a list
# Name of the MongoDB database to use.
SEFARIA_DB = 'sefaria' # Change if you named your db something else
SEFARIA_DB_USER = '' # Leave user and password blank if not using Mongo Auth
SEFARIA_DB_PASSWORD = ''
APSCHEDULER_NAME = "apscheduler"


""" These are some examples of possible caches. more here: https://docs.djangoproject.com/en/1.11/topics/cache/"""
# CACHES = {
#     "shared": {
#         'BACKEND': 'django.core.cache.backends.dummy.DummyCache',
#     },
#     "default": {
#         'BACKEND': 'django.core.cache.backends.dummy.DummyCache',
#     },
# }

"""uncomment for local cache"""
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CACHES = {
    'shared': {
        'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
        'LOCATION': f'{BASE_DIR}/sefaria/django_cache/',
    },
    'default': {
        'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
        'LOCATION': f'{BASE_DIR}/sefaria/django_cache/',
    },
    "persistent": {
        'BACKEND': 'sefaria.system.caches.SimpleMongoDBCache',
        "OPTIONS": {
            "DATABASE": SEFARIA_DB,
            "COLLECTION": "django_cache",  # default: django_cache
        },
        "TIMEOUT": None,
    },
}

SESSION_CACHE_ALIAS = "default"
USER_AGENTS_CACHE = 'default'
SHARED_DATA_CACHE_ALIAS = 'shared'

"""THIS CACHE DEFINITION IS FOR USE WITH NODE AND SERVER SIDE RENDERING"""
"""
CACHES = {
    "shared": {
        "BACKEND": "django_redis.cache.RedisCache",
        "LOCATION": "redis://127.0.0.1:6379/1", #The URI used to look like this "127.0.0.1:6379:0"
        "OPTIONS": {
            "CLIENT_CLASS": "django_redis.client.DefaultClient",
            #"SERIALIZER": "django_redis.serializers.json.JSONSerializer", #this is the default, we override it to ensure_ascii=False
            "SERIALIZER": "sefaria.system.serializers.JSONSerializer",
        },
        "TIMEOUT": None,
    },
    "default": {
        "BACKEND": "django_redis.cache.RedisCache",
        "LOCATION": "redis://127.0.0.1:6379/0", #The URI used to look like this "127.0.0.1:6379:0"
        "OPTIONS": {
            "CLIENT_CLASS": "django_redis.client.DefaultClient",
            #"PASSWORD": "secretpassword", # Optional
        },
        "TIMEOUT": 60 * 60 * 24 * 30,
    },
}
"""

SITE_PACKAGE = "sites.sefaria"




################ These are things you DO NOT NEED to touch unless you know what you are doing. ##############################
DEBUG = True
ALLOWED_HOSTS = ['localhost', '127.0.0.1', '[::1]']
OFFLINE = False
DOWN_FOR_MAINTENANCE = False
MAINTENANCE_MESSAGE = ""

# Location of Strapi CMS instance
# For local development, Strapi is located at http://localhost:1337 by default
STRAPI_LOCATION = "https://cms.sefaria.org"
STRAPI_PORT = 443


MANAGERS = ADMINS

SECRET_KEY = 'insert your long random secret key here !'


EMAIL_HOST = 'localhost'
EMAIL_PORT = 1025
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'

# Example using anymail, replaces block above
# EMAIL_BACKEND = 'anymail.backends.mandrill.EmailBackend'
# DEFAULT_FROM_EMAIL = "Sefaria <hello@sefaria.org>"
# ANYMAIL = {
#    "MANDRILL_API_KEY": "your api key",
# }


# ElasticSearch server
# URL to connect to ES server.
# Set this to https://sefaria.org/api/search to connect to production search.
# If ElasticSearch server has a password use the following format: http(s)://{username}:{password}@{base_url}
SEARCH_URL = "http://localhost:9200"

SEARCH_INDEX_ON_SAVE = False  # Whether to send texts and source sheet to Search Host for indexing after save
SEARCH_INDEX_NAME_TEXT = 'text'  # name of the ElasticSearch index to use
SEARCH_INDEX_NAME_SHEET = 'sheet'

# Node Server
USE_NODE = False
NODE_HOST = "http://localhost:4040"
NODE_TIMEOUT = 10

SEFARIA_DATA_PATH = '/path/to/your/Sefaria-Data' # used for Data
SEFARIA_EXPORT_PATH = '/path/to/your/Sefaria-Data/export' # used for exporting texts


GOOGLE_GTAG = 'G-B0LMV7FCME'
GOOGLE_TAG_MANAGER_CODE = 'you tag manager code here'

HOTJAR_ID = None

# Determine which CRM connection implementations to use
CRM_TYPE = "NONE"  # "SALESFORCE" || "NATIONBUILDER" || "NONE"

# Integration with a NationBuilder list
NATIONBUILDER_SLUG = ""
NATIONBUILDER_TOKEN = ""
NATIONBUILDER_CLIENT_ID = ""
NATIONBUILDER_CLIENT_SECRET = ""

# Integration with Salesforce
SALESFORCE_BASE_URL = ""
SALESFORCE_CLIENT_ID = ""
SALESFORCE_CLIENT_SECRET = ""

# Issue bans to Varnish on update.
USE_VARNISH = False
FRONT_END_URL = "http://localhost:8000"  # This one wants the http://
VARNISH_ADM_ADDR = "localhost:6082" # And this one doesn't
VARNISH_HOST = "localhost"
VARNISH_FRNT_PORT = 8040
VARNISH_SECRET = "/etc/varnish/secret"
# Use ESI for user box in header.
USE_VARNISH_ESI = False

# Prevent modification of Index records
DISABLE_INDEX_SAVE = False

# Turns off search autocomplete suggestions, which are reinitialized on every server reload
# which can be annoying for local development.
DISABLE_AUTOCOMPLETER = False

# Turns on loading of machine learning models to run linker
ENABLE_LINKER = True
GPU_SERVER_URL = "http://127.0.0.1:5000"

# Caching with Cloudflare
CLOUDFLARE_ZONE = ""
CLOUDFLARE_EMAIL = ""
CLOUDFLARE_TOKEN = ""

# Multiserver
MULTISERVER_ENABLED = False
MULTISERVER_REDIS_SERVER = "127.0.0.1"
MULTISERVER_REDIS_PORT = 6379
MULTISERVER_REDIS_DB = 0
MULTISERVER_REDIS_EVENT_CHANNEL = "msync"   # Message queue on Redis
MULTISERVER_REDIS_CONFIRM_CHANNEL = "mconfirm"   # Message queue on Redis


# OAUTH these fields dont need to be filled in. they are only required for oauth2client to __init__ successfully
GOOGLE_OAUTH2_CLIENT_ID = ""
GOOGLE_OAUTH2_CLIENT_SECRET = ""
# This is the field that is actually used
GOOGLE_OAUTH2_CLIENT_SECRET_FILEPATH = ""

GOOGLE_APPLICATION_CREDENTIALS_FILEPATH = "/Users/yon/google-cloud-secret/BackupManagerKey.json"

GEOIP_DATABASE = 'data/geoip/GeoLiteCity.dat'
GEOIPV6_DATABASE = 'data/geoip/GeoLiteCityv6.dat'

RAW_REF_MODEL_BY_LANG_FILEPATH = {
    "en": "/Users/yon/projects/models/en_ref_model",
    "he": "/Users/yon/projects/models/he_ref_model"
}

RAW_REF_PART_MODEL_BY_LANG_FILEPATH = {
    "en": "/Users/yon/projects/models/en_subref_model",
    "he": "/Users/yon/projects/models/he_subref_model"
}
USERID = 171118

# Simple JWT
SIMPLE_JWT = {
    'ACCESS_TOKEN_LIFETIME': timedelta(days=1),
    'REFRESH_TOKEN_LIFETIME': timedelta(days=90),
    'ROTATE_REFRESH_TOKENS': True,
    'SIGNING_KEY': 'a signing key: at least 256 bits',
}


# Celery - the following section defines variables to connect to the celery broker. This can be either redis or redis sentinel
# Either define SENTINEL_HEADLESS_URL if using sentinel or REDIS_URL for a simple redis instance
# If using sentinel, also pass any variables prefixed with SENTINEL. Otherwise, they can be left as default.
# All other variables need to be defined if connecting to either redis or redis sentinel

# Celery
REDIS_PORT = 6379
REDIS_PASSWORD = ""
CELERY_ENABLED = True
CELERY_REDIS_BROKER_DB_NUM = 0
CELERY_REDIS_RESULT_BACKEND_DB_NUM = 1
CELERY_QUEUES = {'tasks': 'web', 'llm': 'llm', 'linker': 'linker'}
SENTINEL_HEADLESS_URL = None
SENTINEL_PORT = 26379
SENTINEL_TRANSPORT_OPTS = {}
SENTINEL_PASSWORD = None
REDIS_URL = "redis://127.0.0.1"
# END Celery

# Key which identifies the Sefaria app as opposed to a user
# using our API outside of the app. Mainly for registration
MOBILE_APP_KEY = "MOBILE_APP_KEY"

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        "json_formatter": {
            "()": structlog.stdlib.ProcessorFormatter,
            "processor": structlog.processors.JSONRenderer(),
        },
    },
    'handlers': {
        'default': {
            "class": "logging.StreamHandler",
            "formatter": "json_formatter",
        },
    },
    'loggers': {
        '': {
            'handlers': ['default'],
            'propagate': False,
        },
        'django': {
            'handlers': ['default'],
            'propagate': False,
        },
        'django.request': {
            'handlers': ['default'],
            'propagate': False,
        },
    }
}

# LOG SQL CALLS
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'level': 'DEBUG',
            'class': 'logging.StreamHandler',
        },
    },
    'loggers': {
        'django': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': True,
        },
    },
}

structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.add_logger_name,
        sefaria_logging.add_severity,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.StackInfoRenderer(),
        sefaria_logging.log_exception_info,
        structlog.processors.UnicodeDecoder(),
        sefaria_logging.decompose_request_info,
        structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
    ],
    context_class=structlog.threadlocal.wrap_dict(dict),
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

SENTRY_DSN = None
CLIENT_SENTRY_DSN = None

# Fail gracefully when decorator conditional_graceful_exception on function. This should be set to True on production
# Example: If a text or ref cannot be properly loaded, fail gracefully and let the server continue to run
FAIL_GRACEFULLY = False
if "pytest" in sys.modules:
    FAIL_GRACEFULLY = False
CELERY_ENABLED = False
SLACK_URL = ''

```

### sefaria/forms.py

```
# -*- coding: utf-8 -*-
"""
Override of Django forms for new users and password reset.

Includes logic for subscribing to mailing list on register and for
"User Seeds" -- pre-creating accounts that may already be in a Group.
"""
import structlog

from django import forms
from django.contrib.auth.models import User, Group
from django.contrib.auth.forms import *
from django.utils.translation import ugettext_lazy as _

from emailusernames.forms import EmailUserCreationForm, EmailAuthenticationForm
from emailusernames.utils import get_user, user_exists
from captcha.fields import ReCaptchaField
from captcha.widgets import ReCaptchaV2Checkbox

from sefaria.helper.crm.crm_mediator import CrmMediator
from sefaria.settings import DEBUG
from sefaria.settings import MOBILE_APP_KEY
from django.utils.translation import get_language
logger = structlog.get_logger(__name__)

SEED_GROUP = "User Seeds"


class SefariaDeleteUserForm(EmailAuthenticationForm):
    email = forms.EmailField(max_length=75, widget=forms.EmailInput(attrs={'placeholder': _("Email Address to delete")}))
    password = forms.CharField(widget=forms.PasswordInput(attrs={'placeholder': _("Admin Password")}))

class SefariaDeleteSheet(forms.Form):
    sid = forms.CharField(max_length=20, widget=forms.TextInput(attrs={'placeholder': _("Sheet ID to delete")}))
    password = forms.CharField(widget=forms.PasswordInput(attrs={'placeholder': _("Admin Password")}))


class SefariaLoginForm(EmailAuthenticationForm):
    email = forms.EmailField(max_length=75, widget=forms.EmailInput(attrs={'placeholder': _("Email Address")}))
    password = forms.CharField(widget=forms.PasswordInput(attrs={'placeholder': _("Password")}))


class SefariaNewUserForm(EmailUserCreationForm):
    email = forms.EmailField(max_length=75,
                             widget=forms.EmailInput(attrs={'placeholder': _("Email Address"), 'autocomplete': 'off'}))
    first_name = forms.CharField(widget=forms.TextInput(attrs={'placeholder': _("First Name"), 'autocomplete': 'off'}))
    last_name = forms.CharField(widget=forms.TextInput(attrs={'placeholder': _("Last Name"), 'autocomplete': 'off'}))
    password1 = forms.CharField(widget=forms.PasswordInput(attrs={'placeholder': _("Password"), 'autocomplete': 'off'}))
    subscribe_educator = forms.BooleanField(label=_("I am an educator"), help_text=_("I am an educator"), initial=False,
                                            required=False)

    captcha_lang = "iw" if get_language() == 'he' else "en"
    captcha = ReCaptchaField(
        widget=ReCaptchaV2Checkbox(
            attrs={
                'data-theme': 'white'
                # 'data-size': 'compact',
            },
            # api_params={'hl': captcha_lang}
        )
    )

    class Meta:
        model = User
        fields = ("email",)

    def __init__(self, *args, **kwargs):
        super(EmailUserCreationForm, self).__init__(*args, **kwargs)
        del self.fields['password2']
        self.fields.keyOrder = ["email", "first_name", "last_name", "password1", "captcha"]
        self.fields.keyOrder.append("subscribe_educator")

    def clean_email(self):
        email = self.cleaned_data["email"]
        if user_exists(email):
            user = get_user(email)
            if not user.groups.filter(name=SEED_GROUP).exists():
                raise forms.ValidationError(_("A user with that email already exists."))
        return email

    def save(self, commit=True):
        email = self.cleaned_data["email"]
        if user_exists(email):
            # A 'User Seed' existing for this email address.
            user = get_user(email)
            user.set_password(self.cleaned_data["password1"])
            seed_group = Group.objects.get(name=SEED_GROUP)
            user.groups.remove(seed_group)
        else:
            user = super(SefariaNewUserForm, self).save(commit=False)

        user.first_name = self.cleaned_data["first_name"]
        user.last_name = self.cleaned_data["last_name"]

        if commit:
            user.save()

        try:
            crm_mediator = CrmMediator()
            crm_mediator.create_crm_user(user.email, first_name=user.first_name,
                                     last_name=user.last_name, lang=get_language(),
                                     educator=self.cleaned_data["subscribe_educator"])
        except Exception as e:
            logger.error(f"failed to add user to CRM: {e}")

        return user


class SefariaNewUserFormAPI(SefariaNewUserForm):
    mobile_app_key = forms.CharField(widget=forms.HiddenInput())

    def __init__(self, *args, **kwargs):
        super(SefariaNewUserForm, self).__init__(*args, **kwargs)
        # don't require captcha on API form
        # instead, require that the correct app_key is sent
        self.fields.pop('captcha')

    def clean_mobile_app_key(self):
        mobile_app_key = self.cleaned_data["mobile_app_key"]
        if mobile_app_key != MOBILE_APP_KEY:
            raise forms.ValidationError(_("Incorrect mobile_app_key provided"))


# TODO: Check back on me
# This class doesn't seem to be getting called at all -- it's referenced in urls.py,
# but I'm not 100% convinced anything coded here actually sends the email template outside of the django defaults (rmn)
#
class SefariaPasswordResetForm(PasswordResetForm):
    email = forms.EmailField(max_length=75,
                             widget=forms.TextInput(attrs={'placeholder': _("Email Address"), 'autocomplete': 'off'}))


class SefariaSetPasswordForm(SetPasswordForm):
    new_password1 = forms.CharField(
        label=_("New password"),
        widget=forms.PasswordInput(attrs={'placeholder': _("Enter New Password")}),
        strip=False,
        help_text=password_validation.password_validators_help_text_html(),
    )
    new_password2 = forms.CharField(
        label=_("New password confirmation"),
        strip=False,
        widget=forms.PasswordInput(attrs={'placeholder': _("Repeat New Password")}),
    )

```

### sefaria/system/logging.py

```

from structlog.processors import _figure_out_exc_info
from structlog._frames import _format_exception


def log_exception_info(logger, method_name, event_dict):
    """
    Replace an ``exc_info`` field by a ``message`` string field:
    """
    exc_info = event_dict.pop("exc_info", None)
    if exc_info:
        event_dict["message"] = _format_exception(
            _figure_out_exc_info(exc_info)
        )

    return event_dict


def decompose_request_info(logger, method_name, event_dict):
    """

    :param logger:
    :param method_name:
    :param event_dict:
    :return:
    """
    req_obj = event_dict.pop("request", None)
    if req_obj is not None:
        event_dict["httpRequest"] = {
            "requestUrl": req_obj.get_full_path(),
            "requestMethod": req_obj.method
        }
    return event_dict


def add_severity(logger, method_name, event_dict):
    """

    :param logger:
    :param method_name:
    :param event_dict:
    :return:
    """
    event_dict["severity"] = method_name

    return event_dict

```

### sefaria/system/caches.py

```
# -*- coding: utf-8 -*-


# Author:
#  Karol Sikora <karol.sikora@laboratorium.ee>, (c) 2012
#  Alireza Savand <alireza.savand@gmail.com>, (c) 2013, 2014, 2015
#  Olivier Hoareau <olivier.p.hoareau@gmail.com>, (c) 2018

# Adapted from https://github.com/Olivier-OH/django-cache-with-mongodb

from __future__ import unicode_literals, print_function

try:
    import cPickle as pickle
except ImportError:
    import pickle
import base64
import re
from datetime import datetime, timedelta
import functools

import pymongo
from django.core.exceptions import ImproperlyConfigured
from django.utils import timezone
from django.core.cache.backends.base import BaseCache, DEFAULT_TIMEOUT
from pymongo.errors import OperationFailure, ExecutionTimeout
from sefaria.system.database import db


def get_host_and_port(location):
    location = location or 'localhost:27017'
    split_value = location.split(':')
    if len(split_value) == 1:
        return split_value[0], 27017
    elif len(split_value) > 1:
        return split_value[0], split_value[1]
    else:
        return None, None


def reconnect(retries=3):
    def _decorator(f):
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            tries = 0
            while tries < retries:
                try:
                    return f(*args, **kwargs)
                except pymongo.errors.AutoReconnect:
                    tries += 1
            raise pymongo.errors.ConnectionFailure('Could not reconnect to mongodb after {} retries.'.format(retries))

        return wrapper

    return _decorator


class SimpleMongoDBCache(BaseCache):
    def __init__(self, location, params):
        options = params.get('OPTIONS', {})

        BaseCache.__init__(self, params)

        self._host, self._port = get_host_and_port(location)

        self._database = options.get('DATABASE', None)
        #self._username = options.get('USERNAME') or None
        #self._password = options.get('PASSWORD') or None

        self._collection_name = options.get('COLLECTION', None) or 'django_cache'

        if self.default_timeout is not None and self.default_timeout <= 0:
            self.default_timeout = None

    def make_key(self, key, version=None):
        """
        Additional regexp to remove $ and . characters,
        as they cause special behaviour in mongodb
        """
        key = super(SimpleMongoDBCache, self).make_key(key, version)

        return re.sub(r'\$|\.', '_', key)

    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):
        key = self.make_key(key, version)
        self.validate_key(key)

        return self._base_set('add', key, value, timeout)

    def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):
        key = self.make_key(key, version)
        self.validate_key(key)

        return self._base_set('set', key, value, timeout)

    def _base_set(self, mode, key, value, timeout=DEFAULT_TIMEOUT):
        if timeout is DEFAULT_TIMEOUT:
            timeout = self.default_timeout

        now = timezone.now()
        if timeout not in (None, -1):
            expires = now + timedelta(seconds=timeout)
        else:
            expires = None
        coll = self._get_collection()

        if mode == 'add' and self.has_key(key):
            return False

        try:
            coll.update_one(
                {'key': key},
                {'$set': {'data': value, 'expires': expires, 'last_change': now}},
                upsert=True,
            )
        # TODO: check threadsafety!
        except (OperationFailure, ExecutionTimeout) as e:
            raise e
        else:
            return True

    def get(self, key, default=None, version=None):
        coll = self._get_collection()
        key = self.make_key(key, version)
        self.validate_key(key)
        now = timezone.now()

        data = coll.find_one({
            '$and':
                [
                    {
                        'key': key
                    },
                    {'$or': [
                        {'expires': {'$gt': now}},
                        {'expires': None},
                    ]}
                ]
        })
        if not data:
            return default

        return data['data']

    def get_many(self, keys, version=None):
        coll = self._get_collection()
        now = datetime.utcnow()
        out = {}
        parsed_keys = {}
        now = timezone.now()

        for key in keys:
            pkey = self.make_key(key, version)
            self.validate_key(pkey)
            parsed_keys[pkey] = key

        data = coll.find({
            '$and':
                [
                    {
                        'key': {'$in': parsed_keys.keys()}
                    },
                    {'$or': [
                        {'expires': {'$gt': now}},
                        {'expires': None},
                    ]}
                ]
        }
        )
        for result in data:
            out[parsed_keys[result['key']]] = result['data']

        return out

    def delete(self, key, version=None):
        key = self.make_key(key, version)
        self.validate_key(key)
        coll = self._get_collection()
        if not 'capped' in self._db.command("collstats", self._collection_name):
            coll.remove({'key': key})
        else:
            coll.update_one({'key': key}, {'$set': {'expires': timezone.now()}})

    def has_key(self, key, version=None):
        coll = self._get_collection()
        key = self.make_key(key, version)
        self.validate_key(key)
        now = timezone.now()

        data = coll.find(
            {'$and':
                [
                    {'key': key},
                    {'$or': [
                        {'expires': {'$gt': now}},
                        {'expires': None},
                    ]}
                ]
            }
        )

        return data.count() > 0

    def clear(self):
        coll = self._get_collection()
        collstats = self._db.command("collstats", self._collection_name)
        if not 'capped' in collstats or not collstats['capped']:
            coll.remove({})
        else:
            coll.update({}, {'$set': {'expires': timezone.now()}})

    def _get_collection(self):
        if getattr(self, '_coll', None) is None:
            self._initialize_collection()

        return self._coll

    def _initialize_collection(self):
        self._db = db
        if self._collection_name not in self._db.list_collection_names():
            options = {}

            self._db.create_collection(self._collection_name, **options)
            collection = self._db[self._collection_name]

            if self.default_timeout is not None:
                # Create a TTL index on "expires" field
                collection.create_index(
                    [("expires", pymongo.DESCENDING), ],
                    expireAfterSeconds=0,
                )
                # Create an index on "key"/"expires" fields
                collection.create_index([('key', pymongo.ASCENDING), ('expires', pymongo.ASCENDING), ])
            collection.create_index([("key", pymongo.ASCENDING)], background=True)
        self._coll = self._db[self._collection_name]
```

### sefaria/system/validators.py

```
"""
Pre-written validation functions
Useful for validating model schemas when overriding AbstractMongoRecord._validate()
"""

import urllib.parse
from urllib.parse import urlparse
from django.core.validators import URLValidator
from django.core.exceptions import ValidationError
from sefaria.system.exceptions import SchemaValidationException, SchemaInvalidKeyException, SchemaRequiredFieldException\
    , InvalidHTTPMethodException, InvalidURLException


def validate_url(url):
    try:
        # Attempt to parse the URL
        validator = URLValidator()
        validator(url)
        return True

    except ValidationError:
        # URL parsing failed
        raise InvalidURLException(url)


def validate_http_method(method):
    """
    Validate if a string represents a valid HTTP API method.

    Args:
        method (str): The HTTP method to validate.

    Raises:
        InvalidHTTPMethodException: If the method is not valid.

    Returns:
        bool: True if the method is valid, False otherwise.
    """
    valid_methods = ["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS", "HEAD"]

    # Convert the method to uppercase and check if it's in the list of valid methods
    if method.upper() in valid_methods:
        return True
    else:
        raise InvalidHTTPMethodException(method)

```

### sefaria/system/database.py

```
"""
database.py -- connection to MongoDB
The system attribute _called_from_test is set in the py.test conftest.py file
"""
import sys
import pymongo
import urllib.parse
from pymongo.errors import OperationFailure

from sefaria.settings import *

def check_db_exists(db_name):
    dbnames = client.list_database_names()
    return db_name in dbnames


def connect_to_db(db_name):
    if not check_db_exists(db_name):
        raise SystemError(f'Database {db_name} does not exist!')
    return client[db_name]

def get_test_db():
    return client[TEST_DB]


if hasattr(sys, '_doc_build'):
    db = ""
else:
    # TEST_DB = SEFARIA_DB + "_test"
    TEST_DB = SEFARIA_DB
    
    #If we have jsut a single instance mongo (such as for development) the MONGO_HOST param should contain jsut the host string e.g "localhost")
    if MONGO_REPLICASET_NAME is None:
        if SEFARIA_DB_USER and SEFARIA_DB_PASSWORD:
            client = pymongo.MongoClient(MONGO_HOST, MONGO_PORT, username=SEFARIA_DB_USER, password=SEFARIA_DB_PASSWORD)
        else:
            client = pymongo.MongoClient(MONGO_HOST, MONGO_PORT)
    #Else if we are using a replica set mongo, we need to connect with a URI that containts a comma separated list of 'host:port' strings
    else:
        if SEFARIA_DB_USER and SEFARIA_DB_PASSWORD:
            # and also escape user/pass
            username = urllib.parse.quote_plus(SEFARIA_DB_USER)
            password = urllib.parse.quote_plus(SEFARIA_DB_PASSWORD)
            connection_uri = 'mongodb://{}:{}@{}/?ssl=false&readPreference=primaryPreferred&replicaSet={}'.format(username, password, MONGO_HOST, MONGO_REPLICASET_NAME)
        else:
            connection_uri = 'mongodb://{}/?ssl=false&readPreference=primaryPreferred&replicaSet={}'.format(MONGO_HOST, MONGO_REPLICASET_NAME)
        # Now connect to the mongo server
        client = pymongo.MongoClient(connection_uri)



    # Now set the db variable to point to the Sefaria database in the server
    if not hasattr(sys, '_called_from_test'):
        db = connect_to_db(SEFARIA_DB)
    else:
        db = connect_to_db(TEST_DB)



def drop_test():
    global client
    client.drop_database(TEST_DB)


# Not used
# def refresh_test():
#     global client
#     drop_test()
#     # copydb deprecated in 4.2.  https://docs.mongodb.com/v4.0/release-notes/4.0-compatibility/#deprecate-copydb-clone-cmds
#     client.admin.command('copydb',
#                          fromdb=SEFARIA_DB,
#                          todb=TEST_DB)


def ensure_indices(active_db=None):
    active_db = active_db or db
    indices = [
        ('following', ["follower"],{}),
        ('following', ["followee"],{}),
        ('groups', ["name"], {}),
        ('groups', ["sheets"], {}),
        ('groups', ["slug"], {'unique': True}),
        ('groups', ["privateSlug"], {'unique': True}),
        ('groups', ["members"], {}),
        ('groups', ["admins"], {}),
        ('history', ["revision"],{}),
        ('history', ["method"],{}),
        ('history', [[("ref", pymongo.ASCENDING), ("version", pymongo.ASCENDING), ("language", pymongo.ASCENDING)]],{}),
        ('history', ["date"],{}),
        ('history', ["ref"],{}),
        ('history', ["user"],{}),
        ('history', ["rev_type"],{}),
        ('history', ["version"],{}),
        ('history', ["new.refs"],{}),
        ('history', ["new.ref"],{}),
        ('history', ["old.refs"],{}),
        ('history', ["old.ref"],{}),
        ('history', ["title"],{}),
        ('index', ["title"],{}),
        ('index_queue', [[("lang", pymongo.ASCENDING), ("version", pymongo.ASCENDING), ("ref", pymongo.ASCENDING)]],{'unique': True}),
        ('index', ["categories.0"], {}),
        ('index', ["order.0"], {}),
        ('index', ["order.1"], {}),
        # ('links', [[("refs.0",  1), ("refs.1", 1)]], {"unique": True}),
        ('links', [[("refs", pymongo.ASCENDING), ("generated_by", pymongo.ASCENDING)]],{}),
        ('links', ["refs.0"],{}),
        ('links', ["refs.1"],{}),
        ('links', ["expandedRefs0"],{}),
        ('links', ["expandedRefs1"],{}),
        ('links', ["source_text_oid"],{}),
        ('links', ["is_first_comment"],{}),
        ('links', ["inline_citation"],{}),
        ('metrics', ["timestamp"], {'unique': True}),
        ('media', ["ref.sefaria_ref"], {}),
        ('notes', [[("owner", pymongo.ASCENDING), ("ref", pymongo.ASCENDING), ("public", pymongo.ASCENDING)]],{}),
        ('notifications', [[("uid", pymongo.ASCENDING), ("read", pymongo.ASCENDING)]],{}),
        ('notifications', ["uid"],{}),
        ('notifications', ["content.sheet_id"], {}),
        ('parshiot', ["date"],{}),
        ('place', [[("point", pymongo.GEOSPHERE)]],{}),
        ('place', [[("area", pymongo.GEOSPHERE)]],{}),
        ('place', ["key"],{}),
        ('person', ["key"],{}),
        ('profiles', ["slug"],{}),
        ('profiles', ["id"],{}),
        ('sheets', ["id"],{}),
        ('sheets', ["dateModified"],{}),
        ('sheets', ["sources.ref"],{}),
        ('sheets', ["includedRefs"],{}),
        ('sheets', ["expandedRefs"], {}),
        ('sheets', ["tags"],{}),
        ('sheets', ["owner"],{}),
        ('sheets', ["assignment_id"],{}),
        ('sheets', ["is_featured"],{}),
        ('sheets', ["displayedCollection"], {}),
        ('sheets', ["sheetLanguage"], {}),
        ('sheets', [[("views", pymongo.DESCENDING)]],{}),
        ('sheets', ["categories"], {}),
        ('links', [[("owner", pymongo.ASCENDING), ("date_modified", pymongo.DESCENDING)]], {}),
        ('texts', ["title"],{}),
        ('texts', [[("priority", pymongo.DESCENDING), ("_id", pymongo.ASCENDING)]],{}),
        ('texts', [[("versionTitle", pymongo.ASCENDING), ("langauge", pymongo.ASCENDING)]],{}),
        ('texts', ["actualLanguage"], {}),
        ('topics', ["titles.text"], {}),
        ('topic_links', ["class"], {}),
        ('topic_links', ["expandedRefs"], {}),
        ('topic_links', ["toTopic"], {}),
        ('topic_links', ["fromTopic"], {}),
        ('word_form', ["form"],{}),
        ('word_form', ["c_form"],{}),
        ('word_form', ["refs"], {}),
        ('term', ["titles.text"], {'unique': True}),
        ('term', ["category"],{}),
        ('lexicon_entry', [[("headword", pymongo.ASCENDING), ("parent_lexicon", pymongo.ASCENDING)]],{}),
        ('user_story', ["uid"],{}),
        ('user_story', [[("uid", pymongo.ASCENDING), ("timestamp", pymongo.DESCENDING)]],{}),
        ('user_story', [[("timestamp", pymongo.DESCENDING)]],{}),
        ('passage', ["ref_list"],{}),
        ('user_history', ["uid"],{}),
        ('user_history', ["sheet_id"],{}),
        ('user_history', ["datetime"],{}),
        ('user_history', ["ref"], {}),
        ('user_history', [[("time_stamp", pymongo.DESCENDING)]], {}),
        ('user_history', [[("uid", pymongo.ASCENDING), ("server_time_stamp", pymongo.ASCENDING)]], {}),
        ('user_history', [[("uid", pymongo.ASCENDING), ("saved", pymongo.ASCENDING)]], {}),
        ('user_history', [[("uid", pymongo.ASCENDING), ("ref", pymongo.ASCENDING)]], {}),
        ('user_history', [[("uid", pymongo.ASCENDING), ("book", pymongo.ASCENDING), ("last_place", pymongo.ASCENDING)]], {}),
        ('user_history', [[("uid", pymongo.ASCENDING), ("secondary", pymongo.ASCENDING), ("last_place", pymongo.ASCENDING), ("time_stamp", pymongo.ASCENDING)]], {}),
        ('user_history', [[("uid", pymongo.ASCENDING), ("secondary", pymongo.ASCENDING), ("time_stamp", pymongo.ASCENDING)]], {}),
        ('trend', ["name"],{}),
        ('trend', ["uid"],{}),
        ('webpages', ["refs"],{}),
        ('webpages', ["expandedRefs"],{}),
        ('websites', ["domains"], {}),
        ('manuscript_pages', ['expanded_refs'], {}),
        ('manuscript_pages', [[("manuscript_slug", pymongo.ASCENDING), ("page_id", pymongo.ASCENDING)]], {'unique': True}),
        ('manuscripts', ['slug'], {}),
        ('manuscripts', ['title'], {}),
        ('messages', [[("room_id", pymongo.ASCENDING), ("timestamp", pymongo.DESCENDING)]], {}),
        ('vstate', ["title"], {}),
        ('vstate', ["flags.enComplete"], {}),
        ('guide', ["expanded_refs"], {}),
        ('topics', ["parasha"], {}),

    ]

    for col, args, kwargs in indices:
        try:
            getattr(active_db, col).create_index(*args, **kwargs)
        except OperationFailure as e:
            print("Collection: {}, args: {}, kwargs: {}\n{}".format(col, args, kwargs, e))

```

### sefaria/system/serializers.py

```
import json

from django.core.serializers.json import DjangoJSONEncoder


class BaseSerializer:
    def __init__(self, options):
        pass

    def dumps(self, value):
        raise NotImplementedError

    def loads(self, value):
        raise NotImplementedError


class JSONSerializer(BaseSerializer):
    def dumps(self, value):
        return json.dumps(value, cls=DjangoJSONEncoder, ensure_ascii=False).encode()

    def loads(self, value):
        return json.loads(value.decode())



```

### sefaria/system/tests/test_database.py

```

import sefaria.system.database as d
import pymongo
import sefaria.model.lock as lock
from sefaria.settings import *
import pytest

#This one is purposefully circumvented on Travis, to speed up build time.
@pytest.mark.xfail(reason="unknown")
def test_db_name():
    assert d.db.name == d.TEST_DB

#todo: why failing?
@pytest.mark.xfail(reason="unknown")
def test_test_db():
    """
    Create a record using the sefaria API against the test db, and then verify it from a new db connection
    """

    ref = "Mishnah Oktzin 1:5"
    lang = "en"
    version = "Sefaria Community Translation"
    user = 0

    lock_query = {
        "ref": ref,
        "lang": lang,
        "version": version
    }

    lock.release_lock(ref, lang, version)
    lock.set_lock(ref, lang, version, user)
    test_db = get_test_connection()
    assert test_db.locks.find_one(lock_query)
    lock.release_lock(ref, lang, version)
    assert not test_db.locks.find_one(lock_query)


def get_test_connection():
    connection = pymongo.Connection(MONGO_HOST)
    db = connection[d.TEST_DB]
    if SEFARIA_DB_USER and SEFARIA_DB_PASSWORD:
        db.authenticate(SEFARIA_DB_USER, SEFARIA_DB_PASSWORD)
    return db
```

### sefaria/system/tests/test_decorators.py

```
import base64
import pytest
import json

import sefaria.system.decorators as d
import sefaria.system.exceptions as e
from django.http import HttpResponse
from django.test import RequestFactory
from django.contrib.auth.models import AnonymousUser
from sefaria.decorators import webhook_auth_or_staff_required
from django.conf import settings

@d.catch_error_as_json
def call_user_error():
    return raise_user_error()


def raise_user_error():
    raise e.InputError("You really shouldn't do that")


@d.catch_error_as_json
def call_exception():
    return raise_exception()


def raise_exception():
    raise Exception("System Error!")

@pytest.mark.xfail(reason="unknown")
def test_catch_error():
    httpr = call_user_error()
    assert getattr(httpr, "content")
    r = json.loads(httpr.content)
    assert "error" in r
    assert r["error"] == "You really shouldn't do that"


def test_pass_exception():
    with pytest.raises(Exception):
        r = call_exception()

### Test Decorator for Webhook Auth ###

# Constants for testing
VALID_USERNAME = "webhookuser1"
VALID_PASSWORD = "supersecret"

@pytest.fixture
def rf():
    return RequestFactory()

@pytest.fixture(autouse=True)
def patch_credentials():
    settings.WEBHOOK_USERNAME = VALID_USERNAME
    settings.WEBHOOK_PASSWORD = VALID_PASSWORD

def dummy_view(request, *args, **kwargs):
    return HttpResponse("Success")

def get_basic_auth_header(username: str, password: str) -> dict:
    credentials = f"{username}:{password}"
    # Encode credentials to base64, b64encode expects bytes so we encode the string first
    encoded = base64.b64encode(credentials.encode()).decode()
    return {"HTTP_AUTHORIZATION": f"Basic {encoded}"}


def test_valid_basic_auth(rf):
    request = rf.get("/fake-endpoint/")
    request.META.update(get_basic_auth_header(VALID_USERNAME, VALID_PASSWORD))
    request.user = AnonymousUser()

    wrapped = webhook_auth_or_staff_required(dummy_view)
    response = wrapped(request)
    assert response.status_code == 200
    assert response.content == b"Success"


def test_invalid_basic_auth(rf):
    request = rf.get("/fake-endpoint/")
    request.META.update(get_basic_auth_header("wrong", "creds"))
    request.user = AnonymousUser()

    wrapped = webhook_auth_or_staff_required(dummy_view)
    response = wrapped(request)
    assert response.status_code == 401
    assert b"Invalid credentials" in response.content


def test_missing_auth_header_anonymous_user(rf):
    request = rf.get("/fake-endpoint/")
    request.user = AnonymousUser()

    wrapped = webhook_auth_or_staff_required(dummy_view)
    response = wrapped(request)
    assert response.status_code  == 302


def test_valid_staff_user(rf, django_user_model):
    user = django_user_model.objects.create_user(username="staff", password="pass", is_staff=True)
    request = rf.get("/fake-endpoint/")
    request.user = user

    wrapped = webhook_auth_or_staff_required(dummy_view)
    response = wrapped(request)
    assert response.status_code == 200
    assert response.content == b"Success"


def test_nonstaff_user_without_auth(rf, django_user_model):
    user = django_user_model.objects.create_user(username="nonstaff", password="pass", is_staff=False)
    request = rf.get("/fake-endpoint/")
    request.user = user

    wrapped = webhook_auth_or_staff_required(dummy_view)
    response = wrapped(request)
    assert response.status_code == 302

```

### sefaria/system/tests/test_varnish.py

```
from sefaria.settings import USE_VARNISH

if USE_VARNISH:
    import sefaria.system.varnish.wrapper as v
    from sefaria.model import Ref

    class Test_Varnish(object):

        def test_url_regex(self):
            if USE_VARNISH:
                assert v.url_regex(Ref("Exodus 15")) == r'Exodus(\\.15$|\\.15\\.)'
                assert v.url_regex(Ref("Exodus 15:15-17")) == r'Exodus(\\.15\\.15$|\\.15\\.15\\.|\\.15\\.16$|\\.15\\.16\\.|\\.15\\.17$|\\.15\\.17\\.)'
                assert v.url_regex(Ref("Yoma 14a")) == r'Yoma(\\.14a$|\\.14a\\.)'
                assert v.url_regex(Ref("Yoma 14a:12-15")) == r'Yoma(\\.14a\\.12$|\\.14a\\.12\\.|\\.14a\\.13$|\\.14a\\.13\\.|\\.14a\\.14$|\\.14a\\.14\\.|\\.14a\\.15$|\\.14a\\.15\\.)'
                assert v.url_regex(Ref("Yoma")) == r'Yoma($|\\.)'
                assert v.url_regex(Ref("Rashi on Genesis 1.1")) == r'Rashi\\_on\\_Genesis(\\.1\\.1$|\\.1\\.1\\.)'
```

### sefaria/system/cache.py

```

import hashlib
import sys
from datetime import datetime
from functools import wraps

from django.http import HttpRequest
from django.core.cache import DEFAULT_CACHE_ALIAS

from sefaria import settings

import structlog
logger = structlog.get_logger(__name__)

if not hasattr(sys, '_doc_build'):
    from django.core.cache import cache
    from django.core.cache import caches

SHARED_DATA_CACHE_ALIAS = getattr(settings, 'SHARED_DATA_CACHE_ALIAS', DEFAULT_CACHE_ALIAS)
LONG_TERM_CACHE_ALIAS = getattr(settings, 'LONG_TERM_CACHE_ALIAS', DEFAULT_CACHE_ALIAS)

#functions from here: http://james.lin.net.nz/2011/09/08/python-decorator-caching-your-functions/
#and here: https://github.com/rchrd2/django-cache-decorator

# New cache instance reconnect-apparently


def get_cache_factory(cache_type):
    if cache_type is None:
        cache_type = 'default'
    return caches[cache_type]


#get the cache key for storage
def cache_get_key_arr(*args, **kwargs):
    args_key_array = []
    for arg in args:
        args_key_array.append(str(arg))
    for key,arg in sorted(list(kwargs.items()), key=lambda x: x[0]):
        args_key_array.append(str(key))
        args_key_array.append(str(arg))
    return args_key_array


def cache_get_key(key_arr):
    return hashlib.md5("".join(key_arr).encode('utf-8')).hexdigest()


def django_cache(action="get", timeout=None, cache_key='', cache_prefix=None, default_on_miss=False, default_on_miss_value=None, cache_type=None, decorate_data_with_key=False):
    """
    Easily add caching to a function in django
    """
    if not cache_key:
        cache_key = None

    def decorator(fn):
        fn.__dict__["django_cache"] = True
        @wraps(fn)
        def wrapper(*args, **kwargs):
            #logger.debug([args, kwargs])

            # Inner scope variables are read-only so we set a new var
            _cache_key = cache_key
            do_actual_func = False

            if not _cache_key:
                cachekey_args = args[:]
                if len(cachekey_args) and isinstance(cachekey_args[0], HttpRequest): # we dont want a HttpRequest to form part of the cache key, it wont be replicatable.
                    cachekey_args = cachekey_args[1:]
                _cache_ke_arr = cache_get_key_arr(cache_prefix if cache_prefix else fn.__name__, *cachekey_args, **kwargs)
                _cache_key = cache_get_key(_cache_ke_arr)

            if action in ["reset", "set"]:
                do_actual_func = True
                """try:
                    delete_cache_elem(_cache_key, cache_type=cache_type)
                except:
                    pass"""
                result = None
            else:
                #logger.debug(['_cach_key.......',_cache_key])
                result = get_cache_elem(_cache_key, cache_type=cache_type)
                if decorate_data_with_key:
                    result = result["data"]

            if not result:
                if default_on_miss is False or do_actual_func:
                    result = fn(*args, **kwargs)
                    if decorate_data_with_key:
                        result = {
                            'key': "_".join(_cache_ke_arr),
                            'data': result
                        }
                    set_cache_elem(_cache_key, result, timeout=timeout, cache_type=cache_type)
                else:
                    result = default_on_miss_value
                    logger.critical("No cached data was found for {}".format(fn.__name__))

            return result
        return wrapper
    return decorator
#-------------------------------------------------------------#


def get_cache_elem(key, cache_type=None):
    cache_instance = get_cache_factory(cache_type)
    return cache_instance.get(key)


def get_shared_cache_elem(key):
    return get_cache_elem(key, cache_type=SHARED_DATA_CACHE_ALIAS)


def set_cache_elem(key, value, timeout = None, cache_type=None):
    cache_instance = get_cache_factory(cache_type)
    return cache_instance.set(key, value, timeout)


def set_shared_cache_elem(key, value, timeout=None):
    return set_cache_elem(key, value, timeout, cache_type=SHARED_DATA_CACHE_ALIAS)


def delete_cache_elem(key, cache_type=None):
    cache_instance = get_cache_factory(cache_type)
    if isinstance(key, (list, tuple)):
        try:
            return cache_instance.delete_many(key)
        except (AttributeError, NameError, TypeError):
            retval = False
            for k in key:
                retval = retval or cache_instance.delete(k)
            return retval
    return cache_instance.delete(key)


def delete_shared_cache_elem(key):
    return delete_cache_elem(key, cache_type=SHARED_DATA_CACHE_ALIAS)


def get_template_cache(fragment_name='', *args):
    cache_key = 'template.cache.%s.%s' % (fragment_name, hashlib.md5(':'.join([arg for arg in args]).encode('utf-8')).hexdigest())
    return get_cache_elem(cache_key)


def delete_template_cache(fragment_name='', *args):
    delete_cache_elem('template.cache.%s.%s' % (fragment_name, hashlib.md5(':'.join([arg for arg in args]).encode('utf-8')).hexdigest()))


class InMemoryCache():
    data = {}
    timeouts = {}

    def set(self, key, val, timeout=None):
        self.data[key] = val
        if timeout:
            self.timeouts[key] = (timeout, datetime.now().timestamp())

    def get(self, key):
        timeout = self.timeouts.get(key, None)
        if timeout and timeout[0] + timeout[1] < datetime.now().timestamp():
            self.set(key, None, timeout=timeout[0])
            return None

        return self.data.get(key,  None)

    def reset_all(self):
        for k in self.data:
            self.data[k] = None


in_memory_cache = InMemoryCache()
```

### sefaria/system/__init__.py

```

```

### sefaria/system/cloudflare.py

```
import requests
import json
from django.contrib.sites.models import Site

from sefaria.settings import CLOUDFLARE_ZONE, CLOUDFLARE_EMAIL, CLOUDFLARE_TOKEN, USE_CLOUDFLARE, STATICFILES_DIRS
from sefaria.utils.util import list_chunks, in_directory, get_directory_content

import structlog
logger = structlog.get_logger(__name__)


class SefariaCloudflareManager(object):

    valid_cached_dirs = ["static"]
    max_cloudflare_payload_size = 30

    def purge_cloudflare_url(self, path, preprocessed=False):
        """ Calls the Cloudflare API to invalidate cache for the file at current site and `path`"""
        return self.purge_multiple_cloudflare_urls([path], preprocessed=preprocessed)

    def purge_cloudflare(self):
        """Purge the entire Cloudflare cache"""
        import requests
        import json

        url = 'https://api.cloudflare.com/client/v4/zones/%s/purge_cache' % CLOUDFLARE_ZONE
        headers = {
            "X-Auth-Email": CLOUDFLARE_EMAIL,
            "X-Auth-Key": CLOUDFLARE_TOKEN,
            "Content-Type": "application/json",
        }
        r = requests.delete(url, data=json.dumps({"purge_everything":True}), headers=headers)
        logger.info(r.json())

    def _file_in_cached_dirs(self, file):
        return any(in_directory(file, dirname) for dirname in self.valid_cached_dirs)

    def _filter_files_payload(self, files):
        return [item for item in files if self._file_in_cached_dirs(item)]

    def purge_batch_cloudflare_urls(self, files, filter_cached_dirs=True, preprocessed=False):
        """
        Calls the Cloudflare API to invalidate cache for multiple input files.
        Splits into required length of cloudflare max files. Makes sure urls are valid at current site and `path`
        """
        if filter_cached_dirs:
            files = self._filter_files_payload(files)
        files_chunks = list_chunks(files, self.max_cloudflare_payload_size)
        for chunk in files_chunks:
            self.purge_multiple_cloudflare_urls(chunk, preprocessed=preprocessed)

    def purge_static_files_from_cloudflare(self, timestamp=None):
        files_to_purge = []
        for dirname in self.valid_cached_dirs:
            files_to_purge += get_directory_content(dirname, modified_after=timestamp)
        self.purge_batch_cloudflare_urls(files_to_purge, filter_cached_dirs=False, preprocessed=False)

    def purge_multiple_cloudflare_urls(self, files, preprocessed=False):
        """ Calls the Cloudflare API to invalidate cache for the given files"""
        if len(files) > self.max_cloudflare_payload_size:
            logger.error("Too many files to purge {}".format(files))
            raise ValueError("Too many files passed to purge.")
        if not preprocessed:
            current_site = Site.objects.get_current()
            domain = current_site.domain
            files = ["https://{}/{}".format(domain, path) for path in files]
        url = 'https://api.cloudflare.com/client/v4/zones/%s/purge_cache' % CLOUDFLARE_ZONE
        logger.info("About to purge: {}".format(files))
        payload = {"files": files}
        headers = {
            "X-Auth-Email": CLOUDFLARE_EMAIL,
            "X-Auth-Key": CLOUDFLARE_TOKEN,
            "Content-Type": "application/json",
        }
        r = requests.delete(url, data=json.dumps(payload), headers=headers)
        r = r.json()
        if not r["success"]:
            logger.warn(r)
        else:
            logger.info(r)

```

### sefaria/system/varnish/wrapper.py

```
# Varnish wrapper used by web server.
# There is also a parallel file thin_wrapper.py, which does not rely on core code - used for the multiserver monitor.

import re
import urllib.request, urllib.parse, urllib.error

from .common import ban_url, purge_url, FRONT_END_URL
from sefaria.model import *
from sefaria.system.exceptions import InputError
from sefaria.utils.util import graceful_exception


import structlog
logger = structlog.get_logger(__name__)


@graceful_exception(logger=logger, return_value=None)
def invalidate_ref(oref, lang=None, version=None, purge=False):
    """
    Called when 'ref' is changed.
    We aim to PURGE the main page, so that the results of any save will be immediately visible to the person editing.
    All other implications are handled with a blanket BAN.

    todo: Tune this so as not to ban when the version changed is not a displayed version
    """
    if not isinstance(oref, Ref):
        return
    
    if getattr(oref.index_node, "depth", False) and len(oref.sections) >= oref.index_node.depth - 1:
        oref = oref.section_ref()

    if version:
        version = urllib.parse.quote(version.replace(" ", "_").encode("utf-8"))
    if purge:
        # Purge this section level ref, so that immediate responses will return good results
        purge_url("{}/api/texts/{}".format(FRONT_END_URL, oref.url()))
        if version and lang:
            try:
                purge_url("{}/api/texts/{}/{}/{}".format(FRONT_END_URL, oref.url(), lang, version))
            except Exception as e:
                logger.exception(e)
        # Hacky to add these
        purge_url("{}/api/texts/{}?commentary=1&sheets=1".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/texts/{}?sheets=1".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/texts/{}?commentary=0".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/texts/{}?commentary=0&pad=0".format(FRONT_END_URL, oref.url()))
        if version and lang:
            try:
                purge_url("{}/api/texts/{}/{}/{}?commentary=0".format(FRONT_END_URL, oref.url(), lang, version))
            except Exception as e:
                logger.exception(e)
        purge_url("{}/api/links/{}".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/links/{}?with_text=0".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/links/{}?with_text=1".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/related/{}".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/related/{}?with_sheet_links=1".format(FRONT_END_URL, oref.url()))
        purge_url("{}/api/related/{}?with_sheet_links=0".format(FRONT_END_URL, oref.url()))

    # Ban anything underneath this section
    ban_url("/api/texts/{}".format(url_regex(oref)))
    ban_url("/api/links/{}".format(url_regex(oref)))
    ban_url("/api/related/{}".format(url_regex(oref)))


def invalidate_linked(oref):
    for linkref in {r.section_ref() for r in oref.linkset().refs_from(oref)}:
        try:
            invalidate_ref(linkref)
        except UnicodeDecodeError:
            logger.warn("Unable to invalidate {}. We cannot invalidate unicode at this time".format(linkref.normal()))


@graceful_exception(logger=logger, return_value=None, exception_type=UnicodeDecodeError)
def invalidate_counts(indx):
    if isinstance(indx, Index):
        oref = Ref(indx.title)
        url = oref.url()
    elif isinstance(indx, str):
        url = indx.replace(" ", "_").replace(":", ".")
    else:
        logger.warn("Could not parse index '{}' to purge counts from Varnish.".format(indx))
        return

    purge_url("{}/api/preview/{}".format(FRONT_END_URL, url))
    purge_url("{}/api/counts/{}".format(FRONT_END_URL, url))
    purge_url("{}/api/v2/index/{}?with_content_counts=1".format(FRONT_END_URL, url))

    # Assume this is unnecesary, given that the specific URLs will have been purged/banned by the save action
    # oref = Ref(indx.title)
    # invalidate_ref(oref)


@graceful_exception(logger=logger, return_value=None)
def invalidate_index(indx):
    if isinstance(indx, Index):
        try:
            oref = Ref(indx.title)
            url = oref.url()
        except InputError as e:
            logger.warn("In sf.varnish.invalidate_index(): failed to instantiate ref for index name: {}".format(indx.title))
            return
    elif isinstance(indx, str):
        url = indx.replace(" ", "_").replace(":", ".")
    else:
        logger.warn("Could not parse index '{}' to purge from Varnish.".format(indx))
        return

    purge_url("{}/api/index/{}".format(FRONT_END_URL, url))
    purge_url("{}/api/v2/raw/index/{}".format(FRONT_END_URL, url))
    purge_url("{}/api/v2/index/{}".format(FRONT_END_URL, url))
    purge_url("{}/api/v2/index/{}?with_content_counts=1".format(FRONT_END_URL, url))


@graceful_exception(logger=logger, return_value=None)
def invalidate_title(title):
    title = title.replace(" ", "_").replace(":", ".")
    invalidate_index(title)
    invalidate_counts(title)
    ban_url("/api/texts/{}".format(title))
    ban_url("/api/links/{}".format(title))


def invalidate_all():
    ban_url(".*")


def url_regex(ref):
    """
    :return string: Returns a non-anchored regular expression part that will match normally formed URLs of this Ref and any more specific Ref.
    E.g., "Genesis 1" yields an RE that match "Genesis.1" and "Genesis.1.3"
    Result is hyper slashed, as Varnish likes.
    """

    assert isinstance(ref, Ref)

    patterns = []

    if ref.is_range():
        if ref.is_spanning():
            s_refs = ref.split_spanning_ref()
            normals = []
            for s_ref in s_refs:
                normals += [r.normal() for r in s_ref.range_list()]
        else:
            normals = [r.normal() for r in ref.range_list()]

        for r in normals:
            sections = re.sub("^%s" % re.escape(ref.book), '', r).replace(":", r"\\.").replace(" ", r"\\.")
            patterns.append("%s$" % sections)   # exact match
            patterns.append(r"%s\\?" % sections) # Exact match with '?' afterwards
            patterns.append(r"%s\\/" % sections) # Exact match with '/' afterwards
            patterns.append(r"%s\\." % sections)   # more granualar, exact match followed by .
    else:
        sections = re.sub("^%s" % re.escape(ref.book), '', ref.normal()).replace(":", r"\\.").replace(" ", r"\\.")
        patterns.append("%s$" % sections)   # exact match
        patterns.append(r"%s\\?" % sections)  # Exact match with '?' afterwards
        patterns.append(r"%s\\/" % sections) # Exact match with '/' afterwards
        if ref.index_node.has_titled_continuation():
            patterns.append("{}({}).".format(sections, "|".join([s.replace(" ","_") for s in ref.index_node.title_separators])))

        elif ref.index_node.has_numeric_continuation():
            patterns.append(r"%s\\." % sections)   # more granualar, exact match followed by .

    return r"%s(%s)" % (re.escape(ref.book).replace(" ","_").replace("\\", "\\\\"), "|".join(patterns))



```

### sefaria/system/varnish/__init__.py

```

```

### sefaria/system/varnish/common.py

```
import subprocess
from urllib.parse import urlparse
from http.client import HTTPConnection
from sefaria.settings import VARNISH_ADM_ADDR, VARNISH_HOST, VARNISH_FRNT_PORT, VARNISH_SECRET, FRONT_END_URL

from sefaria.utils.util import graceful_exception

import structlog
logger = structlog.get_logger(__name__)


@graceful_exception(logger=logger, return_value=None)
def ban_url(url):
    args = ["varnishadm", "-T", VARNISH_ADM_ADDR, "-S", VARNISH_SECRET, "ban", "obj.http.url ~ {}".format(url)]
    subprocess.run(args, check=True)


@graceful_exception(logger=logger, return_value=None)
def purge_url(url):
    """
    Do an HTTP PURGE of the given asset.
    The URL is run through urlparse and must point to the varnish instance not the varnishadm
    """
    url = urlparse(url)
    connection = HTTPConnection(VARNISH_HOST, VARNISH_FRNT_PORT)
    path = url.path or '/'
    connection.request('PURGE', '%s?%s' % (path, url.query) if url.query else path, '',
                       {'Host': url.hostname})
    response = connection.getresponse()
    if response.status != 200:
        logger.error('Purge of {}{} on host {} failed with status: {}'.format(path,
                                                                                  "?" + url.query if url.query else '',
                                                                                  url.hostname,
                                                                                  response.status))
    return response



```

### sefaria/system/varnish/thin_wrapper.py

```
# Varnish wrapper that does not depend on core code.  Used for the multiserver monitor.

from .common import ban_url, purge_url, FRONT_END_URL
from sefaria.utils.util import graceful_exception

import structlog
logger = structlog.get_logger(__name__)


@graceful_exception(logger=logger, return_value=None)
def invalidate_title(title):
    title = title.replace(" ", "_").replace(":", ".")

    # Parallel to sefaria.system.varnish.wrapper.invalidate_index()
    purge_url("{}/api/index/{}".format(FRONT_END_URL, title))
    purge_url("{}/api/v2/raw/index/{}".format(FRONT_END_URL, title))
    purge_url("{}/api/v2/index/{}".format(FRONT_END_URL, title))
    purge_url("{}/api/v2/index/{}?with_content_counts=1".format(FRONT_END_URL, title))

    # Parallel to sefaria.system.varnish.wrapper.invalidate_counts()
    purge_url("{}/api/preview/{}".format(FRONT_END_URL, title))
    purge_url("{}/api/counts/{}".format(FRONT_END_URL, title))
    purge_url("{}/api/v2/index/{}?with_content_counts=1".format(FRONT_END_URL, title))

    # Parallel to base of sefaria.system.varnish.wrapper.invalidate_title()
    ban_url("/api/texts/{}".format(title))
    ban_url("/api/links/{}".format(title))

```

### sefaria/system/multiserver/coordinator.py

```
import json
import uuid

from django.core.exceptions import MiddlewareNotUsed

from sefaria.settings import MULTISERVER_ENABLED, MULTISERVER_REDIS_EVENT_CHANNEL, MULTISERVER_REDIS_CONFIRM_CHANNEL

from .messaging import MessagingNode

import structlog
logger = structlog.get_logger(__name__)


class ServerCoordinator(MessagingNode):
    """
    Runs on each instance of the server.
    publish_event() - Used for publishing events to other servers
    sync() - used for listening for events. Invoked periodically from MultiServerEventListenerMiddleware
    """
    subscription_channels = [MULTISERVER_REDIS_EVENT_CHANNEL]

    def publish_event(self, obj, method, args = None):
        """

        :param obj:
        :param method:
        :param args:
        :return:
        """
        # Check to see if there's any messages in the queue before pushing/popping a new one.
        ## Edge case - needs thought - does the order of operations make for trouble in this case?##
        self.sync()

        payload = {
            "obj": obj,
            "method": method,
            "args": args or [],
            "id": uuid.uuid4().hex
        }
        msg_data = json.dumps(payload)

        import socket
        import os
        logger.info("publish_event from {}:{} - {}".format(socket.gethostname(), os.getpid(), msg_data))
        try:
            self.redis_client.publish(MULTISERVER_REDIS_EVENT_CHANNEL, msg_data)
        except Exception:
            logger.error("Failed to connect to Redis instance while doing message publish.")
            return

        # Since we are subscribed to this channel as well, throw away the message we just sent.
        # It would be nice to assume that nothing new came through in the microseconds that it took to publish ##
        # But the below should insulate against even that case ##
        try:
            popped_msg = self.pubsub.get_message()
        except Exception:
            logger.error("Failed to connect to Redis instance while doing message publish listen.")
            popped_msg = None

        while popped_msg:
            if popped_msg["data"] != msg_data:
                logger.warning("Multiserver Message collision!")
                self._process_message(popped_msg)
            try:
                popped_msg = self.pubsub.get_message()
            except Exception:
                logger.error("Failed to connect to Redis instance while doing message publish listen.")
                popped_msg = None

    def sync(self):
        self._check_initialization()
        try:
            msg = self.pubsub.get_message()
        except Exception:
            logger.error("Failed to connect to Redis instance while doing multiserver sync.")
            return
        if not msg or msg["type"] == "subscribe":
            return

        if msg["type"] != "message":
            logger.error("Surprising redis message type: {}".format(msg["type"]))

        self._process_message(msg)
        self.sync()  # While there are still live messages, keep processing them.

    def _process_message(self, msg):
        """
        :param msg: JSON encoded message.
         Expecting a message that looks like this:
         {'channel': 'msync',
          'data': {
            "obj": obj,
            "method": method,
            "args": args or [],
            "id": uuid.uuid4().hex
          }
          'pattern': None,
          'type': 'message',
         }

        :return:
        """


        # A list of all of the objects that be referenced
        from sefaria.model import library
        import sefaria.system.cache as scache
        import sefaria.model.text as text
        from sefaria.system.cache import in_memory_cache

        import socket
        import os
        host = socket.gethostname()
        pid = os.getpid()

        data = json.loads(msg["data"])

        obj = locals()[data["obj"]]
        method = getattr(obj, data["method"])

        try:
            method(*data["args"])
            logger.info("Processing succeeded for {} on {}:{}".format(self.event_description(data), host, pid))

            confirm_msg = {
                'event_id': data["id"],
                'host': host,
                'pid': pid,
                'status': 'success'
            }

        except Exception as e:
            logger.error("Processing failed for {} on {}:{} - {}".format(self.event_description(data), host, pid, str(e)))

            confirm_msg = {
                'event_id': data["id"],
                'host': host,
                'pid': pid,
                'status': 'error',
                'error': str(e)
            }

        # Send confirmation
        msg_data = json.dumps(confirm_msg)
        logger.info("Sending confirm from {}:{} - {}".format(host, pid, msg["data"]))
        try:
            self.redis_client.publish(MULTISERVER_REDIS_CONFIRM_CHANNEL, msg_data)
        except Exception:
            logger.error("Failed to connect to Redis instance while doing confirm publish")


class MultiServerEventListenerMiddleware(object):
    delay = 20  # Will check for library updates every X requests.  0 means every request.

    def __init__(self, get_response):
        self.get_response = get_response

        if not MULTISERVER_ENABLED:
            raise MiddlewareNotUsed
        self.req_counter = 0

    def __call__(self, request):
        if self.req_counter == self.delay:
            server_coordinator.sync()
            self.req_counter = 0
        else:
            self.req_counter += 1

        response = self.get_response(request)
        return response

server_coordinator = ServerCoordinator() if MULTISERVER_ENABLED else None

```

### sefaria/system/multiserver/monitor.py

```
import json
import time

from sefaria.settings import MULTISERVER_REDIS_EVENT_CHANNEL, MULTISERVER_REDIS_CONFIRM_CHANNEL

import structlog
logger = structlog.get_logger(__name__)

from .messaging import MessagingNode
from sefaria.system.varnish.thin_wrapper import invalidate_title


class MultiServerMonitor(MessagingNode):
    subscription_channels = [MULTISERVER_REDIS_EVENT_CHANNEL, MULTISERVER_REDIS_CONFIRM_CHANNEL]

    def __init__(self):
        super(MultiServerMonitor, self).__init__()
        self.connect()
        self.events = {}
        self.event_order = []

    def listen(self):
        while True:
            time.sleep(0.2)
            self.process_messages()

    def process_messages(self):
        """

        :return:
        """
        try:
            msg = self.pubsub.get_message()
        except Exception:
            logger.error("Failed to connect to Redis instance while getting new message")
            return
        if not msg:
            return

        if msg["type"] != "message":
            logger.error("Surprising redis message type: {}".format(msg))

        elif msg["channel"] == MULTISERVER_REDIS_EVENT_CHANNEL:
            data = json.loads(msg["data"])
            self._process_event(data)
        elif msg["channel"] == MULTISERVER_REDIS_CONFIRM_CHANNEL:
            data = json.loads(msg["data"])
            self._process_confirm(data)
        else:
            logger.error("Surprising redis message channel: {}".format(msg["channel"]))

        # There may be more than one message waiting
        self.process_messages()

    def _process_event(self, data):
        """

        :param data:
        :return:
        """
        event_id = data["id"]
        try:
            (_, subscribers) = self.redis_client.execute_command('PUBSUB', 'NUMSUB', MULTISERVER_REDIS_EVENT_CHANNEL)
        except Exception:
            logger.error("Failed to connect to Redis instance while getting subscriber count")
            return
        expected = int(subscribers - 2)  # No confirms from the publisher or the monitor => subscribers - 2
        self.events[event_id] = {
            "data": data,
            "expected": expected,
            "confirmed": 0,
            "confirmations": [],
            "complete": False}
        self.event_order += [event_id]
        logger.info("Received new event: {} - expecting {} confirmations".format(
            self.event_description(data), expected))

    def _process_confirm(self, data):
        """

        :param data:
        :return:
        """

        # todo: check status - success/error
        event_id = data["event_id"]
        event_record = self.events.get(event_id)

        if not event_record:
            logger.error("Got confirmation of unknown event. {}".format(data))
            return

        event_record["confirmed"] += 1
        event_record["confirmations"] += [data]
        logger.info("Received {} of {} confirmations for {}".format(
            event_record["confirmed"], event_record["expected"], data["event_id"]))

        if event_record["confirmed"] == event_record["expected"]:
            event_record["complete"] = True
            logger.info("Received all {} responses for {}".format(
                event_record["confirmed"], data["event_id"]))
            self._process_completion(event_record["data"])

    def _process_completion(self, data):
        """

        :param data:
            data["obj"]
            data["method"]
            data["args"]
            data["id"]
        :return:
        """
        if data["obj"] == "library":

            if data["method"] == "refresh_index_record_in_cache":
                title = data["args"][-1]  # Sometimes this is first arg, sometimes second.  Always last.
                logger.info("Invalidating {} in Varnish".format(title))
                invalidate_title(title)

            if data["method"] == "remove_index_record_from_cache":
                title = data["args"][0]
                logger.info("Invalidating {} in Varnish".format(title))
                invalidate_title(title)



```

### sefaria/system/multiserver/__init__.py

```

```

### sefaria/system/multiserver/messaging.py

```
import time
import redis
from sefaria.settings import MULTISERVER_REDIS_SERVER, MULTISERVER_REDIS_PORT, MULTISERVER_REDIS_DB

import structlog
logger = structlog.get_logger(__name__)


class MessagingNode(object):
    subscription_channels = []

    def connect(self):
        logger.info("Initializing {} with subscriptions: {}".format(self.__class__.__name__, self.subscription_channels))
        try:
            self.redis_client = redis.StrictRedis(host=MULTISERVER_REDIS_SERVER, port=MULTISERVER_REDIS_PORT, db=MULTISERVER_REDIS_DB, decode_responses=True, encoding="utf-8")
            self.pubsub = self.redis_client.pubsub()
        except Exception:
            logger.error("Failed to establish connection to Redis")
            return
        if len(self.subscription_channels):
            self.pubsub.subscribe(*self.subscription_channels)
            time.sleep(0.2)
            for _ in self.subscription_channels:
                self._pop_subscription_msg()

    def _pop_subscription_msg(self):
        m = self.pubsub.get_message()
        if not m:
            logger.error("No subscribe message found")
        elif m["type"] != "subscribe":
            logger.error("Expecting subscribe message, found: {}".format(m))

    def _check_initialization(self):
        if not getattr(self, "redis_client", None) or not getattr(self, "pubsub", None):
            self.connect()

    @staticmethod
    def event_description(data):
        return "{}.{}({}) [{}]".format(data["obj"], data["method"], str(data["args"]), data["id"])
```

### sefaria/system/context_processors.py

```
# -*- coding: utf-8 -*-

"""
Djagno Context Processors, for decorating all HTTP requests with common data.
"""
import json
from datetime import datetime
from functools import wraps

from django.template.loader import render_to_string

from sefaria.settings import *
from sefaria.site.site_settings import SITE_SETTINGS
from sefaria.model import library
from sefaria.model.user_profile import UserProfile, UserHistorySet, UserWrapper
from sefaria.utils import calendars
from sefaria.utils.util import short_to_long_lang_code
from sefaria.utils.hebrew import hebrew_parasha_name
from reader.views import render_react_component, _get_user_calendar_params

import structlog
logger = structlog.get_logger(__name__)


def builtin_only(view):
    """
    Marks processors only needed when using on Django builtin auth views.
    """
    @wraps(view)
    def wrapper(request):
        if request.path == "/login" or request.path.startswith("/password"):
            return view(request)
        else:
            return {}
    return wrapper


def data_only(view):
    """
    Marks processors only needed when setting the data JS.
    Passed in Source Sheets which rely on S1 JS.
    """
    @wraps(view)
    def wrapper(request):
        if request.path == "/sefaria.js" or request.path.startswith("/data.") or request.path.startswith("/sheets/"):
            return view(request)
        else:
            return {}
    return wrapper


def user_only(view):
    """
    Marks processors only needed on user visible pages.
    """
    @wraps(view)
    def wrapper(request):
        #exclude = ('/linker.js')
        if request.path == '/linker.js' or request.path.startswith("/api/") or request.path.startswith("/data."):
            return {}
        else:
            return view(request)
    return wrapper


def global_settings(request):
    return {
        "SEARCH_INDEX_NAME_TEXT": SEARCH_INDEX_NAME_TEXT,
        "SEARCH_INDEX_NAME_SHEET":SEARCH_INDEX_NAME_SHEET,
        "STRAPI_LOCATION":        STRAPI_LOCATION,
        "STRAPI_PORT":            STRAPI_PORT,
        "GOOGLE_TAG_MANAGER_CODE":GOOGLE_TAG_MANAGER_CODE,
        "GOOGLE_GTAG":            GOOGLE_GTAG,
        "HOTJAR_ID":              HOTJAR_ID,
        "DEBUG":                  DEBUG,
        "OFFLINE":                OFFLINE,
        "SITE_SETTINGS":          SITE_SETTINGS,
        "CLIENT_SENTRY_DSN":      CLIENT_SENTRY_DSN,
    }


@builtin_only
def base_props(request):
    from reader.views import base_props
    return {"propsJSON": json.dumps(base_props(request), ensure_ascii=False)}


@user_only
def cache_timestamp(request):
    return {
        "last_cached": library.get_last_cached_time(),
        "last_cached_short": round(library.get_last_cached_time())
    }


@data_only
def large_data(request):
    return {
        "toc": library.get_toc(),
        "toc_json": library.get_toc_json(),
        "topic_toc": library.get_topic_toc(),
        "topic_toc_json": library.get_topic_toc_json(),
        "titles_json": library.get_text_titles_json(),
        "terms_json": library.get_simple_term_mapping_json(),
        'virtual_books': library.get_virtual_books()
    }


HEADER = {
    'logged_in': {'english': None, 'hebrew': None},
    'logged_out': {'english': None, 'hebrew': None},
    'logged_in_mobile': {'english': None, 'hebrew': None},
    'logged_out_mobile': {'english': None, 'hebrew': None},
}
@user_only
def header_html(request):
    """
    Uses React to prerender a logged in and and logged out header for use in pages that extend `base.html`.
    Cached in memory -- restarting Django is necessary for catch any HTML changes to header.
    """
    global HEADER
    if USE_NODE:
        lang = request.interfaceLang
        LOGGED_OUT_HEADER = HEADER['logged_out'][lang] or \
            render_react_component("ReaderApp", {"headerMode": True,
                                                 "_uid": None,
                                                 "interfaceLang": lang,
                                                 "_siteSettings": SITE_SETTINGS})

        LOGGED_IN_HEADER = HEADER['logged_in'][lang] or \
            render_react_component("ReaderApp", {"headerMode": True,
                                                 "_uid": True,
                                                 "interfaceLang": lang,
                                                 "notificationCount": 0,
                                                 "profile_pic_url": "",
                                                 "full_name": "",
                                                 "_siteSettings": SITE_SETTINGS})

        MOBILE_LOGGED_OUT_HEADER = HEADER["logged_out_mobile"][lang] or \
            render_react_component("ReaderApp", {"headerMode": True,
                                                 "_uid": None,
                                                 "interfaceLang": lang,
                                                 "multiPanel": False,
                                                 "_siteSettings": SITE_SETTINGS})

        MOBILE_LOGGED_IN_HEADER = HEADER["logged_in_mobile"][lang] or \
            render_react_component("ReaderApp", {"headerMode": True,
                                                 "_uid": True,
                                                 "interfaceLang": lang,
                                                 "notificationCount": 0,
                                                 "profile_pic_url": "",
                                                 "full_name": "",
                                                 "multiPanel": False,
                                                 "_siteSettings": SITE_SETTINGS})



        LOGGED_OUT_HEADER = "" if "appLoading" in LOGGED_OUT_HEADER else LOGGED_OUT_HEADER
        LOGGED_IN_HEADER = "" if "appLoading" in LOGGED_IN_HEADER else LOGGED_IN_HEADER
        MOBILE_LOGGED_OUT_HEADER = "" if "appLoading" in MOBILE_LOGGED_OUT_HEADER else MOBILE_LOGGED_OUT_HEADER
        MOBILE_LOGGED_IN_HEADER = "" if "appLoading" in MOBILE_LOGGED_IN_HEADER else MOBILE_LOGGED_IN_HEADER
        HEADER['logged_out'][lang] = LOGGED_OUT_HEADER
        HEADER['logged_in'][lang] = LOGGED_IN_HEADER
        HEADER['logged_out_mobile'][lang] = MOBILE_LOGGED_OUT_HEADER
        HEADER['logged_in_mobile'][lang] = MOBILE_LOGGED_IN_HEADER
    else:
        LOGGED_OUT_HEADER = ""
        LOGGED_IN_HEADER = ""
        MOBILE_LOGGED_OUT_HEADER = ""
        MOBILE_LOGGED_IN_HEADER = ""
    
    return {
        "logged_in_header":  LOGGED_IN_HEADER,
        "logged_out_header": LOGGED_OUT_HEADER,
        "logged_in_mobile_header":     MOBILE_LOGGED_IN_HEADER,
        "logged_out_mobile_header": MOBILE_LOGGED_OUT_HEADER,
    }


FOOTER = {'english': None, 'hebrew': None}
@user_only
def footer_html(request):
    global FOOTER
    lang = request.interfaceLang
    if USE_NODE:
        FOOTER[lang] = FOOTER[lang] or render_react_component("Footer", {"interfaceLang": request.interfaceLang, "_siteSettings": SITE_SETTINGS})
        FOOTER[lang] = "" if "appLoading" in FOOTER[lang] else FOOTER[lang]
    else:
        FOOTER[lang] = ""
    return {
        "footer": FOOTER[lang]
    }


@user_only
def body_flags(request):
    return {"EMBED": "embed" in request.GET}

```

### sefaria/system/exceptions.py

```
"""
UserError exceptions get handled by the front-end API views (via the catch_error decorator)
    and turned into JSON wrapped error messages for the front end.

All other exceptions get handled by the default Django error handling system.
"""


class InputError(Exception):
    """ Exception thrown when bad input of some sort is found.  This is the parent exception for parsing exceptions. """
    pass


class PartialRefInputError(InputError):
    """ Special Case Exception to throw when an input error is partially correct"""
    def __init__(self, message, matched_part, valid_continuations):

        # Call the base class constructor with the parameters it needs
        super(InputError, self).__init__(message)

        # Now for your custom code...
        self.matched_part = matched_part
        self.valid_continuations = valid_continuations


class BookNameError(InputError):
    """ Thrown when a book title is searched for and not found.  """
    pass


class DuplicateRecordError(InputError):
    """ Thrown when trying to save a record that would duplicate existing information """
    pass


class IndexSchemaError(InputError):
    pass


class NoVersionFoundError(InputError):
    pass


class DictionaryEntryNotFoundError(InputError):
    def __init__(self, message, lexicon_name=None, base_title=None, word=None):
        super(DictionaryEntryNotFoundError, self).__init__(message)
        self.lexicon_name = lexicon_name
        self.base_title = base_title
        self.word = word


class SheetNotFoundError(InputError):
    pass


class ManuscriptError(Exception):
    pass


class MissingKeyError(Exception):
    pass


class SluggedMongoRecordMissingError(Exception):
    pass


class SchemaValidationException(Exception):
    def __init__(self, key, expected_type):
        self.key = key
        self.expected_type = expected_type
        self.message = f"Invalid value for key '{key}'. Expected type: {expected_type}"
        super().__init__(self.message)


class SchemaRequiredFieldException(Exception):
    def __init__(self, key):
        self.key = key
        self.message = f"Required field '{key}' is missing."
        super().__init__(self.message)


class SchemaInvalidKeyException(Exception):
    def __init__(self, key):
        self.key = key
        self.message = f"Invalid key '{key}' found in data dictionary."
        super().__init__(self.message)


class InvalidURLException(Exception):
    def __init__(self, url):
        self.url = url
        self.message = f"'{url}' is not a valid URL."
        super().__init__(self.message)


class InvalidHTTPMethodException(Exception):
    def __init__(self, method):
        self.method = method
        self.message = f"'{method}' is not a valid HTTP API method."
        super().__init__(self.message)


class ComplexBookLevelRefError(InputError):
    def __init__(self, book_ref):
        self.book_ref = book_ref
        self.message = (f"You passed '{book_ref}', please pass a more specific ref for this book, and try again. "
                        f"'{book_ref}' is a \'complex\' book-level ref. We only support book-level "
                        f"refs in cases of texts with a 'simple' structure. To learn more about the "
                        f"structure of a text on Sefaria, "
                        f"see: https://developers.sefaria.org/docs/the-structure-of-a-simple-text")
        super().__init__(self.message)

```

### sefaria/system/testing.py

```
# -*- coding: utf-8 -*-

import os
import secrets

test_uid = os.getenv("DEPLOY_ENV", secrets.token_hex(3))

```

### sefaria/system/middleware.py

```
import sys 
import tempfile
import cProfile
import pstats
from io import StringIO

from django.conf import settings
from django.utils import translation
from django.shortcuts import redirect
from django.http import HttpResponse

from sefaria.settings import *
from sefaria.site.site_settings import SITE_SETTINGS
from sefaria.model.user_profile import UserProfile
from sefaria.utils.util import short_to_long_lang_code, get_lang_codes_for_territory
from sefaria.system.cache import get_shared_cache_elem, set_shared_cache_elem
from django.utils.deprecation import MiddlewareMixin
from urllib.parse import quote
import structlog
logger = structlog.get_logger(__name__)


class SharedCacheMiddleware(MiddlewareMixin):
    def process_request(self, request):
        last_cached = get_shared_cache_elem("last_cached")
        if not last_cached:
            regen_in_progress = get_shared_cache_elem("regenerating")
            if not regen_in_progress:
                set_shared_cache_elem("regenerating", True)
                request.init_shared_cache = True


class LocationSettingsMiddleware(MiddlewareMixin):
    """
        Determines if the user should see diaspora content or Israeli.
    """
    def process_request(self, request):
        loc = request.META.get("HTTP_CF_IPCOUNTRY", None)
        if not loc:
            try:
                from sefaria.settings import PINNED_IPCOUNTRY
                loc = PINNED_IPCOUNTRY
            except:
                loc = "us"
        request.diaspora = False if loc in ("il", "IL", "Il") else True


class LanguageSettingsMiddleware(MiddlewareMixin):
    """
    Determines Interface and Content Language settings for each request.
    """
    def process_request(self, request):
        excluded = ('/linker.js', '/linker.v2.js', '/linker.v3.js', "/api/", "/interface/", "/apple-app-site-association", STATIC_URL)
        if any([request.path.startswith(start) for start in excluded]):
            request.interfaceLang = "english"
            request.contentLang = "bilingual"
            request.translation_language_preference = None
            request.version_preferences_by_corpus = {}
            request.translation_language_preference_suggestion = None
            return # Save looking up a UserProfile, or redirecting when not needed

        profile = UserProfile(id=request.user.id) if request.user.is_authenticated else None
        # INTERFACE 
        # Our logic for setting interface lang checks (1) User profile, (2) cookie, (3) geolocation, (4) HTTP language code
        interface = None
        if request.user.is_authenticated and not interface:
            interface = profile.settings["interface_language"] if "interface_language" in profile.settings else interface 
        if not interface: 
            # Pull language setting from cookie, location (set by Cloudflare) or Accept-Lanugage header or default to english
            interface = request.COOKIES.get('interfaceLang') or request.META.get("HTTP_CF_IPCOUNTRY") or request.LANGUAGE_CODE or 'english'
            interface = 'hebrew' if interface in ('IL', 'he', 'he-il') else interface
            # Don't allow languages other than what we currently handle
            interface = 'english' if interface not in ('english', 'hebrew') else interface

        # Check if the current domain is pinned to  particular language in settings
        domain_lang = current_domain_lang(request)

        if domain_lang and domain_lang != interface:
            # For crawlers, don't redirect -- just return the pinned language
            no_direct = ("Googlebot", "Bingbot", "Slurp", "DuckDuckBot", "Baiduspider", 
                            "YandexBot", "Facebot", "facebookexternalhit", "ia_archiver", "Sogou",
                            "python-request", "curl", "Wget", "sefaria-node")
            if any([bot in request.META.get('HTTP_USER_AGENT', '') for bot in no_direct]):
                interface = domain_lang
            else:
                redirect_domain = None
                for domain in DOMAIN_LANGUAGES:
                    if DOMAIN_LANGUAGES[domain] == interface:
                        redirect_domain = domain
                if redirect_domain:
                    # When detected language doesn't match current domain langauge, redirect
                    path = request.get_full_path()
                    path = path + ("&" if "?" in path else "?") + "set-language-cookie"
                    return redirect(redirect_domain + path)
                    # If no pinned domain exists for the language the user wants,
                    # the user will stay on this domain with the detected language

        # CONTENT
        default_content_lang = 'hebrew' if interface == 'hebrew' else 'bilingual'
        # Pull language setting from cookie or Accept-Lanugage header or default to english
        content = request.GET.get('lang') or request.COOKIES.get('contentLang') or default_content_lang
        content = short_to_long_lang_code(content)
        # Don't allow languages other than what we currently handle
        content = default_content_lang if content not in ('english', 'hebrew', 'bilingual') else content
        # Note: URL parameters may override values set here, handled in reader view.

        if not SITE_SETTINGS["TORAH_SPECIFIC"]:
            content = "english"
            interface = "english"

        # TRANSLATION LANGUAGE PREFERENCE
        translation_language_preference = (profile is not None and profile.settings.get("translation_language_preference", None)) or request.COOKIES.get("translation_language_preference", None)
        langs_in_country = get_lang_codes_for_territory(request.META.get("HTTP_CF_IPCOUNTRY", None))
        translation_language_preference_suggestion = None
        trans_lang_pref_suggested = (profile is not None and profile.settings.get("translation_language_preference_suggested", False)) or request.COOKIES.get("translation_language_preference_suggested", False)
        if translation_language_preference is None and not trans_lang_pref_suggested:
            supported_translation_langs = set(SITE_SETTINGS['SUPPORTED_TRANSLATION_LANGUAGES'])
            for lang in langs_in_country:
                if lang in supported_translation_langs:
                    translation_language_preference_suggestion = lang
                    break             
        if translation_language_preference_suggestion == "en":
            # dont ever suggest English to our users
            translation_language_preference_suggestion = None

        # VERSION PREFERENCE
        import json
        from urllib.parse import unquote
        version_preferences_by_corpus_cookie = json.loads(unquote(request.COOKIES.get("version_preferences_by_corpus", "null")))
        request.version_preferences_by_corpus = (profile is not None and getattr(profile, "version_preferences_by_corpus", None)) or version_preferences_by_corpus_cookie or {}
        request.LANGUAGE_CODE = interface[0:2]
        request.interfaceLang = interface
        request.contentLang   = content
        request.translation_language_preference = translation_language_preference
        request.translation_language_preference_suggestion = translation_language_preference_suggestion

        translation.activate(request.LANGUAGE_CODE)


class LanguageCookieMiddleware(MiddlewareMixin):
    """
    If `set-language-cookie` param is set, set a cookie the interfaceLange of current domain, 
    then redirect to a URL without the param (so the urls with the param don't get loose in wild).
    Allows one domain to set a cookie on another. 
    """
    def process_request(self, request):
        lang = current_domain_lang(request)
        if "set-language-cookie" in request.GET and lang:
            domain = [d for d in DOMAIN_LANGUAGES if DOMAIN_LANGUAGES[d] == lang][0]
            path = quote(request.path, safe='/')
            params = request.GET.copy()
            params.pop("set-language-cookie")
            params_string = params.urlencode()
            params_string = "?" + params_string if params_string else ""
            response = redirect(domain + path + params_string)
            response.set_cookie("interfaceLang", lang)
            if request.user.is_authenticated:
                p = UserProfile(id=request.user.id)
                p.settings["interface_language"] = lang
                p.save()
            return response


def current_domain_lang(request):
    """
    Returns the pinned language for the current domain, or None if current domain is not pinned.
    """
    current_domain = request.get_host()
    domain_lang = None
    for protocol in ("https://", "http://"):
        full_domain = protocol + current_domain
        if full_domain in DOMAIN_LANGUAGES:
            domain_lang = DOMAIN_LANGUAGES[full_domain]
    return domain_lang


class CORSDebugMiddleware(MiddlewareMixin):
    def process_response(self, request, response):
        """
        CORS headers are normally added in nginx response.
        However, nginx isn't normally running when debugging with localhost
        """
        origin = request.get_host()
        if ('localhost' in origin or '127.0.0.1' in origin) and DEBUG:
            response["Access-Control-Allow-Origin"] = "*"
            response["Access-Control-Allow-Methods"] = "POST, GET"
            response["Access-Control-Allow-Headers"] = "*"
        return response


class ProfileMiddleware(MiddlewareMixin):
    """
    Displays profiling for any view.
    http://yoursite.com/yourview/?prof

    Add the "prof" key to query string by appending ?prof (or &prof=)
    and you'll see the profiling results in your browser.
    It's set up to only be available in django's debug mode,
    but you really shouldn't add this middleware to any production configuration.
    * Only tested on Linux
    """
    def process_request(self, request):
        if settings.DEBUG and 'prof' in request.GET:
            self.prof = cProfile.Profile()

    def process_view(self, request, callback, callback_args, callback_kwargs):
        if settings.DEBUG and 'prof' in request.GET:
            return self.prof.runcall(callback, request, *callback_args, **callback_kwargs)

    def process_response(self, request, response):
        if settings.DEBUG and 'prof' in request.GET:
            self.prof.create_stats()

            io = StringIO()
            stats = pstats.Stats(self.prof, stream=io)

            stats.strip_dirs().sort_stats(request.GET.get('sort', "cumulative"))
            stats.print_stats()

            response = HttpResponse('<pre>%s</pre>' % io.getvalue())
        return response

```

### sefaria/system/decorators.py

```
from functools import wraps, partial
from typing import Any

from django.http import HttpResponse, Http404
from django.template import RequestContext
from django.shortcuts import render_to_response

from sefaria.client.util import jsonResponse
import sefaria.system.exceptions as exps
import sefaria.settings

import collections
import bleach
import structlog
logger = structlog.get_logger(__name__)

from sefaria.settings import FAIL_GRACEFULLY


# TODO: we really need to fix the way we are using json responses. Django 1.7 introduced a baked in JsonResponse.
def json_response_decorator(func):
    """
    A decorator thats takes a view response and turns it
    into json. If a callback is added through GET or POST
    the response is JSONP.
    """

    @wraps(func)
    def decorator(request, *args, **kwargs):
        return jsonResponse(func(request, *args, **kwargs), callback=request.GET.get("callback", None))
    return decorator


def catch_error_as_json(func):
    """
    Decorator that catches InputErrors and translates them into JSON 'error' dicts for front end consumption.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            result = func(*args, **kwargs)
        except exps.InputError as e:
            logger.warning("An exception occurred processing request for '{}' while running {}. Caught as JSON".format(args[0].path, func.__name__), exc_info=True)
            request = args[0]
            return jsonResponse({"error": str(e)}, callback=request.GET.get("callback", None))
        return result
    return wrapper


def catch_error_as_http(func):
    """
    Decorator that catches InputErrors and returns an error page.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            result = func(*args, **kwargs)
        except exps.InputError as e:
            logger.warning("An exception occurred processing request for '{}' while running {}. Caught as HTTP".format(args[0].path, func.__name__), exc_info=True)
            raise Http404
        except Http404:
            raise
        except Exception as e:
            logger.exception("An exception occurred processing request for '{}' while running {}. Caught as HTTP".format(args[0].path, func.__name__))
            return render_to_response(args[0], 'static/generic.html',
                             {"content": "There was an error processing your request: {}".format(str(e))})
        return result
    return wrapper


def log(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        """Assumes that function doesn't change input data"""
        #logger.debug("Calling: " + func + "(" + args + kwargs + ")")
        result = func(*args, **kwargs)
        msg = func.__name__ + "(" + ",".join([str(a) for a in args])
        msg += ", " + str(kwargs) if kwargs else ""
        msg += "):\n\t" + str(result)
        logger.debug(msg)
        return result
    return wrapper


def sanitize_get_params(func):
    """
    For view functions where first param is `request`
    Uses bleach to protect against XSS attacks in GET requests
    """
    @wraps(func)
    def wrapper(request, *args, **kwargs):
        request.GET = request.GET.copy()  #  see https://stackoverflow.com/questions/18930234/django-modifying-the-request-object/18931697
        for k, v in list(request.GET.items()):
            request.GET[k] = bleach.clean(v)
        args = [bleach.clean(a) if isinstance(a, str) else a for a in args]  # while we're at it, clean any other vars passed
        result = func(request, *args, **kwargs)
        return result
    return wrapper


def conditional_graceful_exception(logLevel: str = "exception", exception_type: Exception = Exception) -> Any:
    """
    Decorator that catches exceptions and logs them if FAIL_GRACEFULLY is True.
    For instance, when loading the server on prod, we want to fail gracefully if a text or ref cannot be properly loaded.
    However, when running text creation functions, we want to fail loudly so that we can fix the problem.

    :param logLevel: "exception" or "warning"
    :param return_value: function return value if exception is caught
    :param exception_type: type of exception to catch
    :return: return_value no error, if FAIL_GRACEFULLY is True log the error, otherwise raise exception

    """
    def argumented_decorator(func):
        @wraps(func)
        def decorated_function(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except exception_type as e:
                if FAIL_GRACEFULLY:
                    if logger:
                        logger.exception(str(e)) if logLevel == "exception" else logger.warning(str(e))
                else:
                    raise e
        return decorated_function
    return argumented_decorator


class memoized(object):
    """Decorator. Caches a function's return value each time it is called.
    If called later with the same arguments, the cached value is returned
    (not reevaluated).
    Handling of kwargs is simplistic.  There are situations where it could break down.  Currently works dependably for one kwarg.
    """

    def __init__(self, func):
        self.func = func
        self.cache = {}

    def __call__(self, *args, **kwargs):
        if not isinstance(args, collections.abc.Hashable):
            # uncacheable. a list, for instance.
            # better to not cache than blow up.
            return self.func(*args, **kwargs)
        key = (args + tuple(kwargs.items())) if kwargs else args
        if key in self.cache:
            return self.cache[key]
        else:
            value = self.func(*args, **kwargs)
            self.cache[key] = value
            return value

    def __repr__(self):
        '''Return the function's docstring.'''
        return self.func.__doc__

    def __get__(self, obj, objtype):
        '''Support instance methods.'''
        return partial(self.__call__, obj)
```

### sefaria/stats.py

```
import re
from datetime import datetime
from collections import defaultdict
from random import randrange

from sefaria.model import *
from django.contrib.auth.models import User
from sefaria.system.exceptions import InputError
from sefaria.sheets import save_sheet, sheet_topics_counts
from sefaria.utils.util import strip_tags
from sefaria.system.database import db


class SheetStats(object):
	def __init__(self):
		self.show_count         = 20
		self.refs               = defaultdict(int)
		self.texts              = defaultdict(int)
		self.categories         = defaultdict(int)
		self.books              = defaultdict(int)
		self.refs_by_category   = defaultdict(lambda: defaultdict(int))
		self.refs_by_tag        = defaultdict(lambda: defaultdict(int))
		self.fragments          = defaultdict(list)
		self.languages          = defaultdict(int)
		self.untrans_texts      = defaultdict(int)
		self.untrans_categories = defaultdict(int)
		self.untrans_refs       = defaultdict(int)
		self.sources_count      = 0
		self.untrans_count      = 0
		self.comments_count     = 0
		self.outside_count      = 0
		self.fragments_count    = 0

	def run(self, query=None, test=0):
		"""
		Builds and sorts counts across all sheets.
		If `test` is not 0, only sample 1 in every `test` sheets to count.
		"""
		print("Loading sheets...")
		proj = {"sources.ref": 1, "tags": 1, "options": 1, "status": 1, "id": 1}
		if query:
			sheets            = db.sheets.find(query, proj)
			self.total = sheets.count()
			print("%d matching query" % sheets.count())
		else:
			sheets            = db.sheets.find()
			self.total        = sheets.count()
			print("%d Total" % self.total)
			self.public_total = db.sheets.find({"status": "public"}, proj).count()
			print("%d Public" % self.public_total)
		
		print("Processing tags...")
		self.top_tags     = sheet_topics_counts({})

		print("Processing sheets...")
		for sheet in sheets: 
			if test == 0 or randrange(test) == 1:
				self.count_sheet(sheet)

		print("Sorting...")
		self.sort()
		print("Done.")
	
	def count_sheet(self, sheet):
		id = sheet.get("id", 1)
		if id % 1000 == 0:
			print('{0}%\r'.format(((id * 100)/self.total)))
		self.count_sources(sheet.get("sources", []), sheet.get("tags", []), sheet.get("id", -1))
		if "options" in sheet and "language" in sheet["options"]:
			self.languages[sheet["options"]["language"]] += 1
		else:
			self.languages["bilingual"] += 1

	def count_sources(self, sources, tags, sheet_id):
		for s in sources:
			try:
				if "ref" in s and s["ref"] is not None:
					self.sources_count += 1
					oref = Ref(s["ref"]).padded_ref()
					self.refs[oref.normal()] += 1
					self.texts[oref.book] += 1
					self.categories[oref.index.categories[0]] += 1
					self.books[oref.index.title] += 1
					self.refs_by_category[oref.index.categories[0]][oref.normal()] += 1
					for tag in tags:
						self.refs_by_tag[tag][oref.normal()] += 1

					try:
						is_translated = oref.is_text_translated()
					except:
						is_translated = False 
					if not is_translated:
						self.untrans_categories[oref.index.categories[0]] += 1
						self.untrans_texts[oref.book] += 1
						self.untrans_refs[s["ref"]] += 1
						self.untrans_count += 1

						en = strip_tags(s.get("text", {}).get("en", ""))
						if len(en) > 25:
							self.fragments[s["ref"]].append(sheet_id)
							self.fragments_count += 1
				
				elif "comment" in s:
					self.comments_count += 1
				
				elif "outsideText" in s or "outsideBiText" in s:
					self.outside_count += 1
			except:
				continue

	def sort(self):
		self.sorted_refs               = sorted(iter(self.refs.items()), key=lambda x: -x[1])
		self.sorted_texts              = sorted(iter(self.texts.items()), key=lambda x: -x[1])
		self.sorted_categories         = sorted(iter(self.categories.items()), key=lambda x: -x[1])
		self.sorted_books              = sorted(iter(self.books.items()), key=lambda x: -x[1])
		self.sorted_untrans_refs       = sorted(iter(self.untrans_refs.items()), key=lambda x: -x[1])
		self.sorted_untrans_texts      = sorted(iter(self.untrans_texts.items()), key=lambda x: -x[1])
		self.sorted_untrans_categories = sorted(iter(self.untrans_categories.items()), key=lambda x: -x[1])
		self.sorted_fragments          = sorted(iter(self.fragments.items()), key=lambda x: -len(x[1]))

		self.sorted_refs_by_tag = {}
		for ref in self.refs_by_tag:
			self.sorted_refs_by_tag[ref] = sorted(iter(self.refs_by_tag[ref].items()), key=lambda x: -x[1])

		self.sorted_refs_by_category = {}
		for ref in self.refs_by_category:
			self.sorted_refs_by_category[ref] = sorted(iter(self.refs_by_category[ref].items()), key=lambda x: -x[1])

	def collapse_ref_counts(self, refs):
		"""
		Takes and returns a list of ref, count tuples. 
		Merge together results for refs for which one is a specification of the other.
		E.g., if "Shabbat 21a", and "Shabbat 21:5" both appear, add their counts together and keep the mores specific ref.
		"""
		collapsed_refs = {}
		for ref1 in refs:
			matched = False
			for ref2 in refs:
				if ref1 == ref2:
					continue
				oRef1, oRef2 = Ref(ref1[0]), Ref(ref2[0])
				if oRef2.contains(oRef1):
					collapsed_refs[ref1[0]] = ref1[1] + ref2[1]
				if oRef2.contains(oRef1) or oRef1.contains(oRef2):
					matched = True
			if not matched:
				collapsed_refs[ref1[0]] = ref1[1]
		return sorted(iter(collapsed_refs.items()), key=lambda x: -x[1])
		 
	def print_stats(self):
		show_count = self.show_count
		print("*********************************\n")
		print("%d Total Sheets" % self.total)
		if hasattr(self,"public_total"):
			print("%d Public Sheets" % self.public_total)
		print("\n")
		print("%0.1f%% Bilingual" % (100 * self.languages["bilingual"] / float(self.total)))
		print("%0.1f%% Hebrew" % (100 * self.languages["hebrew"] / float(self.total)))
		print("%0.1f%% English" % (100 * self.languages["english"] / float(self.total)))
		print("\n")
		print("\n%d Sources" % self.sources_count)
		print("%d Untranslated Sources" % self.comments_count)
		print("\n")
		print("%d Comments" % self.comments_count)
		print("%d Outside Texts" % self.outside_count)
		print("\n")
		print("%d Potential Fragments (translations in sheets not saved in DB)" % self.fragments_count)

		print("\n******* Top Sources ********\n")
		for item in self.sorted_refs[:show_count]:
			print("%s: %d" % (item[0], item[1]))

		print("\n******* Top Texts ********\n")
		for item in self.sorted_texts[:show_count]:
			print("%s: %d" % (item[0], item[1]))

		print("\n******* Top Categories ********\n")
		for item in self.sorted_categories[:show_count]:
			print("%s: %d" % (item[0], item[1]))

		print("\n******* Top Untranslated Sources ********\n")
		for item in self.sorted_untrans_refs[:show_count]:
			print("%s: %d" % (item[0], item[1]))

		print("\n******* Top Untranslated Texts ********\n")
		for item in self.sorted_untrans_texts[:show_count]:
			print("%s: %d" % (item[0], item[1]))

		print("\n******* Top Untranslated Categories ********\n")
		for item in self.sorted_untrans_categories[:show_count]:
			print("%s: %d" % (item[0], item[1]))

		print("\n******* Top Fragments ********\n")
		for item in self.sorted_fragments[:show_count]:
			print("%s: %d" % (item[0], len(item[1])))

		print("\n******* Top Refs by Category ********\n")
		for cat in self.sorted_refs_by_category:
			print("%s: %s" % (cat, self.sorted_refs_by_category[cat][0][0]))

		print("\n******* Top Refs by Tag ********\n")
		for tag in self.top_tags[:50]:
			print("%s: %s" % (tag["tag"], self.sorted_refs_by_tag[tag["tag"]][0][0]))
		
	def save_top_sources_sheet(self):
		sheet = {
			"title": "Top Sources in All Source Sheets - %s" % datetime.now().strftime("%B %Y"),
			"sources": [{"ref": ref[0]} for ref in self.sorted_refs[:self.show_count]],
			"options": {"numbered": 1, "divineNames": "noSub"}
		}
		save_sheet(sheet, 1)

	def save_top_sources_by_category(self):
		sheet = {
			"title": "Top Sources by Category - %s" % datetime.now().strftime("%B %Y"),
			"sources": [{"ref": self.sorted_refs_by_category[cat][0][0], "title": cat} for cat in self.sorted_refs_by_category],
			"options": {"numbered": 0, "divineNames": "noSub"}
		}
		save_sheet(sheet, 1)

	def save_top_sources_by_tag(self):
		sheet = {
			"title": "Top Sources by Tag - %s" % datetime.now().strftime("%B %Y"),
			"sources": [{"ref": self.sorted_refs_by_tag[tag["tag"]][0][0], "title": tag["tag"]} for tag in self.top_tags[:50]],
			"options": {"numbered": 0, "divineNames": "noSub"}
		}
		save_sheet(sheet, 1)

	def save_top_for_category(self, cat, collapse=False):
		top_books_list = []
		for book in self.sorted_books:
			idx = library.get_index(book[0])
			if idx.categories[0] == cat and "Commentary" not in idx.categories:
				top_books_list.append("{} ({:,})".format(book[0], book[1]))
		top_books = "<ol><li>" + "</li><li>".join(top_books_list[:10]) + "</li></ol>"
		sources = [{"comment": "Most frequently used tractates (full list below):<br>%s" % top_books}]

		refs = self.sorted_refs_by_category[cat][:50]
		refs = self.collapse_ref_counts(refs)[:20] if collapse else refs[:20]

		sources += [{"ref": ref[0]} for ref in refs]

		all_top_books = "<ol><li>" + "</li><li>".join(top_books_list) + "</li></ol>"
		sources += [{"comment": "Most frequently used tractates: %s" % all_top_books}]

		sheet = {
			"title": "Top Sources in %s - %s" % (cat, datetime.now().strftime("%B %Y")),
			"sources": sources,
			"options": {"numbered": 1, "divineNames": "noSub"}
		}
		save_sheet(sheet, 1)

	def save_top_sheets(self):
		self.save_top_sources_sheet()
		self.save_top_sources_by_tag()
		self.save_top_sources_by_category()


def total_sheet_views_by_query(query):
	"""Returns the total number of views for sheets that match `query`"""
	result = db.sheets.aggregate([ 
		{ "$match": query},
		{ 
			"$group": { 
				"_id": None, 
				"total": { 
					"$sum": "$views" 
				} 
			} 
		} ] )
	return list(result)[0]["total"]


def most_popular_refs_in_sheets(pattern, public_only=True):
	counts = defaultdict(int)

	sheets = db.sheets.find({"includedRefs": {"$regex": pattern}})
	for sheet in sheets:
		for ref in sheet["includedRefs"]:
			if re.match(pattern, ref):
				counts[ref] += 1

	top = sorted(iter(counts.items()), key=lambda x: -x[1])

	return top


def account_creation_stats():
	"""
	Counts the number of accounts created each month for all users and separately for 
	accounts which have the Hebrew interface set (proxy for Israeli users).
	Returns a string summary.
	"""
	import os
	os.system('pip install pandas')
	import pandas as pd
	
	users = list(User.objects.all().values('date_joined', 'email', 'first_name', 'last_name', 'id', 'last_login'))

	df = pd.DataFrame(users)
	month_joined = df['date_joined'].groupby(df.date_joined.dt.to_period("M")).agg('count')

	# Filter to users who have hebrew interface language, proxy for Israel
	hebrew_user_ids = db.profiles.find({"settings.interface_language": "hebrew"}, {"id": 1, "settings": 1}).distinct("id")
	hebrew_users = [user for user in users if user["id"] in hebrew_user_ids]
	df_hebrew = pd.DataFrame(hebrew_users)
	hebrew_month_joined = df_hebrew['date_joined'].groupby(df_hebrew.date_joined.dt.to_period("M")).agg('count')


	pd.set_option('display.max_rows', None)
	results = "\nAll Users\n************\n\n" + \
				month_joined.to_string() + \
				"\nHebrew Users\n************\n\n" + \
				hebrew_month_joined.to_string()
	
	return results


# query = {"datetime": {"$gte": datetime(2020, 9, 1)}}
def user_activity_stats(query={}, return_string=False):
	"""
	Metrics based on the user history collection
	- Active users in various monthly windows
	- Montly returning users percentage
	"""
	import os
	os.system('pip install pandas')
	import pandas as pd

	months = db.user_history.aggregate([
		{
			"$match": query
		},
		{
			"$project": {
				"_id": 0,
				"uid": 1,
				"date": {"$dateToString": {"date": "$datetime", "format": "%Y-%m"}}				#"year": {"$year": "$datetime"},
			}
		},
		{
			"$group": {
				"_id": "$date",
				"uids": {"$addToSet": "$uid"}
			}
		},
		{
			"$sort": {"_id": 1}
		}
	])

	months = list(months)

	for i in range(len(months)):
		# Number of user who visit in monthly windows
		active_increments = (1,3,6,12)
		for j in active_increments:
			start = i - j + 1 if i - j + 1 > 0 else 0
			end = i + 1
			months_slice = months[start:end]
			actives = {uid for month in months_slice for uid in month["uids"] }
			months[i]["{} month active".format(j)] = len(actives)

		# Number of users who visited last month and this month over number who visited last month
		returning_users = len(set(months[i]["uids"]) & set(months[i-1]["uids"])) if i != 0 else 0
		months[i]["monthly retention"] = int(100 * returning_users / len(months[i-1]["uids"])) if i != 0 else 0
		months[i]["monthly returning"] = returning_users

	results = "Month, 1 Month Active, 3 Month Active, 6 Month Active, 12 Month Active, 1 Month Returning, Monthly Retention\n"
	for month in months:
		results += "{}, {}, {}, {}, {}, {}, {}%\n".format(month["_id"], month["1 month active"], month["3 month active"], month["6 month active"], month["12 month active"], month["monthly returning"], month["monthly retention"])
		del month["uids"]

	return results if return_string else months


```

### sefaria/model/abstract.py

```
# -*- coding: utf-8 -*-

"""
abstract.py - abstract classes for Sefaria models
"""
from cerberus import Validator
import collections
import structlog
import copy
import bleach
import re

#Should we import "from abc import ABCMeta, abstractmethod" and make these explicity abstract?
#

from bson.objectid import ObjectId

from sefaria.system.database import db
from sefaria.system.exceptions import InputError, SluggedMongoRecordMissingError

logger = structlog.get_logger(__name__)


class AbstractMongoRecord(object):
    """
    AbstractMongoRecord - superclass of classes representing mongo records.
    "collection" attribute is set on subclass
    """
    collection = None  # name of MongoDB collection
    id_field = "_id"  # Mongo ID field
    criteria_field = "_id"  # Primary ID used to find existing records
    criteria_override_field = None  # If a record type uses a different primary key (such as 'title' for Index records), and the presence of an override field in a save indicates that the primary attribute is changing ("oldTitle" in Index records) then this class attribute has that override field name used.
    required_attrs = []  # list of names of required attributes
    optional_attrs = []  # list of names of optional attributes
    attr_schemas = {}    # schemas to validate that an attribute is in the right format. Keys are attribute names, values are schemas in Cerberus format.
    track_pkeys = False
    pkeys = []   # list of fields that others may depend on
    history_noun = None  # Label for history records
    ALLOWED_TAGS = bleach.ALLOWED_TAGS + ["p", "br"]  # not sure why p/br isn't included. dont see any security risks
    ALLOWED_ATTRS = bleach.ALLOWED_ATTRIBUTES

    def __init__(self, attrs=None):
        if attrs is None:
            attrs = {}
        self._init_defaults()
        self.pkeys_orig_values = {}
        self.load_from_dict(attrs, True)
            
    def load_by_id(self, _id=None):
        if _id is None:
            raise Exception(type(self).__name__ + ".load() expects an _id as an argument. None provided.")

        if isinstance(_id, str):
            # allow _id as either string or ObjectId
            _id = ObjectId(_id)
        return self.load({"_id": _id})

    def load(self, query, proj=None):
        obj = getattr(db, self.collection).find_one(query, proj)
        if obj:
            assert set(obj.keys()) <= set(self._saveable_attr_keys()), \
                "{} record loaded with unhandled key(s): {}".format(
                    type(self).__name__,
                    set(obj.keys()) - set(self._saveable_attr_keys())
                )
            self.load_from_dict(obj, True)
            return self
        return None  # used to check for existence of record.

    # careful that this doesn't defeat itself, if/when a cache catches constructor calls
    def copy(self):
        attrs = self._saveable_attrs()
        del attrs[self.id_field]
        return self.__class__(copy.deepcopy(attrs))

    def load_from_dict(self, d, is_init=False):
        """
        Add values from a dict to an existing object.
        Used internally both to initialize new objects and to update existing objects

        :param dict d: The dictionary used to update the object
        :param bool is_init: Indicates whether this dictionary is initializing (as opposed to updating) an object.  If this is true, the primary keys are tracked from this load, and any change will trigger an 'attributeChange' notification.
        :return: the object
        """
        for key, value in list(d.items()):
            setattr(self, key, value)
        if is_init and not self.is_new():
            self._set_pkeys()
        self._set_derived_attributes()
        return self

    def update(self, query, attrs):
        """
        :param query: Query to find existing object to update.
        :param attrs: Dictionary of attributes to update.
        :return: the object
        """
        if not self.load(query):
            raise InputError("No existing {} record found to update for {}".format(type(self).__name__, str(query)))
        self.load_from_dict(attrs)
        return self.save()

    def save(self, override_dependencies=False):
        """
        Save the object to the Mongo data store.
        On completion, will emit a 'save' notification.  If a tracked attribute has changed, will emit an 'attributeChange' notification.
        if override_dependencies is set to True, no notifications will be emitted.
        :return: the object
        """
        is_new_obj = self.is_new()

        self._normalize()
        self._validate()
        self._sanitize()
        self._pre_save()

        props = self._saveable_attrs()

        if self.track_pkeys and not is_new_obj:
            if not (len(self.pkeys_orig_values) == len(self.pkeys)):
                raise Exception("Aborted unsafe {} save. {} not fully tracked.".format(type(self).__name__, self.pkeys))

        if is_new_obj:
            result = getattr(db, self.collection).insert_one(props)
            self._id = result.inserted_id
        else:
            result = getattr(db, self.collection).replace_one({"_id":self._id}, props, upsert=True)
            if not result.matched_count and result.upserted_id:
                raise Exception("{} inserted when expecting an update.".format(type(self).__name__))

        if self.track_pkeys and not is_new_obj and not override_dependencies:
            for key, old_value in list(self.pkeys_orig_values.items()):
                if old_value != getattr(self, key, None):
                    notify(self, "attributeChange", attr=key, old=old_value, new=getattr(self, key))

        if not override_dependencies:
            notify(self, "save", orig_vals=self.pkeys_orig_values, is_new=is_new_obj)

        # Set new values as pkey_orig_values so that future changes will be caught
        if self.track_pkeys:
            for pkey in self.pkeys:
                self.pkeys_orig_values[pkey] = getattr(self, pkey, None)

        return self

    def can_delete(self):
        """
        This method can raise an error or output a reason for the failure to delete.
        :return:
        """
        return True

    def delete(self, force=False, override_dependencies=False):
        """
        Just before the delete is executed, will emit a 'delete' notification.

        :param force: delete object, even if it fails a `can_delete()` check
        :param override_dependencies: if override_dependencies is set to True, no notifications will be emitted.
        :return:
        """
        if not self.can_delete():
            if force:
                logger.warning("Forcing delete of {}.".format(str(self)))
            else:
                logger.warning("Failed to delete {}.".format(str(self)))
                return

        if self.is_new():
            raise InputError("Can not delete {} that doesn't exist in database.".format(type(self).__name__))

        if not override_dependencies:
            notify(self, "delete")
        getattr(db, self.collection).delete_one({"_id": self._id})

    def delete_by_query(self, query, force=False):
        r = self.load(query)
        if r:
            r.delete(force=force)

    def is_new(self):
        return getattr(self, "_id", None) is None

    def _saveable_attr_keys(self):
        return self.required_attrs + self.optional_attrs + [self.id_field]

    def _saveable_attrs(self):
        return {k: getattr(self, k) for k in self._saveable_attr_keys() if hasattr(self, k)}

    def contents(self, **kwargs):
        """ Build a savable/portable dictionary from the object
        Extended by subclasses with derived attributes passed along with portable object
        :return: dict
        """
        d = self._saveable_attrs()
        try:
            del d[self.id_field]
        except KeyError:
            pass
        if kwargs.get("with_string_id", False) and hasattr(self, "_id"):
            d["_id"] = str(self._id)
        return d

    def _set_pkeys(self):
        if self.track_pkeys:
            for pkey in self.pkeys:
                self.pkeys_orig_values[pkey] = getattr(self, pkey, None)

    def is_key_changed(self, key):
        assert self.track_pkeys and key in self.pkeys, "Failed to track key {} in {}".format(key, self.__class__.__name__)
        if self.is_new():
            return bool(getattr(self, key, False))
        return self.pkeys_orig_values[key] != getattr(self, key, None)

    def _init_defaults(self):
        pass

    def _set_derived_attributes(self):
        pass

    def _validate(self):
        """
        Test self for validity
        :return: True on success
        Throws Exception on failure
        """

        attrs = vars(self)

        """" This fails when the object has been created but not yet saved.
        if not getattr(self, self.id_field, None):
            logger.debug(type(self).__name__ + ".is_valid: No id field " + self.id_field + " found.")
            return False
        """

        for attr in self.required_attrs:
            #properties. which are virtual instance members, do not get returned by vars()
            if attr not in attrs and not getattr(self, attr, None):
                raise InputError(type(self).__name__ + "._validate(): Required attribute: " + attr + " not in " + ", ".join(attrs))

        """ This check seems like a good idea, but stumbles as soon as we have internal attrs
        for attr in attrs:
            if attr not in self.required_attrs and attr not in self.optional_attrs and attr != self.id_field:
                logger.debug(type(self).__name__ + ".is_valid: Provided attribute: " + attr +
                             " not in " + ",".join(self.required_attrs) + " or " + ",".join(self.optional_attrs))
                return False
        """
        schema = self.attr_schemas
        for key in schema:
            schema[key]['allow_unknown'] = schema[key].get('allow_unknown', False)  # allow unknowns only in the root
        v = Validator(schema, allow_unknown=True)
        if not v.validate(self._saveable_attrs()):
            raise InputError(v.errors)
        return True

    def _normalize(self):
        pass

    def _pre_save(self):
        pass

    def _sanitize(self):
        """
        bleach all input to protect against security risks
        """
        all_attrs = self.required_attrs + self.optional_attrs
        for attr in all_attrs:
            val = getattr(self, attr, None)
            if isinstance(val, str):
                setattr(self, attr, bleach.clean(val, tags=self.ALLOWED_TAGS, attributes=self.ALLOWED_ATTRS))

    def same_record(self, other):
        if getattr(self, "_id", None) and getattr(other, "_id", None):
            return ObjectId(self._id) == ObjectId(other._id)
        return False

    def __eq__(self, other):
        """

        """
        if type(other) is type(self):
            return self._saveable_attrs() == other._saveable_attrs()
        return False

    def __ne__(self, other):
        return not self.__eq__(other)

    @classmethod
    def all_subclasses(cls) -> set:
        # get all subclasses recursively
        # see https://stackoverflow.com/a/3862957/4246723
        return set(cls.__subclasses__()).union([sub_sub_cls for sub_cls in cls.__subclasses__() for sub_sub_cls in sub_cls.all_subclasses()])


class AbstractMongoSet(collections.abc.Iterable):
    """
    A set of mongo records from a single collection
    """
    recordClass = AbstractMongoRecord

    def __init__(self, query=None, page=0, limit=0, sort=None, proj=None, skip=None, hint=None, record_kwargs=None):   # default sort used to be =[("_id", 1)]
        self.query = query or {}
        self.record_kwargs = record_kwargs or {}  # kwargs to pass to record when instantiating
        self.raw_records = getattr(db, self.recordClass.collection).find(self.query, proj)
        if sort:
            self.raw_records = self.raw_records.sort(sort)
        self.skip = skip if skip is not None else page * limit
        self.raw_records = self.raw_records.skip(self.skip).limit(limit)
        self.hint = hint
        self.limit = limit
        if hint:
            self.raw_records.hint(hint)
        #self.has_more = limit != 0 and self.raw_records.count() == limit
        self.records = None
        self.current = 0
        self.max = None
        self._local_iter = None

    def __iter__(self):
        self._read_records()
        return iter(self.records)

    def __getitem__(self, item):
        self._read_records()
        return self.records[item]

    def _read_records(self):
        if self.records is None:
            self.records = []
            for rec in self.raw_records:
                self.records.append(self.recordClass(attrs=rec, **self.record_kwargs))
            self.max = len(self.records)

    def __len__(self):
        if not self.max:
            self._read_records()
        return self.max

    def array(self):
        self._read_records()
        return self.records

    def distinct(self, field):
        return self.raw_records.distinct(field)

    def count(self):
        if self.max:
            return self.max
        else:
            kwargs = {k: getattr(self, k) for k in ["skip", "limit", "hint"] if getattr(self, k, None)}
            return int(getattr(db, self.recordClass.collection).count_documents(self.query, **kwargs))

    def update(self, attrs):
        for rec in self:
            rec.load_from_dict(attrs).save()

    def delete(self, force=False, bulk_delete=False):
        if bulk_delete: # Bulk deletion is more performant but will not trigger dependencies.
            getattr(db, self.recordClass.collection).delete_many(self.query)
        else:
            for rec in self:
                rec.delete(force=force)

    def save(self):
        for rec in self:
            rec.save()

    def remove(self, condition_callback):
        self._read_records()
        self.records = [r for r in self.records if not condition_callback(r)]
        self.max = len(self.records)
        return self

    def contents(self, **kwargs):
        return [r.contents(**kwargs) for r in self]


class SluggedAbstractMongoRecordMeta(type):

    def __init__(cls, name, parents, dct):
        super().__init__(name, parents, dct)
        cls._init_cache = {}  # cache for instances instantiated using cls.init()


class SluggedAbstractMongoRecord(AbstractMongoRecord, metaclass=SluggedAbstractMongoRecordMeta):
    """
    Use instead of AbstractMongoRecord when model has unique slug field
    """

    slug_fields = None  # List[str]: Names of slug fields on model. Most commonly will be ["slug"] but there are cases where multiple slug fields are useful.
    cacheable = False

    @classmethod
    def init(cls, slug: str, slug_field_idx: int = None) -> 'AbstractMongoRecord':
        """
        Convenience func to avoid using .load() when you're only passing a slug
        Applicable only if class defines `slug_fields`
        @param slug:
        @param slug_field_idx: Optional index of slug field in case `cls` has multiple slug fields. Index should be between 0 and len(cls.slug_fields) - 1
        @return: instance of `cls` with slug `slug`
        """
        if len(cls.slug_fields) != 1 and slug_field_idx is None:
            raise Exception("Can only call init() if exactly one slug field is defined or `slug_field_idx` is passed as"
                            " a parameter.")
        slug_field_idx = slug_field_idx or 0
        if not cls.cacheable or slug not in cls._init_cache:
            instance = cls().load({cls.slug_fields[slug_field_idx]: slug})
            if cls.cacheable:
                cls._init_cache[slug] = instance
            else:
                return instance
        return cls._init_cache[slug]

    def normalize_slug_field(self, slug_field):
        """
        Set the slug (stored in self[slug_field]) using the first available number at the end if duplicates exist
        """
        slug = self.normalize_slug(getattr(self, slug_field))
        dupe_count = 0
        _id = getattr(self, '_id', None)  # _id is not necessarily set b/c record might not have been saved yet
        temp_slug = slug
        while getattr(db, self.collection).find_one({slug_field: temp_slug, "_id": {"$ne": _id}}):
            dupe_count += 1
            temp_slug = "{}{}".format(slug, dupe_count)
        return temp_slug

    @staticmethod
    def normalize_slug(slug):
        slug = slug.lower()
        slug = slug.replace("", "H").replace("", "h")
        slug = re.sub(r"[ /]", "-", slug.strip())
        slug = re.sub(r"[^a-z0-9()\--]", "", slug)  # parens are for disambiguation on topics
        slug = re.sub(r"-+", "-", slug)
        return slug

    def _normalize(self):
        super()._normalize()
        if self.slug_fields is not None:
            for slug_field in self.slug_fields:
                setattr(self, slug_field, self.normalize_slug_field(slug_field))

    @classmethod
    def validate_slug_exists(cls, slug: str, slug_field_idx: int = None):
        """
        Validate that `slug` points to an existing object of type `cls`. Pass `slug_field` if `cls` has multiple slugs
        associated with it (e.g. TopicLinkType)
        @param slug: Slug to look up
        @param slug_field_idx: Optional index of slug field in case `cls` has multiple slug fields. Index should be
        between 0 and len(cls.slug_fields) - 1
        @return: raises SluggedMongoRecordMissingError is slug doesn't match an existing object
        """
        instance = cls.init(slug, slug_field_idx)
        if not instance:
            raise SluggedMongoRecordMissingError(f"{cls.__name__} with slug '{slug}' does not exist.")


class Cloneable:

    def clone(self, **kwargs) -> 'Cloneable':
        """
        Return new object with all the same data except modifications specified in kwargs
        """
        return self.__class__(**{**self.__dict__, **kwargs})


def get_subclasses(c):
    subclasses = c.__subclasses__()
    for d in list(subclasses):
        subclasses.extend(get_subclasses(d))

    return subclasses


def get_record_classes(concrete=True, dynamic_classes=False):
    sc = get_subclasses(AbstractMongoRecord)
    if concrete:
        sc = [s for s in sc if s.collection is not None]
    if not dynamic_classes:
        from sefaria.model.lexicon import DictionaryEntry
        sc = [s for s in sc if not issubclass(s, DictionaryEntry)]
    return sc


def get_set_classes():
    return get_subclasses(AbstractMongoSet)


"""
    Metaclass to provides a caching mechanism for objects of classes using this metaclass.
    Based on: http://chimera.labs.oreilly.com/books/1230000000393/ch09.html#metacreational

    Not yet used
"""


class CachingType(type):

    def __init__(cls, name, parents, dct):
        super(CachingType, cls).__init__(name, parents, dct)
        cls.__cache = {}

    def __call__(cls, *args, **kwargs):
        key = make_hashable(args), make_hashable(kwargs)
        if key in cls.__cache:
            return cls.__cache[key]
        else:
            obj = super(CachingType, cls).__call__(*args)
            cls.__cache[key] = obj
            return obj


def make_hashable(obj):
    """WARNING: This function only works on a limited subset of objects
    Make a range of objects hashable.
    Accepts embedded dictionaries, lists or tuples (including namedtuples)"""
    if isinstance(obj, collections.Hashable):
        #Fine to be hashed without any changes
        return obj
    elif isinstance(obj, collections.Mapping):
        #Convert into a frozenset instead
        items = list(obj.items())
        for i, item in enumerate(items):
            items[i] = make_hashable(item)
        return frozenset(items)
    elif isinstance(obj, collections.Iterable):
        #Convert into a tuple instead
        ret=[type(obj)]
        for i, item in enumerate(obj):
            ret.append(make_hashable(item))
        return tuple(ret)
    #Use the id of the object
    return id(obj)


"""
Register for model dependencies.
If instances of Model X depend on field f in Model Class Y:
- X subscribes with: subscribe(Y, "f", X.callback)
- On a chance of an instance of f, Y calls: notify(Y, "f", old_value, new_value)

todo: currently doesn't respect any inheritance
todo: find a way to test that dependencies have been regsitered correctly


>>> from sefaria.model import *
>>> def handle(old, new):
...     print "Old : " + old
...     print "New : " + new
...
>>> subscribe(index.Index, "title", handle)
>>> notify(index.Index(), "title", "yellow", "green")
Old : yellow
New : green
"""

deps = {}


def notify(inst, action, **kwargs):
    """
    :param inst: An object instance
    :param action: Currently used: "save", "attributeChange", "delete", "create", ... could also be "change"
    """

    actions_reqs = {
        "attributeChange": ["attr"],
        "save": [],
        "delete": [],
        "create": []
    }
    assert inst
    assert action in actions_reqs

    for arg in actions_reqs[action]:
        if not kwargs.get(arg, None):
            raise Exception("Missing required argument {} in notify {}, {}".format(arg, inst, action))

    if action == "attributeChange":
        callbacks = deps.get((type(inst), action, kwargs["attr"]), None)
        logger.debug("Notify: " + str(inst) + "." + str(kwargs["attr"]) + ": " + str(kwargs["old"]) + " is becoming " + str(kwargs["new"]))
    else:
        logger.debug("Notify: " + str(inst) + " is being " + action + "d.")
        callbacks = deps.get((type(inst), action, None), [])

    if not callbacks:
        return

    for callback in callbacks:
        logger.debug("Notify: Calling " + callback.__name__ + "() for " + inst.__class__.__name__ + " " + action)
        callback(inst, **kwargs)


def subscribe(callback, klass, action, attr=None):
    if not deps.get((klass, action, attr), None):
        deps[(klass, action, attr)] = []
    deps[(klass, action, attr)].append(callback)


def cascade(set_class, attr):
    """
    Handles generic value cascading, for simple key reference changes.
    See examples in dependencies.py
    :param set_class: The set class of the impacted model
    :param attr: The name of the impacted class attribute (fk) that holds the references to the changed attribute (pk)
        There is support for any level of nested attributes, e.g. "contents.properties.value"
    :return: a function that will update 'attr' in 'set_class' and can be passed to subscribe()
    """
    from functools import reduce

    attrs = attr.split(".")
    if len(attrs) == 1:
        return lambda obj, **kwargs: set_class({attr: kwargs["old"]}).update({attr: kwargs["new"]})
    else:
        def foo(obj, **kwargs):
            for rec in set_class({attr: kwargs["old"]}):
                dict_parent = reduce(lambda d, k: d[k], attrs[1:-1], getattr(rec, attrs[0]))
                dict_parent[attrs[-1]] = kwargs["new"]
                setattr(rec, attrs[0], getattr(rec, attrs[0]))
                rec.save()
        return foo


def cascade_to_list(set_class, attr):
    """
    Handles generic value cascading, for keys in attributes that hold lists of keys.
    See examples in dependencies.py
    :param set_class: The set class of the impacted model
    :param attr: The name of the impacted class attribute (fk) that holds the list of references to the changed attribute (pk)
    :return: a function that will update 'attr' in 'set_class' and can be passed to subscribe()
    """
    def foo(obj, **kwargs):
        for rec in set_class({attr: kwargs["old"]}):
            setattr(rec, attr, [kwargs["new"] if e == kwargs["old"] else e for e in getattr(rec, attr)])
            rec.save()

    return foo


def cascade_delete(set_class, fk_attr, pk_attr):
    """
    Handles generic delete cascading, for simple key reference changes.
    See examples in dependencies.py
    :param set_class: The set class of the impacted model
    :param fk_attr: The name of the impacted class attribute (fk) that holds the references to the primary identifier (pk)
            There is support for nested attributes of arbitrary depth - e.g. "contents.subcontents.value"
    :return: a function that will delete values of 'set_class' where 'attr' matches
    """
    return lambda obj, **kwargs: set_class({fk_attr: getattr(obj, pk_attr)}).delete()


def cascade_delete_to_list(set_class, fk_attr, pk_attr):
    """
    Handles generic delete cascading, for keys in attributes that hold lists of keys.
    See examples in dependencies.py
    :param set_class: The set class of the impacted model
    :param fk_attr: The name of the impacted class attribute (fk) that holds the list of references to the primary identifier (pk)
    :param pk_attr:
    :return: a function that will update 'attr' in 'set_class' and can be passed to subscribe()
    """
    def foo(obj, **kwargs):
        for rec in set_class({fk_attr: getattr(obj, pk_attr)}):
            setattr(rec, fk_attr, [e for e in getattr(rec, fk_attr) if e != getattr(obj, pk_attr)])
            rec.save()

    return foo

```

### sefaria/model/queue.py

```
"""
queue.py
Writes to MongoDB Collection: index_queue
"""

import structlog
logger = structlog.get_logger(__name__)

from . import abstract as abst


class IndexQueue(abst.AbstractMongoRecord):
    """
    """
    collection = 'index_queue'

    required_attrs = [
        "lang",
        "type",
        "version",
        "ref"
    ]
    optional_attrs = [

    ]

    #todo: This is written generically.  Do we want elsewhere?
    def save(self):
        duplicate_query = {}
        for attr in self.required_attrs:
            duplicate_query[attr] = getattr(self, attr, None)
        if self.__class__().load(duplicate_query):
            logger.warning("Aborting save of {}.  Duplicate found: {}".format(self.__class__.__name__, vars(self)))
        else:
            super(self.__class__, self).save()


class IndexQueueSet(abst.AbstractMongoSet):
    recordClass = IndexQueue
```

### sefaria/model/autospell.py

```
# -*- coding: utf-8 -*-

"""
http://norvig.com/spell-correct.html
http://scottlobdell.me/2015/02/writing-autocomplete-engine-scratch-python/
"""
from collections import defaultdict
from typing import List, Iterable
import math
import datrie
from unidecode import unidecode
from django.contrib.auth.models import User
from sefaria.model import *
from sefaria.model.schema import SheetLibraryNode
from sefaria.model.tests.topic_test import topic_pool
from sefaria.utils import hebrew
from sefaria.model.following import aggregate_profiles
import re2 as re
import structlog
logger = structlog.get_logger(__name__)


letter_scope = "\u05b0\u05b1\u05b2\u05b3\u05b4\u05b5\u05b6\u05b7\u05b8\u05b9\u05ba\u05bb\u05bc\u05bd" \
            + "\u05c1\u05c2" \
            + "\u05d0\u05d1\u05d2\u05d3\u05d4\u05d5\u05d6\u05d7\u05d8\u05d9\u05da\u05db\u05dc\u05dd\u05de\u05df" \
            + "\u05e0\u05e1\u05e2\u05e3\u05e4\u05e5\u05e6\u05e7\u05e8\u05e9\u05ea" \
            + "\u200e\u200f\u2013\u201c\u201d\ufeff" \
            + " Iabcdefghijklmnopqrstuvwxyz1234567890[]`:;.-,*$()'&?/\""


def normalizer(lang):
    if lang == "he":
        return lambda x: "".join([c if c in letter_scope else unidecode(c) for c in hebrew.normalize_final_letters_in_str(x)])
    return lambda x: "".join([c if c in letter_scope else unidecode(c) for c in str.lower(x)])


splitter = re.compile(r"[\s,]+")


class AutoCompleter(object):
    """
    An AutoCompleter object provides completion services - it is the object in this module designed to be used by the Library.
    It instantiates objects that provide string completion according to different algorithms.
    """
    def __init__(self, lang, lib, include_titles=True, include_categories=False,
                 include_parasha=False, include_lexicons=False, include_users=False, include_collections=False,
                 include_topics=False, min_topics=10, *args, **kwargs):
        """

        :param lang:
        :param library:
        :param titles: List of all titles in this language
        :param args:
        :param kwargs:
        """
        assert lang in ["en", "he"]

        self.lang = lang
        self.library = lib
        self.normalizer = normalizer(lang)
        self.title_trie = TitleTrie(lang, *args, **kwargs)
        self.spell_checker = SpellChecker(lang)
        self.ngram_matcher = NGramMatcher(lang)
        self.other_lang_ac = None
        self.max_completion_length = 200   # Max # of chars of input string, beyond which no completion search is done
        self.max_autocorrect_length = 20   # Max # of chars of input string, beyond which no autocorrect search is done
        # self.prefer_longest = True  # True for titles, False for dictionary entries.  AC w/ combo of two may be tricky.

        PAD = 1000000 # padding for object type ordering.  Allows for internal ordering within type.

        # Titles in library
        if include_titles:
            title_node_dict = self.library.get_title_node_dict(lang)
            tnd_items = [(t, d) for t, d in list(title_node_dict.items()) if not isinstance(d, SheetLibraryNode)]
            titles = [t for t, d in tnd_items]
            normal_titles = [self.normalizer(t) for t, d in tnd_items]
            self.title_trie.add_titles_from_title_node_dict(tnd_items, normal_titles, 1 * PAD)
            self.spell_checker.train_phrases(normal_titles)
            self.ngram_matcher.train_phrases(titles, normal_titles)
        if include_categories:
            categories = self._get_main_categories(library.get_toc_tree().get_root())
            category_names = [c.primary_title(lang) for c in categories]
            normal_category_names = [self.normalizer(c) for c in category_names]
            self.title_trie.add_titles_from_set(categories, "all_node_titles", "primary_title", "full_path", 2 * PAD)
            self.spell_checker.train_phrases(category_names)
            self.ngram_matcher.train_phrases(category_names, normal_category_names)
        if include_parasha:
            parashot = TermSet({"scheme": "Parasha"})
            parasha_names = [n for p in parashot for n in p.get_titles(lang)]
            normal_parasha_names = [self.normalizer(p) for p in parasha_names]
            self.title_trie.add_titles_from_set(parashot, "get_titles", "get_primary_title", "name", 3 * PAD)
            self.spell_checker.train_phrases(parasha_names)
            self.ngram_matcher.train_phrases(parasha_names, normal_parasha_names)
        if include_topics:
            ts_gte10 = TopicSet({"shouldDisplay":{"$ne":False}, "numSources":{"$gte":min_topics}, "subclass": {"$ne": "author"}})
            authors = AuthorTopicSet()  # include all authors
            ts = ts_gte10.array() + authors.array()
            tnames = [name for t in ts for name in t.get_titles(lang)]
            normal_topics_names = [self.normalizer(n) for n in tnames]

            def sub_order_fn(t: Topic) -> int:
                sub_order = PAD - getattr(t, 'numSources', 0) - 1
                if isinstance(t, AuthorTopic):
                    # give a bonus to authors so they don't get drowned out by topics
                    sub_order -= 100
                return sub_order
            self.title_trie.add_titles_from_set(ts, "get_titles", "get_primary_title", "slug", 4 * PAD, sub_order_fn)
            self.spell_checker.train_phrases(tnames)
            self.ngram_matcher.train_phrases(tnames, normal_topics_names)
        if include_users:
            profiles = aggregate_profiles()
            users = User.objects.in_bulk(profiles.keys())
            unames = []
            normal_user_names = []
            for id, u in users.items():
                fullname = u.first_name + " " + u.last_name
                normal_name = self.normalizer(fullname)
                self.title_trie[normal_name] = {
                    "title": fullname,
                    "type": "User",
                    "key": profiles[id]["user"]["slug"],
                    "pic": profiles[id]["user"]["profile_pic_url_small"],
                    "order": (7 * PAD) - profiles[id]["count"],  # lower is earlier
                    "is_primary": True,
                }
                unames += [fullname]
                normal_user_names += [normal_name]
            self.spell_checker.train_phrases(unames)
            self.ngram_matcher.train_phrases(unames, normal_user_names)
        if include_collections:
            cs = CollectionSet({"listed": True, "moderationStatus": {"$ne": "nolist"}})
            cnames = [name for c in cs for name in c.all_names(lang)]
            normal_collection_names = [self.normalizer(n) for n in cnames]
            self.title_trie.add_titles_from_set(cs, "all_names", "primary_name", "slug", 6 * PAD)
            self.spell_checker.train_phrases(cnames)
            self.ngram_matcher.train_phrases(cnames, normal_collection_names)
        if include_lexicons:
            # languages get muddy for lexicons
            # self.prefer_longest = False
            wfs = WordFormSet({"generated_by": {"$ne": "replace_shorthand"}})

            for wf in wfs:
                self.title_trie[self.normalizer(wf.form)] = {
                    "title": wf.form,
                    "key": wf.form,
                    "type": "word_form",
                    "is_primary": True,
                    "order": (2 * PAD),
                }
                if not hasattr(wf, "c_form"):
                    continue
                self.title_trie[self.normalizer(wf.c_form)] = {
                    "title": wf.c_form,
                    "key": wf.form,
                    "type": "word_form",
                    "is_primary": True,
                    "order": (2 * PAD),
                }

            forms = [getattr(wf, "c_form", wf.form) for wf in wfs]
            normal_forms = [self.normalizer(wf) for wf in forms]
            self.spell_checker.train_phrases(forms)
            self.ngram_matcher.train_phrases(forms, normal_forms)

    def set_other_lang_ac(self, ac):
        self.other_lang_ac = ac

    @staticmethod
    def _get_main_categories(otoc):
        cats = []
        for child in otoc.children:
            if child.children and child.primary_title("en") != "Commentary":
                cats += [child]
            for grandchild in child.children:
                if grandchild.children and grandchild.primary_title("en") != "Commentary":
                    cats += [grandchild]
        return cats

    def get_object(self, instring):
        """
        If there is a string matching instring in the title trie, return the data for default object stored for that string.
        Otherwise, return None
        :param instring:
        :return:
        """
        normal = self.normalizer(instring)
        try:
            return self.title_trie[normal][0]
        except KeyError:
            return None

    def get_data(self, instring):
        """
        If there is a string matching instring in the title trie, return the data stored for that string.
        Otherwise, return None
        :param instring:
        :return:
        """
        normal = self.normalizer(instring)
        try:
            return self.title_trie[normal]
        except KeyError:
            return None

    def complete(self, instring, limit=0, redirected=False, type=None, topic_pool=None, exact_continuations=False, order_by_matched_length=False):
        """
        Wrapper for Completions object - prioritizes and aggregates completion results.
        In the case where there are no results, tries to swap keyboards and get completion results from the other language.
        :param instring:
        :param limit: Number of results.  0 is unlimited.
        :param redirected: Is this request redirected from the other language?  Prevents infinite loops.
        :return: completions list, completion objects list
        """
        instring = instring.strip()  # A terminal space causes some kind of awful "include everything" behavior
        instring = self.normalizer(instring)
        if len(instring) >= self.max_completion_length:
            return [], []
        cm = Completions(self, self.lang, instring, limit,
                         do_autocorrect=len(instring) < self.max_autocorrect_length, type=type, topic_pool=topic_pool, exact_continuations=exact_continuations, order_by_matched_length=order_by_matched_length)
        cm.process()
        if cm.has_results():
            return cm.get_completion_strings(), cm.get_completion_objects()

        # No results. Try letter swap
        if not redirected and self.other_lang_ac and not exact_continuations:
            swapped_string = hebrew.swap_keyboards_for_string(instring)
            return self.other_lang_ac.complete(swapped_string, limit, redirected=True)

        return [], []

    '''
    def next_steps_from_node(self, instring):
        """
        Used in the case when the instring matches a node.  Provides the continuations of that string for its children nodes.
        :param instring:
        :return:
        """
        # Assume that instring is the name of a node.  Extend with a comma, and get next nodes in the Trie
        normal_string = self.normalizer(instring)
        try:
            titles_and_objects = [(v["title"], v) for k, all_v in self.title_trie.items(normal_string + ",") for v in all_v]
            titles_and_objects.sort(key=lambda v: len(v[0]))   # better than sort would be the shallow option of pygtrie, but datrie doesn't have
            return [t for t,o in titles_and_objects], [o for t,o in titles_and_objects]
        except KeyError:
            return []
    '''



class Completions(object):

    _type_norm_map = {
        "Collection": "Collection",
        "AuthorTopic": "Topic",
        "TocCategory": "TocCategory",
        "PersonTopic": "Topic",
        "Topic": "Topic",
        "ref": "ref",
        "Term": "Term",
        "User": "User"}

    def __init__(self, auto_completer, lang, instring, limit=0, do_autocorrect = True, type=None, topic_pool=None, exact_continuations=False, order_by_matched_length=False):
        """
        An object that contains a single search, delegates to different methods of completions, and aggregates results.
        :param auto_completer:
        :param lang:
        :param instring:
        :param limit: Number of results.  0 is unlimited.
        :param do_autocorrect: Defaults to true.  Set to false to prevent resource burn on long strings.
        """
        assert lang in ["en", "he"]

        self.auto_completer = auto_completer
        self.lang = lang
        self.instring = instring
        self.normal_string = normalizer(lang)(instring)
        self.limit = limit
        self.keys_covered = set()
        self.completions = []  # titles to return
        self.completion_objects = []
        self.do_autocorrect = do_autocorrect
        self._completion_strings = []
        self._raw_completion_strings = []  # May have dupes
        self._completion_objects = []
        self._candidate_type_counters = defaultdict(int)
        self._type_limit = 3
        self.type = type
        self.topic_pool = topic_pool
        self.exact_continuations = exact_continuations
        self.order_by_matched_length = order_by_matched_length

    def has_results(self):
        return len(self._completion_objects) > 0

    def get_completion_objects(self):
        return self._completion_objects

    def get_completion_strings(self):
        return self._completion_strings

    def process(self):
        """
        Execute the completion search
        :return:
        """
        self._collect_candidates()
        self._trim_results()

    def _trim_results(self):
        seen = set()

        if self.limit == 0:
            self._completion_strings = [x for x in self._raw_completion_strings if x not in seen and not seen.add(x)]
            return

        obj_count = 0
        for x in self._raw_completion_strings:
            obj_count += 1
            if x in seen:
                continue
            else:
                seen.add(x)
                self._completion_strings += [x]
            if len(seen) >= self.limit:
                break

        self._completion_objects = self._completion_objects[:obj_count]

        return

    def _candidate_order(self, c):
        self._candidate_type_counters[c[1]["type"]] += 1
        if self._candidate_type_counters[c[1]["type"]] <= self._type_limit:
            return c[1]["order"]
        else:
            return c[1]["order"] * 100

    def _filter_completions_by_type(self, completion_strings, completion_objects):
        filtered_completions = [
            (cs, co)
            for cs, co in zip(completion_strings, completion_objects)
            if self._has_required_type(co)
        ]
        list1, list2 = zip(*filtered_completions) if filtered_completions else ([], [])
        return list(list1), list(list2)

    def _has_required_type(self, completion_object):
        if not self.type:
            return True

        co_type = completion_object["type"]
        normalized_type = self._type_norm_map[co_type]

        if normalized_type != self.type:
            return False

        if normalized_type == 'Topic' and self.topic_pool:
            return self.topic_pool in completion_object["topic_pools"]

        return True


    def _collect_candidates(self):
        # Match titles that begin exactly this way
        cs, co = self.get_new_continuations_from_string(self.normal_string)
        cs, co = self._filter_completions_by_type(cs, co)

        joined = list(zip(cs, co))
        if len(joined):
            # joined.sort(key=lambda w: w[1]["order"])
            joined.sort(key=self._candidate_order)
            if self.order_by_matched_length:
                joined.sort(key=lambda i: len(i[0]))
            self._raw_completion_strings, self._completion_objects = [list(_) for _ in zip(*joined)]
        else:
            self._raw_completion_strings, self._completion_objects = [], []

        if self.limit and len(set(self._raw_completion_strings)) >= self.limit:
            return

        # This string of characters deeper in the string
        self._collect_candidates_later_in_string(do_autocorrect=False)

        if not self.do_autocorrect:
            return 

        # single misspellings
        single_edits = self.auto_completer.spell_checker.single_edits(self.normal_string)
        for edit in single_edits:
            cs, co =  self.get_new_continuations_from_string(edit)
            cs, co = self._filter_completions_by_type(cs, co)

            self._raw_completion_strings += cs
            self._completion_objects += co
            if self._is_past_limit() or self.exact_continuations:
                return

        # A minor variations of this string of characters deeper in the string
        self._collect_candidates_later_in_string(do_autocorrect=True)

        return

    def _is_past_limit(self):
        return self.limit and len(set(self._raw_completion_strings)) >= self.limit

    def _collect_candidates_later_in_string(self, do_autocorrect=True):
        if do_autocorrect:
            tokens = self.auto_completer.spell_checker.correct_phrase(self.normal_string)
        else:
            tokens = splitter.split(self.normal_string)

        try:
            for suggestion in self.auto_completer.ngram_matcher.guess_titles(tokens):
                k = normalizer(self.lang)(suggestion)
                try:
                    all_v = self.auto_completer.title_trie[k]
                    _, all_v = self._filter_completions_by_type(all_v, all_v)
                except KeyError:
                    all_v = []
                for v in all_v:
                    if (v["type"], v["key"]) not in self.keys_covered:
                        self._completion_objects += [v]
                        self._raw_completion_strings += [v["title"]]
                        self.keys_covered.add((v["type"], v["key"]))
                        if self._is_past_limit():
                            return
        except ValueError:
            pass

    def get_new_continuations_from_string(self, str):
        """
        Find titles beginning with this string.
        Adds titles to self.completions, noting covered nodes in self.nodes_covered
        :param str: String of beginning characters
        :return:
        """

        try:
            # skip = -1 if self.auto_completer.prefer_longest else 1
            all_continuations = self.auto_completer.title_trie.items(str)
            all_continuations.sort(key=lambda i: len(i[0]))
        except KeyError:
            return []

        # Use one title for each book before any duplicate match titles
        # Prefer primary titles
        # todo: don't list all subtree titles, if string doesn't cover base title
        completions = []
        completion_objects = []
        non_primary_matches = []
        for k, all_v in all_continuations:
            for v in all_v:
                if v["is_primary"] and (v["type"], v["key"]) not in self.keys_covered:
                    if v["type"] == "ref" or v["type"] == "word_form" or v["type"] == "Topic":
                        completion_objects += [v]
                        completions += [v["title"]]
                    else:
                        completion_objects.insert(0, v)
                        completions.insert(0, v["title"])
                    self.keys_covered.add((v["type"], v["key"]))
                else:
                    non_primary_matches += [(k, v)]

        # Iterate through non primary ones, until we cover the whole node-space
        for k, v in non_primary_matches:
            if (v["type"], v["key"]) not in self.keys_covered:
                if v["type"] == "ref" and len(v["title"]) <= 4:  # The > 4 looks to get rid of "Gen" "Exod" and the like.
                    continue
                completions += [v["title"]]
                completion_objects += [v]
                self.keys_covered.add((v["type"], v["key"]))

        return [completions, completion_objects]


class LexiconTrie(datrie.Trie):

    def __init__(self, lexicon_name):
        super(LexiconTrie, self).__init__(letter_scope)

        for entry in LexiconEntrySet({"parent_lexicon": lexicon_name}, sort=[("_id", -1)]):
            self[hebrew.strip_nikkud(entry.headword)] = self.get(hebrew.strip_nikkud(entry.headword), []) + [entry.headword]
            for ahw in entry.get_alt_headwords():
                self[hebrew.strip_nikkud(ahw)] = self.get(hebrew.strip_nikkud(ahw), []) + [entry.headword]


class TitleTrie(datrie.Trie):
    """
    Character Trie built up of the titles in the library.
    Stored items are lists of dicts, each dict having details about one system object.
    {
        "title": string
        "key": string
        "type": string
        "is_primary": bool
    }
    """

    def __init__(self, lang, *args, **kwargs):
        assert lang in ["en", "he"]
        super(TitleTrie, self).__init__(letter_scope)
        self.lang = lang
        self.normalizer = normalizer(lang)

    def __setitem__(self, key, value):
        try:
            item = self[key]
            assert isinstance(item, list)

            super(TitleTrie, self).__setitem__(key, item + [value])
        except KeyError:
            super(TitleTrie, self).__setitem__(key, [value])

    def add_titles_from_title_node_dict(self, tnd_items, normal_titles, order):
        for (title, snode), norm_title in zip(tnd_items, normal_titles):
            self[norm_title] = {
                "title": title,
                "key": snode.full_title("en"),
                "type": "ref",
                "is_primary": title == snode.full_title(self.lang),
                "order": order
            }

    def add_titles_from_set(self, recordset, all_names_method, primary_name_method, keyattr, base_order, sub_order_fn=None):
        """

        :param recordset: Instance of a subclass of AbstractMongoSet, or a List of objects
        :param all_names_method: Name of method that will return list of titles, when passed lang
        :param primary_name_method: Name of method that will return primary title, when passed lang
        :param keyattr: Name of attribute that will give key to object
        :param sub_order_fn: optional function which takes an AbstractMongoRecord as a parameter and returns an integer between 0 and PAD-1 inclusive. the lower the number, the higher ranked this object will be among objects of the same type.
        :return:
        """
        done = set()
        for obj in recordset:
            key = getattr(obj, keyattr, None)
            if not key:
                continue
            sub_order = 0 if sub_order_fn is None else sub_order_fn(obj)
            title = getattr(obj, primary_name_method)(self.lang)
            if title:
                norm_title = self.normalizer(title)
                done.add(norm_title)
                self[norm_title] = {
                    "title": title,
                    "type": obj.__class__.__name__,
                    "key": tuple(key) if isinstance(key, list) else key,
                    "is_primary": True,
                    "order": base_order + sub_order,
                    "topic_pools": obj.get_pools() if isinstance(obj, Topic) else []
                }

            titles = getattr(obj, all_names_method)(self.lang)
            for title in titles:
                norm_title = self.normalizer(title)
                if norm_title in done:
                    continue
                done.add(norm_title)
                self[norm_title] = {
                    "title": title,
                    "type": obj.__class__.__name__,
                    "key": tuple(key) if isinstance(key, list) else key,
                    "is_primary": False,
                    "order": base_order + sub_order,
                    "topic_pools": obj.get_pools() if isinstance(obj, Topic) else []
                }


class SpellChecker(object):
    """
    Utilities to find small edits of a given string,
    and also to find edits of a given string that result in words in our title list.
    """
    def __init__(self, lang):
        assert lang in ["en", "he"]
        self.lang = lang
        self.normalizer = normalizer(lang)
        if lang == "en":
            self.letters = "abcdefghijklmnopqrstuvwxyz'."
        else:
            self.letters = hebrew.ALPHABET_22 + hebrew.GERESH + hebrew.GERSHAYIM + '".' + "'"
        self.WORDS = defaultdict(int)

    def train_phrases(self, phrases):
        """
        :param phrases: A list of normalized (lowercased, etc) strings
        :return:
        """
        for p in phrases:
            for w in splitter.split(p):
                if not w:
                    continue
                self.WORDS[w] += 1

    def single_edits(self, word, hold_first_letter=True):
        """All edits that are one edit away from `word`."""
        start      = 1 if hold_first_letter else 0
        splits     = [(word[:i], word[i:])    for i in range(start, len(word) + 1)]
        deletes    = [L + R[1:]               for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
        replaces   = [L + c + R[1:]           for L, R in splits if R for c in self.letters]
        inserts    = [L + c + R               for L, R in splits for c in self.letters]
        return set(deletes + transposes + replaces + inserts)

    def _known_edits2(self, word):
        """All edits that are two edits away from `word`."""
        return (e2 for e1 in self.single_edits(word) for e2 in self.single_edits(e1) if e2 in self.WORDS)

    def _known(self, words):
        """The subset of `words` that appear in the dictionary of WORDS."""
        return set(w for w in words if w in self.WORDS)

    def correct_token(self, token):
        candidates = self._known([token]) or self._known(self.single_edits(token)) or [token] #self._known_edits2(token) or [token]
        return max(candidates, key=self.WORDS.get)

    def correct_phrase(self, text):
        normal_text = self.normalizer(text)
        tokens = splitter.split(normal_text)
        return [self.correct_token(token) for token in tokens if token]


class NGramMatcher(object):
    """
    Utility to find titles in our list that roughly match a given string. 
    """

    # MIN_N_GRAM_SIZE = 3

    def __init__(self, lang):
        assert lang in ["en", "he"]
        self.lang = lang
        self.normalizer = normalizer(lang)
        self.token_to_titles = defaultdict(list)
        self.token_trie = datrie.BaseTrie(letter_scope)
        self._tfidf_scorer = TfidfScorer()

    def train_phrases(self, titles, normal_titles):
        for title, normal_title in zip(titles, normal_titles):
            tokens = tuple(splitter.split(normal_title))
            self._tfidf_scorer.train_tokens(tokens)
            for token in tokens:
                if not token:
                    continue
                self.token_to_titles[token].append((title, tokens))
        for k in self.token_to_titles.keys():
            self.token_trie[k] = 1

    def _get_real_tokens_from_possible_n_grams(self, tokens):
        return {token: self.token_trie.keys(token) for token in tokens}

    def _get_scored_titles(self, real_token_map):
        total_ngrams = len(real_token_map)
        possibilities__scores = []
        possibilties_score_map = defaultdict(int)
        title_ngram_map = defaultdict(set)  # map of ngram inputs that matched this title (through mapping of ngrams to real tokens)
        for ngram_token, real_tokens in real_token_map.items():
            for real_token in real_tokens:
                possibilities = self.token_to_titles.get(real_token, [])
                for (title, title_tokens) in possibilities:
                    possibilties_score_map[title] += self._tfidf_scorer.score_token(real_token, title_tokens)
                    title_ngram_map[title].add(ngram_token)

        for title, matched_token_score in possibilties_score_map.items():
            matched_ngrams = title_ngram_map[title]
            score = matched_token_score - (total_ngrams - len(matched_ngrams))
            possibilities__scores.append((title, score))
        return possibilities__scores

    def _filtered_results(self, titles__scores):
        score_threshold = 0.5  # NOTE: score is no longer between 0 and 1. This threshold is somewhat arbitrary and may need adjusting.
        return [tuple_obj[0] for tuple_obj in titles__scores if tuple_obj[1] >= score_threshold]

    def guess_titles(self, tokens):
        real_token_map = self._get_real_tokens_from_possible_n_grams(tokens)
        titles__scores = self._get_scored_titles(real_token_map)
        titles__scores.sort(key=lambda t: t[1], reverse=True)
        return self._filtered_results(titles__scores)


class TfidfScorer:

    def __init__(self):
        self._token_idf_map = {}
        self._missing_idf_value = 0
        self._total_documents = 0

    def train_tokens(self, tokens: Iterable[str]) -> None:
        self._total_documents += 1
        token_document_count_map = defaultdict(int)
        for token in set(tokens):
            token_document_count_map[token] += 1
        for token, count in token_document_count_map.items():
            idf = math.log(self._total_documents / (1 + token_document_count_map[token]))
            self._token_idf_map[token] = idf
        self._missing_idf_value = math.log(self._total_documents)

    def score_token(self, query_token: str, doc_tokens):
        tf = 1 / (1 + len(doc_tokens))  # approximation of tf excluding # of times token appears in document. this seems like a small factor for AC and adds function calls.
        idf = self._token_idf_map.get(query_token, self._missing_idf_value)
        return tf * idf


```

### sefaria/model/link.py

```
"""
link.py
Writes to MongoDB Collection: links
"""

import regex as re
from bson.objectid import ObjectId
from sefaria.model.text import AbstractTextRecord, VersionSet
from sefaria.system.exceptions import DuplicateRecordError, InputError, BookNameError
from sefaria.system.database import db
from . import abstract as abst
from . import text

import structlog
logger = structlog.get_logger(__name__)


class Link(abst.AbstractMongoRecord):
    """
    A link between two texts (or more specifically, two references)
    """
    collection = 'links'
    history_noun = 'link'

    required_attrs = [
        "type",             # string of connection type
        "refs",             # list of refs connected
    ]

    optional_attrs = [
        "expandedRefs0",    # list of refs corresponding to `refs.0`, but breaking ranging refs down into individual segments
        "expandedRefs1",    # list of refs corresponding to `refs.1`, but breaking ranging refs down into individual segments
        "anchorText",       # string of dibbur hamatchil (largely depcrated) 
        "availableLangs",   # list of lists corresponding to `refs` showing languages available, e.g. [["he"], ["he", "en"]]  
        "highlightedWords", # list of strings to be highlighted when presenting a text as a connection
        "auto",             # bool whether generated by automatic process
        "generated_by",     # string in ("add_commentary_links", "add_links_from_text", "mishnah_map")
        "source_text_oid",  # oid of text from which link was generated
        "is_first_comment", # set this flag to denote its the first comment link between the two texts in the link
        "first_comment_indexes", # Used when is_first_comment is True. List of the two indexes of the refs.
        "first_comment_section_ref", # Used when is_first_comment is True. First comment section ref.
        "inline_reference",  # dict with keys "data-commentator" and "data-order" to match an inline reference (itag)
        "charLevelData",     # list of length 2. Containing 2 dicts coresponding to the refs list, each dict consists of the following keys: ["startChar","endChar","versionTitle","language"]. *if one of the refs is a Pasuk the startChar and endChar keys are startWord and endWord. This attribute was created for the quotation finder
        "score",             # int. represents how "good"/accurate the link is. introduced for quotations finder
        "inline_citation",    # bool acts as a flag for wrapped refs logic to run on the segments where this citation is inline.
        "versions",          # only for cases when type is `essay`: list of versionTitles corresponding to `refs`, where first versionTitle corresponds to Index of first ref, and each value is a dictionary of language and title of version
        "displayedText"       # only for cases when type is `essay`: dictionary of en and he strings to be displayed
    ]

    def _normalize(self):
        self.auto = getattr(self, 'auto', False)
        self.generated_by = getattr(self, "generated_by", None)
        self.source_text_oid = getattr(self, "source_text_oid", None)
        self.type = getattr(self, "type", "").lower()
        self.refs = [text.Ref(self.refs[0]).normal(), text.Ref(self.refs[1]).normal()]

        if getattr(self, "_id", None):
            self._id = ObjectId(self._id)

    def _validate(self):
        assert super(Link, self)._validate()

        if self.type == "essay":   # when type is 'essay', versionTitles should correspond to indices referred to in self.refs
            assert hasattr(self, "versions") and hasattr(self, "displayedText"), "You must set versions and displayedText fields for type 'essay'."
            assert "en" in self.displayedText[0] and "he" in self.displayedText[0] and "en" in self.displayedText[1] and "he" in self.displayedText[1], \
                "displayedText field must be a list of dictionaries with 'he' and 'en' fields."
            for ref, version in zip(self.refs, self.versions):
                versionTitle = version["title"]
                versionLanguage = version["language"] if "language" in version else None
                index_title = text.Ref(ref).index.title
                if versionTitle not in ["ALL", "NONE"]:
                    assert VersionSet({"title": index_title, "versionTitle": versionTitle, "language": versionLanguage}).count() > 0, \
                        f"No version found for book '{index_title}', with versionTitle '{versionTitle}', and language '{versionLanguage}'"


        if False in self.refs:
            return False

        if hasattr(self, "charLevelData"):
            try:
                assert type(self.charLevelData) is list
                assert len(self.charLevelData) == 2
                assert type(self.charLevelData[0]) is dict
                assert type(self.charLevelData[1]) is dict
            except AssertionError:
                raise InputError(f'Structure of the charLevelData in Link is wrong. link refs: {self.refs[0]} - {self.refs[1]}. charLevelData should be a list of length 2 containing 2 dicts coresponding to the refs list, each dict consists of the following keys: ["startChar","endChar","versionTitle","language"]')
            assert self.charLevelData[0]['versionTitle'] in [v['versionTitle'] for v in text.Ref(self.refs[0]).version_list()], 'Dictionaries in charLevelData should be in correspondence to the "refs" list'
        return True

    def _pre_save(self):
        if getattr(self, "_id", None) is None:
            # Don't bother saving a connection that already exists, or that has a more precise link already
            if self.refs != sorted(self.refs):
                if hasattr(self, 'charLevelData'):
                    self.charLevelData.reverse()
                if getattr(self, "versions", False) and getattr(self, "displayedText", False):
                    # if reversed self.refs, make sure to reverse self.versions and self.displayedText
                    self.versions = self.versions[::-1]
                    self.displayedText = self.displayedText[::-1]
            self.refs = sorted(self.refs)  # make sure ref order is deterministic
            samelink = Link().load({"refs": self.refs})

            if not samelink:
                #check for samelink section level vs ranged ref
                oref0 = text.Ref(self.refs[0])
                oref1 = text.Ref(self.refs[1])
                section0 = oref0.section_ref()
                section1 = oref1.section_ref()
                if oref0.is_range() and oref0.all_segment_refs() == section0.all_segment_refs():
                    samelink = Link().load({"$and": [{"refs": section0.normal()}, {"refs": self.refs[1]}]})
                elif oref0.is_section_level():
                    ranged0 = text.Ref(f"{oref0.all_segment_refs()[0].normal()}-{oref0.all_segment_refs()[-1].normal()}") #without nomalizing the first ref can have '<d>' which cause invalid ranged ref
                    samelink = Link().load({"$and": [{"refs": ranged0.normal()}, {"refs": self.refs[1]}]})
                elif oref1.is_range() and oref1.all_segment_refs() == section1.all_segment_refs():
                    samelink = Link().load({"$and": [{"refs": section1.normal()}, {"refs": self.refs[0]}]})
                elif oref1.is_section_level():
                    ranged1 = text.Ref(f"{oref1.all_segment_refs()[0].normal()}-{oref1.all_segment_refs()[-1].normal()}")
                    samelink = Link().load({"$and": [{"refs": ranged1.normal()}, {"refs": self.refs[0]}]})

            if samelink:
                if hasattr(self, 'score') and hasattr(self, 'charLevelData'):
                    samelink.score = self.score
                    samelink.charLevelData = self.charLevelData
                    samelink.save()
                    raise DuplicateRecordError("Updated existing link with the new score and charLevelData data")

                elif not self.auto and self.type and not samelink.type:
                    samelink.type = self.type
                    samelink.save()
                    raise DuplicateRecordError("Updated existing link with new type: {}".format(self.type))

                elif self.auto and not samelink.auto:
                    samelink.auto = self.auto
                    samelink.generated_by = self.generated_by
                    samelink.source_text_oid = self.source_text_oid
                    samelink.type = self.type
                    samelink.refs = self.refs  #in case the refs are reversed. switch them around
                    samelink.save()
                    raise DuplicateRecordError("Updated existing link with auto generation data {} - {}".format(self.refs[0], self.refs[1]))
                else:
                    raise DuplicateRecordError("Link already exists {} - {}. Try editing instead.".format(self.refs[0], self.refs[1]))

            else:
                #find a potential link that already has a more precise ref of either of this link's refs.
                preciselink = Link().load(
                    {'$and':[text.Ref(self.refs[0]).ref_regex_query(), text.Ref(self.refs[1]).ref_regex_query()]}
                )

                if preciselink:
                    # logger.debug("save_link: More specific link exists: " + link["refs"][1] + " and " + preciselink["refs"][1])
                    if getattr(self, "_override_preciselink", False):
                        preciselink.delete()
                        self.generated_by = self.generated_by+'_preciselink_override'
                        #and the new link will be posted (supposedly)
                    else:
                        raise DuplicateRecordError("A more precise link already exists: {} - {}".format(preciselink.refs[0], preciselink.refs[1]))
                # else: # this is a good new link

        if not getattr(self, "_skip_lang_check", False):
            self._set_available_langs()

        if not getattr(self, "_skip_expanded_refs_set", False):
            self._set_expanded_refs()

    def _sanitize(self):
        """
        bleach all input to protect against security risks
        """
        all_attrs = self.required_attrs + self.optional_attrs
        for attr in all_attrs:
            val = getattr(self, attr, None)
            if isinstance(val, str):
                setattr(self, attr, AbstractTextRecord.remove_html(val))

    def _set_available_langs(self):
        LANGS_CHECKED = ["he", "en"]
        
        def lang_list(ref):
            return [lang for lang in LANGS_CHECKED if text.Ref(ref).is_text_fully_available(lang)]
        self.availableLangs = [lang_list(ref) for ref in self.refs]

    def _set_expanded_refs(self):
        self.expandedRefs0 = [oref.normal() for oref in text.Ref(self.refs[0]).all_segment_refs()]
        self.expandedRefs1 = [oref.normal() for oref in text.Ref(self.refs[1]).all_segment_refs()]

    def ref_opposite(self, from_ref, as_tuple=False):
        """
        Return the Ref in this link that is opposite the one matched by `from_ref`.
        The matching of from_ref uses Ref.regex().  Matches are to the specific ref, or below.
        If neither Ref matches from_ref, None is returned.
        :param from_ref: A Ref object
        :param as_tuple: If true, return a tuple (Ref,Ref), where the first Ref is the given from_ref,
        or one more specific, and the second Ref is the opposing Ref in the link.
        :return:
        """
        reg = re.compile(from_ref.regex())
        if reg.match(self.refs[1]):
            from_tref = self.refs[1]
            opposite_tref = self.refs[0]
        elif reg.match(self.refs[0]):
            from_tref = self.refs[0]
            opposite_tref = self.refs[1]
        else:
            return None

        if opposite_tref:
            try:
                if as_tuple:
                    return text.Ref(from_tref), text.Ref(opposite_tref)
                return text.Ref(opposite_tref)
            except InputError:
                return None


class LinkSet(abst.AbstractMongoSet):
    recordClass = Link

    def __init__(self, query_or_ref={}, page=0, limit=0):
        '''
        LinkSet can be initialized with a query dictionary, as any other MongoSet.
        It can also be initialized with a :py:class: `sefaria.text.Ref` object,
        and will use the :py:meth: `sefaria.text.Ref.regex()` method to return the set of Links that refer to that Ref or below.
        :param query_or_ref: A query dict, or a :py:class: `sefaria.text.Ref` object
        '''
        try:
            regex_list = query_or_ref.regex(as_list=True)
            ref_clauses = [{"expandedRefs0": {"$regex": r}} for r in regex_list]
            ref_clauses += [{"expandedRefs1": {"$regex": r}} for r in regex_list]
            super(LinkSet, self).__init__({"$or": ref_clauses}, page, limit)
        except AttributeError:
            super(LinkSet, self).__init__(query_or_ref, page, limit)

    def filter(self, sources):
        """
        Filter LinkSet according to 'sources' which may be either
        - a string, naming a text or category to include
        - an array of strings, naming multiple texts or categories to include

        ! Returns a list of Links, not a LinkSet
        """
        if isinstance(sources, str):
            return self.filter([sources])

        # Expand Categories
        categories = text.library.get_text_categories()
        expanded_sources = []
        for source in sources:
            expanded_sources += [source] if source not in categories else text.library.get_indexes_in_category(source)

        regexes = [text.Ref(source).regex() for source in expanded_sources]
        filtered = []
        for source in self:
            if any([re.match(regex, source.refs[0]) for regex in regexes] + [re.match(regex, source.refs[1]) for regex in regexes]):
                filtered.append(source)

        return filtered

    # This could be implemented with Link.ref_opposite, but we should speed test it first.
    def refs_from(self, from_ref, as_tuple=False, as_link=False):
        """
        Get a collection of Refs that are opposite the given Ref, or a more specific Ref, in this link set.
        Note that if from_ref is more specific than the criterion that created the linkSet,
        then the results of this function will implicitly be filtered according to from_ref.
        :param from_ref: A Ref object
        :param as_tuple: If true, return a collection of tuples (Ref,Ref), where the first Ref is the given from_ref,
        or one more specific, and the second Ref is the opposing Ref in the link.
        :return: List of Ref objects
        """
        reg = re.compile(from_ref.regex())
        refs = []
        for link in self:
            if reg.match(link.refs[1]):
                from_tref = link.refs[1]
                opposite_tref = link.refs[0]
            elif reg.match(link.refs[0]):
                from_tref = link.refs[0]
                opposite_tref = link.refs[1]
            else:
                opposite_tref = False

            if opposite_tref:
                try:
                    if as_link:
                        refs.append((link, text.Ref(opposite_tref)))
                    elif as_tuple:
                        refs.append((text.Ref(from_tref), text.Ref(opposite_tref)))
                    else:
                        refs.append(text.Ref(opposite_tref))
                except:
                    pass
        return refs

    @classmethod
    def get_first_ref_in_linkset(cls, base_text, dependant_text):
        """
        Given a linkset
        :param from_ref: A Ref object
        :param as_tuple: If true, return a collection of tuples (Ref,Ref), where the first Ref is the given from_ref,
        or one more specific, and the second Ref is the opposing Ref in the link.
        :return: List of Ref objects
        """
        retlink = None
        orig_ref = text.Ref(dependant_text)
        base_text_ref = text.Ref(base_text)
        ls = cls(
            {'$and': [orig_ref.ref_regex_query(), base_text_ref.ref_regex_query()],
             "generated_by": {"$ne": "add_links_from_text"}}
        )
        refs_from = ls.refs_from(base_text_ref, as_link=True)
        sorted_refs_from = sorted(refs_from, key=lambda r: r[1].order_id())
        if len(sorted_refs_from):
            retlink = sorted_refs_from[0][0]
        return retlink

    def summary(self, relative_ref):
        """
        Returns a summary of the counts and categories in this link set,
        relative to 'relative_ref'.
        """
        results = {}
        for link in self:
            ref = link.refs[0] if link.refs[1] == relative_ref.normal() else link.refs[1]
            try:
                oref = text.Ref(ref)
            except:
                continue
            cat  = oref.primary_category
            if (cat not in results):
                results[cat] = {"count": 0, "books": {}}
            results[cat]["count"] += 1
            if (oref.book not in results[cat]["books"]):
                results[cat]["books"][oref.book] = 0
            results[cat]["books"][oref.book] += 1

        return [{"name": key, "count": results[key]["count"], "books": results[key]["books"] } for key in list(results.keys())]


def process_index_title_change_in_links(indx, **kwargs):
    print("Cascading Links {} to {}".format(kwargs['old'], kwargs['new']))

    # ensure that the regex library we're using here is the same regex library being used in `Ref.regex`
    from .text import re as reg_reg
    patterns = [pattern.replace(reg_reg.escape(indx.title), reg_reg.escape(kwargs["old"]))
                for pattern in text.Ref(indx.title).regex(as_list=True)]
    queries = [{'refs': {'$regex': pattern}} for pattern in patterns]
    links = LinkSet({"$or": queries})
    for l in links:
        l.refs = [r.replace(kwargs["old"], kwargs["new"], 1) if re.search('|'.join(patterns), r) else r for r in l.refs]
        l.expandedRefs0 = [r.replace(kwargs["old"], kwargs["new"], 1) if re.search('|'.join(patterns), r) else r for r in l.expandedRefs0]
        l.expandedRefs1 = [r.replace(kwargs["old"], kwargs["new"], 1) if re.search('|'.join(patterns), r) else r for r in l.expandedRefs1]
        try:
            l._skip_lang_check = True
            l._skip_expanded_refs_set = True
            l.save()
        except InputError: #todo: this belongs in a better place - perhaps in abstract
            logger.warning("Deleting link that failed to save: {} - {}".format(l.refs[0], l.refs[1]))
            l.delete()


def process_index_delete_in_links(indx, **kwargs):
    from sefaria.model.text import prepare_index_regex_for_dependency_process
    pattern = prepare_index_regex_for_dependency_process(indx)
    LinkSet({"refs": {"$regex": pattern}}).delete()


def update_link_language_availabiliy(oref, lang=None, available=None):
    """
    Updates langauge availibility tags in links connected to `oref`.
    If `lang` and `available` a present set the values provided.
    If not, re-save the links, triggering a lookup of content availability. 
    """
    links = oref.linkset()
    
    if lang and available is not None:
        for link in links:
            if not getattr(link, "availableLangs", None):
                link.save()
                continue
            pos = 0 if oref.overlaps(text.Ref(link.refs[0])) else 1

            if available:
                link.availableLangs[pos].append(lang)
                link.availableLangs[pos] = list(set(link.availableLangs[pos]))
            else:
                link.availableLangs[pos] = [alang for alang in link.availableLangs[pos] if alang != lang]
            link._skip_lang_check = True
            link.save()
    else:
        links.save()


#get_link_counts() and get_book_link_collection() are used in Link Explorer.
#They have some client formatting code in them; it may make sense to move them up to sefaria.client or sefaria.helper
def get_link_counts(cat1, cat2):
    """
    Returns a list of book to book link counts for books within `cat1` and `cat2`
    Parameters may name either a category or a individual book
    """
    titles = []
    for c in [cat1, cat2]:
        ts = text.library.get_indexes_in_corpus(c) or text.library.get_indexes_in_category(c)
        if len(ts) == 0:
            try:
                text.library.get_index(c)
                ts.append(c)
            except BookNameError:
                return {"error": "No results for {}".format(c)}
        titles.append(ts)

    result = []
    for title1 in titles[0]:
        for title2 in titles[1]:
            re1 = r"^{} \d".format(title1)
            re2 = r"^{} \d".format(title2)
            links = LinkSet({"$and": [{"refs": {"$regex": re1}}, {"refs": {"$regex": re2}}]})
            count = links.count()
            if count:
                result.append({"book1": title1, "book2": title2, "count": count})

    return result


# todo: check vis-a-vis commentary refactor
def get_category_commentator_linkset(cat, commentator):
    return LinkSet({"$or": [
                        {"$and": [{"refs": {"$regex": r"{} \d".format(t)}},
                                  {"refs": {"$regex": "^{} on {}".format(commentator, t)}}
                                  ]
                         }
                        for t in text.library.get_indexes_in_category(cat)]
                    })


def get_category_category_linkset(cat1, cat2):
    """
    Return LinkSet of links between the given book and category.
    :param book: String
    :param cat: String
    :return:
    """
    queries = []
    titles = []
    regexes = []
    clauses = []

    for i, cat in enumerate([cat1, cat2]):
        queries += [{"$and": [{"categories": cat}, {'dependence': {'$in': [False, None]}}]}]
        titles += [text.library.get_indexes_in_corpus(cat) or text.library.get_indexes_in_category(cat)]
        if len(titles[i]) == 0:
            raise IndexError("No results for {}".format(queries[i]))

        regexes += [[]]
        for t in titles[i]:
            regexes[i] += text.Ref(t).regex(as_list=True)

        clauses += [[]]
        for rgx in regexes[i]:
            if cat1 == cat2:
                clauses[i] += [{"refs.{}".format(i): {"$regex": rgx}}]
            else:
                clauses[i] += [{"refs": {"$regex": rgx}}]

    return LinkSet({"$and": [{"$or": clauses[0]}, {"$or": clauses[1]}]})


def get_book_category_linkset(book, cat):
    """
    Return LinkSet of links between the given book and category, or between two books.
    :param book: String
    :param cat: String
    :return:
    """
    titles = text.library.get_indexes_in_corpus(cat) or text.library.get_indexes_in_category(cat)
    if len(titles) == 0:
        try:
            text.library.get_index(cat)
            titles = [cat]
        except BookNameError:
            return {"error": "No results for {}".format(cat)}

    book_re = text.Ref(book).regex()
    cat_re = r'^({}) \d'.format('|'.join(titles)) #todo: generalize this regex

    cat_re = r'^({})'.format('|'.join([text.Ref(title).regex() for title in titles]))

    return LinkSet({"$and": [{"refs": {"$regex": book_re}}, {"refs": {"$regex": cat_re}}]})


def get_book_link_collection(book, cat):
    """
    Format results of get_book_category_linkset for front end use by the Explorer.
    :param book: String
    :param cat: String
    :return:
    """
    links = get_book_category_linkset(book, cat)

    link_re = r'^(?P<title>.+) (?P<loc>\d.*)$'
    ret = []

    for link in links:
        l1 = re.match(link_re, link.refs[0])
        l2 = re.match(link_re, link.refs[1])
        if not l1 or not l2:
            continue
        ret.append({
            "r1": {"title": l1.group("title"), "loc": l1.group("loc")},
            "r2": {"title": l2.group("title"), "loc": l2.group("loc")}
        })
    return ret

```

### sefaria/model/guide.py

```
# coding=utf-8
import regex as re
from sefaria.system.exceptions import InputError

from . import abstract as abst
from . import text
from sefaria.model.text import Ref

import structlog
logger = structlog.get_logger(__name__)


class Guide(abst.AbstractMongoRecord):
    """
    Learning Guides for sidebar connection panel.

    Data structure:
        ref: A string that references the specific section of the Pesach Haggadah being discussed.
        questions: An array of questions, each containing:
            question: A string representing the question being asked about the Pesach Haggadah.
                commentaries: An array of commentaries related to the question, each containing:
                    commentaryRef: A string that serves as a reference to the specific commentary on the Pesach Haggadah.
                    summaryText: A string providing a summary of the commentary's main points or interpretations.

Formally:
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Pesach Haggadah Commentary",
  "type": "object",
  "properties": {
    "ref": {
      "type": "string",
      "description": "A reference to the specific section of the Pesach Haggadah being discussed."
    },
    "questions": {
      "type": "array",
      "description": "An array of questions related to the Pesach Haggadah.",
      "items": {
        "type": "object",
        "properties": {
          "question": {
            "type": "string",
            "description": "The question being asked about the Pesach Haggadah."
          },
          "commentaries": {
            "type": "array",
            "description": "An array of commentaries related to the question.",
            "items": {
              "type": "object",
              "properties": {
                "commentaryRef": {
                  "type": "string",
                  "description": "A reference to the specific commentary on the Pesach Haggadah."
                },
                "summaryText": {
                  "type": "string",
                  "description": "A summary of the commentary's main points or interpretations."
                }
              },
              "required": ["commentaryRef", "summaryText"],
              "additionalProperties": false
            }
          }
        },
        "required": ["question", "commentaries"],
        "additionalProperties": false
      }
    }
  },
  "required": ["ref", "questions"],
  "additionalProperties": false
}


    Example data:

      "ref": "Pesach Haggadah, Kadesh 2",
      "questions": [
        {
          "question": "Why four cups of wine?",
          "commentaries": [
            {
              "commentaryRef": "Simchat HaRegel on Pesach Haggadah, Kadesh 2:1",
              "summaryText": "Chida explores whether the festival Kiddush is a rabbinic or Torah obligation, why we introduce it with 'Savrei', and what it means that Israel was chosen from the nations."
            },
            {
              "commentaryRef": "Maarechet Heidenheim on Pesach Haggadah, Kadesh 2:2",
              "summaryText": "Tevele Bondi gives three reasons for the four cups: Pharaoh's four decrees against Israel, the four promises of redemption in Exodus, and a midrash relating them to four redeemers."
            },
            {
              "commentaryRef": "Marbeh Lesaper on Pesach Haggadah, Kadesh 2:1",
              "summaryText": "Yedidiah Weil cites the Mekhilta that the four cups honor four mitzvot the Israelites kept in Egypt: avoiding unchastity, slander, name changes, and language/clothing changes."
            },
            {
              "commentaryRef": "Divrei Negidim on Pesach Haggadah, Kadesh 2:3",
              "summaryText": "Yehudah Rosenberg explains the cups mystically as representing higher spiritual redemption. He also connects them to the four matriarchs."
            }
          ]
        },
        {
          "question": "What is Shabbat HaGadol?",
          "commentaries": [
            {
              "commentaryRef": "Divrei Negidim on Pesach Haggadah, Kadesh 2:1",
              "summaryText": "Yehudah Rosenberg explains Shabbat HaGadol is named for the 'great day' of future redemption. He also says it spiritually prepares for Pesach, like Shabbat Teshuva before Yom Kippur."
            }
          ]
        },
        {
          "question": "Why wear a kittel at seder?",
          "commentaries": [
            {
              "commentaryRef": "Divrei Negidim on Pesach Haggadah, Kadesh 2:5",
              "summaryText": "Yehudah Rosenberg says wearing a white kittel represents the simple, higher spiritual source of the redemption, like the High Priest's white garments on Yom Kippur."
            }
          ]
        }
      ]
    }

    Todo?
    In the future may want to support broader Refs and included ExpandedRefs field.

    """
    collection = 'guide'
    required_attrs = [
        'ref',  # May be section level, segment level, or ranged segment level.
        'expanded_refs',  # list of segment level refs
        'questions',
    ]

    def __init__(self, attrs=None):
        self.expanded_refs = []
        super(Guide, self).__init__(attrs)

    def _normalize(self):
        self.ref = Ref(self.ref).normal()
        self.set_expanded_refs()

    def set_expanded_refs(self):
        self.expanded_refs = [r.normal() for r in Ref(self.ref).all_segment_refs()]

    def load_by_ref(self, ref):
        return self.load({"ref": ref.normal()})

    def contents(self):
        d = super(Guide, self).contents()
        d["anchorRef"] = d["ref"]
        d["anchorRefExpanded"] = d["expanded_refs"]
        return d

class GuideSet(abst.AbstractMongoSet):
    recordClass = Guide

    @classmethod
    def load_set_for_client(cls, tref):
        try:
            oref = Ref(tref)
        except InputError:
            return []

        segment_refs = [r.normal() for r in oref.all_segment_refs()]

        documents = cls({"expanded_refs": {"$in": segment_refs}}).contents()  # Presuming exact matches of normal refs
        return documents

```

### sefaria/model/timeperiod.py

```
# -*- coding: utf-8 -*-
"""

"""

from . import abstract as abst
from . import schema

import structlog
logger = structlog.get_logger(__name__)


""" This data, from Joshua Parker - http://www.joshua-parker.net/sages/
Loaded from MySQL DB with Sefaria-Data/sources/Sages_DB/parse_eras_from_sages.py
+---------------+------------+-----------------+-------------------------------+-----------------------+
| period_symbol | period_seq | period_era      | period_generation             | period_dates          |
+---------------+------------+-----------------+-------------------------------+-----------------------+
| AV            |        1.0 | Avot            |                               | before 13th c. BCE    |
| MS            |        2.0 | Moshe Rabbeinu  |                               | 13th c. BCE           |
| NR            |        3.0 | Former Prophets |                               | 13th c. - 6th c. BCE  |
| NA            |        4.0 | Latter Prophets |                               | 6th c. - 3rd c. BCE   |
| KG            |        5.0 | Great Assembly  |                               | 3rd c. BCE            |
| PT            |        6.0 | pre-Tannaic     |                               | 3rd c.  1st c. BCE   |
| Z1            |        7.0 | Zugot           | first generation              | 2nd c. BCE            |
| Z2            |        8.0 | Zugot           | second generation             | 2nd c. BCE            |
| Z3            |        9.0 | Zugot           | third generation              | 1st c. BCE            |
| Z4            |       10.0 | Zugot           | fourth generation             | 1st c. BCE            |
| Z5            |       11.0 | Zugot           | fifth generation              | 30 BCE  20 CE        |
| T1            |       12.0 | Tannaim         | first generation              | 20  40 CE            |
| T             |       12.1 | Tannaim         | unknown generation            | 20 -200 CE            |
| T2            |       13.0 | Tannaim         | second generation             | 40  80 CE            |
| T3            |       14.0 | Tannaim         | third generation              | 80  110 CE           |
| T4            |       15.0 | Tannaim         | fourth generation             | 110  135 CE          |
| T4/T5         |       15.5 | Tannaim         | fourth and fifth generations  | 110 - 170 CE          |
| T5            |       16.0 | Tannaim         | fifth generation              | 135  170 CE          |
| T6            |       17.0 | Tannaim         | sixth generation              | 170  200 CE          |
| TA            |       18.0 | Tannaim/Amoraim | transition                    | 200  220 CE          |
| A1            |       19.0 | Amoraim         | first generation              | 220  250 CE          |
| A1/A2         |       19.5 | Amoraim         | first and second generations  | 220 - 290 CE          |
| A2            |       20.0 | Amoraim         | second generation             | 250  290 CE          |
| A2/A3         |       20.5 | Amoraim         | second and third generations  | 250 - 320 CE          |
| A3            |       21.0 | Amoraim         | third generation              | 290  320 CE          |
| A3/A4         |       21.5 | Amoraim         | third and fourth generations  | 290 - 350 CE          |
| A4            |       22.0 | Amoraim         | fourth generation             | 320  350 CE          |
| A4/A5         |       22.5 | Amoraim         | fourth and fifth generations  | 320 - 375 CE          |
| A5            |       23.0 | Amoraim         | fifth generation              | 350  375 CE          |
| A5/A6         |       23.5 | Amoraim         | fifth and sixth generations   | 350 - 425 CE          |
| A6            |       24.0 | Amoraim         | sixth generation              | 375  425 CE          |
| A6/A7         |       24.5 | Amoraim         | sixth and seventh generations | 375 - 460 CE          |
| A7            |       25.0 | Amoraim         | seventh generation            | 425  460 CE          |
| A8            |       26.0 | Amoraim         | eighth generation             | 460  500 CE          |
| SV            |       27.0 | Savoraim        |                               | 500 - 540 CE          |
| GN            |       28.0 | Geonim          |                               | 6th c. - 11th c. CE   |
| RI            |       29.0 | Rishonim        |                               | 11th c. - 15th c. CE  |
| AH            |       30.0 | Acharonim       |                               | 15th c. CE - present  |
+---------------+------------+-----------------+-------------------------------+-----------------------+
"""

DASH = ''

class TimePeriod(abst.AbstractMongoRecord):
    """
    TimePeriod is used both for the saved time periods - Eras and Generations
    and for the adhoc in memory TimePeriods generated from e.g. the Person model
    """
    collection = 'time_period'
    track_pkeys = True
    pkeys = ["symbol"]

    required_attrs = [
        "symbol",
        "type",  # "Era", "Generation", "Two Generations"
        "names"
    ]
    optional_attrs = [
        "start",
        "startIsApprox",
        "end",
        "endIsApprox",
        "order",
        "range_string"
    ]

    def __str__(self):
        return vars(self).__str__()

    def __repr__(self):
        return self.__str__()

    # Names
    # todo: This is the same as on Person, and very similar to Terms - abstract out
    def _init_defaults(self):
        self.name_group = None

    def _set_derived_attributes(self):
        self.name_group = schema.TitleGroup(getattr(self, "names", None))

    def _normalize(self):
        self.names = self.name_group.titles
        if getattr(self, "start", False):
            self.start = int(self.start)

        if getattr(self, "end", False):
            self.end = int(self.end)

    def all_names(self, lang="en"):
        return self.name_group.all_titles(lang)

    def primary_name(self, lang="en"):
        return self.name_group.primary_title(lang)

    def secondary_names(self, lang="en"):
        return self.name_group.secondary_titles(lang)

    def add_name(self, name, lang, primary=False, replace_primary=False):
        return self.name_group.add_title(name, lang, primary=primary, replace_primary=replace_primary)

    def getYearLabels(self, lang):
        start = getattr(self, "start", None)
        end = getattr(self, "end", None)
        if start is None:
            return "", ""
        if end is None:
            end = start

        if int(start) < 0 < int(end):
            return ("BCE ", "CE") if lang == "en" else ('"' + ' ', "")
        elif int(end) > 0:
            return ("", "CE") if lang == "en" else ("", "")
        else:  # self.end <= 0
            return ("", "BCE") if lang == "en" else ("", '"')

    def getApproximateMarkers(self, lang):
        marker = "c." if lang == "en" else ""
        return (
            marker if getattr(self, "startIsApprox", None) else "",
            marker if getattr(self, "endIsApprox", None) else ""
        )

    def period_string(self, lang):
        name = ""

        if getattr(self, "start", None) is not None:  # and getattr(self, "end", None) is not None:
            labels = self.getYearLabels(lang)
            approxMarker = self.getApproximateMarkers(lang)

            if lang == "en":
                if getattr(self, "symbol", "") == "CO" or getattr(self, "end", None) is None:
                    name += " ({}{} {} {} )".format(
                        approxMarker[0],
                        abs(int(self.start)),
                        labels[1],
                        DASH)
                    return name
                elif int(self.start) == int(self.end):
                    name += " ({}{} {})".format(
                        approxMarker[0],
                        abs(int(self.start)),
                        labels[1])
                else:
                    name += " ({}{} {} {} {}{} {})".format(
                        approxMarker[0],
                        abs(int(self.start)),
                        labels[0],
                        DASH,
                        approxMarker[1],
                        abs(int(self.end)),
                        labels[1])
            if lang == "he":
                if getattr(self, "symbol", "") == "CO" or getattr(self, "end", None) is None:
                    name += " ({} {} {} {} )".format(
                        abs(int(self.start)),
                        labels[1],
                        approxMarker[0],
                        DASH)
                    return name
                elif int(self.start) == int(self.end):
                    name += " ({}{}{})".format(
                        abs(int(self.end)),
                        " " + labels[1] if labels[1] else "",
                        " " + approxMarker[1] if approxMarker[1] else "")
                else:
                    both_approx = approxMarker[0] and approxMarker[1]
                    if both_approx:
                        name += " ({}{} {} {}{} {})".format(
                            abs(int(self.start)),
                            " " + labels[0] if labels[0] else "",
                            DASH,
                            abs(int(self.end)),
                            " " + labels[1] if labels[1] else "",
                            approxMarker[1]
                        )
                    else:
                        name += " ({}{}{} {} {}{}{})".format(
                            abs(int(self.start)),
                            " " + labels[0] if labels[0] else "",
                            " " + approxMarker[0] if approxMarker[0] else "",
                            DASH,
                            abs(int(self.end)),
                            " " + labels[1] if labels[1] else "",
                            " " + approxMarker[1] if approxMarker[1] else ""
                        )

        return name

    def get_era(self):
        """
        Given a generation, get the Era for that generation
        :return:
        """
        #This info should be stored on Generations.  It doesn't change.
        if self.type == "Era":
            return self
        t = TimePeriod().load({"type": "Era",
                        "start": {"$lte": self.start},
                        "end": {"$gte": self.end}})

        return t or None

    def get_people_in_generation(self, include_doubles = True):
        from . import topic
        if self.type == "Generation":
            if include_doubles:
                return topic.Topic({"properties.generation.value": {"$regex": self.symbol}})
            else:
                return topic.Topic({"properties.generation.value": self.symbol})

    def determine_year_estimate(self):
        start = getattr(self, 'start', None)
        end = getattr(self, 'end', None)
        if start != None and end != None:
            return round((int(start) + int(end)) / 2)
        elif start != None:
            return int(start)
        elif end != None:
            return int(end)

class TimePeriodSet(abst.AbstractMongoSet):
    recordClass = TimePeriod

    @staticmethod
    def _get_typed_set(type):
        return TimePeriodSet({"type": type}, sort=[["order", 1]])

    @staticmethod
    def get_eras():
        return TimePeriodSet._get_typed_set("Era")

    @staticmethod
    def get_generations(include_doubles = False):
        arg = {"$in": ["Generation", "Two Generations"]} if include_doubles else "Generation"
        return TimePeriodSet._get_typed_set(arg)

class LifePeriod(TimePeriod):

    def period_string(self, lang):

        if getattr(self, "start", None) == None and getattr(self, "end", None) == None:
            return ""

        labels = self.getYearLabels(lang)
        approxMarker = self.getApproximateMarkers(lang)
        abs_birth = abs(int(getattr(self, "start", 0)))
        abs_death = abs(int(getattr(self, "end", 0)))
        if lang == "en":
            birth = 'b.'
            death = 'd.'
            order_vars_by_lang = lambda year, label, approx: (approx, '', year, label)
        else:
            birth = ''
            death = ''
            order_vars_by_lang = lambda year, label, approx: (year, ' ', label, approx)

        if getattr(self, "symbol", "") == "CO" or getattr(self, "end", None) is None:
            name = '{} {}{}{} {}'.format(birth, *order_vars_by_lang(abs_birth, labels[1], approxMarker[0]))
        elif getattr(self, "start", None) is None:
            name = '{} {}{}{} {}'.format(death, *order_vars_by_lang(abs_death, labels[1], approxMarker[0]))
        elif int(self.start) == int(self.end):
            name = '{}{}{} {}'.format(*order_vars_by_lang(abs_birth, labels[1], approxMarker[0]))
        else:
            both_approx = approxMarker[0] and approxMarker[1]
            if lang == 'he' and  both_approx:
                birth_string = '{}{}{}'.format(*order_vars_by_lang(abs_birth, labels[0], approxMarker[0])[:-1])
            else:
                birth_string = '{}{}{} {}'.format(*order_vars_by_lang(abs_birth, labels[0], approxMarker[0]))
            death_string = '{}{}{} {}'.format(*order_vars_by_lang(abs_death, labels[1], approxMarker[0]))
            name = f'{birth_string} {DASH} {death_string}'

        name = f' ({" ".join(name.split())})'
        return name


```

### sefaria/model/story.py

```
# -*- coding: utf-8 -*-

"""
story.py
"""

from sefaria.utils.util import strip_tags
from . import user_profile

import structlog
logger = structlog.get_logger(__name__)


class Story(object):
# Deprecated in Nov 2022.  Retained for static methods used in sheet and trend statistics
# TODO: Refactor out of existence

    @staticmethod
    def sheet_metadata(sheet_id, return_id=False):
        from sefaria.sheets import get_sheet_metadata
        metadata = get_sheet_metadata(sheet_id)
        if not metadata:
            return None
        return Story.build_sheet_metadata_dict(metadata, sheet_id, return_id=return_id)

    @staticmethod
    def build_sheet_metadata_dict(metadata, sheet_id, return_id=False):
        d = {
            "sheet_title": strip_tags(metadata["title"]),
            "sheet_summary": strip_tags(metadata["summary"]) if "summary" in metadata else "",
            "publisher_id": metadata["owner"],
            "sheet_via": metadata.get("via", None)
        }
        if return_id:
            d["sheet_id"] = sheet_id
        return d

    @staticmethod
    def publisher_metadata(publisher_id, return_id=False):
        udata = user_profile.public_user_data(publisher_id)
        d = {
            "publisher_name": udata["name"],
            "publisher_url": udata["profileUrl"],
            "publisher_image": udata["imageUrl"],
            "publisher_position": udata["position"],
            "publisher_organization": udata["organization"],
        }
        if return_id:
            d["publisher_id"] = publisher_id

        return d

    @staticmethod
    def sheet_metadata_bulk(sid_list, return_id=False, public=True):
        from sefaria.sheets import get_sheet_metadata_bulk
        metadata_list = get_sheet_metadata_bulk(sid_list, public=public)
        return [Story.build_sheet_metadata_dict(metadata, metadata['id'], return_id) for metadata in metadata_list]

```

### sefaria/model/place.py

```

import geojson
from . import abstract as abst
from . import schema
from sefaria.system.exceptions import InputError
import structlog
from geopy.geocoders import Nominatim
from sefaria.utils.hebrew import get_he_key
logger = structlog.get_logger(__name__)

class Place(abst.AbstractMongoRecord):
    """
    Homo Sapiens
    """
    collection = 'place'
    track_pkeys = True
    pkeys = ["key"]

    required_attrs = [
        "key",
        "names",
        "point"
    ]
    optional_attrs = [
        "area"
    ]

    # Names
    # This is the same as on TimePeriod, and very similar to Terms & Person - abstract out
    def _init_defaults(self):
        self.name_group = None

    def _set_derived_attributes(self):
        self.name_group = schema.TitleGroup(getattr(self, "names", None))

    def _normalize(self):
        super(Place, self)._normalize()
        self.names = self.name_group.titles
        #if not self.key and self.primary_name("en"):
        #    self.key = self.primary_name("en")

    def all_names(self, lang=None):
        return self.name_group.all_titles(lang)

    def primary_name(self, lang=None):
        return self.name_group.primary_title(lang)

    def secondary_names(self, lang=None):
        return self.name_group.secondary_titles(lang)
    
    @classmethod 
    def create_new_place(cls, en, he=None):
        p = cls().load({'key': en})
        if p:
            return p
        p = cls({'key': en})
        p.name_group.add_title(en, 'en', True, True)
        if he:
            p.name_group.add_title(he, 'he', True, True)
        p.city_to_coordinates(en)
        p.save()
        return p

    def city_to_coordinates(self, city):
        geolocator = Nominatim(user_agent='hello@sefaria.org')
        location = geolocator.geocode(city)
        if location and location.raw['type'] in ['administrative', 'city', 'town', 'municipality', 'neighbourhood', 'village']:
            self.point_location(lon=location.longitude, lat=location.latitude)
        else:
            raise InputError(f"{city} is not a real city.")


    def point_location(self, lon=None, lat=None):
        if lat is None and lon is None:
            return getattr(self, "point", None)
        if lat is None or lon is None:
            raise InputError("Bad coordinates passed to Place.point_location: {}, {}".format(lon, lat))
        self.point = geojson.Point((lon, lat))

    def area_location(self, geoj=None):
        if geoj is None:
            return self.area
        self.area = geoj



class PlaceSet(abst.AbstractMongoSet):
    recordClass = Place

    def asGeoJson(self, with_polygons=False, as_string=False):
        features = []
        for place in self:
            point = place.point_location()
            area = None
            if with_polygons:
                area = place.area_location()
            feature = area or point
            if feature:
                features.append(geojson.Feature(geometry=feature, id=place.key, properties={"name": place.key}))
        if as_string:
            return geojson.dumps(geojson.FeatureCollection(features))
        else:
            return geojson.FeatureCollection(features)

def process_index_place_change(indx, **kwargs):
    key = kwargs['attr']
    he_key = get_he_key(key)
    he_new_val = getattr(indx, he_key, '')
    if kwargs['new'] != '':
        Place.create_new_place(en=kwargs['new'], he=he_new_val)

def process_topic_place_change(topic_obj, **kwargs):
    keys = ["birthPlace", "deathPlace"]
    for key in keys:
        if kwargs.get(key, False):  # only change property value if key is in data, otherwise it indicates no change
            new_val = kwargs[key]
            if new_val != '':
                he_key = get_he_key(key)
                he_new_val = kwargs.get(he_key, '')
                place = Place.create_new_place(en=new_val, he=he_new_val)
                topic_obj.properties[key] = {"value": place.primary_name('en'), 'dataSource': 'learning-team-editing-tool'}
            else:
                topic_obj.properties.pop(key, None)


```

### sefaria/model/manuscript.py

```
# encoding=utf-8
#from __future__ import annotations

from sefaria.system.exceptions import InputError, DuplicateRecordError, ManuscriptError
from sefaria.system.database import db
from sefaria.model.abstract import AbstractMongoRecord, AbstractMongoSet, SluggedAbstractMongoRecord
from sefaria.model.text import Ref
import structlog
logger = structlog.get_logger(__name__)





class Manuscript(SluggedAbstractMongoRecord):
    pkeys = ['slug']
    track_pkeys = True

    collection = 'manuscripts'
    required_attrs = [
        'slug',  # unique, derived from title.
        'title',
        'he_title',
        'source',
        'description',
        'he_description'
    ]
    optional_attrs = ['license']

    def normalize_slug_field(self, slug_field: str) -> str:
        """
        Duplicates are forbidden for Manuscript slugs. Character normalization only, duplicates raise
        a DuplicateRecordError
        :param slug_field: not an attribute, this should just be a string that will be normalized.
        :return: normalized string
        """
        slug = self.normalize_slug(slug_field)
        mongo_id = getattr(self, '_id', None)
        duplicate = getattr(db, self.collection).find_one({'slug': slug, "_id": {"$ne": mongo_id}})
        if duplicate:
            raise DuplicateRecordError(f"Record with the title {slug_field} already exists")

        return slug

    def _normalize(self):
        title = getattr(self, 'title', None)
        if not title:
            raise InputError('title not set')
        self.slug = self.normalize_slug_field(title)


class ManuscriptSet(AbstractMongoSet):
    recordClass = Manuscript


class ManuscriptPage(AbstractMongoRecord):

    collection = 'manuscript_pages'
    required_attrs = [
        'manuscript_slug',
        'page_id',  # manuscript_slug & page_id must be unique
        'image_url',
        'thumbnail_url',
        'contained_refs',  # list of section level (possibly ranged) refs
        'expanded_refs',   # list of segment level refs
    ]

    def __init__(self, attrs=None):
        self.contained_refs = []  # an empty list is a valid value
        self.expanded_refs = []
        super(ManuscriptPage, self).__init__(attrs)

    def _pre_save(self):
        self.expanded_refs = list(set(self.expanded_refs))  # clear out duplicates

        # make sure we're not adding duplicates
        manuscript_id, page_id = getattr(self, 'manuscript_slug', None), getattr(self, 'page_id', None)
        if manuscript_id is None or page_id is None:  # important to check for None explicitly, page_id=0 is valid
            raise ManuscriptError('No manuscript_id or page_id')
        if self.is_new():
            duplicate = ManuscriptPage().load({
                'manuscript_id': manuscript_id,
                'page_id': page_id
            })
            if duplicate:
                raise DuplicateRecordError("Record already exists. Please update existing instead of adding new.")

    def _validate(self):
        super(ManuscriptPage, self)._validate()

        # check that the manuscript this page is part of exists in the database
        if self.get_manuscript() is None:
            raise ManuscriptError("Manuscript missing in database")

        for tref in self.contained_refs:
            if not Ref.is_ref(tref):
                raise ManuscriptError(f'{tref} is not a valid Ref')

        test_refs = self.get_ref_objects()
        while test_refs:
            current_ref = test_refs.pop()
            for tr in test_refs:
                if current_ref.overlaps(tr):
                    raise ManuscriptError(f'Overlap between contained refs {tr} and {current_ref}')

            if not len(test_refs):
                break

    def validate(self, verbose=False) -> bool:
        """
        helper method, useful for seeing if the underlying data is valid without raising any errors
        :param verbose:
        :return: bool
        """
        try:
            self._validate()
        except ManuscriptError as e:
            if verbose:
                print(f'Validation failed with the following error: {e}')
            return False
        return True

    def get_ref_objects(self):
        try:
            return [Ref(tref) for tref in self.contained_refs]
        except InputError:
            raise ManuscriptError('bad ref associated with this Manuscript Page')

    @staticmethod
    def get_expanded_refs_for_source(oref: Ref):
        return [r.normal() for r in oref.all_segment_refs()]

    def set_expanded_refs(self):
        expanded_refs = []
        for oref in self.get_ref_objects():
            expanded_refs.extend(self.get_expanded_refs_for_source(oref))
        self.expanded_refs = expanded_refs

    def add_ref(self, tref):
        try:
            new_oref = Ref(tref)
        except InputError as e:
            raise ManuscriptError(e)
        for oref in self.get_ref_objects():
            if oref.overlaps(new_oref):
                raise ManuscriptError(f'Overlap between contained refs {oref} and {new_oref}')
        self.contained_refs.append(tref)
        self.expanded_refs.extend(self.get_expanded_refs_for_source(new_oref))

    def remove_ref(self, tref):
        try:
            tref_index = self.contained_refs.index(tref)
        except ValueError:
            raise ValueError(f'Cannot remove {tref}: it is not contained on this image')

        self.contained_refs.pop(tref_index)
        self.set_expanded_refs()

    def get_manuscript(self):
        return Manuscript().load({'slug': self.manuscript_slug})

    @staticmethod
    def get_slug_for_title(title):
        return Manuscript.normalize_slug(title)


class ManuscriptPageSet(AbstractMongoSet):
    recordClass = ManuscriptPage

    @staticmethod
    def expanded_ref_query_for_ref(oref: Ref):
        return [{'expanded_refs': {'$regex': r}} for r in oref.regex(as_list=True)]

    @classmethod
    def load_by_ref(cls, oref):
        return cls({'$or': cls.expanded_ref_query_for_ref(oref)}, hint='expanded_refs_1')

    @classmethod
    def load_set_for_client(cls, tref: str):
        """
        This method returns an array of results that can be converted to JSON instead of Sefaria MongoSet instances.
        This method uses a mongo aggregation to JOIN the manuscript with the manuscript page.
        :param tref:
        :return:
        """
        try:
            oref = Ref(tref)
        except InputError:
            return []

        segment_refs = [r.normal() for r in oref.all_segment_refs()]
        results, manuscripts = [], {}
        documents = cls.load_by_ref(oref)

        for document in documents:
            contained_refs, expanded = document.contained_refs, document.expanded_refs
            anchor_ref_list, anchor_ref_expanded_list = oref.get_all_anchor_refs(segment_refs, contained_refs, expanded)

            for anchor_ref, anchor_ref_expanded in zip(anchor_ref_list, anchor_ref_expanded_list):
                contents = document.contents()
                contents["anchorRef"] = anchor_ref.normal()
                contents["anchorRefExpanded"] = [r.normal() for r in anchor_ref_expanded]
                del contents['contained_refs']
                del contents['expanded_refs']

                if document.manuscript_slug in manuscripts:
                    manuscript = manuscripts[document.manuscript_slug]
                else:
                    manuscript = Manuscript().load({'slug': document.manuscript_slug})
                    manuscripts[manuscript.slug] = manuscript
                man_contents = manuscript.contents()
                contents['manuscript'] = man_contents

                results.append(contents)
        return results


def process_index_title_change_in_manuscript_links(indx, **kwargs):
    from sefaria.system.exceptions import InputError

    print("Cascading ManuscriptPage from {} to {}".format(kwargs['old'], kwargs['new']))

    # ensure that the regex library we're using here is the same regex library being used in `Ref.regex`
    from .text import re as reg_reg
    patterns = [pattern.replace(reg_reg.escape(indx.title), reg_reg.escape(kwargs["old"]))
                for pattern in Ref(indx.title).regex(as_list=True)]
    queries = [{'expanded_refs': {'$regex': pattern}} for pattern in patterns]
    objs = ManuscriptPageSet({"$or": queries})
    for o in objs:
        o.contained_refs = [r.replace(kwargs["old"], kwargs["new"], 1) if reg_reg.search('|'.join(patterns), r) else r for r in o.contained_refs]
        o.expanded_refs = [r.replace(kwargs["old"], kwargs["new"], 1) if reg_reg.search('|'.join(patterns), r) else r
                            for r in o.expanded_refs]
        try:
            o.save()
        except InputError:
            logger.warning("Failed to convert ref data from: {} to {}".format(kwargs['old'], kwargs['new']))


def process_slug_change_in_manuscript(man, **kwargs):
    ManuscriptPageSet({"manuscript_slug": kwargs["old"]}).update({"manuscript_slug": kwargs["new"]})


def process_manucript_deletion(man, **kwargs):
    ManuscriptPageSet({"manuscript_slug": man.slug}).delete()

```

### sefaria/model/media.py

```
# coding=utf-8
from urllib.parse import urlparse
import regex as re
from datetime import datetime
from collections import defaultdict

from . import abstract as abst
from . import text
from sefaria.system.database import db
from sefaria.model.text import Ref

import structlog
logger = structlog.get_logger(__name__)


class Media(abst.AbstractMongoRecord):
    """
    Media for sidebar connection panel.
    """
    collection = 'media'
    required_attrs = [
        "media_url",
        "source_he",
        "source",
        "media_type",
        "ref",
        "license",
        "source_site",
        "description",
        "description_he",
    ]

    def _normalize(self): # what does this do?
        self.ref = Ref(self.ref).normal()

    def client_contents(self, ref):
        d = self.contents()
        t = {}
        t["media_url"]     = ref["media_url"]
        t["source"]   = d["source"]
        t["source_he"]   = d["source_he"]
        t['start_time'] = ref['start_time']
        t['end_time'] = ref['end_time']
        t['anchorRef'] = ref['sefaria_ref']
        t['license'] = d['license']
        t['source_site'] = d['source_site']
        t['description'] = d['description']
        t['description_he'] = d['description_he']
        return t

class MediaSet(abst.AbstractMongoSet):
    recordClass = Media

def get_media_for_ref(tref):
    oref = text.Ref(tref)
    regex_list = oref.regex(as_list=True)
    ref_clauses = [{"ref.sefaria_ref": {"$regex": r}} for r in regex_list]
    query = {"$or": ref_clauses }
    results = MediaSet(query=query)
    client_results = []
    ref_re = "("+'|'.join(regex_list)+")"
    matched_ref = []
    for media in results:
        for r in media.ref:
            if re.match(ref_re, r['sefaria_ref']):
                r['media_url'] = media.media_url
                matched_ref.append(r)
    for ref in matched_ref:
        media_contents = media.client_contents(ref)
        client_results.append(media_contents)

    return client_results

```

### sefaria/model/marked_up_text_chunk.py

```
from . import abstract as abst
from sefaria.model.text import TextChunk, Ref
from sefaria.system.exceptions import InputError, DuplicateRecordError

class MarkedUpTextChunk(abst.AbstractMongoRecord):
    """
    MarkedUpTextChunk objects define the quotations and links inside Sefaria texts
    Probably, every Quoting Commentary will have a MarkedUpTextChunk object
    """
    collection = "marked_up_text_chunks"
    criteria_field = "ref"
    track_pkeys = True
    pkeys = ["ref", "versionTitle", "language"]

    required_attrs = [
        "ref",
        "versionTitle",
        "language",
        "spans"
    ]

    attr_schemas = {
        "ref": {"type": "string", "required": True},
        "versionTitle": {"type": "string", "required": True},
        "language": {"type": "string", "allowed": ["en", "he"], "required": True},
        "spans": {
            "type": "list",
            "empty": False,
            "schema": {
                "type": "dict",
                "schema": {
                    "charRange": {
                        "type": "list",
                        "schema": {"type": "integer"},
                        "minlength": 2,
                        "maxlength": 2,
                        "required": True
                    },
                    "text": {"type": "string", "required": True},
                    "type": {
                        "type": "string",
                        "allowed": ["quote", "citation"],
                        "required": True
                    },
                    "ref": {"type": "string", "required": True}
                }
            },
            "required": True
        }
    }


    def _validate(self):
        super()._validate()
        oref = Ref(self.ref)
        if not oref.is_segment_level():
            raise InputError(type(self).__name__ + "._validate(): Ref must be at segment level: " + oref.normal())
        tc = TextChunk(oref, lang=self.language, vtitle=self.versionTitle)

        if not tc.text:
            raise InputError(type(self).__name__ + "._validate(): Corresponding TextChunk is empty")

        # Enforce uniqueness
        pkey_query = {k: getattr(self, k) for k in self.pkeys}

        existing = self.__class__().load(pkey_query)
        if existing and existing._id != getattr(self, "_id", None):
            raise DuplicateRecordError(f"{type(self).__name__}._validate(): Duplicate primary key {self.pkeys}, found {pkey_query} to already exist in the database.")


        for span in self.spans:
            text = tc.text
            citation_text = text[span['charRange'][0]:span['charRange'][1]]
            if citation_text != span['text']:
                raise InputError(f"{type(self).__name__}._validate(): Span text does not match the text in the corresponding TextChunk for {span['ref']}"
                                 f": expected '{span['text']}', found '{citation_text}'.")

        return True

    def _normalize(self):
        self.ref = Ref(self.ref).normal()
        for span in self.spans:
            span['ref'] = Ref(span['ref']).normal()

    def __str__(self):
        return "TextSpan: {}".format(self.ref)

```

### sefaria/model/portal.py

```
from . import abstract as abst
from sefaria.system.validators import validate_url, validate_http_method
import structlog

logger = structlog.get_logger(__name__)


class Portal(abst.SluggedAbstractMongoRecord):
    collection = 'portals'
    slug_fields = ['slug']

    required_attrs = [
        "slug",
        "about",
        "name",
    ]
    optional_attrs = [
        "mobile",
        "newsletter",
        "organization"
    ]
    attr_schemas = {
        'about': {'type': 'dict',
                   'schema': {'title': {'type': 'dict',
                                        'required': True,
                                        'schema': {'en': {'type': 'string', 'required': True},
                                                   'he': {'type': 'string', 'required': True}}},
                              'title_url': {'type': 'string'},
                              'image_uri': {'type': 'string'},
                              'image_caption': {'type': 'dict',
                                                'schema': {'en': {'type': 'string'}, 'he': {'type': 'string'}}},
                              'description': {'type': 'dict',
                                              'schema': {'en': {'type': 'string', 'required': True},
                                                         'he': {'type': 'string', 'required': True}}}}},
         'mobile': {'type': 'dict',
                    'schema': {'title': {'type': 'dict',
                                         'required': True,
                                         'schema': {'en': {'type': 'string', 'required': True},
                                                    'he': {'type': 'string', 'required': True}}},
                               'description': {'type': 'dict',
                                               'schema': {'en': {'type': 'string'}, 'he': {'type': 'string'}}},
                               'android_link': {'type': 'string'},
                               'ios_link': {'type': 'string'}}},
         'organization': {'type': 'dict',
                          'schema': {'title': {'type': 'dict',
                                               'required': True,
                                               'schema': {'en': {'type': 'string', 'required': True},
                                                          'he': {'type': 'string', 'required': True}}},
                                     'description': {'type': 'dict',
                                                     'schema': {'en': {'type': 'string', 'required': True},
                                                                'he': {'type': 'string', 'required': True}}}}},
         'newsletter': {'type': 'dict',
                        'schema': {'title': {'type': 'dict',
                                             'required': True,
                                             'schema': {'en': {'type': 'string', 'required': True},
                                                        'he': {'type': 'string', 'required': True}}},
                                   'description': {'type': 'dict',
                                                   'schema': {'en': {'type': 'string', 'required': True},
                                                              'he': {'type': 'string', 'required': True}}},
                                   'title_url': {'type': 'string'},
                                   'api_schema': {'type': 'dict',
                                                  'schema': {'http_method': {'type': 'string', 'required': True},
                                                             'payload': {'type': 'dict',
                                                                         'schema': {
                                                                             'first_name_key': {'type': 'string'},
                                                                             'last_name_key': {'type': 'string'},
                                                                             'email_key': {'type': 'string'}}}}}}}
    }

    def _validate(self):
        super(Portal, self)._validate()
        if hasattr(self, "about"):
            title_url = self.about.get("title_url")
            if title_url: validate_url(title_url)
        if hasattr(self, "mobile"):
            android_link = self.mobile.get("android_link")
            if android_link: validate_url(android_link)
            ios_link = self.mobile.get("ios_link")
            if ios_link: validate_url(ios_link)
        if hasattr(self, "newsletter"):
            http_method = self.newsletter.get("api_schema", {}).get("http_method")
            if http_method: validate_http_method(http_method)
        return True


class PortalSet(abst.AbstractMongoSet):
    recordClass = Portal

```

### sefaria/model/passage.py

```
# coding=utf-8
from . import abstract as abst
from . import text
import structlog
logger = structlog.get_logger(__name__)


class Passage(abst.AbstractMongoRecord):
    """
    Sugyot
    """
    collection = 'passage'

    required_attrs = [
        "full_ref",  # ""
        "type",     # "Mishnah" or "Sugya"
        "ref_list"   # []
    ]
    optional_attrs = [
        "same_as",    # []
        "source"
    ]

    possible_types = ["Mishnah", "Sugya", "passage", "biblical-story"]

    @classmethod
    def containing_segment(cls, ref):
        #get shortest passage containing this segment ref
        assert isinstance(ref, text.Ref)
        assert ref.is_segment_level()
        passages = PassageSet({"ref_list": ref.starting_ref().normal()})
        return min(passages, key=lambda passage: len(passage.ref_list)) if passages else None

    def _normalize(self):
        super(Passage, self)._normalize()
        self.ref_list = [r.normal() for r in self.ref().range_list()]
        if type == "Mishnah":
            pass
            # Look up mishnah-in-talmud links for this mishnah, get the Mishnah location and put in sameAs

    def _validate(self):
        super(Passage, self)._validate()
        assert self.type in self.possible_types

    def ref(self):
        return text.Ref(self.full_ref)


class PassageSet(abst.AbstractMongoSet):
    recordClass = Passage



```

### sefaria/model/webpage.py

```
# coding=utf-8
from urllib.parse import urlparse
import regex as re
from collections import defaultdict
from django.core.validators import URLValidator
from django.core.exceptions import ValidationError
from . import abstract as abst
from . import text
from sefaria.system.database import db
from sefaria.system.cache import in_memory_cache
import bleach
import structlog
logger = structlog.get_logger(__name__)
from collections import Counter
from sefaria.utils.calendars import daf_yomi, parashat_hashavua_and_haftara
from sefaria.utils.util import truncate_string
from datetime import datetime, timedelta
from sefaria.system.exceptions import InputError
from tqdm import tqdm
from sefaria.model import *


class WebPage(abst.AbstractMongoRecord):
    collection = 'webpages'

    required_attrs = [
        "url",
        "title",
        "refs",
        "lastUpdated",
    ]
    optional_attrs = [
        "description",
        "expandedRefs",
        "body",
        "linkerHits",
        'authors',
        'articleSource',
        'type'
    ]

    def load(self, url_or_query):
        query = {"url": WebPage.normalize_url(url_or_query)} if isinstance(url_or_query, str) else url_or_query
        return super(WebPage, self).load(query)

    def _set_derived_attributes(self):
        if getattr(self, "url", None):
            self.domain      = WebPage.domain_for_url(self.url)
            self._site_data  = WebPage.site_data_for_domain(self.domain)
            self.site_name   = self._site_data["name"] if self._site_data else self.domain
            self.favicon = f"https://www.google.com/s2/favicons?domain={self._site_data['domains'][0]}" if self._site_data else None
            self.whitelisted = self._site_data["is_whitelisted"] if self._site_data else False

    def _init_defaults(self):
        self.linkerHits = 0
        self.lastUpdated = datetime.now()

    def _normalize_data_sent_from_linker(self):
        self.url = self.normalize_url(self.url)
        self.refs = self._normalize_refs(getattr(self, "refs", []))
        self.title = self.clean_title(getattr(self, "title", ""), getattr(self, "_site_data", {}), getattr(self, "site_name", ""))
        self.description = self.clean_description(getattr(self, "description", ""))

    @staticmethod
    def _normalize_refs(refs):
        refs = {text.Ref(ref).normal() for ref in refs if text.Ref.is_ref(ref)}
        return list(refs)

    def _normalize(self):
        super(WebPage, self)._normalize()
        self._normalize_data_sent_from_linker()
        self.expandedRefs = text.Ref.expand_refs(self.refs)

    def _validate(self):
        validator = URLValidator()
        validator(self.url)
        if hasattr(self, 'type'):
            assert self.type == 'article', "WebPage's type can be 'article' or not exist"
        else:
            assert not hasattr(self, 'articleSource'), "only WebPage of type 'article' can have 'articleSource' attribute"
        articleSource = getattr(self, 'articleSource', None)
        if articleSource:
            assert 'title' in articleSource, "articleSource of WebPage should have title"
            assert all(key in ['title', 'related_parts'] for key in articleSource), "articleSource of WebPage can have only the keys 'title' and 'related_parts'"
        super(WebPage, self)._validate()

    def _sanitize(self):
        all_attrs = self.required_attrs + self.optional_attrs
        for attr in all_attrs:
            if attr == 'url':
                continue
            val = getattr(self, attr, None)
            if isinstance(val, str):
                setattr(self, attr, bleach.clean(val, tags=self.ALLOWED_TAGS, attributes=self.ALLOWED_ATTRS))

    def get_website(self, dict_only=False):
        # returns the corresponding WebSite.  If dict_only is True, grabs the dictionary of the WebSite from cache
        domain = WebPage.domain_for_url(WebPage.normalize_url(self.url))
        if dict_only is False:
            return WebSite().load({"domains": domain})
        else:
            sites = get_website_cache()
            for site in sites:
                if domain in site["domains"]:
                    return site
            return {}

    @staticmethod
    def normalize_url(url):
        rewrite_rules = {
            "use https": lambda url: re.sub(r"^http://", "https://", url),
            "remove hash": lambda url: re.sub(r"#.*", "", url),
            "remove url params": lambda url: re.sub(r"\?.+", "", url),
            "remove utm params": lambda url: re.sub(r"\?utm_.+", "", url),
            "remove fbclid param": lambda url: re.sub(r"\?fbclid=.+", "", url),
            "remove www": lambda url: re.sub(r"^(https?://)?www\.", r"\1", url),
            "remove mediawiki params": lambda url: re.sub(r"&amp;.+", "", url),
            "remove sort param": lambda url: re.sub(r"\?sort=.+", "", url),
            "remove all params after id": lambda url: re.sub(r"(\?id=\d+).+$", r"\1", url)
        }
        global_rules = ["remove hash", "remove utm params", "remove fbclid param", "remove www", "use https"]
        domain = WebPage.domain_for_url(url)
        site_rules = global_rules
        site_data = WebPage.site_data_for_domain(domain)
        if site_data and site_data["is_whitelisted"]:
            site_rules += [x for x in site_data.get("normalization_rules", []) if x not in global_rules]
        for rule in site_rules:
            url = rewrite_rules[rule](url)

        return url

    @staticmethod
    def domain_for_url(url):
        return urlparse(url).netloc

    def should_be_excluded(self):
        """ Returns true if this webpage should not be included in our index
        because it matches a title/url we want to exclude or has no refs"""
        if len(self.refs) == 0:
            return True
        bleached_url = bleach.clean(self.url.encode('utf-8'), tags=self.ALLOWED_TAGS, attributes=self.ALLOWED_ATTRS)
        if len(bleached_url) > 1000:
            # url field is indexed. Mongo doesn't allow indexing a field over 1000 bytes
            from sefaria.system.database import db
            db.webpages_long_urls.insert_one(self.contents())
            return True
        url_regex = WebPage.excluded_pages_url_regex(self.domain)
        url_match = re.search(url_regex, self.url) if url_regex is not None else False
        #url_regex is None when bad_urls is empty, so we should not exclude this domain
        title_regex = WebPage.excluded_pages_title_regex()
        return bool(url_match) or re.search(title_regex, self.title)

    def delete_if_should_be_excluded(self):
        if not self.should_be_excluded():
            return False
        if not self.is_new():
            self.delete()
        return True

    def add_hit(self):
        self.linkerHits += 1
        self.lastUpdated = datetime.now()

    @staticmethod
    def excluded_pages_url_regex(looking_for_domain=None):
        bad_urls = []
        sites = get_website_cache()
        for site in sites:
            if looking_for_domain is None or looking_for_domain in site["domains"]:
                bad_urls += site.get("bad_urls", [])
                for domain_in_site in site["domains"]:
                    if site["is_whitelisted"]:
                        bad_urls += [re.escape(domain_in_site)+"/search.*?$"]

        if len(bad_urls) == 0:
            return None
        else:
            return "({})".format("|".join(bad_urls))

    @staticmethod
    def excluded_pages_title_regex():
        bad_titles = [
            r"Page \d+ of \d+",  # Rabbi Sacks paged archives
            r"Page [nN]ot [fF]ound$",  # 404 pages include links to content
            r"^JTS Torah Online$"  # JTS search result pages
        ]
        return "({})".format("|".join(bad_titles))

    @staticmethod
    def site_data_for_domain(domain):
        sites = get_website_cache()
        for site in sites:
            for site_domain in site["domains"]:
                if site_domain == domain or domain.endswith("." + site_domain):
                    return site
        return None

    @staticmethod
    def add_or_update_from_linker(webpage_contents: dict, add_hit=True):
        """
        Adds an entry for the WebPage represented by `data` or updates an existing entry with the same normalized URL
        Returns True is data was saved, False if data was determined to be excluded

        @param webpage_contents: a dict representing the contents of a `WebPage`
        @param add_hit: True if you want to add hit to webpage in webpages collection
        """
        temp_webpage = WebPage(webpage_contents)
        temp_webpage._normalize_data_sent_from_linker()
        webpage = WebPage().load(temp_webpage.url)
        if webpage:
            if temp_webpage.title == webpage.title and temp_webpage.description == getattr(webpage, "description", "") and set(webpage_contents["refs"]) == set(webpage.refs):
                return "excluded", webpage  # no new data
            contents_to_overwrite = {
                "url": temp_webpage.url,
                "title": temp_webpage.title or webpage.title,
                "refs": temp_webpage.refs,
                "description": temp_webpage.description,
            }
            webpage.load_from_dict(contents_to_overwrite)
        else:
            webpage = temp_webpage

        if webpage.delete_if_should_be_excluded():
            return "excluded", None

        if add_hit:
            webpage.add_hit()
        try:
            webpage.save()
        except ValidationError:
            # something is wrong with the webpage URL
            return "excluded", None
        return "saved", webpage

    def client_contents(self):
        d = self.contents()
        d["domain"]     = self.domain
        d["siteName"]   = self.site_name
        d["favicon"] = self.favicon
        d['authors'] = getattr(self, 'authors', None)
        d['articleSource'] = getattr(self, 'articleSource', None)
        del d["lastUpdated"]
        d = self.clean_client_contents(d)
        return d

    @staticmethod
    def clean_client_contents(d):
        d["title"]       = WebPage.clean_title(d["title"], d.get("_site_data", {}), d.get("site_name", ""))
        d["description"] = WebPage.clean_description(d.get("description", ""))
        return d

    @staticmethod
    def clean_title(title, site_data, site_name):
        if site_data == {} or site_data is None:
            return title
        title = str(title)
        title = title.replace("&amp;", "&")
        brands = [site_name] + site_data.get("title_branding", [])
        separators = [("-", ' '), ("|", ' '), ("", ' '), ("", ' '), ("", ' '), ("", ' '), (":", ''), ("", ' ')]
        for separator, padding in separators:
            for brand in brands:
                if site_data.get("initial_title_branding", False):
                    brand_str = f"{brand}{padding}{separator} "
                    if title.startswith(brand_str):
                        title = title[len(brand_str):]
                else:
                    brand_str = f" {separator}{padding}{brand}"
                    if title.endswith(brand_str):
                        title = title[:-len(brand_str)]

        return title

    @staticmethod
    def clean_description(description):
        if description is None:
            return ""
        for uhoh_string in ["*/", "*******"]:
            if description.find(uhoh_string) != -1:
                return None
        description = description.replace("&amp;", "&")
        description = description.replace("&nbsp;", " ")
        return truncate_string(description, 150, 170)


class WebPageSet(abst.AbstractMongoSet):
    recordClass = WebPage


class WebSite(abst.AbstractMongoRecord):
    collection = 'websites'

    required_attrs = [
        "name",
        "domains",
        "is_whitelisted"
    ]
    optional_attrs = [
        "bad_urls",
        "normalization_rules",
        "title_branding",
        "initial_title_branding",
        "linker_installed",
        "num_webpages",
        "exclude_from_tracking",
        "whitelist_selectors",
        'lastUpdated',
    ]

    def __key(self):
        return (self.name, self.domains[0])

    def __hash__(self):
        return hash(self.__key())

    def __eq__(self, other):
        if isinstance(other, WebSite):
            return self.__key() == other.__key()
        return NotImplemented

    def get_num_webpages(self):
        if getattr(self, 'num_webpages', None) is None:
            self.num_webpages = WebPageSet({"url": {"$regex": "|".join(self.domains)}}).count()
            self.save()
        return self.num_webpages


class WebSiteSet(abst.AbstractMongoSet):
    recordClass = WebSite


def get_website_cache():
    sites = in_memory_cache.get("websites_data")
    if sites in [None, []]:
        sites = [w.contents() for w in WebSiteSet()]
        in_memory_cache.set("websites_data", sites)
        return sites
    return sites


def get_webpages_for_ref(tref):
    from pymongo.errors import OperationFailure
    oref = text.Ref(tref)
    segment_refs = [r.normal() for r in oref.all_segment_refs()]
    results = WebPageSet(query={"expandedRefs": {"$in": segment_refs}}, hint="expandedRefs_1", sort=None)
    try:
        results = results.array()
    except OperationFailure as e:
        # If documents are too large or there are too many results, fail gracefully
        logger.warn(f"WebPageSet for ref {tref} failed due to Error: {repr(e)}")
        return []
    webpage_objs = {}      # webpage_obj is an actual WebPage()
    webpage_results = {}  # webpage_results is dictionary that API returns
    
    for webpage in results:
        if not webpage.whitelisted or len(webpage.title) == 0:
            continue
          
        webpage_key = webpage.title+"|".join(sorted(webpage.refs))
        prev_webpage_obj = webpage_objs.get(webpage_key, None)
        if prev_webpage_obj is None or prev_webpage_obj.lastUpdated < webpage.lastUpdated:
            anchor_ref_list, anchor_ref_expanded_list = oref.get_all_anchor_refs(segment_refs, webpage.refs,
                                                                                 webpage.expandedRefs)
            for anchor_ref, anchor_ref_expanded in zip(anchor_ref_list, anchor_ref_expanded_list):
                webpage_contents = webpage.client_contents()
                webpage_contents["anchorRef"] = anchor_ref.normal()
                webpage_contents["anchorRefExpanded"] = [r.normal() for r in anchor_ref_expanded]
                webpage_objs[webpage_key] = webpage
                webpage_results[webpage_key] = webpage_contents

    return list(webpage_results.values())


def test_normalization():
    pages = WebPageSet()
    count = 0
    for page in pages:
        norm = WebPage.normalize_url(page.url)
        if page.url != norm:
            print(page.url.encode("utf-8"))
            print(norm.encode("utf-8"))
            print("\n")
            count += 1

    print("{} pages normalized".format(count))


def dedupe_webpages(webpages, test=True):
    """Normalizes URLs of all webpages and deletes multiple entries that normalize to the same URL"""
    norm_count = 0
    dedupe_count = 0
    for i, webpage in tqdm(enumerate(webpages)):
        norm = WebPage.normalize_url(webpage.url)
        if webpage.url != norm:
            normpage = WebPage().load(norm)
            if normpage:
                dedupe_count += 1
                if test:
                    print("DEDUPE")
                    print(webpage.url.encode("utf-8"))
                    print(norm.encode("utf-8"))
                    print("\n")
                else:
                    normpage.linkerHits += webpage.linkerHits
                    if normpage.lastUpdated < webpage.lastUpdated:
                        normpage.lastUpdated = webpage.lastUpdated
                        normpage.refs = webpage.refs
                        normpage.expandedRefs = webpage.expandedRefs
                    normpage.save()
                    webpage.delete()

            else:
                norm_count += 1
                if test:
                    print("NORM")
                    print(webpage.url.encode("utf-8"))
                    print(norm.encode("utf-8"))
                    print("\n")
                else:
                    webpage.save()
    print("{} pages removed as duplicates".format(dedupe_count))
    print("{} pages normalized".format(norm_count))



def dedupe_identical_urls(test=True):
    dupes = db.webpages.aggregate([
        {"$group": {
            "_id": "$url",
            "uniqueIds": {"$addToSet": "$_id"},
            "count": {"$sum": 1}
            }
        },
        {"$match": {
            "count": {"$gt": 1}
            }
        },
        {"$sort": {
            "count": -1
            }
        }
    ], allowDiskUse=True)

    url_count = 0
    removed_count = 0
    for dupe in dupes:
        url_count += 1
        pages = WebPageSet({"_id": {"$in": dupe["uniqueIds"]}})
        merged_page_data = {
            "url": dupe["_id"], "linkerHits": 0, "lastUpdated": datetime.min
        }
        if test:
            print("\nReplacing: ")
        for page in pages:
            if test:
                print(page.contents())
            merged_page_data["linkerHits"] += page.linkerHits
            if "refs" not in merged_page_data.keys() or merged_page_data["lastUpdated"] < page.lastUpdated:
                merged_page_data.update({
                    "refs": page.refs,
                    "expandedRefs": page.expandedRefs,
                    "title": page.title,
                    "description": getattr(page, "description", ""),
                    "lastUpdated": page.lastUpdated
                })
        removed_count += (pages.count() - 1)

        merged_page = WebPage(merged_page_data)
        if test:
            print("with")
            print(merged_page.contents())
        else:
            pages.delete()
            merged_page.save()

    print("\n{} pages with identical urls removed from {} url groups.".format(removed_count, url_count))


def clean_webpages(test=True):
    url_bad_regexes = WebPage.excluded_pages_url_regex()[:-1] + "|\d{3}\.\d{3}\.\d{3}\.\d{3})"  #delete any page that matches the regex produced by excluded_pages_url_regex() or in IP form



    """ Delete webpages matching patterns deemed not worth including"""
    pages = WebPageSet({"$or": [
            {"url": {"$regex": url_bad_regexes}},
            {"title": {"$regex": WebPage.excluded_pages_title_regex()}},
            {"refs": {"$eq": []}},
             {"domain": ""}
        ]})

    for page in WebPageSet({"$expr": {"$gt": [{"$strLenCP": "$url"}, 1000]}}):
        # url field is indexed. Mongo doesn't allow indexing a field over 1000 bytes
        from sefaria.system.database import db
        db.webpages_long_urls.insert_one(page.contents())
        print(f"Moving {page.url} to long urls DB...")
        page.delete()


    if not test:
        pages.delete()
        print("Deleted {} pages.".format(pages.count()))
    else:
        for page in pages:
            print(page.url)
        print("\n {} pages would be deleted".format(pages.count()))



def webpages_stats():
    webpages = WebPageSet(proj={"expandedRefs": False})
    total_pages  = webpages.count()
    total_links  = []
    websites = {}
    year_data = Counter()

    for webpage in tqdm(webpages):
        website = webpage.get_website()
        if website:
            if website not in websites:
                websites[website] = 0
            websites[website] += 1
        total_links += webpage.refs
        year = int((datetime.today() - webpage.lastUpdated).days / 365.0)
        year_data[year] += 1

    total_links = len(set(total_links))

    for website, num in websites.items():
        website.num_webpages = num
        website.save()

    return (total_pages, total_links, year_data)


def find_webpages_without_websites(webpages, test=True, hit_threshold=50, last_linker_activity_day=20):
    from datetime import datetime, timedelta
    new_active_sites = Counter()   # WebSites we don't yet have in DB, but we have corresponding WebPages accessed recently
    unactive_unacknowledged_sites = {}  # WebSites we don't yet have in DB, and we have correpsonding WebPages but they have not been accessed recently

    active_threshold = datetime.today() - timedelta(days=last_linker_activity_day)   # used for creating new sites
    unactive_threshold = datetime.today() - timedelta(days=(last_linker_activity_day+10))   # used for deleting old pages
    # if we have more than hit_threshold webpages for a website accessed after active_threshold, create new website for these pages
    # lets say there are 45 pages in last 20 days so we dont create a new site. if active_threshold were the same as unactive_threshold, we would delete these.
    # if we then get 5 new pages in the next hour, they won't correspond to an actual site. the way to deal with this
    # is to make sure the unactive_threshold, which determines which pages we delete, is significantly older than the active_threshold. let's pick 10 days

    for i, webpage in tqdm(enumerate(webpages)):
        website = webpage.get_website(dict_only=True)
        if website == {}:
            if webpage.lastUpdated > active_threshold:
                new_active_sites[webpage.domain] += 1
            elif webpage.lastUpdated < unactive_threshold:
                if webpage.domain not in unactive_unacknowledged_sites:
                    unactive_unacknowledged_sites[webpage.domain] = []
                unactive_unacknowledged_sites[webpage.domain].append(webpage)

    sites_added = {}
    for site, hits in new_active_sites.items():
        if hits > hit_threshold:
            sites_added[site] = f"{site} should be created because it has {hits} pages in last {last_linker_activity_day} days"

    for site, hits in unactive_unacknowledged_sites.items():
        if site not in new_active_sites.keys():  # if True, site has not been updated recently
            print("Deleting {} with {} pages".format(site, len(unactive_unacknowledged_sites[site])))
            for webpage in unactive_unacknowledged_sites[site]:
                if not test:
                    webpage.delete()

    return sites_added

def find_sites_to_be_excluded(webpages):
    # returns all sites dictionary and each entry has a Counter of refs
    all_sites = {}
    for webpage in tqdm(webpages):
        website = webpage.get_website(dict_only=True)
        if website != {}:
            if website["name"] not in all_sites:
                all_sites[website["name"]] = Counter()
            for ref in webpage.refs:
                all_sites[website["name"]][ref] += 1
    return all_sites

def find_sites_to_be_excluded_absolute(flag=100):
    # this function looks for any website which has more webpages than 'flag' of any ref
    all_sites = find_sites_to_be_excluded()
    sites_to_exclude = {}
    for website in all_sites:
        sites_to_exclude[website] = ""
        if len(all_sites[website]) > 0:
            most_common = all_sites[website].most_common(10)
            for common in most_common:
                if common[1] > flag:
                    sites_to_exclude[website] += f"{website} may need exclusions set due to Ref {common[0]} with {common[1]} pages.\n"
    return sites_to_exclude

def find_sites_to_be_excluded_relative(webpages, flag=25, relative_percent=3):
    # this function looks for any website which has more webpages than 'flag' of any ref AND the amount of pages of this ref is a significant percentage of site's total refs
    sites_to_exclude = defaultdict(list)
    all_sites = find_sites_to_be_excluded(webpages)
    for website in all_sites:
        total = sum(all_sites[website].values())
        top_10 = all_sites[website].most_common(10)
        for c in top_10:
            if c[1] > flag and 100.0*float(c[1])/total > relative_percent:
                sites_to_exclude[website].append(c)
    return sites_to_exclude

def check_daf_yomi_and_parashat_hashavua(sites):
    previous = datetime.now() - timedelta(10)
    recent_daf = daf_yomi(previous)[0]["ref"]
    recent_parasha = parashat_hashavua_and_haftara(previous)[0]["ref"]

    future_daf = datetime.now() + timedelta(500)
    future_daf = daf_yomi(future_daf)[0]["ref"]

    future_parasha = datetime.now() + timedelta(180)
    future_parasha = parashat_hashavua_and_haftara(future_parasha)[0]["ref"]
    poss_issues = {}
    for site in sites:
        poss_issues[site] = {}
        poss_issues[site]["Daf"] = 0
        poss_issues[site]["Parasha"] = 0
        for type, future, recent in [("Daf", future_daf, recent_daf), ("Parasha", future_parasha, recent_parasha)]:
            future_range = text.Ref(future)
            recent_range = text.Ref(recent)
            for ref, count in sites[site].items():
                try:
                    ref = text.Ref(ref)
                    if recent_range.contains(ref):
                        poss_issues[site][type] += count
                    if future_range.contains(ref):
                        poss_issues[site][type] -= count
                except InputError as e:
                    print(e)

    for site in poss_issues:
        daf = poss_issues[site]["Daf"]
        parasha = poss_issues[site]["Parasha"]
        if daf > 10:
            print("{} may have daf yomi on every page.".format(site))
        if parasha > 10:
            print("{} may have parasha on every page.".format(site))

def find_sites_that_may_have_removed_linker(last_linker_activity_day=20):
    """
    Checks for each site whether there has been a webpage hit with the linker in the last `last_linker_activity_day` days
    Prints an alert for each site that doesn't meet this criterion
    """
    sites_to_delete = {}
    sites_to_keep = {}
    from datetime import datetime, timedelta
    last_active_threshold = datetime.today() - timedelta(days=last_linker_activity_day)
    webpages_without_websites = 0
    for data in get_website_cache():
         if data["is_whitelisted"]:  # we only care about whitelisted sites
            for domain in data['domains']:
                ws = WebPageSet({"url": {"$regex": re.escape(domain)}}, limit=1, sort=[['lastUpdated', -1]])
                keep = True
                if ws.count() == 0:
                    sites_to_delete[domain] = f"{domain} has no pages"
                    keep = False
                else:
                    webpage = ws[0]  # lastUpdated webpage for this domain
                    website = webpage.get_website()
                    if website:
                        website.linker_installed = webpage.lastUpdated > last_active_threshold
                        if not website.linker_installed:
                            keep = False
                            print(f"Alert! {domain} has removed the linker!")
                            sites_to_delete[domain] = f"{domain} has {website.get_num_webpages()} pages, but has not used the linker in {last_linker_activity_day} days. {webpage.url} is the newest page."
                    else:
                        print("Alert! Can't find website {} corresponding to webpage {}".format(data["name"], webpage.url))
                        webpages_without_websites += 1
                        continue
                if keep:
                    assert domain not in sites_to_delete
                    sites_to_keep[domain] = True

    if webpages_without_websites > 0:
        print("Found {} webpages without websites".format(webpages_without_websites))
    return sites_to_delete


```

### sefaria/model/linker/named_entity_resolver.py

```
import dataclasses
from typing import List, Dict, Type, Set
import re2 as re
from functools import reduce
from collections import defaultdict
from sefaria.model.linker.ref_part import RawNamedEntity
from sefaria.model.topic import Topic
from sefaria.utils.hebrew import strip_cantillation
from sefaria.system.exceptions import InputError


class ResolvedNamedEntity:

    def __init__(self, raw_named_entity: RawNamedEntity, topics: List[Topic]):
        self.raw_entity = raw_named_entity
        self.topics = topics

    @property
    def topic(self):
        if len(self.topics) != 1:
            raise InputError(f"ResolvedNamedEntity is ambiguous and has {len(self.topics)} topics so you can't access "
                             ".topic.")
        return self.topics[0]

    @property
    def is_ambiguous(self):
        return len(self.topics) != 1

    @property
    def resolution_failed(self):
        return len(self.topics) == 0


class TitleGenerator:

    expansions = {}

    @classmethod
    def generate(cls, title: str) -> List[str]:
        expansions = [title]
        for reg, reg_expansions in cls.expansions.items():
            for reg_expansion in reg_expansions:
                potential_expansion = re.sub(reg, reg_expansion, title)
                if potential_expansion == title: continue
                expansions += [potential_expansion]
        expansions = [strip_cantillation(t, strip_vowels=True) for t in expansions]
        return expansions


class PersonTitleGenerator(TitleGenerator):

    expansions = {
        r' b\. ': [' ben ', ' bar ', ', son of ', ', the son of ', ' son of ', ' the son of ', ' Bar ', ' Ben '],
        r'^Ben ': ['ben '],
        r'^Bar ': ['bar '],
        r'^Rabbi ': ['R. '],
        r'^Rebbi ': ['R. '],
    }


class FallbackTitleGenerator(TitleGenerator):

    expansions = {
        '^The ': ['the '],
    }


@dataclasses.dataclass
class NamedEntityTitleExpanderRoute:
    type_slug: str
    generator: Type[TitleGenerator]


class NamedEntityTitleExpander:
    type_generator_router = [
        NamedEntityTitleExpanderRoute('people', PersonTitleGenerator),
        NamedEntityTitleExpanderRoute('entity', FallbackTitleGenerator),
    ]

    def __init__(self, lang: str):
        self._lang = lang

    def expand(self, topic: Topic) -> List[str]:
        for route in self.type_generator_router:
            if topic.has_types({route.type_slug}):
                return self._expand_titles_with_generator(topic, route.generator)
        return self._get_topic_titles(topic)

    def _get_topic_titles(self, topic: Topic) -> List[str]:
        return topic.get_titles(lang=self._lang, with_disambiguation=False)

    def _expand_titles_with_generator(self, topic: Topic, generator: Type[TitleGenerator]) -> List[str]:
        expansions = []
        for title in self._get_topic_titles(topic):
            expansions += generator.generate(title)
        return expansions


class TopicMatcher:

    def __init__(self, lang: str, named_entity_types_to_topics: Dict[str, Dict[str, List[str]]]):
        self._lang = lang
        self._title_expander = NamedEntityTitleExpander(lang)
        topics_by_type = {
            named_entity_type: self.__generate_topic_list_from_spec(topic_spec)
            for named_entity_type, topic_spec in named_entity_types_to_topics.items()
        }
        all_topics = reduce(lambda a, b: a + b, topics_by_type.values(), [])
        self._slug_topic_map = {t.slug: t for t in all_topics}
        self._title_slug_map_by_type = {
            named_entity_type: self.__get_title_map_for_topics(topics_by_type[named_entity_type])
            for named_entity_type, topic_spec in named_entity_types_to_topics.items()
        }

    def __get_title_map_for_topics(self, topics: List[Topic]) -> Dict[str, Set[str]]:
        title_slug_map = defaultdict(set)
        unique_topics = {t.slug: t for t in topics}.values()
        for topic in unique_topics:
            for title in self._title_expander.expand(topic):
                title_slug_map[title].add(topic.slug)
        return title_slug_map

    @staticmethod
    def __generate_topic_list_from_spec(topic_spec: Dict[str, List[str]]) -> List[Topic]:
        topics = []
        for root in topic_spec.get('ontology_roots', []):
            topics += Topic.init(root).topics_by_link_type_recursively()
        topics += [Topic.init(slug) for slug in topic_spec.get('single_slugs', [])]
        return topics

    def match(self, named_entity: RawNamedEntity) -> List[Topic]:
        slugs = self._title_slug_map_by_type.get(named_entity.type.name, {}).get(named_entity.text, [])
        return [self._slug_topic_map[slug] for slug in slugs]


class NamedEntityResolver:

    def __init__(self, topic_matcher: TopicMatcher):
        self._topic_matcher = topic_matcher

    def bulk_resolve(self, raw_named_entities: List[RawNamedEntity]) -> List[ResolvedNamedEntity]:
        resolved = []
        for named_entity in raw_named_entities:
            matched_topics = self._topic_matcher.match(named_entity)
            resolved += [ResolvedNamedEntity(named_entity, matched_topics)]
        return resolved




```

### sefaria/model/linker/tests/named_entity_resolver_tests.py

```
import pytest
from sefaria.model.linker.named_entity_resolver import PersonTitleGenerator


@pytest.mark.parametrize(('title', 'expected_output'), [
    ['Rabbi b. Ben', ['Rabbi b. Ben', 'Rabbi ben Ben', 'Rabbi bar Ben', 'Rabbi, son of Ben', 'Rabbi, the son of Ben',
                      'Rabbi son of Ben', 'Rabbi the son of Ben', 'Rabbi Bar Ben', 'Rabbi Ben Ben', 'R. b. Ben']],
    ['Rabbi ben Ben', ['R. ben Ben', 'Rabbi ben Ben']],
    ['Bar Kochba', ['Bar Kochba', 'bar Kochba']],
])
def test_person_title_generator(title, expected_output):
    expected_output = sorted(expected_output)
    actual_output = sorted(PersonTitleGenerator.generate(title))
    assert actual_output == expected_output

```

### sefaria/model/linker/tests/test_linker_paragraphs.py

```
import pytest
from sefaria.model.linker.linker import Linker


@pytest.mark.parametrize(
    "input_str,expected_paragraphs,expected_spans",
    [
        ("Paragraph one.\nParagraph two.", ["Paragraph one.", "Paragraph two."], [(14, 15)]),
        ("A\n\nB", ["A", "B"], [(1, 3)]),
        ("No breaks here", ["No breaks here"], []),
        ("First\nSecond\nThird", ["First", "Second", "Third"], [(5, 6), (12, 13)]),
        ("  Leading\n\nTrailing  ", ["  Leading", "Trailing  "], [(9, 11)]),
        ("Para1\n\nPara2\n\nPara3", ["Para1", "Para2", "Para3"], [(5, 7), (12, 14)]),
        ("\nStart with break", ['', "Start with break"], [(0, 1)]),
        ("End with break\n", ["End with break", ''], [(14, 15)]),
    ]
)
def test_break_input_into_paragraphs(input_str, expected_paragraphs, expected_spans):
    linker = Linker.__new__(Linker)  # bypass __init__
    paragraphs, spans = linker._Linker__break_input_into_paragraphs(input_str)
    assert paragraphs == expected_paragraphs
    assert spans == expected_spans

```

### sefaria/model/linker/tests/resolved_ref.py

```
import pytest
from unittest.mock import Mock
from sefaria.model.linker.referenceable_book_node import ReferenceableBookNode, NumberedReferenceableBookNode, NamedReferenceableBookNode, MapReferenceableBookNode
from sefaria.model.linker.ref_resolver import ResolvedRef
from sefaria.model.text import Ref


def make_num_node(ref: Ref, depth=0) -> ReferenceableBookNode:
    ja_node = ref.index_node
    if ja_node.has_default_child():
        ja_node = ja_node.get_default_child()
    node = NumberedReferenceableBookNode(ja_node)
    for _ in range(depth):
        node = node.get_children()[0]
    return node


def make_named_node(title: str, node_path: list[str], is_alt_struct_path: bool):
    index = Ref(title).index
    if is_alt_struct_path:
        # first element in path is alt struct name
        node = index.get_alt_structure(node_path[0])
        node_path = node_path[1:]
    else:
        node = index.nodes
    while len(node_path) > 0:
        node = next((child for child in node.children if child.get_primary_title('en') == node_path[0]), None)
        if node is None:
            raise ValueError(f'Could not find node with title "{node_path[0]}".')
        node_path = node_path[1:]
    return NamedReferenceableBookNode(node)


zohar_volume1_intro_node = MapReferenceableBookNode(Ref('Zohar').index.get_alt_structure("Daf").children[0].children[0])
zohar_first_daf_node = zohar_volume1_intro_node.get_children()[0]


@pytest.mark.parametrize(('node_a', 'node_b', 'self_tref', 'other_tref', 'is_contained'), [
    [make_num_node(Ref('Genesis')), make_num_node(Ref('Genesis'), 1), None, None, True],  # Generic pasuk node is contained in generic perek node
    [make_num_node(Ref('Genesis')), make_num_node(Ref('Genesis'), 1), "Genesis 1", "Genesis 1:2", True],  # Specific pasuk ref is contained in specific perek ref
    [make_num_node(Ref('Genesis')), make_num_node(Ref('Genesis'), 1), "Genesis 2", "Genesis 1:2", False],  # case where specific pasuk isn't contained in specific perek
    [make_named_node('Sefer HaChinukh', ['Parasha', 'Lech Lecha'], True),  make_num_node(Ref("Sefer HaChinukh")), None, "Sefer HaChinukh, 2", True],  # ref is contained in alt struct node (which has no ref)
    [make_num_node(Ref("Sefer HaChinukh")), make_named_node('Sefer HaChinukh', ['Parasha', 'Lech Lecha'], True), "Sefer HaChinukh, 2", None, True],  # alt struct node with only one ref is contained in that ref
    [make_num_node(Ref("Sefer HaChinukh")), make_named_node('Sefer HaChinukh', ['Parasha', 'Bo'], True), "Sefer HaChinukh, 4", None, False],  # alt struct node with multiple refs is not contained in a single one of refs
    [make_named_node('Sefer HaChinukh', ['Parasha', 'Lech Lecha'], True),  make_num_node(Ref("Sefer HaChinukh")), None, "Sefer HaChinukh, 3", False],  # ref outside of alt struct node isn't contained in it
    [zohar_volume1_intro_node, zohar_first_daf_node, None, 'Zohar, Volume I, Introduction 1b', True],  # zohar altStruct ref
    [zohar_first_daf_node, zohar_volume1_intro_node, 'Zohar, Volume I, Introduction 1b', None, False],  # zohar altStruct ref
])
def test_contains(node_a: ReferenceableBookNode, node_b: ReferenceableBookNode, self_tref: str, other_tref: str, is_contained: bool):
    self_oref = self_tref and Ref(self_tref)
    other_oref = other_tref and Ref(other_tref)
    rr_a = ResolvedRef(Mock(), Mock(), node_a, self_oref)
    rr_b = ResolvedRef(Mock(), Mock(), node_b, other_oref)
    assert rr_a.contains(rr_b) == is_contained

```

### sefaria/model/linker/tests/linker_test.py

```
from sefaria.model.linker.ref_part import RangedRawRefParts, SectionContext
from sefaria.model.linker.referenceable_book_node import DiburHamatchilNodeSet, NumberedReferenceableBookNode
from sefaria.model.linker.ref_resolver import ResolvedRef, ResolutionThoroughness, RefResolver, IbidHistory
from .linker_test_utils import *
from sefaria.model import schema
from sefaria.settings import ENABLE_LINKER

if not ENABLE_LINKER:
    pytest.skip("Linker not enabled", allow_module_level=True)


def test_referenceable_child():
    from sefaria.model.schema import AddressAmud
    i = library.get_index("Rashi on Berakhot")
    assert i.nodes.depth == 3
    ref_node = NumberedReferenceableBookNode(i.nodes)
    children = ref_node.get_children(Ref("Rashi on Berakhot 2a"))
    child = next((child for child in children if child.__class__ == NumberedReferenceableBookNode and child._address_class.__class__ == AddressAmud), None)
    assert child is not None

    # one more level
    child = child.get_children(Ref("Rashi on Berakhot 2a"))[0]
    assert child.__class__ == DiburHamatchilNodeSet
    assert child.query == {'container_refs': 'Rashi on Berakhot 2a'}


def test_resolved_raw_ref_clone():
    index = library.get_index("Berakhot")
    raw_ref, context_ref, lang, _ = create_raw_ref_data(["@", "@", "# "])
    rrr = ResolvedRef(raw_ref, [], index.nodes, Ref("Berakhot"))
    rrr_clone = rrr.clone(ref=Ref("Genesis"))
    assert rrr_clone.ref == Ref("Genesis")


crrd = create_raw_ref_data


@pytest.mark.parametrize(('resolver_data', 'expected_trefs'), [
    # Numbered JAs
    [crrd(["@", "@", "# "]), ("Jerusalem Talmud Berakhot 9:1:11-19", "Jerusalem Talmud Berakhot 2:1:14-19")],  # ambig venice or vilna yerushalmi daf
    [crrd(["@", "@", "# "]), ("Jerusalem Talmud Berakhot 9:1:31-2:9",)],  # venice yerushalmi daf
    [crrd(["@Jerusalem", "@Talmud", "@Yoma", "#5a"], lang='en'), ("Jerusalem Talmud Yoma 1:1:20-25",)],
    [crrd(["@Babylonian", "@Talmud", "@Sukkah", "#49b"], lang='en'), ("Sukkah 49b",)],
    [crrd(["@", "@", "# "]), ("Berakhot 2",)],   # amud-less talmud
    [crrd(["@", "# "]), ("Berakhot 2",)],  # amud-less talmud
    [crrd(["@", "@", "# ."]), ("Shabbat 2a",)],  # amud-ful talmud
    [crrd(['@', '#   ', '@']), ("Makkot 2a",)],  # out of order with prefix on title
    [crrd(['@ ', '# ', '# ']), ("Genesis 13:1",)],
    [crrd(['@ ', '# ', '# ']), ("Genesis 13:1",)],  # sections out of order
    [crrd(['@', '#', '#']), ("Exodus 1:2",)],  # used to also match Exodus 2:1 b/c would allow mixing integer parts

    # Roman numerals
    [crrd(['@Job', '#III', '#5'], lang='en'), ("Job 3:5",)],
    [crrd(['@Job', '#ix', '#5'], lang='en'), ("Job 9:5",)],
    [crrd(['@Job', '#IV .', '#5'], lang='en'), ("Job 4:5",)],
    [crrd(['@Job', '#xli.', '#5'], lang='en'), ("Job 41:5",)],
    [crrd(['@Job', '#CIV', '#5'], lang='en'), tuple()],  # too high
    [crrd(['@Job', '#iiii', '#5'], lang='en'), tuple()],  # invalid roman numeral

    # Amud split into two parts
    [crrd(['@', '@', '#', '#']), ("Yevamot 61a",)],
    [crrd(["@", "@", "#", "#"]), ("Tosafot on Pesachim 106a",)],  # amud for commentary that has DH
    [crrd(["@", "@", "#", "#", "*\"  "]), ("Tosafot on Pesachim 106a:1:1",)],  # amud for commentary that has DH
    [crrd(['@', '# ', '# ', '@']), ("Makkot 2a",)],
    [crrd(['@', '# ', '# ', '@']), ("Makkot 2a",)],  # out of order daf and amud
    [crrd(['@', '# ', '# ', '@']), tuple()],
    [crrd(['@', '# ', '# ', '^', '# ', '@']), ("Makkot 2",)],
    [crrd(['@', '# ', '# ', '^', '# ', '# ', '@']), ("Makkot 2a-3b",)],
    [crrd(['@', '#', '#']), ["Mishnah Shabbat 1:2"]],  # shouldn't match Shabbat 2a by reversing order of parts
    [crrd(['@', '#', '#']), ["Shabbat 2a", "Mishnah Shabbat 2:1"]],  # ambiguous case

    # Parsha -> sections
    [crrd(["@Parshat Vayikra", "#2", "#3"], lang='en'), ('Leviticus 2:3',)],
    [crrd(["@Parshat Tzav", "#2", "#3"], lang='en'), tuple()],  # validate that sections fall within parsha
    pytest.param(crrd(["@Parshat Noach", "#6", "#3"], lang='en'), tuple(), marks=pytest.mark.xfail(reason="currently dont check if pasuk/perek pair fall in parsha, only perek")),

    # Aliases for perakim
    [crrd(["@", "@", "# "]), ("Mishnah Berakhot 1",)],
    [crrd(["@", "@", "# "]), ("Mishnah Berakhot 9",)],
    [crrd(['#" ', '@ ']), ("Mishnah Berakhot 9",)],
    [crrd(['#"', '@']), ['Kilayim 3']],
    [crrd(['#"', '@']), ['Kilayim 3']],

    # Named alt structs
    [crrd(["@  ", "@"]), ("Pesachim 65b:10-73b:16",)],  # talmud perek (that's ambiguous)
    [crrd(["@  "]), ("Pesachim 65b:10-73b:16", "Berakhot 51b:11-53b:33", "Jerusalem Talmud Berakhot 8:1:1-8:7", "Jerusalem Talmud Pesachim 6:1:1-6:4", "Jerusalem Talmud Demai 2:1:1-5:4")],  # talmud perek without book that's ambiguous
    [crrd(["@\"", "@  ", "@"]), ("Rashi on Beitzah 15b:1-23b:10",)],  # rashi perek
    [crrd(["@\"", "@ "]), ("Rashi on Berakhot 2a:1-13a:15",)],  # rashi perek
    [crrd(["@\"", "@   ", "@", "*\"   "]), ("Rashi on Nazir 2a:1:1",)],  # rashi perek dibur hamatchil

    # Numbered alt structs
    [crrd(["# ", "@"]), ("Pesachim 2a:1-21a:7", "Mishnah Pesachim 1")],  # numbered talmud perek
    [crrd(['#"', '@']), ("Pesachim 2a:1-21a:7", "Mishnah Pesachim 1")],  # numbered talmud perek
    [crrd(["# ", "@"]), ("Pesachim 58a:1-65b:9", "Mishnah Pesachim 5")],  # numbered talmud perek
    [crrd(['#"', '@']), ("Pesachim 58a:1-65b:9", "Mishnah Pesachim 5", "Pesachim 85")],  # numbered talmud perek
    [crrd(["# ", "@"]), ("Mishnah Pesachim 10", "Pesachim 99b:1-121b:3")],  # numbered talmud perek
    [crrd(['@\'', '#\"', '@']), ("Niddah 48a:11-54b:9",)],  # prefixes in front of perek name

    # Using addressTypes of alt structs
    [crrd(["@JT", "@Bikkurim", "#Chapter 2"], lang="en"), ("Jerusalem Talmud Bikkurim 2",)],
    [crrd(["@Tosafot Rabbi Akiva Eiger", "@Shabbat", "#Letter 87"], lang="en"), ("Tosafot Rabbi Akiva Eiger on Mishnah Shabbat 7.2.1",)],
    [crrd(["@JT", "@Berakhot", "#2a"], lang="en"), ("Jerusalem Talmud Berakhot 1:1:2-4", "Jerusalem Talmud Berakhot 1:1:7-11",)],  # ambig b/w Venice and Vilna
    [crrd(["@JT", "@Berakhot", "#Chapter 1", "#2a"], lang="en"), ("Jerusalem Talmud Berakhot 1:1:2-4", "Jerusalem Talmud Berakhot 1:1:7-11",)],
    [crrd(["@JT", "@Peah", "#10b"], lang="en"), ("Jerusalem Talmud Peah 2:1:1-4",)],  # Venice not ambig
    [crrd(["@JT", "@Peah", "#Chapter 3", "#15b"], lang="en"), ("Jerusalem Talmud Peah 3:2:4-4:3",)],  # Venice not ambig because of chapter
    [crrd(["@JT", "@Peah", "#15c"], lang="en"), ("Jerusalem Talmud Peah 1:1:20-30",)],  # Folio address
    [crrd(["@Chapter 1"], lang="en"), tuple()],  # It used to be that Bavli perakim where Chapter N which causes problems for global scope

    # Dibur hamatchils
    [crrd(["@\"", "@ ", "*\"  "]), ("Rashi on Beitzah 15b:8:1",)],
    [crrd(["@\"", "@", "*\"  "]), ("Rashi on Beitzah 15b:8:1",)],
    [crrd(["@\"", "@ ", "*\"  "]), ("Rashi on Rosh Hashanah 29b:5:3",)],
    [crrd(['@', '#  "', '@"', '*']), ("Tosafot on Shevuot 25a:11:1",)],
    [crrd(["@\"", "#   ", "@", "*\"  "]), ("Rashi on Sukkah 2a:1:1",)], # rashi dibur hamatchil
    [crrd(["@\"", "@", "# ", "# ", "*\" "]), ("Rashi on Genesis 1:1:1", "Rashi on Genesis 1:1:2")],

    # Ranged refs
    [crrd(['@ ', '# ', '# ', '^', '# ', '# ']), ("Genesis 13:1-14:4",)],
    [crrd(['@', '#', '#', '^-', '#', '#']), ("Genesis 13:1-14:4",)],
    [crrd(['@', '# ', '# ', '^-', '#']), ("Deuteronomy 14:40-45",)],
    pytest.param(crrd(['@', '@', '#', '#', '^-', '#']), ["Ketubot 112"], marks=pytest.mark.xfail(reason="Deciding that we can't handle daf and amud being separate in this case because difficult to know we need to merge")),
    [crrd(['@', '@', '# ', '^-', '#']), ["Ketubot 112"]],

    # Base text context
    [crrd(['@\'', '#" "', '*" '], "Rashi on Berakhot 2a"), ("Tosafot on Berakhot 27b:14:2",)],  # shared context child via graph context

    # Mis-classified part types
    [crrd(['@"', "#", "# ", "# "]), ("Shulchan Arukh, Even HaEzer 28:1",)],

    # Ibid
    [crrd(['&', '#'], prev_trefs=["Genesis 1"]), ["Genesis 7", "Genesis 1:7"]],  # ambiguous ibid
    [crrd(['&Ibid', '#12'], prev_trefs=["Exodus 1:7"], lang='en'), ["Exodus 1:12", "Exodus 12"]],  # ambiguous ibid when context is segment level (not clear if this is really ambiguous. maybe should only have segment level result)
    [crrd(['#'], prev_trefs=["Genesis 1"]), ["Genesis 1:2", "Genesis 2"]],  # ambiguous ibid
    [crrd(['#', '#'], prev_trefs=["Genesis 1:3", "Exodus 1:3"]), ["Genesis 2:7", "Exodus 2:7"]],
    [crrd(['@', '&', '#'], prev_trefs=["Exodus 1:3", "Genesis 1:3"]), ["Genesis 1:7"]],
    [crrd(['&', '#', '#'], prev_trefs=["Genesis 1"]), ["Genesis 2:7"]],
    [crrd(['# '], prev_trefs=["Genesis 1"]), ("Genesis 4",)],
    [crrd(['# '], prev_trefs=["Genesis 1", None]), tuple()],
    [crrd(['@', '*" '], prev_trefs=["Berakhot 27b"]), ("Tosafot on Berakhot 27b:14:2",)],
    [crrd(['# '], 'Berakhot 2a', prev_trefs=["Shabbat 2a"]), ("Berakhot 20", "Shabbat 20")],  # conflicting contexts
    [crrd(['# ', '&'], 'Berakhot 2a', prev_trefs=["Shabbat 2a"]), ("Shabbat 20",)],  # conflicting contexts which can be resolved by explicit sham
    [crrd(["@'", "&", "*\" "], "Gilyon HaShas on Berakhot 30a:2", prev_trefs=["Berakhot 17b"]), ("Tosafot on Berakhot 17b:5:1",)],  # Ibid.
    [crrd(['&', '&', '#'], "Mishnah Berakhot 1", prev_trefs=['Mishnah Shabbat 1:1']), ("Mishnah Shabbat 1:2",)],  # multiple shams. TODO failing because we're not enforcing order
    [crrd(['&'], prev_trefs=['Genesis 1:1']), ('Genesis 1:1',)],
    [crrd(['# ', '^-', '#'], prev_trefs=['Deuteronomy 14']), ("Deuteronomy 14:40-45",)],
    [crrd(['#', '#', '^-', '#'], prev_trefs=['Deuteronomy 1:20']), ("Deuteronomy 13:1-2",)],
    [crrd(['@', '# '], prev_trefs=['Rashi on Berakhot 3a']), ('Berakhot 2',)],  # dont use context when not needed
    [crrd(['# '], prev_trefs=["Genesis 1"]), ("Genesis 4",)],  # prefix in front of section
    [crrd(['@', '#"', '#'], prev_trefs=['Exodus 10:1-13:16']), ['Exodus 12:1']],  # broke with merging logic in final pruning
    [crrd(['@"', '#"', '#'], prev_trefs=["Exodus 16:32"]), ["Ramban on Exodus 16:4"]],
    [crrd(['# '], prev_trefs=["II Kings 17:31"]), ["II Kings 7"]],
    [crrd(['@ ', '#'], prev_trefs=["Arukh HaShulchan, Orach Chaim 400"]), ["Arukh HaShulchan, Orach Chaim 493"]],  # ibid named part that's not root
    [crrd(['@"', '&'], prev_trefs=["Genesis 25:9", "Rashi on Genesis 21:20"]), ["Rashi on Genesis 21:20", "Rashi on Genesis 25:9"]],  # ambiguous ibid
    [crrd(["@Job"], lang='en', prev_trefs=['Job 1:1']), ("Job",)],  # don't use ibid context if there's a match that uses all input

    # Relative (e.g. Lekaman)
    [crrd(["@'", "<", "# \"", "*\" \""], "Gilyon HaShas on Berakhot 2a:2"), ("Tosafot on Berakhot 4b:6:1",)],  # likaman + abbrev in DH
    [crrd(['<', '# '], "Mishnah Berakhot 1", prev_trefs=['Mishnah Shabbat 1']), ("Mishnah Berakhot 1:1",)],  # competing relative and sham

    # Superfluous information
    [crrd(['@Vayikra', '@Leviticus', '#1'], lang='en'), ("Leviticus 1",)],
    [crrd(['@', '# ', '@', '# ']), ['Tosafot on Berakhot 2']],

    # YERUSHALMI EN
    [crrd(['@Bavli', '#2a'], "Jerusalem Talmud Shabbat 1:1", "en"), ("Shabbat 2a",)],
    pytest.param(crrd(['@Berakhot', '#2', '#1'], "Jerusalem Talmud Shabbat 1:1", "en"), ("Jerusalem Talmud Berakhot 2:1",), marks=pytest.mark.xfail(reason="Tricky case. We've decided to always prefer explicit or ibid citations so this case fails.")),
    [crrd(['@Bavli', '#2a', '^/', '#b'], "Jerusalem Talmud Shabbat 1:1", 'en'), ("Shabbat 2",)],
    [crrd(['@Halakha', '#2', '#3'], "Jerusalem Talmud Shabbat 1:1", 'en'), ("Jerusalem Talmud Shabbat 2:3",)],
    [crrd(['#2', '#3'], "Jerusalem Talmud Shabbat 1:1", 'en'), ("Jerusalem Talmud Shabbat 2:3",)],
    [crrd(['@Tosephta', '@eviit', '#1', '#1'], "Jerusalem Talmud Sheviit 1:1:3", 'en'), ("Tosefta Sheviit 1:1", "Tosefta Sheviit (Lieberman) 1:1")],
    [crrd(['@Babli', '#28b', '~,', '#31a'], "Jerusalem Talmud Taanit 1:1:3", 'en'), ("Taanit 28b", "Taanit 31a")],  # non-cts with talmud
    [crrd(['@Exodus', '#21', '#1', '~,', '#3', '~,', '#22', '#5'], "Jerusalem Talmud Taanit 1:1:3", 'en'), ("Exodus 21:1", "Exodus 21:3", "Exodus 22:5")],  # non-cts with tanakh
    pytest.param(crrd(['@Ro Haanah', '#4', '#Notes 42', '^', '#43'], "Jerusalem Talmud Taanit 1:1:3", "en"), ("Jerusalem Talmud Rosh Hashanah 4",), marks=pytest.mark.xfail(reason="currently dont support partial ranged ref match. this fails since Notes is not a valid address type of JT")),
    [crrd(['@Tosaphot', '#85a', '*s.v. '], "Jerusalem Talmud Pesachim 1:1:3", 'en'), ("Tosafot on Pesachim 85a:14:1",)],
    [crrd(['@Unknown book', '#2'], "Jerusalem Talmud Pesachim 1:1:3", 'en'), tuple()],  # make sure title context doesn't match this
    [crrd(['@Tosafot', '@Megillah', '#21b', '*s. v . '], "Jerusalem Talmud Pesachim 1:1:3", 'en'), ("Tosafot on Megillah 21b:7:1",)],  # make sure title context doesn't match this
    [crrd(['@Sifra', '@Behar', '#Paraah 6', '#5'], "Jerusalem Talmud Pesachim 1:1:3", 'en'), ("Sifra, Behar, Section 6 5",)],
    [crrd(['@Sifra', '@Saw', '#Parashah 2(910'], "Jerusalem Talmud Pesachim 1:1:3", 'en'), tuple()],  # if raw ref gets broken into incorrect parts, make sure it handles it correctly

    # gilyon hashas
    [crrd(["@", "*\" "], "Gilyon HaShas on Berakhot 51b:1"), ("Tosafot on Berakhot 51b:8:1",)],  # commentator with implied book and daf from context commentator
    [crrd(["@", "# \"", "@'", "*\" \""], "Gilyon HaShas on Berakhot 21a:3"), ("Tosafot on Kiddushin 15b:3:1", "Tosafot on Kiddushin 15b:4:1",)],  # abbrev in DH. ambiguous.
    [crrd(["@\"", "# \""], "Gilyon HaShas on Berakhot 21a:3"), ("Bava Metzia 88b",)],  # TODO should this match Gilyon HaShas as well?

    # specific books
    # TODO still need to convert the following tests to new `crrd` syntax
    [crrd(['@', '@ ', '# ']), ("Tur, Orach Chaim 1",)],
    [crrd(['@', '@', '#', '#']), ("Sifra, Behar, Chapter 2:4", "Sifra, Behar, Section 2:4")],
    [crrd(['@"', '@', '#', '#']), ("Ramban on Deuteronomy 14:21",)],
    [crrd(['@"', '@', '#"', '#"']), ("Mishneh Torah, Heave Offerings 1:8",)],
    [crrd(['@', '@', '#"', '#"']), ("Jerusalem Talmud Sheviit 6:1",)],
    [crrd(['@  ', '#:']), ("Sanhedrin 97b",)],  # one big ref part that actually matches two separate terms + each part has prefix
    [crrd(['@ ']), ("Deuteronomy 31:1-30",)],  # lamed prefix
    [crrd(['@"', '#"', '@\' ', '#"']), ("Mishneh Torah, Heave Offerings 8:11",)],
    [crrd(["@\"", "# \"", "#\""]), ("Shulchan Arukh, Even HaEzer 155:14",)],
    [crrd(['@"', '@', '# \' "']), ("Rashi on Kiddushin 80a",)],
    # pytest.param(crrd("Gilyon HaShas on Berakhot 48b:1", 'he', '''" " ( ''', [0, 1, slice(3, 5)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ("Sefer HaTashbetz, Part II 161",), marks=pytest.mark.xfail(reason="Don't support Sefer HaTashbetz yet")),  # complex text
    [crrd(['@', '# ']), ["Yevamot 39b"]],
    [crrd(['@  ']), ['Parashat Shelach']],
    [crrd(['@  ', '# ']), ['Tur, Yoreh Deah 1']],
    [crrd(['@', '@', '#', '#']), ['Tosefta Berakhot 1:1', 'Tosefta Berakhot (Lieberman) 1:1']],  # tosefta ambiguity
    [crrd(['@', '@', '#', '#']), ['Tosefta Berakhot 1:16']],  # tosefta ambiguity

    # zohar
    [crrd(['@"', '#"','@ ', '@ ', '# .']), ['Zohar, Lech Lecha 10.78-84']],
    [crrd(['@"', '#"','@ ', '# :']), ['Zohar, Lech Lecha 17.152-18.165']],
    [crrd(['@"', '#"', '# :']), ['Zohar, Lech Lecha 17.152-18.165']],
    [crrd(['@"', '@ ', '# :']), ['Zohar, Lech Lecha 17.152-18.165']],

    [crrd(['@ ', '@']), ['Zohar Chadash, Bereshit']],
    [crrd(['@', '@', '#', '#']), ['Tractate Soferim 2:3']],
    [crrd(['@"', '#', '#']), ["Avot DeRabbi Natan 2:3"]],
    [crrd(['@ ', '#']), ["Tractate Derekh Eretz Zuta, Section on Peace 3"]],
    [crrd(['@" ', '@ ', '#']), ["Tractate Derekh Eretz Zuta, Section on Peace 3"]],
    [crrd(['@ ', '@ ', '#']), ['Sefer HaChinukh 2']],
    [crrd(['@ ', '#']), ['Sefer HaChinukh 2']],

    #ben yehuda project
    [crrd(["@'", '#', '#']), ["Genesis 1:2"]],
    [crrd(['@"', "@'", '#:']), ["Rashi on Ketubot 10b"]],
    [crrd(["@'", '#', '#']), ["Mishnah Mikvaot 1:2"]],
    [crrd(['@', "@'", '#', '#']), ["Tosefta Yoma 1:1", 'Tosefta Yoma (Lieberman) 1:1']],

    # pytest.param(crrd(None, 'he', ',  ,  "', [0, slice(2, 4), 5, slice(6, 9)], [RPT.NAMED, RPT.NAMED, RPT.NAMED, RPT.NUMBERED]), ['Sefer HaChinukh 3'], marks=pytest.mark.xfail(reason="Don't support Aseh as address type yet")),
    # [crrd(None, 'he', '  ', [0, slice(1, 3)], [RPT.NAMED, RPT.NAMED]), ["Mekhilta d'Rabbi Yishmael 17:8-18:27"]],
    # [crrd(None, 'he', '   ', [slice(0, 2), 2, 3], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Mekhilta d'Rabbi Yishmael 21:3"]],
    # [crrd(None, 'he', '   ', [0, 1, 2, 3], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Mekhilta d'Rabbi Yishmael 21:3"]],
    # [crrd(None, 'he', '  ', [0, 1, 2], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Mekhilta d'Rabbi Yishmael 21:3"]],
    # [crrd(None, 'he', '   " ', [0, slice(1, 3), slice(3, 6), 6], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Shemot Rabbah 11:3"]],
    # [crrd(None, 'he', '  ', [slice(0, 3)], [RPT.NAMED]), ["Exodus 1:1-6:1"]],
    # [crrd(None, 'he', '"  ', [slice(0,3), slice(3, 5)], [RPT.NAMED, RPT.NUMBERED]), ["Pirkei DeRabbi Eliezer 23"]],
    # [crrd(None, 'he', '  "', [slice(0, 2), slice(2, 5)], [RPT.NAMED, RPT.NUMBERED]), ["Tanna Debei Eliyahu Rabbah 10"]],
    # pytest.param(crrd(None, 'he', '"  ', [slice(0, 4), 4], [RPT.NAMED, RPT.NUMBERED]), ["Tanna debei Eliyahu Zuta 11"], marks=pytest.mark.xfail(reason="Currently there's an unnecessary SchemaNode 'Seder Eliyahu Zuta' that needs to be removed for this to pass")),
    # [crrd(None, 'he', ' "  ', [slice(0, 4), slice(4, 6)], [RPT.NAMED, RPT.NUMBERED]), ["Mekhilta DeRabbi Shimon Ben Yochai 12"]],

    # [crrd(None, 'he', "  ", [slice(0, 2), 2], [RPT.NAMED, RPT.NUMBERED]), ["Sifrei Bamidbar 142"]],
    # pytest.param(crrd(None, 'he', "  ' ", [slice(0, 2), slice(2, 5)], [RPT.NAMED, RPT.NUMBERED]), ["Sifrei Bamidbar 142"], marks=pytest.mark.xfail(reason="Don't support Piska AddressType")),
    # pytest.param(crrd(None, 'he', '   ', [slice(0, 2), slice(2, 4)], [RPT.NAMED, RPT.NUMBERED]), ["Sifrei Devarim 99"], marks=pytest.mark.xfail(reason="Don't support Piska AddressType")),
    # pytest.param(crrd(None, 'he', '   ', [slice(0, 2), slice(2, 4)], [RPT.NAMED, RPT.NUMBERED]), ["Sifrei Bamidbar 17"], marks=pytest.mark.xfail(reason="Don't support Piska AddressType")),
    # pytest.param(crrd(None, 'he', '     ', [slice(0, 2), slice(2, 4), slice(4, 6)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Sifrei Bamidbar 78"], marks=pytest.mark.xfail(reason="Don't support Piska AddressType")),
    # [crrd(None, 'he', "    ' ", [slice(0, 2), slice(2, 4), slice(4, 7)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Sifrei Devarim 344"]],
    # [crrd(None, 'he', ' " ', [slice(0, 4), 4], [RPT.NAMED, RPT.NUMBERED]), ["Pesikta D'Rav Kahanna 3"]],
    # [crrd(None, 'he', "  ", [slice(0, 2), 2], [RPT.NAMED, RPT.NUMBERED]), ["Pesikta Rabbati 15"]],
    # pytest.param(crrd(None, 'he', "  ", [slice(0, 2), 2], [RPT.NAMED, RPT.NUMBERED]), ["Yalkut Shimoni on Torah 23"], marks=pytest.mark.xfail(reason="Can't infer which Yalkut Shimoni from Remez")),
    # pytest.param(crrd(None, 'he', "    ", [slice(0, 2), 2, 3], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED]), ["Yalkut Shimoni on Torah 23"], marks=pytest.mark.xfail(reason="Can't infer which Yalkut Shimoni from book")),
    # pytest.param(crrd(None, 'he', '   "  ', [slice(0, 6), 6], [RPT.NAMED, RPT.NUMBERED]), ["Yalkut Shimoni on Nach 23"], marks=pytest.mark.xfail(reason="Can't infer which book in Yalkut Shimoni on Nach from Remez")),
    # [crrd(None, 'he', '   ', [slice(0, 2), 2, 3], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED]), ["Yalkut Shimoni on Nach 23"]],
    # [crrd(None, 'he', '  ', [slice(0, 2), 2], [RPT.NAMED, RPT.NUMBERED]), ["Midrash Tehillim 23"]],
    # [crrd(None, 'he', '   ', [slice(0, 2), slice(2, 4)], [RPT.NAMED, RPT.NUMBERED]), ["Midrash Mishlei 23"]],
    # [crrd(None, 'he', '  ', [0, 1, 2], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED]), ["Midrash Tanchuma, Bereshit 13"]],
    # [crrd(None, 'he', '   ', [slice(0,2), 2, 3], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED]), ["Midrash Tanchuma Buber, Bereshit 13"]],
    # [crrd(None, 'he', '   "', [slice(0, 2), slice(2, 6)], [RPT.NAMED, RPT.NUMBERED]), ["Tikkunei Zohar 23b"]],
    # pytest.param(crrd(None, 'he', '" ', [slice(0, 3), 3], [RPT.NAMED, RPT.NUMBERED]),["Tikkunei Zohar 24a:7-24b:1"], marks=pytest.mark.xfail(reason="Currently invalid way to refer to Tikkunei Zohar")),
    # pytest.param(crrd(None, 'he', '"  ', [slice(0, 3), slice(3, 5)], [RPT.NAMED, RPT.NUMBERED]), ["Tikkunei Zohar 24a:7-24b:1"], marks=pytest.mark.xfail(reason="Currently invalid way to refer to Tikkunei Zohar")),
    # pytest.param(crrd(None, 'he', '"   "', [slice(0, 3), 3, slice(4, 8)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Tikkunei Zohar 24b:1"], marks=pytest.mark.xfail(reason="Currently invalid way to refer to Tikkunei Zohar")),
    # [crrd(None, 'he', ' "', [0, slice(1, 4)], [RPT.NAMED, RPT.NAMED]), ["Tikkunei Zohar 1a:1-16b:4"]],
    # [crrd(None, 'he', '" "', [slice(0, 3), slice(3, 6)], [RPT.NAMED, RPT.NUMBERED]), ["Seder Olam Rabbah 11"]],
    # [crrd(None, 'he', '"  ', [slice(0, 3), 3, 4], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["The Book of Maccabees I 3:4"]],
    # [crrd(None, 'he', '    ', [slice(0, 3), 3, 4], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["The Book of Maccabees II 6:2"]],
    # [crrd(None, 'he', ' "  ', [slice(0,4), 4, 5], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Likutei Moharan 23:2"]],
    # pytest.param(crrd(None, 'he', ' "    ', [slice(0, 4), slice(4, 6), slice(6, 8)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Likutei Moharan 23:2"], marks=pytest.mark.xfail(reason="Torah and Ot are not valid AddressTypes yet")),
    # [crrd(None, 'he', ' "   ', [slice(0,4), 4, 5, 6], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Likutei Moharan, Part II 4:2"]],
    # [crrd(None, 'he', '  " "', [0, 1, slice(2, 5), slice(5, 8)], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Sefer Yetzirah 1:1"]],
    # [crrd(None, 'he', '   "  ', [0, 1, slice(3, 6), 6, 7], [RPT.NAMED, RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Sefer Yetzirah Gra Version 2:2"]],
    # [crrd(None, 'he', '  "    ', [slice(0, 2), slice(2, 5), slice(5, 7), 7, 8], [RPT.NAMED, RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Likutei Halakhot, Orach Chaim, Laws of Fringes 2:1"]],
    # [crrd(None, 'he', '     ', [slice(0, 2), slice(2, 6)], [RPT.NAMED, RPT.NAMED]), ["Likutei Halakhot, Choshen Mishpat, Laws of Roof Rails and Preservation of Life"]],
    # [crrd(None, 'he', "   \" \"", [slice(0, 2), 2, slice(3, 6), slice(6, 9)], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ["Jerusalem Talmud Ketubot 1:2"]],
    # [crrd(None, 'he', ' ', [0, 1], [RPT.NAMED, RPT.NUMBERED]), ['Sotah 43']],
    # [crrd(None, 'he', '    "', [slice(0, 2), slice(2, 4), slice(4, 7)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ['Mishnah Berurah 1:1']],
    # [crrd(None, 'he', '    ', [slice(0, 2), slice(2, 4), slice(4, 7)],  [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ['Mishnah Berurah 1:15']],
    # [crrd(None, 'he', '      ', [slice(0, 2), slice(2, 4), slice(4, 7)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ['Mishnah Berurah 1:1']],
    # [crrd(None, 'he', '     ', [slice(0, 2), slice(2, 4), slice(4, 8)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ['Mishnah Berurah 1:1']],
    # [crrd(None, 'he', '" "  " "', [slice(0, 3), slice(3, 6), slice(6, 10), slice(10, 13)], [RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ['Shulchan Arukh, Orach Chayim 329:6']],
    # [crrd(None, 'he', '" "  " "', [slice(0, 3), slice(3, 6), slice(6, 10), slice(10, 13)],
    #       [RPT.NAMED, RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), ['Shulchan Arukh, Orach Chayim 330:11']],
    #
    # # support basic ref instantiation as fall-back
    [crrd(['@Rashi on Genesis', '#1', '#1', '#1'], lang='en'), ["Rashi on Genesis 1:1:1"]],
])
def test_resolve_raw_ref(resolver_data, expected_trefs):
    raw_ref, context_ref, lang, prev_trefs = resolver_data
    linker = library.get_linker(lang)
    ref_resolver = linker._ref_resolver
    ref_resolver.reset_ibid_history()  # reset from previous test runs
    if prev_trefs:
        for prev_tref in prev_trefs:
            if prev_tref is None:
                ref_resolver.reset_ibid_history()
            else:
                ref_resolver._ibid_history.last_refs = Ref(prev_tref)
    print_spans(raw_ref)
    ref_resolver.set_thoroughness(ResolutionThoroughness.HIGH)
    matches = ref_resolver.resolve_raw_ref(context_ref, raw_ref)
    matched_orefs = sorted(reduce(lambda a, b: a + b, [[match.ref] if not match.is_ambiguous else [inner_match.ref for inner_match in match.resolved_raw_refs] for match in matches], []), key=lambda x: x.normal())
    if len(expected_trefs) != len(matched_orefs):
        print(f"Found {len(matched_orefs)} refs instead of {len(expected_trefs)}")
        for matched_oref in matched_orefs:
            print("-", matched_oref.normal())
    assert len(matched_orefs) == len(expected_trefs)
    for expected_tref, matched_oref in zip(sorted(expected_trefs, key=lambda x: x), matched_orefs):
        assert matched_oref == Ref(expected_tref)
class TestResolveRawRef:

    pass


@pytest.mark.parametrize(('context_tref', 'input_str', 'lang', 'expected_trefs', 'expected_pretty_texts'), [
    ["Berakhot 2a", 'It says in the Talmud, "Don\'t steal" which implies it\'s bad to steal.', 'en', tuple(), tuple()],  # Don't match Talmud using Berakhot 2a as ibid context
    [None, 'It says in the Torah, "Don\'t steal" which implies it\'s bad to steal.', 'en', tuple(), tuple()],
    [None, """'  .   (  " " "  :)""", 'he', ("Rashi on Beitzah 15b:8:1",), [' (  " " "  :)']],
    [None, """   ". '   " " :""", 'he', ("Tosafot on Menachot 44a:12:1",), ['  " " ']],
    [None, """'  . '    " ' " :""", 'he',("Tosafot on Menachot 78a:10:1",), ['''   " ' " ''']],
    [None, """cf. Ex. 9:6,12:8""", 'en', ("Exodus 9:6", "Exodus 12:8"), ['Ex. 9:6', '12:8']],
    ["Gilyon HaShas on Berakhot 25b:1", '"   " "   .  "  .', 'he', ("Rashi on Temurah 28b:4:2",), ['"   " "   ']],
    [None, "See Genesis 1:1. It says in the Torah, \"Don't steal\". It also says in 1:3 \"Let there be light\".", "en", ("Genesis 1:1", "Genesis 1:3"), ("Genesis 1:1", "1:3")],
])
def test_full_pipeline_ref_resolver(context_tref, input_str, lang, expected_trefs, expected_pretty_texts):
    context_oref = context_tref and Ref(context_tref)
    linker = library.get_linker(lang)
    doc = linker.link(input_str, context_oref, type_filter='citation')
    resolved = doc.resolved_refs
    resolved_orefs = sorted(reduce(lambda a, b: a + b, [[match.ref] if not match.is_ambiguous else [inner_match.ref for inner_match in match.resolved_raw_refs] for match in resolved], []), key=lambda x: x.normal())
    if len(expected_trefs) != len(resolved_orefs):
        print(f"Found {len(resolved_orefs)} refs instead of {len(expected_trefs)}")
        for matched_oref in resolved_orefs:
            print("-", matched_oref.normal())
    assert len(resolved) == len(expected_trefs)
    for expected_tref, matched_oref in zip(sorted(expected_trefs, key=lambda x: x), resolved_orefs):
        assert matched_oref == Ref(expected_tref)
    for match, expected_pretty_text in zip(resolved, expected_pretty_texts):
        assert input_str[slice(*match.raw_entity.char_indices)] == match.raw_entity.text
        assert match.pretty_text == expected_pretty_text


@pytest.mark.parametrize(('input_addr_str', 'AddressClass','expected_sections'), [
    ['"', schema.AddressPerek, ([8, 88], [8, 88], [schema.AddressPerek, schema.AddressInteger])],
    ['"', schema.AddressTalmud, ([87], [88], [schema.AddressTalmud])],
    ['".', schema.AddressTalmud, ([87], [87], [schema.AddressTalmud])],
    ['" "', schema.AddressTalmud, ([88], [88], [schema.AddressTalmud])],
    ['"', schema.AddressMishnah, ([4, 44], [4, 44], [schema.AddressMishnah, schema.AddressInteger])],
    ['"', schema.AddressPerek, ([1, 100], [1, 100], [schema.AddressPerek, schema.AddressPerek])],
])
def test_get_all_possible_sections_from_string(input_addr_str, AddressClass, expected_sections):
    exp_secs, exp2secs, exp_addrs = expected_sections
    sections, toSections, addr_classes = AddressClass.get_all_possible_sections_from_string('he', input_addr_str)
    sections = sorted(sections)
    toSections = sorted(toSections)
    assert sections == exp_secs
    assert toSections == exp2secs
    assert list(addr_classes) == exp_addrs


@pytest.mark.parametrize(('raw_ref_params', 'expected_section_slices'), [
    # [create_raw_ref_params('he', " :-", [0, 1, 3, 4, 5], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED, RPT.RANGE_SYMBOL, RPT.NUMBERED]), (slice(1,3),slice(4,5))],  # standard case
    # [create_raw_ref_params('he', ":-", [0, 2, 3, 4], [RPT.NUMBERED, RPT.NUMBERED, RPT.RANGE_SYMBOL, RPT.NUMBERED]), (slice(0,2),slice(3,4))],  # only numbered sections
    # [create_raw_ref_params('he', " :-:", [0, 1, 3, 4, 5, 7], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED, RPT.RANGE_SYMBOL, RPT.NUMBERED, RPT.NUMBERED]), (slice(1,3),slice(4,6))],  # full sections and toSections
    # [create_raw_ref_params('he', " :", [0, 1, 3], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED]), (None, None)],  # no ranged symbol
    # [create_raw_ref_params('he', '          ', [slice(0, 2), slice(2, 4), slice(4, 6), 6, slice(7, 9), slice(9, 11)], [RPT.NAMED, RPT.NUMBERED, RPT.NUMBERED, RPT.RANGE_SYMBOL, RPT.NUMBERED, RPT.NUMBERED]), (slice(1,3), slice(4,6))],  # verbose range
])
def test_group_ranged_parts(raw_ref_params, expected_section_slices):
    lang, raw_ref_parts, span = raw_ref_params
    raw_ref = RawRef(span, lang, raw_ref_parts)
    exp_sec_slice, exp2sec_slice = expected_section_slices
    if exp_sec_slice is None:
        expected_raw_ref_parts = raw_ref_parts
    else:
        sections = raw_ref_parts[exp_sec_slice]
        toSections = sections[:]
        toSections[-(exp2sec_slice.stop-exp2sec_slice.start):] = raw_ref_parts[exp2sec_slice]
        expected_ranged_raw_ref_parts = RangedRawRefParts(sections, toSections)
        expected_raw_ref_parts = raw_ref_parts[:exp_sec_slice.start] + \
                                 [expected_ranged_raw_ref_parts] + \
                                 raw_ref_parts[exp2sec_slice.stop:]
        ranged_raw_ref_parts = raw_ref.raw_ref_parts[exp_sec_slice.start]
        assert ranged_raw_ref_parts.sections == expected_ranged_raw_ref_parts.sections
        assert ranged_raw_ref_parts.toSections == expected_ranged_raw_ref_parts.toSections
        start_span = sections[0].span
        end_span = toSections[-1].span
        start_char, _ = start_span.range
        _, end_char = end_span.range
        full_span = start_span.doc.subspan(slice(start_char, end_char))
        assert ranged_raw_ref_parts.span.text == full_span.text
    assert expected_raw_ref_parts == raw_ref.raw_ref_parts


@pytest.mark.parametrize(('context_tref', 'match_title', 'common_title', 'expected_sec_cons'), [
    ['Gilyon HaShas on Berakhot 12b:1', 'Tosafot on Berakhot', 'Berakhot', (('Talmud', 'Daf', 24),)],
    ['Berakhot 2a:1', 'Berakhot', 'Berakhot', (('Talmud', 'Daf', 3),)]  # skip "Line" address which isn't referenceable
])
def test_get_section_contexts(context_tref, match_title, common_title, expected_sec_cons):
    context_ref = Ref(context_tref)
    match_index = library.get_index(match_title)
    common_index = library.get_index(common_title)
    section_contexts = RefResolver._get_section_contexts(context_ref, match_index, common_index)
    if len(section_contexts) != len(expected_sec_cons):
        print(f"Found {len(section_contexts)} sec cons instead of {len(expected_sec_cons)}")
        for sec_con in section_contexts:
            print("-", sec_con)
    assert len(section_contexts) == len(expected_sec_cons)
    for i, (addr_str, sec_name, address) in enumerate(expected_sec_cons):
        assert section_contexts[i] == SectionContext(schema.AddressType.to_class_by_address_type(addr_str), sec_name, address)


def test_address_matches_section_context():
    r = Ref("Berakhot")
    sec_con = SectionContext(schema.AddressType.to_class_by_address_type('Talmud'), 'Daf', 34)
    assert NumberedReferenceableBookNode(r.index_node).matches_section_context(sec_con)


@pytest.mark.parametrize(('last_n_to_store', 'trefs', 'expected_title_len'), [
    [1, ('Job 1', 'Job 2', 'Job 3'), (1, 1, 1)],
    [1, ('Job 1', 'Genesis 2', 'Exodus 3'), (1, 1, 1)],
    [2, ('Job 1', 'Genesis 2', 'Exodus 3'), (1, 2, 2)],

])
def test_ibid_history(last_n_to_store, trefs, expected_title_len):
    ibid = IbidHistory(last_n_to_store)
    orefs = [Ref(tref) for tref in trefs]
    for i, (oref, title_len) in enumerate(zip(orefs, expected_title_len)):
        ibid.last_refs = oref
        end = i-len(orefs)+1
        start = end-title_len
        end = None if end == 0 else end
        curr_refs = orefs[start:end]
        assert ibid._last_titles == [r.index.title for r in curr_refs]
        assert len(ibid._title_ref_map) == title_len
        for curr_ref in curr_refs:
            assert ibid._title_ref_map[curr_ref.index.title] == curr_ref


@pytest.mark.parametrize(('crrd_params',), [
    [[['@', '#', '#']]],  # no change in inds
    [[['@', '#', '0<b>', '#', '0</b>']]],

])
def test_map_new_indices(crrd_params):
    # unnorm data
    raw_ref, _, lang, _ = crrd(*crrd_params)
    text = raw_ref.text
    linker = library.get_linker(lang)
    doc = NEDoc(text)
    indices = raw_ref.char_indices
    part_indices = [p.char_indices for p in raw_ref.raw_ref_parts]
    print_spans(raw_ref)

    # norm data
    n = linker.get_ner()._normalizer
    norm_text = n.normalize(text)
    norm_doc = NEDoc(norm_text)
    norm_part_indices = n.norm_to_unnorm_indices(text, part_indices, reverse=True)
    norm_part_spans = [norm_doc.subspan(slice(s, e)) for (s, e) in norm_part_indices]
    norm_part_char_inds = []
    for span in norm_part_spans:
        start, end = span.range
        norm_part_char_inds += [slice(start, end)]

    part_types = [part.type for part in raw_ref.raw_ref_parts]
    raw_encoded_part_list = EncodedPart.convert_to_raw_encoded_part_list(lang, norm_text, norm_part_char_inds, part_types)
    norm_crrd_params = crrd_params[:]
    norm_crrd_params[0] = raw_encoded_part_list
    norm_raw_ref, _, _, _ = crrd(*norm_crrd_params)

    # test
    assert norm_raw_ref.text == norm_text.strip()
    norm_raw_ref.map_new_char_indices(doc, indices)
    norm_raw_ref.map_new_part_char_indices(part_indices)
    assert norm_raw_ref.text == raw_ref.text
    for norm_part, part in zip(norm_raw_ref.raw_ref_parts, raw_ref.raw_ref_parts):
        assert norm_part.text == part.text

```

### sefaria/model/linker/tests/ne_span_test.py

```
import django
django.setup()
import pytest
from sefaria.model.linker.ne_span import NEDoc, NESpan


class TestNEDoc:
    @pytest.mark.parametrize(
        "slc,expected_text,expected_range",
        [
            (slice(0, 4), "This", (0, 4)),
            (slice(0, 0), "", (0, 0)),
        ]
    )
    def test_subspan(self, slc, expected_text, expected_range):
        doc = NEDoc("This is a test document.")
        span = doc.subspan(slc)
        assert span.text == expected_text
        assert span.range == expected_range

    @pytest.mark.parametrize(
        "slc,expected_text,expected_range",
        [
            (slice(1, 3), "is a", (5, 9)),
            (slice(2, 2), "", (0, 0)),
        ]
    )
    def test_subspan_by_word_indices(self, slc, expected_text, expected_range):
        doc = NEDoc("This is a test document.")
        span = doc.subspan_by_word_indices(slc)
        assert span.text == expected_text
        assert span.range == expected_range

    def test_handles_out_of_range_word_indices(self):
        doc = NEDoc("This is a test document.")
        with pytest.raises(IndexError):
            doc.subspan_by_word_indices(slice(10, 12))

    def test_creates_correct_hash_for_span(self):
        doc = NEDoc("This is a test document.")
        span = NESpan(doc, 0, 4, "test_label")
        assert hash(span) == hash(("This is a test document.", 0, 4, "test_label"))


class TestNESpan:
    @pytest.mark.parametrize(
        "text,start,end,label,expected_text,expected_label,expected_range",
        [
            ("Abraham went to Egypt.", 0, 7, "PERSON", "Abraham", "PERSON", (0, 7)),
            ("Abraham went to Egypt.", 8, 12, "ACTION", "went", "ACTION", (8, 12)),
            ("Abraham went to Egypt.", 13, 15, None, "to", None, (13, 15)),
        ]
    )
    def test_properties(self, text, start, end, label, expected_text, expected_label, expected_range):
        doc = NEDoc(text)
        span = NESpan(doc, start, end, label)
        assert span.text == expected_text
        assert span.label == expected_label
        assert span.range == expected_range
        assert span.doc is doc

    def test_hash_equality_and_inequality(self):
        doc = NEDoc("Abraham went to Egypt.")
        span1 = NESpan(doc, 0, 7, "PERSON")
        span2 = NESpan(doc, 0, 7, "PERSON")
        span3 = NESpan(doc, 0, 7, "LOCATION")
        assert hash(span1) == hash(span2)
        assert hash(span1) != hash(span3)

    @pytest.mark.parametrize(
        "text,start,end,slice_args,expected_text,expected_range",
        [
            ("Abraham went to Egypt.", 0, 7, (0, 3), "Abr", (0, 3)),
            ("Abraham went to Egypt.", 8, 12, (0, 2), "we", (8, 10)),
        ]
    )
    def test_subspan(self, text, start, end, slice_args, expected_text, expected_range):
        doc = NEDoc(text)
        span = NESpan(doc, start, end, "LABEL")
        sub = span.subspan(slice(*slice_args))
        assert isinstance(sub, NESpan)
        assert sub.text == expected_text
        assert sub.range == expected_range
        assert sub.doc is doc

    def test_subspan_by_word_indices(self):
        doc = NEDoc("Abraham went to Egypt.")
        span = NESpan(doc, 0, len(doc.text), "EVENT")
        sub = span.subspan_by_word_indices(slice(2, 4))
        assert sub.text == "to Egypt"
        assert sub.doc is doc

    def test_word_length(self):
        doc = NEDoc("Abraham went to Egypt.")
        span = NESpan(doc, 0, len(doc.text), "EVENT")
        assert span.word_length() == 5

```

### sefaria/model/linker/tests/linker_linker_test.py

```

```

### sefaria/model/linker/tests/__init__.py

```

```

### sefaria/model/linker/tests/category_matcher_test.py

```
import pytest
from unittest.mock import Mock
from sefaria.model.category import Category
from sefaria.model.linker.category_resolver import CategoryMatcher


def make_title(text, lang):
    return {"text": text, "lang": lang}


@pytest.fixture
def mock_category():
    return Category({
        "titles": [make_title("Title1", "en"), make_title("Title2", "en")]
    })


@pytest.fixture
def mock_category_2():
    return Category({
        "titles": [make_title("Title1", "en"), make_title("Title4", "en")]
    })


@pytest.fixture
def mock_raw_ref():
    raw_ref = Mock()
    raw_ref.text = "Title2"
    return raw_ref


@pytest.fixture
def category_matcher(mock_category, mock_category_2):
    return CategoryMatcher(lang="en", category_registry=[mock_category, mock_category_2])


def test_match_single_title(category_matcher, mock_raw_ref, mock_category):
    # Test matching for a valid title in mock_raw_ref
    matched_categories = category_matcher.match(mock_raw_ref)
    assert len(matched_categories) == 1
    assert mock_category in matched_categories


def test_match_no_match(category_matcher, mock_raw_ref):
    # Test case where the raw_ref title does not match any category
    mock_raw_ref.text = "NonexistentTitle"
    matched_categories = category_matcher.match(mock_raw_ref)
    assert matched_categories == []


def test_match_multiple_titles(category_matcher, mock_raw_ref, mock_category, mock_category_2):
    # Test case where multiple categories match the same title
    mock_raw_ref.text = "Title1"

    matched_categories = category_matcher.match(mock_raw_ref)
    assert len(matched_categories) == 2
    assert mock_category in matched_categories
    assert mock_category_2 in matched_categories

```

### sefaria/model/linker/tests/resolved_category_test.py

```
import pytest
from unittest.mock import Mock
from sefaria.model.linker.category_resolver import ResolvedCategory


@pytest.fixture
def mock_raw_ref():
    return Mock()


@pytest.fixture
def mock_category():
    return Mock()


def test_is_ambiguous_with_single_category(mock_raw_ref, mock_category):
    # Test when there is exactly one category
    resolved_category = ResolvedCategory(mock_raw_ref, [mock_category])
    assert not resolved_category.is_ambiguous


def test_is_ambiguous_with_multiple_categories(mock_raw_ref, mock_category):
    # Test when there are multiple categories
    resolved_category = ResolvedCategory(mock_raw_ref, [mock_category, mock_category])
    assert resolved_category.is_ambiguous


def test_resolution_failed_with_no_categories(mock_raw_ref):
    # Test when there are no categories
    resolved_category = ResolvedCategory(mock_raw_ref, [])
    assert resolved_category.resolution_failed


def test_resolution_not_failed_with_categories(mock_raw_ref, mock_category):
    # Test when there are one or more categories
    resolved_category = ResolvedCategory(mock_raw_ref, [mock_category])
    assert not resolved_category.resolution_failed

```

### sefaria/model/linker/tests/non_unique_term_test.py

```
import pytest
from sefaria.model.schema import NonUniqueTerm
from sefaria.model.abstract import SluggedAbstractMongoRecord


@pytest.fixture(scope='module')
def duplicate_terms():
    initial_slug = "rashiBLAHBLAH"
    t = NonUniqueTerm({"slug": initial_slug, "titles": [{
        "text": "Rashi",
        "lang": "en",
        "primary": True
    }]})
    t.save()

    s = NonUniqueTerm({"slug": initial_slug, "titles": [{
        "text": "Rashi",
        "lang": "en",
        "primary": True
    }]})

    s.save()

    yield t, s, initial_slug

    t.delete()
    s.delete()


def test_duplicate_terms(duplicate_terms):
    t, s, initial_slug = duplicate_terms
    assert t.slug == SluggedAbstractMongoRecord.normalize_slug(initial_slug)
    assert s.slug == SluggedAbstractMongoRecord.normalize_slug(initial_slug) + "1"
```

### sefaria/model/linker/tests/linker_test_utils.py

```
import pytest
from typing import List
from functools import reduce
from sefaria.model.linker.ne_span import NEDoc
from sefaria.model.text import Ref, library
from sefaria.model.linker.ref_part import RefPartType, RawRef, RawRefPart
from sefaria.settings import ENABLE_LINKER

if not ENABLE_LINKER:
    pytest.skip("Linker not enabled", allow_module_level=True)


class RefPartTypeNone:
    """
    Represents no ref part type. A RefPart with this type will not be considered in parsing
    """
    pass


class EncodedPart:

    PART_TYPE_MAP = {
        "@": RefPartType.NAMED,
        "#": RefPartType.NUMBERED,
        "*": RefPartType.DH,
        "0": RefPartTypeNone,
        "^": RefPartType.RANGE_SYMBOL,
        "&": RefPartType.IBID,
        "<": RefPartType.RELATIVE,
        "~": RefPartType.NON_CTS,
    }

    def __init__(self, raw_encoded_part):
        self.raw_encoded_part = raw_encoded_part

    def _get_symbol(self):
        return self.raw_encoded_part[0]

    @property
    def text(self):
        return self.raw_encoded_part[1:]

    @property
    def part_type(self):
        symbol = self._get_symbol()

        if symbol not in self.PART_TYPE_MAP:
            raise Exception(f"Symbol '{symbol}' does not exist in EncodedParts.PART_TYPE_MAP. Valid symbols are "
                            f"{', '.join(self.PART_TYPE_MAP.keys())}")

        return self.PART_TYPE_MAP.get(symbol)

    @staticmethod
    def get_symbol_by_part_type(part_type):
        for symbol, temp_part_type in EncodedPart.PART_TYPE_MAP.items():
            if part_type == temp_part_type: return symbol

    @staticmethod
    def convert_to_raw_encoded_part_list(lang, text, span_slices, part_types):
        raw_encoded_part_list = []

        for part_type, span_slice in zip(part_types, span_slices):
            subtext = text[span_slice]
            symbol = EncodedPart.get_symbol_by_part_type(part_type)
            raw_encoded_part_list += [f"{symbol}{subtext}"]

        return raw_encoded_part_list


class EncodedPartList:

    def __init__(self, lang, raw_encoded_part_list: List[str]):
        self.lang = lang
        self.encoded_part_list: List[EncodedPart] = [EncodedPart(x) for x in raw_encoded_part_list]
        self._span = None

    @staticmethod
    def _get_part_separator():
        return " "

    def _get_char_inds(self):
        char_inds = []
        for part in self.encoded_part_list:
            if len(char_inds) > 0:
                last_char_ind = char_inds[-1][-1] + len(self._get_part_separator())
            else:
                last_char_ind = 0
            char_inds += [(last_char_ind, last_char_ind + len(part.text))]
        return char_inds

    @property
    def input_str(self):
        part_texts = [x.text for x in self.encoded_part_list]
        return reduce(lambda a, b: a + self._get_part_separator() + b, part_texts)

    @property
    def part_types(self):
        return [part.part_type for part in self.encoded_part_list]

    @property
    def span(self):
        if not self._span:
            doc = NEDoc(self.input_str)
            self._span = doc.subspan(slice(0, None))
        return self._span

    @property
    def span_slices(self):
        span_slices = []
        for char_start, char_end in self._get_char_inds():
            subspan = self.span.subspan(slice(char_start, char_end))
            span_slices += [slice(*subspan.range)]
        return span_slices

    @property
    def raw_ref_parts(self):
        try:
            part_spans = [self.span.subspan(span_slice) for span_slice in self.span_slices]
        except IndexError as e:
            self.print_debug_info()
            raise e
        raw_ref_parts = []
        for part_type, part_span in zip(self.part_types, part_spans):
            if part_type == RefPartTypeNone: continue
            raw_ref_parts += [RawRefPart(part_type, part_span)]
        return raw_ref_parts

    def get_raw_ref_params(self):
        return self.span, self.lang, self.raw_ref_parts

    def print_debug_info(self):
        print('Input:', self.input_str)
        print('Span indexes:', self.span_slices)
        print('Spans:')
        for i, subspan in enumerate(self.span):
            print(f'{i}) {subspan.text}')


def create_raw_ref_data(raw_encoded_parts: List[str], context_tref=None, lang='he', prev_trefs=None):
    """
    Just reflecting prev_trefs here b/c pytest.parametrize can't handle optional parameters
    """
    encoded_parts = EncodedPartList(lang, raw_encoded_parts)
    raw_ref = RawRef(*encoded_parts.get_raw_ref_params())
    context_oref = context_tref and Ref(context_tref)
    return raw_ref, context_oref, lang, prev_trefs


def print_spans(raw_ref: RawRef):
    print('\nInput:', raw_ref.text)
    print('Spans:')
    for i, part in enumerate(raw_ref.raw_ref_parts):
        print(f'{i}) {part.text}')
```

### sefaria/model/linker/ne_span.py

```
from abc import ABC, abstractmethod
from dataclasses import dataclass
from functools import cached_property
from sefaria.spacy_function_registry import get_spacy_tokenizer
from sefaria.settings import ENABLE_LINKER

if ENABLE_LINKER:
    TOKENIZER = get_spacy_tokenizer()
else:
    TOKENIZER = None


class _Subspannable(ABC):
    """
    Abstract base class for objects that contain text and can be subspanned (meaning, they can be sliced to create smaller spans).
    """

    @property
    @abstractmethod
    def text(self) -> str:
        pass

    @property
    @abstractmethod
    def doc(self) -> 'NEDoc':
        pass

    @abstractmethod
    def _get_subspan_offset(self) -> int:
        pass

    def word_length(self) -> int:
        """
        Returns the number of words in the text.
        Words are defined as runs of non-whitespace characters.
        """
        return len(self.__word_spans)

    def subspan(self, item: slice, span_label: str = None) -> 'NESpan':
        if isinstance(item, slice):
            start = item.start or 0
            end = item.stop
        else:
            raise TypeError("Item must be a slice")
        start += self._get_subspan_offset()
        if end is not None:
            end += self._get_subspan_offset()
        return NESpan(self.doc, start, end, span_label)

    @cached_property
    def __word_spans(self):
        doc = TOKENIZER(self.text)
        # extract start and end character indices of each word
        spans = [(token.idx, token.idx+len(token)) for token in doc if not token.is_space]
        return spans

    def subspan_by_word_indices(self, word_slice: slice) -> 'NESpan':
        """
        Return an NESpan covering words [start_word, end_word), where words
        are runs of non-whitespace. 0-based, end_word is exclusive.
        """
        spans = self.__word_spans
        word_span_slice = spans[word_slice]
        if not word_span_slice:
            if word_slice.start is not None and word_slice.start > len(spans):
                raise IndexError(f"Word indices out of range: {word_slice}. Document has {len(self.__word_spans)} words.")
            # slice is empty, return a span of zero length
            start_char = end_char = 0
        else:
            start_char = word_span_slice[0][0]
            end_char = word_span_slice[-1][1]
        return self.subspan(slice(start_char, end_char))


class NEDoc(_Subspannable):

    def __init__(self, text: str):
        self.__text = text

    @property
    def text(self) -> str:
        return self.__text

    @property
    def doc(self):
        return self

    def _get_subspan_offset(self) -> int:
        return 0


class NESpan(_Subspannable):
    """
    Span of text which represents a named entity before it has been identified with an object in Sefaria's DB
    """
    def __init__(self, doc: NEDoc, start: int, end: int, label: str = None):
        """
        :param doc: The document containing the text
        :param start: Start index of the span in the text
        :param end: End index of the span in the text
        :param label
        """
        self.__doc = doc
        self.__start = start or 0
        self.__end = end if end is not None else len(doc.text)
        self.__label = label

    def __str__(self):
        return f"NESpan(text='{self.text}', label='{self.label}', range={self.range})"

    @property
    def doc(self) -> NEDoc:
        return self.__doc

    @property
    def text(self) -> str:
        return self.__doc.text[self.__start:self.__end]

    @property
    def label(self) -> str:
        return self.__label

    @property
    def range(self) -> [int, int]:
        return self.__start, self.__end

    def __hash__(self):
        return hash((self.__doc.text, self.__start, self.__end, self.__label))

    def _get_subspan_offset(self) -> int:
        return self.__start

```

### sefaria/model/linker/match_template.py

```
from collections import defaultdict
from typing import List, Optional, Iterable
from functools import reduce
from sefaria.model import abstract as abst
from sefaria.model import schema
from .ref_part import TermContext, LEAF_TRIE_ENTRY
from .referenceable_book_node import NamedReferenceableBookNode
import structlog

logger = structlog.get_logger(__name__)


class MatchTemplate(abst.Cloneable):
    """
    Template for matching a SchemaNode to a RawRef
    """
    def __init__(self, term_slugs, scope='combined'):
        self.term_slugs = term_slugs
        self.scope = scope

    def get_terms(self) -> Iterable[schema.NonUniqueTerm]:
        for slug in self.term_slugs:
            yield schema.NonUniqueTerm.init(slug)

    def serialize(self) -> dict:
        serial = {
            "term_slugs": [t.slug for t in self.get_terms()],
        }
        if self.scope != 'combined':
            serial['scope'] = self.scope
        return serial

    def matches_scope(self, other_scope: str) -> bool:
        """
        Does `self`s scope match `other_scope`?
        @param other_scope:
        @return: True if scope matches
        """
        return other_scope == 'any' or self.scope == 'any' or other_scope == self.scope

    terms = property(get_terms)


class MatchTemplateTrie:
    """
    Trie for titles. Keys are titles from match_templates on nodes.
    E.g. if there is match template with term slugs ["term1", "term2"], term1 has title "Term 1", term2 has title "Term 2"
    then an entry in the trie would be {"Term 1": {"Term 2": ...}}
    """
    def __init__(self, lang: str, nodes: List[schema.TitledTreeNode] = None, sub_trie: dict = None, scope: str = None):
        """
        :param lang:
        :param nodes:
        :param sub_trie:
        :param scope: str. scope of the trie. if 'alone', take into account `match_templates` marked with scope "alone" or "any".
        """
        self.lang = lang
        self.scope = scope
        self._trie = self.__init_trie(nodes, sub_trie)

    def __init_trie(self, nodes: List[schema.TitledTreeNode], sub_trie: dict):
        if nodes is None:
            return sub_trie
        return self.__init_trie_with_nodes(nodes)

    def __init_trie_with_nodes(self, nodes: List[schema.TitledTreeNode]):
        trie = {}
        for node in nodes:
            for match_template in node.get_match_templates():
                if not node.is_root() and not match_template.matches_scope(self.scope):
                    continue
                curr_dict_queue = [trie]
                self.__add_all_term_titles_to_trie(match_template.terms, node, curr_dict_queue)
                self.__add_nodes_to_leaves(node, curr_dict_queue)
        return trie

    @staticmethod
    def __log_non_existent_term_warning(node: schema.TitledTreeNode):
        try:
            node_ref = node.ref()
        except:
            node_ref = node.get_primary_title('en')
        logger.warning(f"{node_ref} has match_templates that reference slugs that don't exist."
                       f"Check match_templates and fix.")

    def __add_all_term_titles_to_trie(self, term_list: List[schema.NonUniqueTerm], node: schema.TitledTreeNode, curr_dict_queue: List[dict]):
        for term in term_list:
            if term is None:
                self.__log_non_existent_term_warning(node)
                continue
            self.__add_term_titles_to_trie(term, curr_dict_queue)

    def __add_term_titles_to_trie(self, term, curr_dict_queue: List[dict]):
        len_curr_dict_queue = len(curr_dict_queue)
        for _ in range(len_curr_dict_queue):
            curr_dict = curr_dict_queue.pop(0)
            curr_dict_queue += self.__get_sub_tries_for_term(term, curr_dict)

    @staticmethod
    def __add_nodes_to_leaves(node: schema.TitledTreeNode, curr_dict_queue: List[dict]):
        for curr_dict in curr_dict_queue:
            leaf_node = NamedReferenceableBookNode(node.index if node.is_root() else node)
            if LEAF_TRIE_ENTRY in curr_dict:
                curr_dict[LEAF_TRIE_ENTRY] += [leaf_node]
            else:
                curr_dict[LEAF_TRIE_ENTRY] = [leaf_node]

    @staticmethod
    def __get_sub_trie_for_new_key(key: str, curr_trie: dict) -> dict:
        if key in curr_trie:
            sub_trie = curr_trie[key]
        else:
            sub_trie = {}
            curr_trie[key] = sub_trie
        return sub_trie

    def __get_sub_tries_for_term(self, term: schema.NonUniqueTerm, curr_trie: dict) -> List[dict]:
        sub_tries = []
        for title in term.get_titles(self.lang):
            sub_tries += [self.__get_sub_trie_for_new_key(title, curr_trie)]
        # also add term's key to trie for lookups from context ref parts
        sub_tries += [self.__get_sub_trie_for_new_key(TermContext(term).key(), curr_trie)]
        return sub_tries

    def __getitem__(self, key):
        return self.get(key)        

    def get(self, key, default=None):
        sub_trie = self._trie.get(key, default)
        if sub_trie is None: return
        return MatchTemplateTrie(self.lang, sub_trie=sub_trie, scope=self.scope)

    def has_continuations(self, key: str, key_is_id=False) -> bool:
        """
        Does trie have continuations for `key`?
        :param key: key to look up in trie. may need to be split into multiple keys to find a continuation.
        :param key_is_id: True if key is ID that cannot be split into smaller keys (e.g. slug).
        TODO currently not allowing partial matches here but theoretically possible
        """
        conts, _ = self.get_continuations(key, default=None, key_is_id=key_is_id, allow_partial=False)
        return conts is not None

    @staticmethod
    def _merge_two_tries(a, b):
        "merges b into a"
        for key in b:
            if key in a:
                if isinstance(a[key], dict) and isinstance(b[key], dict):
                    MatchTemplateTrie._merge_two_tries(a[key], b[key])
                elif a[key] == b[key]:
                    pass  # same leaf value
                elif isinstance(a[key], list) and isinstance(b[key], list):
                    a[key] += b[key]
                else:
                    raise Exception('Conflict in _merge_two_tries')
            else:
                a[key] = b[key]
        return a

    @staticmethod
    def _merge_n_tries(*tries):
        if len(tries) == 1:
            return tries[0]
        return reduce(MatchTemplateTrie._merge_two_tries, tries)

    def get_continuations(self, key: str, default=None, key_is_id=False, allow_partial=False):
        continuations, partial_key_end_list = self._get_continuations_recursive(key, key_is_id=key_is_id, allow_partial=allow_partial)
        if len(continuations) == 0:
            return default, None
        merged = self._merge_n_tries(*continuations)
        # TODO unclear how to 'merge' partial_key_end_list. Currently will only work if there's one continuation
        partial_key_end = partial_key_end_list[0] if len(partial_key_end_list) == 1 else None
        return MatchTemplateTrie(self.lang, sub_trie=merged, scope=self.scope), partial_key_end

    def _get_continuations_recursive(self, key: str, prev_sub_tries=None, key_is_id=False, has_partial_matches=False, allow_partial=False):
        from sefaria.utils.hebrew import get_prefixless_inds
        import re

        prev_sub_tries = prev_sub_tries or self._trie
        if key_is_id:
            # dont attempt to split key
            next_sub_tries = [prev_sub_tries[key]] if key in prev_sub_tries else []
            return next_sub_tries, []
        next_sub_tries = []
        partial_key_end_list = []
        key = key.strip()
        starti_list = [0]
        if self.lang == 'he' and len(key) >= 4:
            # In AddressType.get_all_possible_sections_from_string(), we prevent stripping of prefixes from AddressInteger. No simple way to do that with terms that take the place of AddressInteger (e.g. Bavli Perek). len() check is a heuristic.
            starti_list += get_prefixless_inds(key)
        for starti in starti_list:
            for match in reversed(list(re.finditer(r'(\s+|$)', key[starti:]))):
                endi = match.start() + starti
                sub_key = key[starti:endi]
                if sub_key not in prev_sub_tries: continue
                if endi == len(key):
                    next_sub_tries += [prev_sub_tries[sub_key]]
                    partial_key_end_list += [None]
                    continue
                temp_sub_tries, temp_partial_key_end_list = self._get_continuations_recursive(key[endi:], prev_sub_tries[sub_key], has_partial_matches=True, allow_partial=allow_partial)
                next_sub_tries += temp_sub_tries
                partial_key_end_list += temp_partial_key_end_list

        if has_partial_matches and len(next_sub_tries) == 0 and allow_partial and isinstance(prev_sub_tries, dict):
            # partial match without any complete matches
            return [prev_sub_tries], [key]
        if len(partial_key_end_list) > 1:
            # currently we don't consider partial keys if there's more than one match
            full_key_matches = list(filter(lambda x: x[1] is None, zip(next_sub_tries, partial_key_end_list)))
            if len(full_key_matches) == 0:
                return [], []
            next_sub_tries, partial_key_end_list = zip(*full_key_matches)
        return next_sub_tries, partial_key_end_list

    def __contains__(self, key):
        return key in self._trie

    def __iter__(self):
        for item in self._trie:
            yield item

```

### sefaria/model/linker/linker_entity_recognizer.py

```
from typing import Generator, Optional, Union, Any
from functools import reduce
from collections import defaultdict
from sefaria.model.linker.ref_part import RawRef, RawRefPart, RefPartType, RawNamedEntity, NamedEntityType
from sefaria.helper.normalization import NormalizerComposer
from sefaria.model.linker.named_entity_recognizer import AbstractNER
from sefaria.model.linker.ne_span import NESpan, NEDoc


class LinkerEntityRecognizer:
    """
    Wrapper around AbstractNER
    Given AbstractNER for NER and Raw Ref parts, runs them and returns parsed RawNamedEntity including RawRefs.
    Currently, named entities include:
    - refs
    - people
    - groups of people
    """

    def __init__(self, lang: str, named_entity_model: AbstractNER, raw_ref_part_model: AbstractNER):
        """

        @param lang: language that the Recognizer understands (based on how the models were trained)
        @param named_entity_model: NER model which takes a string and recognizes where entities are
        @param raw_ref_part_model: NER model which takes a string raw ref and recognizes the parts of the ref
        """
        self._lang = lang
        self._named_entity_model = named_entity_model
        self._raw_ref_part_model = raw_ref_part_model
        self._normalizer = self.__init_normalizer()

    def __init_normalizer(self) -> NormalizerComposer:
        # see ML Repo library_exporter.py:TextWalker.__init__() which uses same normalization
        # important that normalization is equivalent to normalization done at training time
        normalizer_steps = ['unidecode', 'html', 'double-space']
        if self._lang == 'he':
            normalizer_steps += ['maqaf', 'cantillation']
        return NormalizerComposer(normalizer_steps)

    def bulk_recognize(self, inputs: list[str]) -> list[list[RawNamedEntity]]:
        """
        Return all RawNamedEntity's in `inputs`. If the entity is a citation, parse out the inner RawRefParts and create
        RawRefs.
        @param inputs: List of strings to search for named entities in.
        @return: 2D list of RawNamedEntity's. Includes RawRefs which are a subtype of RawNamedEntity
        """
        all_raw_named_entities = self._bulk_get_raw_named_entities_wo_raw_refs(inputs)
        all_citations, all_non_citations = self._bulk_partition_named_entities_by_citation_type(all_raw_named_entities)
        all_raw_refs = self._bulk_parse_raw_refs(all_citations)
        merged_entities = []
        for inner_non_citations, inner_citations in zip(all_non_citations, all_raw_refs):
            merged_entities += [inner_non_citations + inner_citations]
        return merged_entities

    def recognize(self, input_str: str) -> [list[RawRef], list[RawNamedEntity]]:
        raw_named_entities = self._get_raw_named_entities_wo_raw_refs(input_str)
        citations, non_citations = self._partition_named_entities_by_citation_type(raw_named_entities)
        raw_refs = self._parse_raw_refs(citations)
        return raw_refs, non_citations

    def _bulk_get_raw_named_entities_wo_raw_refs(self, inputs: list[str]) -> list[list[RawNamedEntity]]:
        """
        Finds RawNamedEntities in `inputs` but doesn't parse citations into RawRefs with RawRefParts
        @param inputs:
        @return:
        """
        normalized_inputs = self._normalize_input(inputs)
        all_raw_named_entity_spans = list(self._bulk_get_raw_named_entity_spans(normalized_inputs))
        all_raw_named_entities = []
        for raw_named_entity_spans in all_raw_named_entity_spans:
            temp_raw_named_entities = []
            for span in raw_named_entity_spans:
                ne_type = NamedEntityType.span_label_to_enum(span.label)
                temp_raw_named_entities += [RawNamedEntity(span, ne_type)]
            all_raw_named_entities += [temp_raw_named_entities]
        return all_raw_named_entities

    def _get_raw_named_entities_wo_raw_refs(self, input_str: str) -> list[RawNamedEntity]:
        """
        Finds RawNamedEntities in `input_str` but doesn't parse citations into RawRefs with RawRefParts
        @param input_str:
        @return:
        """
        normalized_input = self._normalize_input([input_str])[0]
        raw_named_entity_spans = self._get_raw_named_entity_spans(normalized_input)
        raw_named_entities = []
        for span in raw_named_entity_spans:
            ne_type = NamedEntityType.span_label_to_enum(span.label)
            raw_named_entities += [RawNamedEntity(span, ne_type)]
        return raw_named_entities

    @staticmethod
    def _bulk_partition_named_entities_by_citation_type(
            all_raw_named_entities: list[list[RawNamedEntity]]
    ) -> [list[list[RawNamedEntity]], list[list[RawNamedEntity]]]:
        """
        Given named entities, partition them into two lists; list of entities that are citations and those that aren't.
        @param all_raw_named_entities:
        @return:
        """
        citations, non_citations = [], []
        for sublist in all_raw_named_entities:
            inner_citations, inner_non_citations = LinkerEntityRecognizer._partition_named_entities_by_citation_type(sublist)
            citations += [inner_citations]
            non_citations += [inner_non_citations]
        return citations, non_citations

    @staticmethod
    def _partition_named_entities_by_citation_type(
            raw_named_entities: list[RawNamedEntity]
    ) -> [list[RawNamedEntity], list[RawNamedEntity]]:
        citations, non_citations = [], []
        for named_entity in raw_named_entities:
            curr_list = citations if named_entity.type == NamedEntityType.CITATION else non_citations
            curr_list += [named_entity]
        return citations, non_citations

    def _bulk_parse_raw_refs(self, all_citation_entities: list[list[RawNamedEntity]]) -> list[list[RawRef]]:
        """
        Runs models on inputs to locate all refs and ref parts
        Note: takes advantage of bulk spaCy operations. It is more efficient to pass multiple strings in input than to
        run this function multiple times
        @param all_citation_entities: List of RawNamedEntity lists, where each inner list corresponds to the named entities found in a string of the input.
        @return: 2D list of RawRefs. Each inner list corresponds to the refs found in a string of the input.
        """
        ref_part_input = reduce(lambda a, b: a + [(sub_b.text, b[0]) for sub_b in b[1]], enumerate(all_citation_entities), [])
        all_raw_ref_part_spans = list(self._bulk_get_raw_ref_part_spans(ref_part_input, as_tuples=True))
        all_raw_ref_part_span_map = defaultdict(list)
        for ref_part_span, input_idx in all_raw_ref_part_spans:
            all_raw_ref_part_span_map[input_idx] += [ref_part_span]

        all_raw_refs = []
        for input_idx, named_entities in enumerate(all_citation_entities):
            raw_ref_part_spans = all_raw_ref_part_span_map[input_idx]
            all_raw_refs += [self._bulk_make_raw_refs(named_entities, raw_ref_part_spans)]
        return all_raw_refs

    def _parse_raw_refs(self, citation_entities: list[RawNamedEntity]) -> list[RawRef]:
        raw_ref_part_spans = list(self._bulk_get_raw_ref_part_spans([e.text for e in citation_entities]))
        return self._bulk_make_raw_refs(citation_entities, raw_ref_part_spans)

    def bulk_map_normal_output_to_original_input(self, input: list[str], raw_ref_list_list: list[list[RawRef]]):
        for temp_input, raw_ref_list in zip(input, raw_ref_list_list):
            self.map_normal_output_to_original_input(temp_input, raw_ref_list)

    def map_normal_output_to_original_input(self, input: str, named_entities: list[RawNamedEntity]) -> None:
        """
        Ref resolution ran on normalized input. Remap raw refs to original (non-normalized) input
        """
        unnorm_doc = NEDoc(input)
        mapping, subst_end_indices = self._normalizer.get_mapping_after_normalization(input)
        conv = self._normalizer.norm_to_unnorm_indices_with_mapping
        norm_inds = [named_entity.char_indices for named_entity in named_entities]
        unnorm_inds = conv(norm_inds, mapping, subst_end_indices)
        unnorm_part_inds = []
        for (named_entity, (norm_raw_ref_start, _)) in zip(named_entities, norm_inds):
            raw_ref_parts = named_entity.raw_ref_parts if isinstance(named_entity, RawRef) else []
            unnorm_part_inds += [conv([[norm_raw_ref_start + i for i in part.char_indices]
                                       for part in raw_ref_parts], mapping, subst_end_indices)]
        for named_entity, temp_unnorm_inds, temp_unnorm_part_inds in zip(named_entities, unnorm_inds, unnorm_part_inds):
            named_entity.map_new_char_indices(unnorm_doc, temp_unnorm_inds)
            if isinstance(named_entity, RawRef):
                named_entity.map_new_part_char_indices(temp_unnorm_part_inds)

    @property
    def named_entity_model(self):
        return self._named_entity_model

    @property
    def raw_ref_part_model(self):
        return self._raw_ref_part_model

    def _normalize_input(self, input: list[str]):
        """
        Normalize input text to match normalization that happened at training time
        """
        return [self._normalizer.normalize(s) for s in input]

    def _get_raw_named_entity_spans(self, st: str) -> list[NESpan]:
        return self._named_entity_model.predict(st)

    def _get_raw_ref_part_spans(self, st: str) -> list[NESpan]:
        return self._raw_ref_part_model.predict(st)

    def _bulk_get_raw_named_entity_spans(self, inputs: Union[list[str], list[tuple[str, Any]]], batch_size=150, as_tuples=False) -> Generator[list[NESpan], None, None]:
        if as_tuples:
            yield from self._named_entity_model.bulk_predict_as_tuples(inputs, batch_size)
        else:
            yield from self._named_entity_model.bulk_predict(inputs, batch_size)

    def _bulk_get_raw_ref_part_spans(self, inputs: Union[list[str], list[tuple[str, Any]]], batch_size=None, as_tuples=False) -> Generator[list[NESpan], None, None]:
        batch_size = batch_size or len(inputs)
        if as_tuples:
            yield from self._raw_ref_part_model.bulk_predict_as_tuples(inputs, batch_size)
        else:
            yield from self._raw_ref_part_model.bulk_predict(inputs, batch_size)

    # TODO the following four functions are very loosely coupled to the LinkerEntityRecognizer and should be moved out
    def _bulk_make_raw_refs(self, named_entities: list[RawNamedEntity], raw_ref_part_spans: list[list[NESpan]]) -> list[RawRef]:
        raw_refs = []
        dh_continuations = self._bulk_make_dh_continuations(named_entities, raw_ref_part_spans)
        for named_entity, part_span_list, temp_dh_continuations in zip(named_entities, raw_ref_part_spans, dh_continuations):
            raw_refs += [self._make_raw_ref(named_entity.span, part_span_list, temp_dh_continuations)]
        return raw_refs

    def _make_raw_ref(self, span: NESpan, part_span_list: list[NESpan], dh_continuations: list[NESpan]) -> RawRef:
        raw_ref_parts = []
        for part_span, dh_continuation in zip(part_span_list, dh_continuations):
            part_type = RefPartType.span_label_to_enum(part_span.label)
            raw_ref_parts += [RawRefPart(part_type, part_span, dh_continuation)]
        return RawRef(span, self._lang, raw_ref_parts)

    @staticmethod
    def _bulk_make_dh_continuations(named_entities: list[RawNamedEntity], raw_ref_part_spans) -> list[list[NESpan]]:
        dh_continuations = []
        for ispan, (named_entity, part_span_list) in enumerate(zip(named_entities, raw_ref_part_spans)):
            temp_dh_continuations = []
            for ipart, part_span in enumerate(part_span_list):
                part_type = RefPartType.span_label_to_enum(part_span.label)
                dh_continuation = None
                if part_type == RefPartType.DH:
                    dh_continuation = LinkerEntityRecognizer._get_dh_continuation(ispan, ipart, named_entities, part_span_list,
                                                                                  named_entity.span, part_span)
                temp_dh_continuations += [dh_continuation]
            dh_continuations += [temp_dh_continuations]
        return dh_continuations

    @staticmethod
    def _get_dh_continuation(ispan: int, ipart: int, named_entities: list[RawNamedEntity], part_span_list: list[NESpan], span: NESpan, part_span: NESpan) -> Optional[NESpan]:
        if ipart == len(part_span_list) - 1:
            curr_doc = span.doc
            _, span_end = span.range
            if ispan == len(named_entities) - 1:
                dh_cont = curr_doc.subspan(slice(span_end, None))
            else:
                next_span_start, _ = named_entities[ispan + 1].span.range
                dh_cont = curr_doc.subspan(slice(span_end, next_span_start))
        else:
            _, part_span_end = part_span.range
            next_part_span_start, _ = part_span_list[ipart + 1].range
            dh_cont = part_span.subspan(slice(part_span_end, next_part_span_start))

        return dh_cont

```

### sefaria/model/linker/has_match_template.py

```
class HasMatchTemplates:
    MATCH_TEMPLATE_ALONE_SCOPES = {'any', 'alone'}

    def get_match_templates(self):
        from sefaria.model.linker.match_template import MatchTemplate
        for raw_match_template in getattr(self, 'match_templates', []):
            yield MatchTemplate(**raw_match_template)

    def get_match_template_trie(self, lang: str):
        from sefaria.model.linker.match_template import MatchTemplateTrie
        return MatchTemplateTrie(lang, nodes=[self], scope='combined')

    def has_scope_alone_match_template(self):
        """
        @return: True if `self` has any match template that has scope = "alone" OR scope = "any"
        """
        return any(template.scope in self.MATCH_TEMPLATE_ALONE_SCOPES for template in self.get_match_templates())



```

### sefaria/model/linker/resolved_ref_refiner.py

```
from abc import ABC, abstractmethod
from itertools import product
from typing import List
from sefaria.model.schema import AddressInteger
from sefaria.model.linker.referenceable_book_node import ReferenceableBookNode, NumberedReferenceableBookNode, NamedReferenceableBookNode, DiburHamatchilNodeSet
from sefaria.model.linker.ref_part import RawRefPart, SectionContext, RefPartType
from sefaria.system.exceptions import InputError
from sefaria.model.text import Ref


class ResolvedRefRefiner(ABC):

    def __init__(self, part_to_match: RawRefPart, node: ReferenceableBookNode, resolved_ref: 'ResolvedRef'):
        self.part_to_match = part_to_match
        self.node = node
        self.resolved_ref = resolved_ref

    def _get_resolved_parts(self):
        return self.resolved_ref.resolved_parts + [self.part_to_match]

    def _clone_resolved_ref(self, **kwargs) -> 'ResolvedRef':
        return self.resolved_ref.clone(**kwargs)

    def _has_prev_unused_numbered_ref_part(self) -> bool:
        """
        Helper function to avoid matching AddressInteger sections out of order
        Returns True if there is a RawRefPart which immediately precedes `raw_ref_part` and is not yet included in this match
        """
        prev_part = self.resolved_ref.raw_entity.prev_num_parts_map.get(self.part_to_match, None)
        if prev_part is None: return False
        return prev_part not in set(self.resolved_ref.resolved_parts)

    def _has_prev_unused_numbered_ref_part_for_node(self, lang: str) -> bool:
        """
        For SchemaNodes or ArrayMapNodes that have numeric equivalents (e.g. an alt struct for perek)
        make sure we are not matching AddressIntegers out of order. See self.has_prev_unused_numbered_ref_part()
        """
        if self.part_to_match.type != RefPartType.NUMBERED or \
                not self.node.get_numeric_equivalent() or \
                not self._has_prev_unused_numbered_ref_part():
            return False
        try:
            possible_sections, possible_to_sections, addr_classes = AddressInteger(0).get_all_possible_sections_from_string(lang, self.part_to_match.text, strip_prefixes=True)
            for sec, toSec, addr_class in zip(possible_sections, possible_to_sections, addr_classes):
                if sec != self.node.get_numeric_equivalent(): continue
                if addr_class == AddressInteger: return True
        except KeyError:
            return False

    @abstractmethod
    def refine(self, lang: str, **kwargs) -> List['ResolvedRef']:
        pass


class ResolvedRefRefinerForDefaultNode(ResolvedRefRefiner):

    def refine(self, lang: str, **kwargs) -> List['ResolvedRef']:
        resolved_parts = self.resolved_ref.resolved_parts
        return [self._clone_resolved_ref(resolved_parts=resolved_parts, node=self.node, ref=self.node.ref())]


class ResolvedRefRefinerForNumberedPart(ResolvedRefRefiner):

    def refine(self, lang: str, **kwargs) -> List['ResolvedRef']:
        if isinstance(self.part_to_match, SectionContext):
            return self.__refine_context_full()
        return self.__refine_context_free(lang, **kwargs)

    def __refine_context_full(self) -> List['ResolvedRef']:
        if self.node is None or not self.node.matches_section_context(self.part_to_match):
            return []
        try:
            refined_ref = self.resolved_ref.ref.subref(self.part_to_match.address)
        except (InputError, IndexError, AssertionError, AttributeError):
            return []
        return [self._clone_resolved_ref(resolved_parts=self._get_resolved_parts(), node=self.node, ref=refined_ref)]

    def __refine_context_free(self, lang: str, fromSections=None) -> List['ResolvedRef']:
        if self.node is None or not isinstance(self.node, NumberedReferenceableBookNode):
            return []
        possible_subrefs, can_match_out_of_order_list = self.node.possible_subrefs(lang, self.resolved_ref.ref, self.part_to_match.text, fromSections)
        refined_refs = []
        for refined_ref, can_match_out_of_order in zip(possible_subrefs, can_match_out_of_order_list):
            if self._has_prev_unused_numbered_ref_part() and not can_match_out_of_order:
                """
                If raw_ref has NUMBERED parts [a, b]
                and part b matches before part a
                and part b gets matched as AddressInteger
                discard match because AddressInteger parts need to match in order
                """
                continue
            refined_refs += [refined_ref]
        return [self._clone_resolved_ref(resolved_parts=self._get_resolved_parts(), node=self.node, ref=refined_ref) for refined_ref in refined_refs]


class ResolvedRefRefinerForRangedPart(ResolvedRefRefiner):

    def __get_refined_matches_for_ranged_sections(self, sections: List['RawRefPart'], node: NumberedReferenceableBookNode, lang, fromSections: list=None):
        resolved_raw_refs: List['ResolvedRef'] = [self._clone_resolved_ref(resolved_parts=self._get_resolved_parts(), node=node)]
        incomplete_resolved_raw_refs = []
        is_first_pass = True
        for section_part in sections:
            queue_len = len(resolved_raw_refs)
            for _ in range(queue_len):
                temp_resolved_raw_ref = resolved_raw_refs.pop(0)
                if not is_first_pass:
                    temp_children = temp_resolved_raw_ref.node.get_children(temp_resolved_raw_ref.ref)
                    temp_resolved_raw_ref.node = None if len(temp_children) == 0 else temp_children[0]
                is_first_pass = False
                temp_resolved_ref_refiner = ResolvedRefRefinerForNumberedPart(section_part, temp_resolved_raw_ref.node, temp_resolved_raw_ref)
                next_resolved_raw_refs = temp_resolved_ref_refiner.refine(lang, fromSections=fromSections)
                resolved_raw_refs += next_resolved_raw_refs
                if len(next_resolved_raw_refs) == 0 and False:
                    # disabling incomplete ranged ref matches to avoid false positives
                    incomplete_resolved_raw_refs += [temp_resolved_raw_ref]
        return resolved_raw_refs, incomplete_resolved_raw_refs

    def refine(self, lang: str, **kwargs) -> List['ResolvedRef']:
        section_resolved_raw_refs, incomplete_section_refs = self.__get_refined_matches_for_ranged_sections(self.part_to_match.sections, self.node, lang)
        toSection_resolved_raw_refs, _ = self.__get_refined_matches_for_ranged_sections(self.part_to_match.toSections, self.node, lang, fromSections=[x.ref.sections for x in section_resolved_raw_refs])
        ranged_resolved_raw_refs = []
        for section, toSection in product(section_resolved_raw_refs, toSection_resolved_raw_refs):
            try:
                ranged_resolved_raw_refs += [self._clone_resolved_ref(resolved_parts=self._get_resolved_parts(), node=section.node, ref=section.ref.to(toSection.ref))]
            except InputError:
                continue
        if len(section_resolved_raw_refs) == 0:
            ranged_resolved_raw_refs += incomplete_section_refs
        return ranged_resolved_raw_refs


class ResolvedRefRefinerForNamedNode(ResolvedRefRefiner):

    def refine(self, lang: str, **kwargs) -> List['ResolvedRef']:
        if self.node.ref_part_title_trie(lang).has_continuations(self.part_to_match.key(), key_is_id=self.part_to_match.key_is_id) \
                and not self._has_prev_unused_numbered_ref_part_for_node(lang):

            return [self._clone_resolved_ref(resolved_parts=self._get_resolved_parts(), node=self.node, ref=self.node.ref())]
        return []


class ResolvedRefRefinerForDiburHamatchilPart(ResolvedRefRefiner):

    def __get_refined_matches_for_dh_part(self, lang, raw_ref_part: RawRefPart, node: DiburHamatchilNodeSet):
        """
        Finds dibur hamatchil ref which best matches `raw_ref_part`
        Currently a simplistic algorithm
        If there is a DH match, return the corresponding 'ResolvedRef'
        """
        if not self.resolved_ref.complies_with_thoroughness_level():
            return []
        best_matches = node.best_fuzzy_matches(lang, raw_ref_part)

        if len(best_matches):
            best_dh = max(best_matches, key=lambda x: x.order_key())
            self.resolved_ref._set_matched_dh(raw_ref_part, best_dh.potential_dh_token_idx)

        return [self._clone_resolved_ref(resolved_parts=self._get_resolved_parts().copy(), node=dh_match.dh_node, ref=Ref(dh_match.dh_node.ref)) for dh_match in best_matches]

    def refine(self, lang: str, **kwargs) -> List['ResolvedRef']:
        node = self.node
        if isinstance(node, NumberedReferenceableBookNode):
            # jagged array node can be skipped entirely if it has a dh child
            # technically doesn't work if there is a referenceable child in between ja and dh node
            node_children = self.node.get_children(self.resolved_ref.ref)
            node = None if len(node_children) == 0 else node_children[0]
        if isinstance(node, DiburHamatchilNodeSet):
            return self.__get_refined_matches_for_dh_part(lang, self.part_to_match, node)
        return []


class ResolvedRefRefinerCatchAll(ResolvedRefRefiner):

    def refine(self, lang: str, **kwargs) -> List['ResolvedRef']:
        return []

```

### sefaria/model/linker/named_entity_recognizer.py

```
from abc import ABC, abstractmethod
import re
from typing import Union, Any
import structlog
from sefaria.model.linker.ne_span import NESpan, NEDoc
try:
    import spacy
except ImportError:
    spacy = None

logger = structlog.get_logger(__name__)


def load_spacy_model(path: str):
    import tarfile
    from tempfile import TemporaryDirectory
    from sefaria.google_storage_manager import GoogleStorageManager
    from sefaria.spacy_function_registry import inner_punct_tokenizer_factory  # this looks unused, but spacy.load() expects this function to be in scope

    using_gpu = spacy.prefer_gpu()
    logger.info(f"Spacy successfully connected to GPU: {using_gpu}")

    if path.startswith("gs://"):
        # file is located in Google Cloud
        # file is expected to be a tar.gz of the contents of the model folder (not the folder itself)
        match = re.match(r"gs://([^/]+)/(.+)$", path)
        bucket_name = match.group(1)
        blob_name = match.group(2)
        model_buffer = GoogleStorageManager.get_filename(blob_name, bucket_name)
        tar_buffer = tarfile.open(fileobj=model_buffer)
        with TemporaryDirectory() as tempdir:
            tar_buffer.extractall(tempdir)
            nlp = spacy.load(tempdir)
    else:
        nlp = spacy.load(path)
    return nlp


class NERFactory:
    @staticmethod
    def create(model_type: str, model_location: str) -> 'AbstractNER':
        if model_type == "spacy":
            return SpacyNER(model_location)
        elif model_type == "huggingface":
            return HuggingFaceNER()
        else:
            raise ValueError(f"Unknown model type: {model_type}")


class AbstractNER(ABC):

    @abstractmethod
    def __init__(self, model_location: str):
        """
        Initializes the inference model with the specified model name.

        :param model_location: The name of the model to load.
        """
        pass

    @abstractmethod
    def predict(self, text: str) -> list[NESpan]:
        """
        Predicts the named entities in the given text.

        :param text: The input text to analyze.
        :return: A list of named entities found in the text.
        """
        pass

    @abstractmethod
    def bulk_predict(self, texts: list[str], batch_size: int) -> list[list[NESpan]]:
        """
        Predicts named entities for a list of texts.

        :param texts: A list of input texts to analyze.
        :param batch_size: Batch size for processing the texts.
        :return: A list of lists, where each inner list contains named entities for the corresponding text.
        """
        pass

    @abstractmethod
    def bulk_predict_as_tuples(self, text__context: list[tuple[str, Any]], batch_size: int) -> tuple[list[list[NESpan]], Any]:
        """
        Predicts named entities for a list of texts with additional context information.

        :param text__context: A list of input texts to analyze. Each text is paired with additional context information.
        :param batch_size: Batch size for processing the texts.
        :return: A tuple containing a list of lists of named entities and additional context information.
        """
        pass


class SpacyNER(AbstractNER):

    def __init__(self, model_location: str):
        self.__ner = load_spacy_model(model_location)

    @staticmethod
    def __doc_to_ne_spans(doc) -> list[NESpan]:
        ne_doc = NEDoc(doc.text)
        return [NESpan(ne_doc, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]

    def predict(self, text: str) -> list[NESpan]:
        return self.__doc_to_ne_spans(self.__ner(text))

    def bulk_predict(self, texts: list[str], batch_size: int) -> list[list[NESpan]]:
        return [self.__doc_to_ne_spans(doc) for doc in self.__ner.pipe(texts, batch_size=batch_size)]

    def bulk_predict_as_tuples(self, text__context: list[tuple[str, Any]], batch_size: int) -> tuple[list[list[NESpan]], Any]:
        ret = []
        for doc__context in self.__ner.pipe(text__context, batch_size=batch_size, as_tuples=True):
            doc, context = doc__context
            ret.append((self.__doc_to_ne_spans(doc), context))
        return ret


class HuggingFaceNER(AbstractNER):

    def __init__(self, model_location: str):
        pass

```

### sefaria/model/linker/linker.py

```
import dataclasses
from typing import List, Optional, Union, Iterable, Tuple
from tqdm import tqdm
from sefaria.model.linker.ne_span import NEDoc
from sefaria.model.text import Ref
from sefaria.model.linker.ref_part import RawRef, RawNamedEntity
from sefaria.model.linker.ref_resolver import RefResolver, ResolutionThoroughness, PossiblyAmbigResolvedRef, ResolvedRef
from sefaria.model.linker.named_entity_resolver import NamedEntityResolver, ResolvedNamedEntity
from sefaria.model.linker.linker_entity_recognizer import LinkerEntityRecognizer
from sefaria.model.linker.category_resolver import CategoryResolver, ResolvedCategory


@dataclasses.dataclass
class LinkedDoc:
    text: str
    resolved_refs: list[PossiblyAmbigResolvedRef]
    resolved_named_entities: list[ResolvedNamedEntity]
    resolved_categories: list[ResolvedCategory]

    @property
    def all_resolved(self) -> List[Union[PossiblyAmbigResolvedRef, ResolvedNamedEntity, ResolvedCategory]]:
        return self.resolved_refs + self.resolved_named_entities + self.resolved_categories


class Linker:

    def __init__(self, ner: LinkerEntityRecognizer, ref_resolver: RefResolver, ne_resolver: NamedEntityResolver, cat_resolver: CategoryResolver):
        self._ner = ner
        self._ref_resolver = ref_resolver
        self._ne_resolver = ne_resolver
        self._cat_resolver = cat_resolver

    def bulk_link(self, inputs: List[str], book_context_refs: Optional[List[Optional[Ref]]] = None, with_failures=False,
                  verbose=False, thoroughness=ResolutionThoroughness.NORMAL, type_filter='all') -> List[LinkedDoc]:
        """
        Bulk operation to link every string in `inputs` with citations and named entities
        `bulk_link()` is faster than running `link()` in a loop because it can pass all strings to the relevant models
        at once.
        @param inputs: String inputs. Each input is processed independently.
        @param book_context_refs: Additional context references that represents the source book that the input came from.
        @param with_failures: True to return all recognized entities, even if they weren't linked.
        @param verbose: True to print progress to the console
        @param thoroughness: How thorough the search to link entities should be. HIGH increases the processing time.
        @param type_filter: Type of entities to return, either 'all', 'citation' or 'named entity'
        @return: list of LinkedDocs
        """
        self._ref_resolver.reset_ibid_history()
        all_named_entities = self._ner.bulk_recognize(inputs)
        docs = []
        book_context_refs = book_context_refs or [None]*len(all_named_entities)
        iterable = self._get_bulk_link_iterable(inputs, all_named_entities, book_context_refs, verbose)
        for input_str, book_context_ref, inner_named_entities in iterable:
            raw_refs, named_entities = self._partition_raw_refs_and_named_entities(inner_named_entities)
            resolved_refs, resolved_named_entities, resolved_cats = [], [], []
            if type_filter in {'all', 'citation'}:
                resolved_refs, resolved_cats = self._bulk_resolve_refs_and_cats(raw_refs, book_context_ref, thoroughness, False)
            if type_filter in {'all', 'named entity'}:
                resolved_named_entities = self._ne_resolver.bulk_resolve(named_entities)
            if not with_failures:
                resolved_refs, resolved_named_entities, resolved_cats = self._remove_failures(resolved_refs, resolved_named_entities, resolved_cats)
            docs += [LinkedDoc(input_str, resolved_refs, resolved_named_entities, resolved_cats)]

        named_entity_list_list = [[rr.raw_entity for rr in doc.all_resolved] for doc in docs]
        self._ner.bulk_map_normal_output_to_original_input(inputs, named_entity_list_list)
        return docs

    def link(self, input_str: str, book_context_ref: Optional[Ref] = None, with_failures=False,
             thoroughness=ResolutionThoroughness.NORMAL, type_filter='all') -> LinkedDoc:
        """
        Link `input_str` with citations and named entities
        @param input_str:
        @param book_context_ref: Additional context reference that represents the source book that the input came from.
        @param with_failures: True to return all recognized entities, even if they weren't linked.
        @param thoroughness: How thorough the search to link entities should be. HIGH increases the processing time.
        @param type_filter: Type of entities to return, either 'all', 'citation' or 'named entity'
        @return:
        """
        raw_refs, named_entities = self._ner.recognize(input_str)
        resolved_refs, resolved_named_entities, resolved_cats = [], [], []
        if type_filter in {'all', 'citation'}:
            resolved_refs, resolved_cats = self._bulk_resolve_refs_and_cats(raw_refs, book_context_ref, thoroughness)
        if type_filter in {'all', 'named entity'}:
            resolved_named_entities = self._ne_resolver.bulk_resolve(named_entities)
        if not with_failures:
            resolved_refs, resolved_named_entities, resolved_cats = self._remove_failures(resolved_refs, resolved_named_entities, resolved_cats)
        doc = LinkedDoc(input_str, resolved_refs, resolved_named_entities, resolved_cats)
        self._ner.map_normal_output_to_original_input(input_str, [x.raw_entity for x in doc.all_resolved])
        return doc

    def __break_input_into_paragraphs(self, input_str: str) -> [list[str], list[tuple[int, int]]]:
        """
        Breaks the input string into paragraphs based on new line characters.
        Returns a list of paragraphs and their corresponding spans.
        @param input_str: The input string to be broken into paragraphs.
        @return: A tuple containing a list of paragraphs and a list of spans.
        """
        import re
        paragraph_break_spans = [m.span() for m in re.finditer(r'\s*\n+\s*', input_str)]
        inputs = []
        offset = 0
        for start, end in paragraph_break_spans:
            inputs.append(input_str[offset:start])
            offset = end
        inputs.append(input_str[offset:])
        return inputs, paragraph_break_spans

    def link_by_paragraph(self, input_str: str, book_context_ref: Optional[Ref] = None, *link_args, **link_kwargs) -> LinkedDoc:
        """
        Similar to `link()` except model is run on each paragraph individually (via a bulk operation)
        This better mimics the way the underlying ML models were trained and tends to lead to better results
        Paragraphs are delineated by new line characters
        @param input_str:
        @param book_context_ref:
        @param link_args: *args to be passed to link()
        @param link_kwargs: **kwargs to be passed to link()
        @return:
        """
        inputs, paragraph_break_spans = self.__break_input_into_paragraphs(input_str)
        linked_docs = self.bulk_link(inputs, [book_context_ref]*len(inputs), *link_args, **link_kwargs)
        resolved_refs = []
        resolved_named_entities = []
        resolved_categories = []
        full_ne_doc = NEDoc(input_str)
        offset = 0
        for curr_input, linked_doc, curr_par_break in zip(inputs, linked_docs, paragraph_break_spans):
            resolved_refs += linked_doc.resolved_refs
            resolved_named_entities += linked_doc.resolved_named_entities
            resolved_categories += linked_doc.resolved_categories

            for resolved in linked_doc.all_resolved:
                named_entity = resolved.raw_entity
                named_entity.align_to_new_doc(full_ne_doc, offset)
                if isinstance(named_entity, RawRef):
                    # named_entity's current start has already been offset so it's the offset we need for each part
                    raw_ref_offset, _ = named_entity.span.range
                    named_entity.align_parts_to_new_doc(full_ne_doc, raw_ref_offset)
            offset = curr_par_break[1]  # Update offset to the end of the current paragraph break
        return LinkedDoc(input_str, resolved_refs, resolved_named_entities, resolved_categories)

    def get_ner(self) -> LinkerEntityRecognizer:
        return self._ner

    def reset_ibid_history(self) -> None:
        """
        Reflecting this function out
        @return:
        """
        self._ref_resolver.reset_ibid_history()

    def _bulk_resolve_refs_and_cats(self, raw_refs, book_context_ref, thoroughness, reset_ibids=True) -> (list[ResolvedRef], list[ResolvedCategory]):
        """
        First match categories, then resolve refs for anything that didn't match a category
        This prevents situations where a category is parsed as a ref using ibid (e.g. Talmud with context Berakhot 2a)
        @param raw_refs:
        @param book_context_ref:
        @param thoroughness:
        @param reset_ibids:
        @return:
        """
        possibly_resolved_cats = self._cat_resolver.bulk_resolve(raw_refs)
        unresolved_raw_refs = [r.raw_entity for r in filter(lambda r: r.resolution_failed, possibly_resolved_cats)]
        resolved_cats = list(filter(lambda r: not r.resolution_failed, possibly_resolved_cats))
        #try to resolve only unresolved categories (unresolved_raw_refs) as refs:
        resolved_refs = self._ref_resolver.bulk_resolve(unresolved_raw_refs, book_context_ref, thoroughness, reset_ibids=reset_ibids)
        return resolved_refs, resolved_cats

    @staticmethod
    def _partition_raw_refs_and_named_entities(raw_refs_and_named_entities: List[RawNamedEntity]) \
            -> Tuple[List[RawRef], List[RawNamedEntity]]:
        raw_refs = [ne for ne in raw_refs_and_named_entities if isinstance(ne, RawRef)]
        named_entities = [ne for ne in raw_refs_and_named_entities if not isinstance(ne, RawRef)]
        return raw_refs, named_entities

    @staticmethod
    def _get_bulk_link_iterable(inputs: List[str], all_named_entities: List[List[RawNamedEntity]],
                                book_context_refs: Optional[List[Optional[Ref]]] = None, verbose=False
                                ) -> Iterable[Tuple[Ref, List[RawNamedEntity]]]:
        iterable = zip(inputs, book_context_refs, all_named_entities)
        if verbose:
            iterable = tqdm(iterable, total=len(book_context_refs))
        return iterable

    @staticmethod
    def _remove_failures(*args):
        out = []
        for arg in args:
            out.append(list(filter(lambda x: not x.resolution_failed, arg)))
        return out

```

### sefaria/model/linker/ref_resolver.py

```
from collections import defaultdict
from typing import List, Union, Dict, Optional, Tuple, Iterable, Set
from enum import IntEnum, Enum
from sefaria.system.exceptions import InputError
from sefaria.model import abstract as abst
from sefaria.model import text
from sefaria.model import schema
from sefaria.model.linker.ref_part import RawRef, RawRefPart, RefPartType, SectionContext, ContextPart, TermContext
from sefaria.model.linker.ne_span import NESpan
from sefaria.model.linker.referenceable_book_node import ReferenceableBookNode
from sefaria.model.linker.match_template import MatchTemplateTrie, LEAF_TRIE_ENTRY
from sefaria.model.linker.resolved_ref_refiner_factory import resolved_ref_refiner_factory
import structlog
logger = structlog.get_logger(__name__)


class ContextType(Enum):
    """
    Types of context which can be used to help resolve refs
    """
    CURRENT_BOOK = "CURRENT_BOOK"
    IBID = "IBID"


# maps ContextTypes that will always (I believe) map to certain RefPartTypes
# they don't necessarily need to map to any RefPart but if they do, they will match these types
CONTEXT_TO_REF_PART_TYPE = {
    ContextType.CURRENT_BOOK: {RefPartType.RELATIVE},
    ContextType.IBID: {RefPartType.IBID}
}


class ResolutionThoroughness(IntEnum):
    NORMAL = 1
    HIGH = 2


class ResolvedRef(abst.Cloneable):
    """
    Partial or complete resolution of a RawRef
    """
    is_ambiguous = False

    def __init__(self, raw_entity: RawRef, resolved_parts: List[RawRefPart], node, ref: text.Ref, context_ref: text.Ref = None, context_type: ContextType = None, context_parts: List[ContextPart] = None, _thoroughness=ResolutionThoroughness.NORMAL, _matched_dh_map=None) -> None:
        self.raw_entity = raw_entity
        self.resolved_parts = resolved_parts
        self.node: ReferenceableBookNode = node
        self.ref = ref
        self.context_ref = context_ref
        self.context_type = context_type
        self.context_parts = context_parts[:] if context_parts else []
        self._thoroughness = _thoroughness
        self._matched_dh_map = _matched_dh_map or {}

    def complies_with_thoroughness_level(self):
        return self._thoroughness >= ResolutionThoroughness.HIGH or not self.ref.is_book_level()

    @property
    def pretty_text(self) -> str:
        """
        Return text of underlying RawRef with modifications to make it nicer
        Currently
        - adds ending parentheses if just outside span
        - adds extra DH words that were matched but aren't in span
        @return:
        """
        new_raw_ref_span = self._get_pretty_dh_span(self.raw_entity.span)
        new_raw_ref_span = self._get_pretty_end_paren_span(new_raw_ref_span)
        return new_raw_ref_span.text

    def _get_pretty_dh_span(self, curr_span: NESpan) -> NESpan:
        curr_start, curr_end = curr_span.range
        for dh_span in self._matched_dh_map.values():
            temp_start, temp_end = dh_span.range
            curr_start = temp_start if temp_start < curr_start else curr_start
            curr_end = temp_end if temp_end > curr_end else curr_end

        return curr_span.doc.subspan(slice(curr_start, curr_end))

    @staticmethod
    def _get_pretty_end_paren_span(curr_span: NESpan) -> NESpan:
        import re

        curr_start, curr_end = curr_span.range
        if re.search(r'\([^)]+$', curr_span.text) is not None:
            for temp_end in range(curr_end, curr_end+5):
                if curr_span.doc.text[temp_end] == ")":
                    curr_end = temp_end + 1
                    break

        return curr_span.doc.subspan(slice(curr_start, curr_end))

    def _set_matched_dh(self, part: RawRefPart, potential_dh_token_idx: int):
        if part.potential_dh_continuation is None: return
        matched_dh_continuation = part.potential_dh_continuation.subspan_by_word_indices(slice(0, potential_dh_token_idx))
        self._matched_dh_map[part] = matched_dh_continuation

    def merge_parts(self, other: 'ResolvedRef') -> None:
        for part in other.resolved_parts:
            if part in self.resolved_parts: continue
            if part.is_context:
                # prepend context parts, so they pass validation that context parts need to precede non-context parts
                self.resolved_parts = [part] + self.resolved_parts
            else:
                self.resolved_parts += [part]
        if not self.ref:
            # self may reference an AltStructNode and therefore doesn't have a ref.
            # Use ref from other which is expected to be equivalent or more specific
            self.ref = other.ref

    def get_resolved_parts(self, include: Iterable[type] = None, exclude: Iterable[type] = None) -> List[RawRefPart]:
        """
        Returns list of resolved_parts according to criteria `include` and `exclude`
        If neither `include` nor `exclude` is passed, return all parts in `self.resolved_parts`
        :param include: if not None, only include parts that are an instance of at least one class specified in `include`
        :param exclude: if not None, exclude parts that are an instance of at least one class specified in `exclude`
        """
        parts = []
        for part in self.resolved_parts:
            if include is not None and not any(isinstance(part, typ) for typ in include):
                continue
            if exclude is not None and any(isinstance(part, typ) for typ in exclude):
                continue
            parts += [part]
        return parts

    def num_resolved(self, include: Iterable[type] = None, exclude: Iterable[type] = None) -> int:
        return len(self.get_resolved_parts(include, exclude))

    @staticmethod
    def count_by_part_type(parts) -> Dict[RefPartType, int]:
        part_type_counts = defaultdict(int)
        for part in parts:
            part_type_counts[part.type] += 1
        return part_type_counts

    def get_node_children(self):
        return self.node.get_children(self.ref)

    def contains(self, other: 'ResolvedRef') -> bool:
        """
        Does `self` contain `other`. If `self.ref` and `other.ref` aren't None, this is just ref comparison.
        Otherwise, see if the schema/altstruct node that back `self` contains `other`'s node.
        Note this function is a bit confusing. It works like this:
        - If `self.ref` and `other.ref` are None, we compare the nodes themselves to see if self is an ancestor of other
        - If `self.ref` is None and `other.ref` isn't, we check that `other.ref` is contained in at least one of `self`'s children (`self` may be an AltStructNode in which case it has no Ref)
        - If `self.ref` isn't None and `other_ref` is None, we check that `self.ref` contains all of `other`'s children (`other` may be an AltStructNode in which case it has no Ref)
        - If `self.ref` and `other.ref` are both defined, we can use Ref.contains()
        @param other:
        @return:
        """
        if not other.node or not self.node:
            return False
        if other.ref and self.ref:
            return self.ref.contains(other.ref)
        try:
            if other.ref is None:
                if self.ref is None:
                    return self.node.is_ancestor_of(other.node)
                # other is alt struct and self has a ref
                # check that every leaf node is contained by self's ref
                return all([self.ref.contains(leaf_ref) for leaf_ref in other.node.leaf_refs()])
            # self is alt struct and other has a ref
            # check if any leaf node contains other's ref
            return any([leaf_ref.contains(other.ref) for leaf_ref in self.node.leaf_refs()])
        except NotImplementedError:
            return False

    @property
    def order_key(self):
        """
        For sorting
        """
        explicit_matched = self.get_resolved_parts(exclude={ContextPart})
        num_context_parts_matched = 0
        # theory is more context is helpful specifically for DH matches because if DH still matches with more context,
        # it's more likely to be correct (as opposed to with numbered sections, it's relatively easy to add more context
        # and doesn't give more confidence that it's correct
        if next(iter(part for part in explicit_matched if part.type == RefPartType.DH), False):
            num_context_parts_matched = self.num_resolved(include={ContextPart})
        return len(explicit_matched), num_context_parts_matched

    @property
    def resolution_failed(self) -> bool:
        return self.ref is None and self.node is None


class AmbiguousResolvedRef:
    """
    Container for multiple ambiguous ResolvedRefs
    """
    is_ambiguous = True

    def __init__(self, resolved_refs: List[ResolvedRef]):
        if len(resolved_refs) == 0:
            raise InputError("Length of `resolved_refs` must be at least 1")
        self.resolved_raw_refs = resolved_refs
        self.raw_entity = resolved_refs[0].raw_entity  # assumption is all resolved_refs share same raw_ref. expose at top level

    @property
    def pretty_text(self):
        # assumption is first resolved refs pretty_text is good enough
        return self.resolved_raw_refs[0].pretty_text

    @property
    def resolution_failed(self) -> bool:
        return False


PossiblyAmbigResolvedRef = Union[ResolvedRef, AmbiguousResolvedRef]


class TermMatcher:
    """
    Used to match raw ref parts to non-unique terms naively.
    Stores all existing terms for speed.
    Used in context matching.
    """
    def __init__(self, lang: str, nonunique_terms: schema.NonUniqueTermSet) -> None:
        self.lang = lang
        self._str2term_map = defaultdict(list)
        for term in nonunique_terms:
            for title in term.get_titles(lang):
                self._str2term_map[title] += [term]

    def match_term(self, ref_part: RawRefPart) -> List[schema.NonUniqueTerm]:
        from sefaria.utils.hebrew import get_prefixless_inds

        matches = []
        if ref_part.type != RefPartType.NAMED: return matches
        starti_inds = [0]
        if self.lang == 'he':
            starti_inds += get_prefixless_inds(ref_part.text)
        for starti in starti_inds:
            matches += self._str2term_map.get(ref_part.text[starti:], [])
        return matches

    def match_terms(self, ref_parts: List[RawRefPart]) -> List[schema.NonUniqueTerm]:
        matches = []
        for part in ref_parts:
            matches += self.match_term(part)
        matches = list({m.slug: m for m in matches}.values())  # unique
        return matches


class IbidHistory:

    def __init__(self, last_n_titles: int = 3, last_n_refs: int = 3):
        self.last_n_titles = last_n_titles
        self.last_n_refs = last_n_refs
        self._last_refs: List[text.Ref] = []
        self._last_titles: List[str] = []
        self._title_ref_map: Dict[str, text.Ref] = {}

    def _get_last_refs(self) -> List[text.Ref]:
        return self._last_refs

    def _set_last_match(self, oref: text.Ref):
        self._last_refs += [oref]
        title = oref.index.title
        if title not in self._title_ref_map:
            self._last_titles += [title]
        self._title_ref_map[oref.index.title] = oref

        # enforce last_n_titles
        if len(self._last_titles) > self.last_n_titles:
            oldest_title = self._last_titles.pop(0)
            del self._title_ref_map[oldest_title]

        # enforce last_n_refs
        if len(self._last_refs) > self.last_n_refs:
            self._last_refs.pop(0)

    last_refs = property(_get_last_refs, _set_last_match)

    def get_ref_by_title(self, title: str) -> Optional[text.Ref]:
        return self._title_ref_map.get(title, None)


class RefResolver:

    def __init__(self, lang: str, ref_part_title_trie: MatchTemplateTrie, term_matcher: TermMatcher) -> None:

        self._lang = lang
        self._ref_part_title_trie = ref_part_title_trie
        self._term_matcher = term_matcher
        self._ibid_history = IbidHistory()
        self._thoroughness = ResolutionThoroughness.NORMAL

    def reset_ibid_history(self):
        self._ibid_history = IbidHistory()

    def bulk_resolve(self, raw_refs: List[RawRef], book_context_ref: Optional[text.Ref] = None,
                     thoroughness=ResolutionThoroughness.NORMAL, reset_ibids=True) -> List[PossiblyAmbigResolvedRef]:
        """
        Main function for resolving refs in text. Given a list of RawRefs, returns ResolvedRefs for each
        @param raw_refs:
        @param book_context_ref:
        @param thoroughness: how thorough should the search be. More thorough == slower. Currently "normal" will avoid searching for DH matches at book level and avoid filtering empty refs
        @param reset_ibids: If true, reset ibid history before resolving
        @return:
        """
        self._thoroughness = thoroughness
        if reset_ibids:
            self.reset_ibid_history()
        resolved = []
        for raw_ref in raw_refs:
            temp_resolved = self._resolve_raw_ref_and_update_ibid_history(raw_ref, book_context_ref)
            resolved += temp_resolved
        return resolved

    def _resolve_raw_ref_and_update_ibid_history(self, raw_ref: RawRef, book_context_ref: text.Ref) -> List[PossiblyAmbigResolvedRef]:
        temp_resolved = self.resolve_raw_ref(book_context_ref, raw_ref)
        self._update_ibid_history(raw_ref, temp_resolved)
        if len(temp_resolved) == 0:
            return [ResolvedRef(raw_ref, [], None, None, context_ref=book_context_ref)]
        return temp_resolved

    def _update_ibid_history(self, raw_ref: RawRef, temp_resolved: List[PossiblyAmbigResolvedRef]):
        if len(temp_resolved) == 0:
            self.reset_ibid_history()
        elif any(r.is_ambiguous for r in temp_resolved) or temp_resolved[-1].ref is None:
            # can't be sure about future ibid inferences
            # TODO can probably salvage parts of history if matches are ambiguous within one book
            # if ref is None, match is likely to AltStructNode
            # TODO this node still has useful info. Try to salvage it.
            self.reset_ibid_history()
        else:
            self._ibid_history.last_refs = temp_resolved[-1].ref

    def get_ref_part_title_trie(self) -> MatchTemplateTrie:
        return self._ref_part_title_trie

    def get_term_matcher(self) -> TermMatcher:
        return self._term_matcher

    def split_non_cts_parts(self, raw_ref: RawRef) -> List[RawRef]:
        if not any(part.type == RefPartType.NON_CTS for part in raw_ref.raw_ref_parts): return [raw_ref]
        split_raw_refs = []
        curr_parts = []
        curr_part_start = 0
        for ipart, part in enumerate(raw_ref.raw_ref_parts):
            if part.type != RefPartType.NON_CTS:
                curr_parts += [part]
            if part.type == RefPartType.NON_CTS or ipart == len(raw_ref.raw_ref_parts) - 1:
                if len(curr_parts) == 0: continue
                curr_part_end = ipart  # exclude curr part which is NON_CTS
                if ipart == len(raw_ref.raw_ref_parts) - 1: curr_part_end = ipart + 1  # include curr part
                try:
                    raw_ref_span = raw_ref.subspan(slice(curr_part_start, curr_part_end))
                    curr_parts = [p.realign_to_new_raw_ref(raw_ref.span, raw_ref_span) for p in curr_parts]
                    split_raw_refs += [RawRef(raw_ref_span, self._lang, curr_parts)]
                except AssertionError:
                    pass
                curr_parts = []
                curr_part_start = ipart+1
        return split_raw_refs

    def set_thoroughness(self, thoroughness: ResolutionThoroughness) -> None:
        self._thoroughness = thoroughness

    def resolve_raw_ref(self, book_context_ref: Optional[text.Ref], raw_ref: RawRef) -> List[PossiblyAmbigResolvedRef]:
        split_raw_refs = self.split_non_cts_parts(raw_ref)
        resolved_list = []
        for i, temp_raw_ref in enumerate(split_raw_refs):
            is_non_cts = i > 0 and len(resolved_list) > 0
            if is_non_cts:
                # TODO assumes context is only first resolved ref
                book_context_ref = None if resolved_list[0].is_ambiguous else resolved_list[0].ref
            context_swap_map = None if book_context_ref is None else getattr(book_context_ref.index.nodes,
                                                                        'ref_resolver_context_swaps', None)
            self._apply_context_swaps(raw_ref, context_swap_map)
            unrefined_matches = self.get_unrefined_ref_part_matches(book_context_ref, temp_raw_ref)
            if is_non_cts:
                # filter unrefined matches to matches that resolved previously
                resolved_titles = {r.ref.index.title for r in resolved_list if not r.is_ambiguous}
                unrefined_matches = list(filter(lambda x: x.ref.index.title in resolved_titles, unrefined_matches))
                # resolution will start at context_ref.sections - len(ref parts). rough heuristic
                for match in unrefined_matches:
                    try:
                        match.ref = match.ref.subref(book_context_ref.sections[:-len(temp_raw_ref.raw_ref_parts)])
                    except (InputError, AttributeError):
                        continue
            temp_resolved_list = self.refine_ref_part_matches(book_context_ref, unrefined_matches)
            if len(temp_resolved_list) > 1:
                resolved_list += [AmbiguousResolvedRef(temp_resolved_list)]
            else:
                resolved_list += temp_resolved_list

        if len(resolved_list) == 0:
            resolved_list += self.resolve_raw_ref_using_ref_instantiation(raw_ref)

        return resolved_list

    @staticmethod
    def resolve_raw_ref_using_ref_instantiation(raw_ref: RawRef) -> List[ResolvedRef]:
        try:
            ref = text.Ref(raw_ref.text)
            return [ResolvedRef(raw_ref, raw_ref.parts_to_match, None, ref)]
        except:
            return []

    def get_unrefined_ref_part_matches(self, book_context_ref: Optional[text.Ref], raw_ref: RawRef) -> List[
            'ResolvedRef']:
        context_free_matches = self._get_unrefined_ref_part_matches_recursive(raw_ref, ref_parts=raw_ref.parts_to_match)
        contexts = [(book_context_ref, ContextType.CURRENT_BOOK)] + [(ibid_ref, ContextType.IBID) for ibid_ref in self._ibid_history.last_refs]
        matches = context_free_matches
        if len(matches) == 0:
            context_full_matches = []
            for context_ref, context_type in contexts:
                context_full_matches += self._get_unrefined_ref_part_matches_for_title_context(context_ref, raw_ref, context_type)
            matches = context_full_matches + context_free_matches
        return matches

    def _get_unrefined_ref_part_matches_for_title_context(self, context_ref: Optional[text.Ref], raw_ref: RawRef, context_type: ContextType) -> List[ResolvedRef]:
        matches = []
        if context_ref is None: return matches
        term_contexts = self._get_term_contexts(context_ref.index.nodes)
        if len(term_contexts) == 0: return matches
        temp_ref_parts = raw_ref.parts_to_match + term_contexts
        temp_matches = self._get_unrefined_ref_part_matches_recursive(raw_ref, ref_parts=temp_ref_parts)
        for match in temp_matches:
            if match.num_resolved(include={TermContext}) == 0: continue
            match.context_ref = context_ref
            match.context_type = context_type
            match.context_parts += term_contexts
            matches += [match]
        return matches

    def _apply_context_swaps(self, raw_ref: RawRef, context_swap_map: Dict[str, str]=None):
        """
        Use `context_swap_map` to swap matching element of `ref_parts`
        Allows us to redefine how a ref part is interpreted depending on the context
        E.g. some rishonim refer to other rishonim based on nicknames

        Modifies `raw_ref` with updated ref_parts
        """
        swapped_ref_parts = []
        term_matcher = self.get_term_matcher()
        if context_swap_map is None: return
        for part in raw_ref.raw_ref_parts:
            # TODO assumes only one match in term_matches
            term_matches = term_matcher.match_term(part)
            found_match = False
            for match in term_matches:
                if match.slug not in context_swap_map: continue
                swapped_ref_parts += [TermContext(schema.NonUniqueTerm.init(slug)) for slug in context_swap_map[match.slug]]
                found_match = True
                break
            if not found_match: swapped_ref_parts += [part]
        raw_ref.parts_to_match = swapped_ref_parts

    def _get_unrefined_ref_part_matches_recursive(self, raw_ref: RawRef, title_trie: MatchTemplateTrie = None, ref_parts: list = None, prev_ref_parts: list = None) -> List[ResolvedRef]:
        """
        We are now considering all types for trie lookups (not just NAMED) since there seem to be no cases of false positives when we consider all part types
        In addition, sometimes the raw ref part type model misclassifies a part type and relaxing the type requirement here allows it to recover.
        The exception is we only will split NAMED parts since this causes some odd parts to split. e.g.   can be considered part of the title of book when  is removed
        """
        title_trie = title_trie or self.get_ref_part_title_trie()
        prev_ref_parts = prev_ref_parts or []
        matches = []
        for part in ref_parts:
            temp_raw_ref = raw_ref
            temp_title_trie, partial_key_end = title_trie.get_continuations(part.key(), allow_partial=True)
            if temp_title_trie is None: continue
            if partial_key_end is None:
                matched_part = part
            elif part.type == RefPartType.NAMED:
                try:
                    temp_raw_ref, apart, bpart = raw_ref.split_part(part, partial_key_end)
                    matched_part = apart
                except InputError:
                    matched_part = part  # fallback on original part
            else:
                continue
            temp_prev_ref_parts = prev_ref_parts + [matched_part]
            if LEAF_TRIE_ENTRY in temp_title_trie:
                for node in temp_title_trie[LEAF_TRIE_ENTRY]:
                    try:
                        ref = node.ref()
                    except InputError:
                        continue
                    matches += [ResolvedRef(temp_raw_ref, temp_prev_ref_parts, node, ref, _thoroughness=self._thoroughness)]
            temp_ref_parts = [temp_part for temp_part in ref_parts if temp_part != part]
            matches += self._get_unrefined_ref_part_matches_recursive(temp_raw_ref, temp_title_trie, ref_parts=temp_ref_parts, prev_ref_parts=temp_prev_ref_parts)

        return ResolvedRefPruner.prune_unrefined_ref_part_matches(matches)

    def refine_ref_part_matches(self, book_context_ref: Optional[text.Ref], matches: List[ResolvedRef]) -> List[ResolvedRef]:
        temp_matches = []
        refs_matched = {match.ref.normal() for match in matches}
        for unrefined_match in matches:
            unused_parts = list(set(unrefined_match.raw_entity.parts_to_match) - set(unrefined_match.resolved_parts))
            context_free_matches = self._get_refined_ref_part_matches_recursive(unrefined_match, unused_parts)

            # context
            # if unrefined_match already used context, make sure it continues to use it
            # otherwise, consider other possible context
            context_ref_list = [book_context_ref, self._ibid_history.get_ref_by_title(unrefined_match.ref.index.title)] if unrefined_match.context_ref is None else [unrefined_match.context_ref]
            context_type_list = [ContextType.CURRENT_BOOK, ContextType.IBID] if unrefined_match.context_ref is None else [unrefined_match.context_type]
            context_full_matches = []
            for context_ref, context_type in zip(context_ref_list, context_type_list):
                context_full_matches += self._get_refined_ref_part_matches_for_section_context(context_ref, context_type, unrefined_match, unused_parts)

            # combine
            if len(context_full_matches) > 0:
                # assumption is we don't want refs that used context at book level, then didn't get refined more when considering context free
                # BUT did get refined more when considering context
                context_free_matches = list(filter(lambda x: not (x.num_resolved(include={ContextPart}) > 0 and x.ref.normal() in refs_matched), context_free_matches))
            temp_matches += context_free_matches + context_full_matches
        return ResolvedRefPruner.prune_refined_ref_part_matches(self._thoroughness, temp_matches)

    @staticmethod
    def _get_section_contexts(context_ref: text.Ref, match_index: text.Index, common_index: text.Index) -> List[SectionContext]:
        """
        Currently doesn't work if any of the indexes are complex texts
        Returns list section contexts extracted from `context_node`
        :param context_ref: context ref where we are searching
        :param match_index: Index of current match we are trying to refine
        :param common_index: Index
        """
        def get_section_set(index: text.Index) -> Set[Tuple[str, str, bool]]:
            root_node = index.nodes.get_default_child() or index.nodes
            try:
                referenceable_sections = getattr(root_node, 'referenceableSections', [True] * len(root_node.addressTypes))
                return set(zip(root_node.addressTypes, root_node.sectionNames, referenceable_sections))
            except AttributeError:
                # complex text
                return set()

        if context_ref.is_range():
            # SectionContext doesn't seem to make sense for ranged refs (It works incidentally when context is parsha
            # and input is "See beginning of parsha pasuk 1" but not sure we want to plan for that case)
            return []

        context_node = context_ref.index_node
        if not hasattr(context_node, 'addressTypes'):
            # complex text
            return []
        referenceable_sections = getattr(context_node, 'referenceableSections', [True]*len(context_node.addressTypes))
        context_sec_list = list(zip(context_node.addressTypes, context_node.sectionNames, referenceable_sections))
        match_sec_set  = get_section_set(match_index)
        common_sec_set = get_section_set(common_index) & match_sec_set & set(context_sec_list)
        if len(common_sec_set) == 0: return []
        sec_contexts = []
        for isec, sec_tuple in enumerate(context_sec_list):
            if sec_tuple in common_sec_set and isec < len(context_ref.sections):
                addr_type_str, sec_name, referenceable = sec_tuple
                if not referenceable: continue
                addr_type = schema.AddressType.to_class_by_address_type(addr_type_str)
                sec_contexts += [SectionContext(addr_type, sec_name, context_ref.sections[isec])]
        return sec_contexts

    @staticmethod
    def _get_all_term_contexts(node: schema.SchemaNode, include_root=False) -> List[TermContext]:
        """
        Return all TermContexts extracted from `node` and all parent nodes until root
        @param node:
        @return:
        """
        term_contexts = []
        curr_node = node
        while curr_node is not None and (include_root or not curr_node.is_root()):
            term_contexts += RefResolver._get_term_contexts(curr_node)
            curr_node = curr_node.parent
        return term_contexts

    @staticmethod
    def _get_term_contexts(node: schema.SchemaNode) -> List[TermContext]:
        match_templates = list(node.get_match_templates())
        if len(match_templates) == 0: return []
        # not clear which match_template to choose. shortest has advantage of adding minimum context to search
        longest_template = min(match_templates, key=lambda x: len(list(x.terms)))
        return [TermContext(term) for term in longest_template.terms]

    def _get_refined_ref_part_matches_for_section_context(self, context_ref: Optional[text.Ref], context_type: ContextType, ref_part_match: ResolvedRef, ref_parts: List[RawRefPart]) -> List[ResolvedRef]:
        """
        Tries to infer sections from context ref and uses them to refine `ref_part_match`
        """
        if context_ref is None: return []
        context_titles = set(getattr(context_ref.index, 'base_text_titles', [])) | {context_ref.index.title}
        match_titles = set(getattr(ref_part_match.ref.index, 'base_text_titles', [])) | {ref_part_match.ref.index.title}
        matches = []
        for common_base_text in (context_titles & match_titles):
            common_index = text.library.get_index(common_base_text)
            sec_contexts = RefResolver._get_section_contexts(context_ref, ref_part_match.ref.index, common_index)
            term_contexts = RefResolver._get_all_term_contexts(context_ref.index_node, include_root=False)
            context_to_consider = sec_contexts + term_contexts
            temp_matches = self._get_refined_ref_part_matches_recursive(ref_part_match, ref_parts + context_to_consider)

            # remove matches which don't use context
            temp_matches = list(filter(lambda x: len(set(x.get_resolved_parts(include={ContextPart})) & set(context_to_consider)) > 0, temp_matches))
            for match in temp_matches:
                match.context_ref = context_ref
                match.context_type = context_type
                match.context_parts += sec_contexts + term_contexts

            matches += temp_matches
        return matches

    def _get_refined_ref_part_matches_recursive(self, match: ResolvedRef, ref_parts: List[RawRefPart]) -> List[ResolvedRef]:
        fully_refined = []
        children = match.get_node_children()
        for part in ref_parts:
            for child in children:
                resolved_ref_refiner = resolved_ref_refiner_factory.create(part, child, match)
                temp_matches = resolved_ref_refiner.refine(self._lang)
                for temp_match in temp_matches:
                    temp_ref_parts = list(set(ref_parts) - set(temp_match.resolved_parts))
                    fully_refined += self._get_refined_ref_part_matches_recursive(temp_match, temp_ref_parts)
        if len(fully_refined) == 0:
            # original match is better than no matches
            return [match]
        return fully_refined


class ResolvedRefPruner:

    def __init__(self):
        pass

    @staticmethod
    def prune_unrefined_ref_part_matches(ref_part_matches: List[ResolvedRef]) -> List[ResolvedRef]:
        index_match_map = defaultdict(list)
        for match in ref_part_matches:
            key = match.node.unique_key()
            index_match_map[key] += [match]
        pruned_matches = []
        for match_list in index_match_map.values():
            pruned_matches += ResolvedRefPruner.remove_subset_sets(match_list, key=lambda match: set(part.char_indices for part in match.get_resolved_parts()))
        return pruned_matches

    @staticmethod
    def remove_subset_sets(items, key=None):
        if key:
            sets_to_filter = [key(x) for x in items]
        else:
            sets_to_filter = items
        items, sets_to_filter = zip(*sorted((zip(items, sets_to_filter)), key=lambda x: len(x[1]), reverse=True))
        result = []
        for i in range(len(sets_to_filter)):
            for j in range(i):
                if sets_to_filter[i].issubset(sets_to_filter[j]):
                    # Break the loop as the sublist is a subset of a previous sublist
                    break
            else:
                # If the sublist is not a subset of any previous sublist, add it to the result
                result.append(items[i])
        return result

    @staticmethod
    def do_explicit_sections_match_before_context_sections(match: ResolvedRef) -> bool:
        first_explicit_section = None
        for part in match.get_resolved_parts():
            if not first_explicit_section and part.type == RefPartType.NUMBERED and not part.is_context:
                first_explicit_section = part
            elif first_explicit_section and part.is_context:
                return True
        return False

    @staticmethod
    def matched_all_explicit_sections(match: ResolvedRef) -> bool:
        resolved_explicit = set(match.get_resolved_parts(exclude={ContextPart}))
        to_match_explicit = {part for part in match.raw_entity.parts_to_match if not part.is_context}

        if match.context_type in CONTEXT_TO_REF_PART_TYPE.keys():
            # remove an equivalent number of context parts that were resolved from to_match_explicit to approximate
            # comparison. this is a bit hacky but seems to work for all known cases so far.
            num_parts_to_remove = match.num_resolved(include={ContextPart})
            for _ in range(num_parts_to_remove):
                part = next((p for p in to_match_explicit if p.type in CONTEXT_TO_REF_PART_TYPE[match.context_type]), None)
                if part is None:
                    break  # no more
                to_match_explicit.remove(part)
        return resolved_explicit == to_match_explicit

    @staticmethod
    def ignored_context_ref_part_type(match: ResolvedRef) -> bool:
        """
        When using context, must include at least same number of ref part types in match as were in context
        Logic being, don't drop a section without replacing it with something equivalent
        Prevents errors like the following:

        Input = [DH]
        Context = [Title] [Section]
        Correct Output = [Title] [Section] [DH]
        Invalid Output = [Title] [DH]

        context_ref_part_type_counts = {NAMED: 1, NUMBERED: 1}
        output_counts = {NAMED: 1, NUMBERED: 1, DH: 1}
        invalid_output_counts = {NAMED: 1, DH: 1}
        """
        context_part_type_counts = match.count_by_part_type(match.context_parts)
        explicit_part_type_counts = match.count_by_part_type(match.get_resolved_parts())
        for part_type, count in context_part_type_counts.items():
            if part_type not in explicit_part_type_counts:
                return True
            explicit_part_type_counts[part_type] -= count
            if explicit_part_type_counts[part_type] < 0:
                return True
        return False

    @staticmethod
    def is_match_correct(match: ResolvedRef) -> bool:
        # make sure no explicit sections matched before context sections
        if ResolvedRefPruner.do_explicit_sections_match_before_context_sections(match):
            return False
        if not ResolvedRefPruner.matched_all_explicit_sections(match):
            return False
        if ResolvedRefPruner.ignored_context_ref_part_type(match):
            return False

        return True

    @staticmethod
    def remove_superfluous_matches(thoroughness: ResolutionThoroughness, resolved_refs: List[ResolvedRef]) -> List[ResolvedRef]:
        # make matches with refs that are essentially equivalent (i.e. refs cover same span) actually equivalent
        resolved_refs.sort(key=lambda x: x.ref.order_id() if x.ref else "ZZZ")
        for i, r in enumerate(resolved_refs[:-1]):
            next_r = resolved_refs[i+1]
            if r.contains(next_r) and next_r.contains(r):
                next_r.ref = r.ref

        # make unique
        resolved_refs = list({r.ref: r for r in resolved_refs}.values())
        if thoroughness >= ResolutionThoroughness.HIGH or len(resolved_refs) > 1:
            # remove matches that have empty refs
            resolved_refs = list(filter(lambda x: x.ref and not x.ref.is_empty(), resolved_refs))
        return resolved_refs

    @staticmethod
    def remove_incorrect_matches(resolved_refs: List[ResolvedRef]) -> List[ResolvedRef]:
        temp_resolved_refs = list(filter(ResolvedRefPruner.is_match_correct, resolved_refs))
        if len(temp_resolved_refs) == 0:
            temp_resolved_refs = ResolvedRefPruner._merge_subset_matches(resolved_refs)
            temp_resolved_refs = list(filter(ResolvedRefPruner.is_match_correct, temp_resolved_refs))
        return temp_resolved_refs

    @staticmethod
    def get_context_free_matches(resolved_refs: List[ResolvedRef]) -> List[ResolvedRef]:
        def match_is_context_free(match: ResolvedRef) -> bool:
            return match.context_ref is None and set(match.get_resolved_parts()) == set(match.raw_entity.parts_to_match)
        return list(filter(match_is_context_free, resolved_refs))

    @staticmethod
    def get_top_matches_by_order_key(resolved_refs: List[ResolvedRef]) -> List[ResolvedRef]:
        resolved_refs.sort(key=lambda x: x.order_key, reverse=True)
        top_order_key = resolved_refs[0].order_key
        top_resolved_refs = []
        for resolved_ref in resolved_refs:
            if resolved_ref.order_key != top_order_key: break
            top_resolved_refs += [resolved_ref]
        return top_resolved_refs

    @staticmethod
    def prune_refined_ref_part_matches(thoroughness, resolved_refs: List[ResolvedRef]) -> List[ResolvedRef]:
        """
        Applies some heuristics to remove false positives
        """
        resolved_refs = ResolvedRefPruner.remove_incorrect_matches(resolved_refs)
        if len(resolved_refs) == 0:
            return resolved_refs

        # if any context-free match uses all input parts, dont need to try context
        context_free_matches = ResolvedRefPruner.get_context_free_matches(resolved_refs)
        if len(context_free_matches) > 0:
            resolved_refs = context_free_matches

        resolved_refs = ResolvedRefPruner.get_top_matches_by_order_key(resolved_refs)
        resolved_refs = ResolvedRefPruner.remove_superfluous_matches(thoroughness, resolved_refs)

        return resolved_refs

    @staticmethod
    def _merge_subset_matches(resolved_refs: List[ResolvedRef]) -> List[ResolvedRef]:
        """
        Merge matches where one ref is contained in another ref
        E.g. if matchA.ref == Ref("Genesis 1") and matchB.ref == Ref("Genesis 1:1"), matchA will be deleted and its parts will be appended to matchB's parts
        """
        def get_sort_key(resolved_ref: ResolvedRef) -> str:
            if resolved_ref.ref is None:
                if resolved_ref.node is None:
                    return "N/A"
                elif hasattr(resolved_ref.node, "ref_order_id"):
                    return resolved_ref.node.ref_order_id()
                else:
                    return "N/A"
            else:
                return resolved_ref.ref.order_id()
        resolved_refs.sort(key=get_sort_key)
        merged_resolved_refs = []
        next_merged = False
        for imatch, match in enumerate(resolved_refs[:-1]):
            next_match = resolved_refs[imatch+1]
            if match.is_ambiguous or match.node is None or next_match.node is None or next_merged:
                merged_resolved_refs += [match]
                next_merged = False
                continue
            if match.ref and next_match.ref and match.ref.index.title != next_match.ref.index.title:
                # optimization, the easiest cases to check for
                merged_resolved_refs += [match]
            elif match.contains(next_match):
                next_match.merge_parts(match)
            elif next_match.contains(match):
                # unfortunately Ref.order_id() doesn't consistently put larger refs before smaller ones
                # e.g. Tosafot on Berakhot 2 precedes Tosafot on Berakhot Chapter 1...
                # check if next match actually contains this match
                match.merge_parts(next_match)
                merged_resolved_refs += [match]
                next_merged = True
            else:
                merged_resolved_refs += [match]
        if len(resolved_refs) > 0:
            # never dealt with last resolved_ref
            merged_resolved_refs += [resolved_refs[-1]]
        return merged_resolved_refs

```

### sefaria/model/linker/ref_part.py

```
from typing import Union, Optional, Iterable
from enum import Enum
from sefaria.system.exceptions import InputError
from sefaria.model import abstract as abst
from sefaria.model import schema
from sefaria.model.linker.ne_span import NESpan, NEDoc
import structlog
logger = structlog.get_logger(__name__)


# keys correspond named entity labels in spacy models
# values are properties in RefPartType
LABEL_TO_REF_PART_TYPE_ATTR = {
    # HE
    "": 'NAMED',
    "": "NUMBERED",
    "": "DH",
    "-": "RANGE_SYMBOL",
    "-": "RELATIVE",
    "": "IBID",
    "-": "NON_CTS",
    # EN
    "title": 'NAMED',
    "number": "NUMBERED",
    "DH": "DH",
    "range-symbol": "RANGE_SYMBOL",
    "dir-ibid": "RELATIVE",
    "ibid": "IBID",
    "non-cts": "NON_CTS",
}


# keys correspond named entity labels in spacy models
# values are properties in NamedEntityType
LABEL_TO_NAMED_ENTITY_TYPE_ATTR = {
    # HE
    "": "CITATION",
    "-": "PERSON",
    "": "GROUP",
    # EN
    "Person": "PERSON",
    "Group": "GROUP",
    "Citation": "CITATION",
}


class TrieEntry:
    """
    Base class for entries in MatchTemplateTrie
    """
    key_is_id = False  # is key an ID which shouldn't be manipulated with string functions?

    def key(self):
        return hash(self)


class LeafTrieEntry:
    pass


# static entry which represents a leaf entry in MatchTemplateTrie
LEAF_TRIE_ENTRY = LeafTrieEntry()


class NamedEntityType(Enum):
    PERSON = "person"
    GROUP = "group"
    CITATION = "citation"

    @classmethod
    def span_label_to_enum(cls, span_label: str) -> 'NamedEntityType':
        """
        Convert span label from spacy named entity to NamedEntityType
        """
        return getattr(cls, LABEL_TO_NAMED_ENTITY_TYPE_ATTR[span_label])


class RefPartType(Enum):
    NAMED = "named"
    NUMBERED = "numbered"
    DH = "dibur_hamatchil"
    RANGE_SYMBOL = "range_symbol"
    RANGE = "range"
    RELATIVE = "relative"
    IBID = "ibid"
    NON_CTS = "non_cts"

    @classmethod
    def span_label_to_enum(cls, span_label: str) -> 'RefPartType':
        """
        Convert span label from spacy named entity to RefPartType
        """
        return getattr(cls, LABEL_TO_REF_PART_TYPE_ATTR[span_label])


class RawRefPart(TrieEntry, abst.Cloneable):
    """
    Immutable part of a RawRef
    Represents a unit of text used to find a match to a SchemaNode
    """
    key_is_id = False
    max_dh_continuation_len = 4  # max num tokens in potential_dh_continuation.

    def __init__(self, type: RefPartType, span: Optional[NESpan], potential_dh_continuation: NESpan = None):
        self.span = span
        self.type = type
        self.potential_dh_continuation = self.__truncate_potential_dh_continuation(potential_dh_continuation)

    def __truncate_potential_dh_continuation(self, potential_dh_continuation: NESpan) -> Optional[NESpan]:
        if potential_dh_continuation is None:
            return potential_dh_continuation
        return potential_dh_continuation.subspan_by_word_indices(slice(0, self.max_dh_continuation_len))

    def __str__(self):
        return f"{self.__class__.__name__}: {self.span}, {self.type}"

    def __repr__(self):
        return f"{self.__class__.__name__}({self.span}, {self.dh_cont_text})"

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self.__hash__() == other.__hash__()

    def __hash__(self):
        return hash(f"{self.type}|{self.span.__hash__()}|{self.dh_cont_text}")

    def __ne__(self, other):
        return not self.__eq__(other)

    def key(self):
        return self.text

    @property
    def text(self):
        return self.span.text

    @property
    def dh_cont_text(self):
        cont = self.potential_dh_continuation
        if not cont:
            return ""
        return cont.text

    def get_dh_text_to_match(self, lang: str) -> Iterable[tuple[str, int]]:
        import re2
        reg = r'^(?:?" )?(.+?)$' if lang == 'he' else r'^(?:s ?\. ?v ?\. )?(.+?)$'
        match = re2.match(reg, self.text)
        if match is None:
            return []
        dh = match.group(1)
        if self.potential_dh_continuation:
            yield from self.__enumerate_potential_dh_continuations(dh)
        # no matter what yield just the dh
        yield dh, 0

    def __enumerate_potential_dh_continuations(self, dh: str) -> Iterable[tuple[str, int]]:
        for potential_dh_token_idx in range(self.potential_dh_continuation.word_length(), 0, -1):
            temp_dh = f"{dh} {self.potential_dh_continuation.subspan_by_word_indices(slice(0, potential_dh_token_idx)).text}"
            yield temp_dh, potential_dh_token_idx

    @property
    def is_context(self):
        return isinstance(self, ContextPart)

    @property
    def char_indices(self) -> [int, int]:
        """
        Return start and end char indices of underlying text
        """
        return self.span.range if self.span else None

    def realign_to_new_raw_ref(self, old_raw_ref_span: NESpan, new_raw_ref_span: NESpan):
        """
        If span of raw_ref backing this ref_part changes, use this to align self.span to new raw_ref span
        """
        part_start, part_end = self.span.range
        old_raw_start, _ = old_raw_ref_span.range
        new_raw_start, _ = new_raw_ref_span.range
        offset = new_raw_start - old_raw_start
        return self.clone(span=new_raw_ref_span.subspan(slice(part_start-offset, part_end-offset)))

    def merge(self, other: 'RawRefPart') -> None:
        """
        Merge spans of two RawRefParts.
        Assumes other has same type as self
        """
        assert other.type == self.type
        self_start, self_end = self.span.range
        other_start, other_end = other.span.range
        if other_start < self_start:
            other.merge(self)
            return
        self.span = self.span.doc[self_start:other_end]


class ContextPart(RawRefPart):
    # currently used to easily differentiate TermContext and SectionContext from a vanilla RawRefPart
    pass


class TermContext(ContextPart):
    """
    Represents context backed by a NonUniqueTerm
    """
    key_is_id = True

    def __init__(self, term: schema.NonUniqueTerm):
        super().__init__(RefPartType.NAMED, None)
        self.term = term

    def key(self):
        return f"{self.__class__.__name__}({self.term.slug})"

    @property
    def text(self):
        return self.__str__()

    def __str__(self):
        return self.__repr__()

    def __repr__(self):
        return self.key()

    def __hash__(self):
        return hash(self.__repr__())


class SectionContext(ContextPart):
    """
    Represents a section in a context ref
    Used for injecting section context into a match which is missing sections (e.g. 'Tosafot on Berakhot DH abcd' is missing a daf)
    NOTE: used to used index of section to help validate. Doesn't work b/c we change sections list on the nodes as we refine them
    """

    def __init__(self, addr_type: schema.AddressType, section_name: str, address: int) -> None:
        """
        :param addr_type: AddressType of section
        :param section_name: Name of section
        :param address: Actual address, to be interpreted by `addr_type`
        """
        super().__init__(RefPartType.NUMBERED, None)
        self.addr_type = addr_type
        self.section_name = section_name
        self.address = address

    @property
    def text(self):
        addr_name = self.addr_type.__class__.__name__
        return f"{self.__class__.__name__}({addr_name}(0), '{self.section_name}', {self.address})"

    def __str__(self):
        return self.text

    def __repr__(self):
        return self.text

    def __hash__(self):
        return hash(f"{self.addr_type.__class__}|{self.section_name}|{self.address}")


class RangedRawRefParts(RawRefPart):
    """
    Container for ref parts that represent the sections and toSections of a ranged ref
    """
    def __init__(self, sections: list[RawRefPart], toSections: list[RawRefPart], **kwargs):
        super().__init__(RefPartType.RANGE, self._get_full_span(sections, toSections))
        self.sections = sections
        self.toSections = toSections

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self.__hash__() == other.__hash__()

    def __hash__(self):
        return hash(hash(p) for p in (self.sections + self.toSections))

    def __ne__(self, other):
        return not self.__eq__(other)

    @staticmethod
    def _get_full_span(sections: list[RawRefPart], toSections: list[RawRefPart]) -> NESpan:
        start_span = sections[0].span
        end_span = toSections[-1].span
        start_char, _ = start_span.range
        _, end_char = end_span.range
        return start_span.doc.subspan(slice(start_char, end_char))


class RawNamedEntity(abst.Cloneable):
    """
    Span of text which represents a named entity before it has been identified with an object in Sefaria's DB
    """

    def __init__(self, span: NESpan, type: NamedEntityType, **cloneable_kwargs) -> None:
        self.span = span
        self.type = type

    def map_new_char_indices(self, new_doc: NEDoc, new_char_indices: [int, int]) -> None:
        """
        Remap self.span to new indices
        """
        self.span = new_doc.subspan(slice(*new_char_indices))

    def align_to_new_doc(self, new_doc: NEDoc, offset: int) -> None:
        """
        @param new_doc: new NEDoc to align to
        @param offset: char offset that aligns chars in `self.span` to `new_doc
        """
        curr_start, curr_end = self.span.range
        new_start, new_end = curr_start+offset, curr_end+offset
        self.span = new_doc.subspan(slice(new_start, new_end))

    @property
    def text(self):
        """
        Return text of underlying span
        """
        return self.span.text

    @property
    def char_indices(self) -> [int, int]:
        """
        Return start and end char indices of underlying text
        """
        return self.span.range if self.span else None


class RawRef(RawNamedEntity):
    """
    Span of text which may represent one or more Refs
    Contains RawRefParts
    """
    def __init__(self, span: NESpan, lang: str, raw_ref_parts: list, **clonable_kwargs) -> None:
        """

        @param lang:
        @param raw_ref_parts:
        @param span:
        @param clonable_kwargs: kwargs when running Clonable.clone()
        """
        super().__init__(span, NamedEntityType.CITATION)
        self.lang = lang
        self.raw_ref_parts = self._group_ranged_parts(raw_ref_parts)
        self.parts_to_match = self.raw_ref_parts  # actual parts that will be matched. different when there are context swaps
        self.prev_num_parts_map = self._get_prev_num_parts_map(self.raw_ref_parts)
        for k, v in clonable_kwargs.items():
            setattr(self, k, v)
        self.span = span

    @staticmethod
    def _group_ranged_parts(raw_ref_parts: list['RawRefPart']) -> list['RawRefPart']:
        """
        Preprocessing function to group together RawRefParts which represent ranged sections
        """
        ranged_symbol_ind = None
        for i, part in enumerate(raw_ref_parts):
            if part.type == RefPartType.RANGE_SYMBOL:
                ranged_symbol_ind = i
                break
        if ranged_symbol_ind is None or ranged_symbol_ind == len(raw_ref_parts) - 1: return raw_ref_parts
        section_slice, toSection_slice = None, None
        for i in range(ranged_symbol_ind-1, -1, -1):
            if i == 0 or raw_ref_parts[i-1].type != RefPartType.NUMBERED:
                section_slice = slice(i, ranged_symbol_ind)
                break
        for i in range(ranged_symbol_ind+1, len(raw_ref_parts)):
            if i == len(raw_ref_parts) - 1 or raw_ref_parts[i+1].type != RefPartType.NUMBERED:
                toSection_slice = slice(ranged_symbol_ind+1, i+1)
                break
        if section_slice is None: return raw_ref_parts
        sections = raw_ref_parts[section_slice]
        toSections = sections[:]
        num_explicit_to_sections = toSection_slice.stop - toSection_slice.start
        toSections[-num_explicit_to_sections:] = raw_ref_parts[toSection_slice]
        new_raw_ref_parts = raw_ref_parts[:section_slice.start] + \
                            [RangedRawRefParts(sections, toSections)] + \
                            raw_ref_parts[toSection_slice.stop:]
        return new_raw_ref_parts

    @staticmethod
    def _get_prev_num_parts_map(raw_ref_parts: list[RawRefPart]) -> dict[RawRefPart, RawRefPart]:
        """
        Helper function to avoid matching NUMBERED RawRefParts that match AddressInteger sections out of order
        AddressInteger sections must resolve in order because resolving out of order would be meaningless
        Returns a map from NUMBERED RawRefParts to directly preceeding NUMBERED RawRefParts
        """
        if len(raw_ref_parts) == 0: return {}
        prev_num_parts_map = {}
        prev_part = raw_ref_parts[0]
        for part in raw_ref_parts[1:]:
            if prev_part.type == RefPartType.NUMBERED and part.type == RefPartType.NUMBERED:
                prev_num_parts_map[part] = prev_part
            prev_part = part
        return prev_num_parts_map

    def subspan(self, part_slice: slice) -> NESpan:
        """
        Return subspan covered by `part_slice`, relative to self.span
        """
        parts = self.raw_ref_parts[part_slice]
        start_char, _ = parts[0].span.range
        _, end_char = parts[-1].span.range

        offset_i, _ = self.span.range
        subspan = self.span.doc.subspan(slice(offset_i+start_char, offset_i+end_char))
        # potentially possible that tokenization of raw ref spans is not identical to that of ref parts. check to make sure.
        comparison_text = parts[0].span.doc.subspan(slice(start_char, end_char)).text
        assert subspan.text == comparison_text, f"{subspan.text} != {comparison_text}"
        return subspan

    def split_part(self, part: RawRefPart, str_end) -> ['RawRef', RawRefPart, RawRefPart]:
        """
        split `part` into two parts based on strings in `str_end`
        Return new RawRef with split parts (doesn't modify self)
        @param part: original part to be split
        @param str_end: end string
        @return: new RawRef with split parts
        """
        start_char, end_char = part.span.range
        pivot = len(part.text) - len(str_end) + start_char
        aspan = part.span.doc.subspan(slice(start_char, pivot))
        bspan = part.span.doc.subspan(slice(pivot, end_char))
        apart = part.clone(span=aspan)
        bpart = part.clone(span=bspan)

        # splice raw_ref_parts
        try:
            orig_part_index = self.raw_ref_parts.index(part)
            new_parts = self.raw_ref_parts[:]
            new_parts[orig_part_index:orig_part_index+1] = [apart, bpart]
        except ValueError:
            new_parts = self.raw_ref_parts
        # splice parts_to_match
        try:
            orig_part_index = self.parts_to_match.index(part)
            new_parts_to_match = self.parts_to_match[:]
            new_parts_to_match[orig_part_index:orig_part_index+1] = [apart, bpart]
        except ValueError:
            new_parts_to_match = self.parts_to_match
        return self.clone(raw_ref_parts=new_parts, parts_to_match=new_parts_to_match), apart, bpart

    def map_new_part_char_indices(self, new_part_char_indices: list[[int, int]]) -> None:
        """
        Remap self.span and all spans of parts to new indices
        """
        start_char, _ = self.char_indices
        for part, temp_part_indices in zip(self.raw_ref_parts, new_part_char_indices):
            part.span = self.span.subspan(slice(*[i-start_char for i in temp_part_indices]))

    def align_parts_to_new_doc(self, new_doc: NEDoc, offset: int) -> None:
        """
        See `RawNamedEntity.align_to_new_doc`
        @param new_doc:
        @param offset:
        @return:
        """
        for part in self.raw_ref_parts:
            curr_start, curr_end = part.span.range
            new_start, new_end = curr_start+offset, curr_end+offset
            part.span = new_doc.subspan(slice(new_start, new_end))

```

### sefaria/model/linker/referenceable_book_node.py

```
import dataclasses
from typing import List, Union, Optional, Tuple, Dict
import copy
from typing import List, Union, Optional
from sefaria.model import abstract as abst
from sefaria.model import text
from sefaria.model import schema
from sefaria.system.exceptions import InputError
from bisect import bisect_right


def subref(ref: text.Ref, section: int):
    if ref.index_node.addressTypes[len(ref.sections)-1] == "Talmud":
        return _talmud_subref(ref, section)
    elif ref.index.categories == ['Tanakh', 'Torah']:
        return _parsha_subref(ref, section)
    else:
        return ref.subref(section)


def _talmud_subref(ref: text.Ref, section: int):
    d = ref._core_dict()
    d['sections'][-1] += (section-1)
    d['toSections'] = d['sections'][:]
    return text.Ref(_obj=d)


def _parsha_subref(ref: text.Ref, section: int):
    parsha_trefs = {n.wholeRef for n in ref.index.get_alt_struct_leaves()}
    if ref.normal() in parsha_trefs:
        book_subref = text.Ref(ref.index.title).subref(section)
        if ref.contains(book_subref):
            return book_subref
        else:
            # section doesn't fall within parsha
            # Note, only validates that perek is in parsha range, doesn't check segment level.
            # Edge case is Parshat Noach 6:3
            raise InputError
    else:
        return ref.subref(section)


JA_NODE_LIST_ATTRS = ('addressTypes', 'sectionNames', 'lengths', 'referenceableSections')


def truncate_serialized_node_to_depth(serial_node: dict, depth: int) -> dict:
    truncated_serial_node = serial_node.copy()
    for list_attr in JA_NODE_LIST_ATTRS:
        if list_attr in serial_node:
            truncated_serial_node[list_attr] = serial_node[list_attr][depth:]
    return truncated_serial_node


def insert_amud_node_values(serial_node: dict) -> (dict, int):
    """
    Insert values to serialized JA node that correspond to an Amud section.
    This section doesn't exist in the JA node but is useful for matching Talmud sections with the linker
    @param serial_node: serialized JA node
    @return: `serial_node` with values for Amud section + the next referenceable depth corresponding to this modified node
    """
    serial_node['depth'] += 1
    next_referenceable_depth = 1
    for key, value in zip(JA_NODE_LIST_ATTRS, ('Amud', 'Amud', 1, True)):
        if key in serial_node:
            serial_node[key].insert(1, value)
    return serial_node, next_referenceable_depth


class ReferenceableBookNode:
    """
    Represents tree of referenceable nodes. In general, this tree is based on the tree in an Index's schema, but with
    some notable differences
    - alt struct nodes are part of the tree, represented as NamedReferenceableBookNodes
    - JaggedArrayNodes get split into N NumberedReferenceableNodes, where N is the depth of the JaggedArrayNode
    - The leave node of a JaggedArrayNode with `isSegmentLevelDiburHamatchil == True` is a DiburHamatchilNodeSet
    """

    def get_children(self, *args, **kwargs) -> List['ReferenceableBookNode']:
        return []

    def is_default(self) -> bool:
        return False

    @property
    def referenceable(self) -> bool:
        return True

    def is_ancestor_of(self, other: 'ReferenceableBookNode') -> bool:
        other_node = other._get_titled_tree_node()
        self_node = self._get_titled_tree_node()
        return self_node.is_ancestor_of(other_node)

    def _get_titled_tree_node(self) -> schema.TitledTreeNode:
        raise NotImplementedError

    def leaf_refs(self) -> list[text.Ref]:
        """
        Get the Refs for the ReferenceableBookNode leaf nodes from `self`
        @return:
        """
        raise NotImplementedError


class IndexNodeReferenceableBookNode(ReferenceableBookNode):
    """
    ReferenceableBookNode backed by node in an Index (either SchemaNode or AltStructNode)
    """

    def __init__(self, titled_tree_node: schema.TitledTreeNode):
        self._titled_tree_node = titled_tree_node

    @property
    def referenceable(self):
        return getattr(self._titled_tree_node, 'referenceable', not self.is_default())

    def _get_titled_tree_node(self) -> schema.TitledTreeNode:
        return self._titled_tree_node

    def is_default(self):
        return self._titled_tree_node.is_default() and self._titled_tree_node.parent is not None

    def ref(self) -> text.Ref:
        return self._titled_tree_node.ref()

    def unique_key(self) -> str:
        return self.ref().normal()

    def ref_order_id(self) -> str:
        if isinstance(self._titled_tree_node, schema.AltStructNode):
            leaves = self._titled_tree_node.get_leaf_nodes()
            # assume leaves are contiguous. If this is wrong, will be disproven later in the function
            if len(leaves) == 0:
                return "N/A"
            approx_ref = leaves[0].ref().to(leaves[-1].ref())
            return approx_ref.order_id()
        return self.ref().order_id()


class NamedReferenceableBookNode(IndexNodeReferenceableBookNode):

    def __init__(self, titled_tree_node_or_index: Union[schema.TitledTreeNode, text.Index]):
        self._titled_tree_node_or_index = titled_tree_node_or_index
        titled_tree_node = titled_tree_node_or_index
        if isinstance(titled_tree_node_or_index, text.Index):
            titled_tree_node = titled_tree_node_or_index.nodes
        super().__init__(titled_tree_node)

    def get_numeric_equivalent(self):
        return getattr(self._titled_tree_node, "numeric_equivalent", None)

    @staticmethod
    def _is_array_map_referenceable(node: schema.ArrayMapNode) -> bool:
        if not getattr(node, "isMapReferenceable", True):
            return False
        if getattr(node, "refs", None):
            return True
        if getattr(node, "wholeRef", None) and getattr(node, "includeSections", None):
            return True
        return False

    def _get_all_children(self) -> List[ReferenceableBookNode]:
        thingy = self._titled_tree_node_or_index
        # the schema node for this referenceable node has a dibur hamatchil child
        if isinstance(thingy, schema.NumberedTitledTreeNode) and thingy.is_segment_level_dibur_hamatchil():
            return [DiburHamatchilNodeSet({"container_refs": self.ref().normal()})]
        # the schema node for this referenceable is a JAN. JANs act as both named and numbered nodes
        if isinstance(thingy, schema.JaggedArrayNode) and len(thingy.children) == 0:
            return [NumberedReferenceableBookNode(thingy)]
        if isinstance(thingy, text.Index):
            children = thingy.referenceable_children()
        elif isinstance(thingy, schema.ArrayMapNode):
            if self._is_array_map_referenceable(thingy):
                return [MapReferenceableBookNode(thingy)]
            else:
                index = thingy.ref().index
                yo = NamedReferenceableBookNode(index)
                return yo.get_children()
        else:
            # Any other type of TitledTreeNode
            children = self._titled_tree_node.children
        children = [self._transform_schema_node_to_referenceable(x) for x in children]
        return children

    def _get_children_from_array_map_node(self, node: schema.ArrayMapNode) -> List[ReferenceableBookNode]:
        pass

    @staticmethod
    def _transform_schema_node_to_referenceable(schema_node: schema.TitledTreeNode) -> ReferenceableBookNode:
        if isinstance(schema_node, schema.JaggedArrayNode) and (schema_node.is_default() or schema_node.parent is None):
            return NumberedReferenceableBookNode(schema_node)
        return NamedReferenceableBookNode(schema_node)

    def get_children(self, *args, **kwargs) -> List[ReferenceableBookNode]:
        '''
        Node can have the attribute 'referenceable' sets to True (which is the default when the attribute ismissing), False or 'optional'.
        When node has referenceable False, it will return its referenceable descendant instead of itself.
        When has referenceable 'optional', it will return the node itself and its referenceable descendant.
        :return: list of the referenceable cihldren of the node
        '''
        nodes = []
        for node in self._get_all_children():
            referenceable = getattr(node, 'referenceable', True)
            if referenceable: #referenceable or optional
                nodes.append(node)
            if referenceable is not True: #unreferenceable or optional
                nodes += node._get_all_children()
        return nodes

    def ref_part_title_trie(self, *args, **kwargs):
        return self._titled_tree_node.get_match_template_trie(*args, **kwargs)

    def leaf_refs(self) -> list[text.Ref]:
        return [n.ref() for n in self._get_titled_tree_node().get_leaf_nodes()]


class NumberedReferenceableBookNode(IndexNodeReferenceableBookNode):

    def __init__(self, ja_node: schema.NumberedTitledTreeNode):
        super().__init__(ja_node)
        self._ja_node: schema.NumberedTitledTreeNode = ja_node

    @property
    def referenceable(self):
        return getattr(self._ja_node, 'referenceable', True)

    def leaf_refs(self) -> list[text.Ref]:
        return [self.ref()]

    def possible_subrefs(self, lang: str, initial_ref: text.Ref, section_str: str, fromSections=None) -> Tuple[List[text.Ref], List[bool]]:
        try:
            possible_sections, possible_to_sections, addr_classes = self._address_class.get_all_possible_sections_from_string(lang, section_str, fromSections, strip_prefixes=True)
        except (IndexError, TypeError, KeyError):
            return [], []
        possible_subrefs = []
        can_match_out_of_order_list = []
        for sec, toSec, addr_class in zip(possible_sections, possible_to_sections, addr_classes):
            try:
                refined_ref = subref(initial_ref, sec)
                if toSec != sec:
                    to_ref = subref(initial_ref, toSec)
                    refined_ref = refined_ref.to(to_ref)
                possible_subrefs += [refined_ref]
                can_match_out_of_order_list += [addr_class.can_match_out_of_order(lang, section_str)]
            except (InputError, IndexError, AssertionError, AttributeError):
                continue
        return possible_subrefs, can_match_out_of_order_list

    @property
    def _address_class(self) -> schema.AddressType:
        return self._ja_node.address_class(0)

    @property
    def _section_name(self) -> str:
        return self._ja_node.sectionNames[0]

    def _get_next_referenceable_depth(self):
        if self.is_default():
            return 0
        next_refereceable_depth = 1
        # if `referenceableSections` is not define, assume they're all referenceable
        referenceable_sections = getattr(self._ja_node, 'referenceableSections', [])
        if len(referenceable_sections) > 0:
            while next_refereceable_depth < len(referenceable_sections) and not referenceable_sections[next_refereceable_depth]:
                next_refereceable_depth += 1
        return next_refereceable_depth

    def _get_serialized_node(self) -> dict:
        serial = copy.deepcopy(self._ja_node.serialize())
        next_referenceable_depth = self._get_next_referenceable_depth()
        if isinstance(self._address_class, schema.AddressTalmud):
            serial, next_referenceable_depth = insert_amud_node_values(serial)
        serial['depth'] -= next_referenceable_depth
        serial['default'] = False  # any JA node that has been modified should lose 'default' flag
        serial['parent'] = self._ja_node
        if serial['depth'] == 0:
            raise ValueError("Can't serialize JaggedArray of depth 0")
        serial = truncate_serialized_node_to_depth(serial, next_referenceable_depth)
        return serial

    def get_children(self, context_ref=None, **kwargs) -> [ReferenceableBookNode]:
        try:
            serial = self._get_serialized_node()
        except ValueError:
            return []
        children = []
        if self._ja_node.is_segment_level_dibur_hamatchil():
            children += [DiburHamatchilNodeSet({"container_refs": context_ref.normal()})]
            if serial['depth'] == 1:
                return children
        new_ja = schema.JaggedArrayNode(serial=serial, index=getattr(self, 'index', None), **kwargs)
        return children + [NumberedReferenceableBookNode(new_ja)]

    def matches_section_context(self, section_context: 'SectionContext') -> bool:
        """
        Does the address in `self` match the address in `section_context`?
        """
        if self._address_class.__class__ != section_context.addr_type.__class__: return False
        if self._section_name != section_context.section_name: return False
        return True


class MapReferenceableBookNode(NumberedReferenceableBookNode):
    """
    Node that can only be referenced by refs in a mapping
    """

    def __init__(self, node: schema.ArrayMapNode):
        ja_node = self.__make_ja_from_array_map(node)
        super().__init__(ja_node)
        self._section_ref_map = self.__make_section_ref_map(node)

    @staticmethod
    def __make_ja_from_array_map(node: schema.ArrayMapNode):
        return MapReferenceableBookNode.__make_ja(**MapReferenceableBookNode.__get_ja_attributes_from_array_map(node))

    @staticmethod
    def __make_ja(addressTypes: List[str], sectionNames: List[str], **ja_node_attrs):
        return schema.JaggedArrayNode(serial={
            "addressTypes": addressTypes,
            "sectionNames": sectionNames,
            **ja_node_attrs,
            "depth": len(addressTypes),
        })

    @staticmethod
    def __get_ja_attributes_from_array_map(node: schema.ArrayMapNode) -> dict:
        if getattr(node, 'refs', None):
            address_types = node.addressTypes
            section_names = node.sectionNames
            return {"addressTypes": address_types, "sectionNames": section_names}
        elif getattr(node, 'wholeRef', None) and getattr(node, 'includeSections', False):
            whole_ref = text.Ref(node.wholeRef)
            schema_node = whole_ref.index_node.serialize()
            return truncate_serialized_node_to_depth(schema_node, -2)
        else:
            return {}

    def __make_section_ref_map(self, node: schema.ArrayMapNode) -> Dict[int, text.Ref]:
        if getattr(node, 'refs', None):
            section_ref_map = {
                self.__get_section_with_offset(ichild, node): text.Ref(tref)
                for ichild, tref in enumerate(node.refs)
            }
        elif getattr(node, 'wholeRef', None) and getattr(node, 'includeSections', False):
            whole_ref = text.Ref(node.wholeRef)
            refs = whole_ref.split_spanning_ref()
            section_ref_map = {}
            for oref in refs:
                section = oref.section_ref().sections[0]
                section_ref_map[section] = oref
        else:
            raise Exception("ArrayMapNode doesn't have expected attributes 'refs' or 'wholeRef'.")
        return section_ref_map

    def __get_section_with_offset(self, i: int, node: schema.ArrayMapNode) -> int:
        addresses = getattr(node, "addresses", None)
        if addresses:
            return addresses[i]
        section = i + 1
        starting_address = getattr(node, "startingAddress", None)
        if starting_address:
            section = i + self._address_class.toNumber("en", starting_address)
        skipped_addresses = getattr(node, "skipped_addresses", None)
        if skipped_addresses:
            skipped_addresses.sort()
            section += bisect_right(skipped_addresses, section)
        return section

    def ref(self):
        raise NotImplementedError(f'{self.__class__} does not have a single ref.')

    def leaf_refs(self) -> list[text.Ref]:
        return list(self._section_ref_map.values())

    def possible_subrefs(self, lang: str, initial_ref: text.Ref, section_str: str, fromSections=None) -> Tuple[List[text.Ref], List[bool]]:
        try:
            possible_sections, possible_to_sections, addr_classes = self._address_class.\
                get_all_possible_sections_from_string(lang, section_str, fromSections, strip_prefixes=True)
        except (IndexError, TypeError, KeyError):
            return [], []
        # map sections to equivalent refs in section_ref_map
        mapped_refs = []
        for sec, to_sec in zip(possible_sections, possible_to_sections):
            mapped_ref = self._section_ref_map.get(sec)
            if mapped_ref and sec == to_sec:
                mapped_refs += [mapped_ref]
        return mapped_refs, [True]*len(mapped_refs)


@dataclasses.dataclass
class DiburHamatchilMatch:
    score: float
    dh: Optional[str]
    potential_dh_token_idx: int
    dh_node: 'DiburHamatchilNode' = None

    def order_key(self):
        dh_len = len(self.dh) if self.dh else 0
        return self.score, dh_len

    def __gt__(self, other: 'DiburHamatchilMatch'):
        return self.order_key() > other.order_key()

    def __ge__(self, other: 'DiburHamatchilMatch'):
        return self.order_key() >= other.order_key()

    def __lt__(self, other: 'DiburHamatchilMatch'):
        return self.order_key() < other.order_key()

    def __le__(self, other: 'DiburHamatchilMatch'):
        return self.order_key() <= other.order_key()


class DiburHamatchilNode(abst.AbstractMongoRecord, ReferenceableBookNode):
    """
    Very likely possible to use VirtualNode and add these nodes as children of JANs and ArrayMapNodes. But that can be a little complicated
    """
    collection = "dibur_hamatchils"
    required_attrs = [
        "dibur_hamatchil",
        "container_refs",
        "ref",
    ]

    def fuzzy_match_score(self, lang, raw_ref_part) -> DiburHamatchilMatch:
        from sefaria.utils.hebrew import hebrew_starts_with
        for dh, dh_index in raw_ref_part.get_dh_text_to_match(lang):
            if hebrew_starts_with(self.dibur_hamatchil, dh):
                return DiburHamatchilMatch(1.0, dh, dh_index)
        return DiburHamatchilMatch(0.0, None, dh_index)


class DiburHamatchilNodeSet(abst.AbstractMongoSet, ReferenceableBookNode):
    recordClass = DiburHamatchilNode

    def best_fuzzy_matches(self, lang, raw_ref_part, score_leeway=0.01, threshold=0.9) -> List[DiburHamatchilMatch]:
        """
        :param lang: either 'he' or 'en'
        :param raw_ref_part: of type "DH" to match
        :param score_leeway: all scores within `score_leeway` of the highest score are returned
        :param threshold: scores below `threshold` aren't returned
        """
        best_list = [DiburHamatchilMatch(0.0, '', 0)]
        for node in self:
            dh_match = node.fuzzy_match_score(lang, raw_ref_part)
            if dh_match.dh is None: continue
            if dh_match >= best_list[-1]:
                dh_match.dh_node = node
                best_list += [dh_match]
        best_match = best_list[-1]
        return [best for best in best_list if best.score > threshold and best.score + score_leeway >= best_match.score
                and len(best.dh) == len(best_match.dh)]

```

### sefaria/model/linker/category_resolver.py

```
from collections import defaultdict
from sefaria.model.category import Category
from sefaria.model.linker.ref_part import RawRef


class ResolvedCategory:

    def __init__(self, raw_ref: RawRef, categories: list[Category]) -> None:
        self.raw_entity = raw_ref
        self.categories = categories

    @property
    def is_ambiguous(self):
        return len(self.categories) != 1

    @property
    def resolution_failed(self):
        return len(self.categories) == 0


class CategoryMatcher:

    def __init__(self, lang: str, category_registry: list[Category]) -> None:
        self._title_to_cat: dict[str, list[Category]] = defaultdict(list)
        for cat in category_registry:
            for match_template in cat.get_match_templates():
                for term in match_template.get_terms():
                    for title in term.get_titles(lang):
                        self._title_to_cat[title] += [cat]

    def match(self, raw_ref: RawRef) -> list[Category]:
        return self._title_to_cat[raw_ref.text]


class CategoryResolver:

    def __init__(self, category_matcher: CategoryMatcher) -> None:
        self._matcher = category_matcher

    def bulk_resolve(self, raw_refs: list[RawRef]) -> list[ResolvedCategory]:
        resolved = []
        for raw_ref in raw_refs:
            matched_categories = self._matcher.match(raw_ref)
            resolved += [ResolvedCategory(raw_ref, matched_categories)]
        return resolved

```

### sefaria/model/linker/resolved_ref_refiner_factory.py

```
from sefaria.model.linker.ref_part import RawRefPart, RefPartType
from sefaria.model.linker.referenceable_book_node import ReferenceableBookNode, NamedReferenceableBookNode, NumberedReferenceableBookNode, MapReferenceableBookNode
from sefaria.model.linker.resolved_ref_refiner import ResolvedRefRefinerForDefaultNode, ResolvedRefRefinerForNumberedPart, ResolvedRefRefinerForDiburHamatchilPart, ResolvedRefRefinerForRangedPart, ResolvedRefRefinerForNamedNode, ResolvedRefRefiner, ResolvedRefRefinerCatchAll


class ResolvedRefRefinerFactoryKey:

    def __init__(self, part_type: RefPartType = None, node_class: type = None, is_default: bool = False):
        self.key = (part_type, node_class, is_default)

    def __eq__(self, other):
        if not isinstance(other, ResolvedRefRefinerFactoryKey):
            return False
        for a, b in zip(self.key, other.key):
            if a is not None and b is not None and a != b:
                return False
        return True


class ResolvedRefRefinerFactory:
    """
    Factory class to create `ResolveRefRefiner`s
    Use `register_refiner()` to register rules when a certain refiner should be used
    Order in which refiners are registered matters. The first rule that matches will be used.
    """

    def __init__(self):
        self.__refiner_list = []

    def register_refiner(self, key: ResolvedRefRefinerFactoryKey, refiner_type: type):
        self.__refiner_list += [(key, refiner_type)]

    def create(self, part: RawRefPart, node: ReferenceableBookNode, resolved_ref: 'ResolvedRef') -> ResolvedRefRefiner:
        refiner = self.__get_refiner_class(part.type, node.__class__, node.is_default())
        return refiner(part, node, resolved_ref)

    def __get_refiner_class(self, part_type: RefPartType, node_class: type, is_default: bool) -> type:
        key = ResolvedRefRefinerFactoryKey(part_type, node_class, is_default)
        for temp_key, temp_refiner in self.__refiner_list:
            if key == temp_key:
                return temp_refiner
        raise ValueError(f"Invalid combination of part and node passed.")


def initialize_resolved_ref_refiner_factory() -> ResolvedRefRefinerFactory:
    factory = ResolvedRefRefinerFactory()
    key = ResolvedRefRefinerFactoryKey
    refiners_to_register = [
        (key(is_default=True), ResolvedRefRefinerForDefaultNode),
        (key(RefPartType.NUMBERED, node_class=NumberedReferenceableBookNode), ResolvedRefRefinerForNumberedPart),
        (key(RefPartType.NUMBERED, node_class=MapReferenceableBookNode), ResolvedRefRefinerForNumberedPart),
        (key(RefPartType.RANGE, node_class=NumberedReferenceableBookNode), ResolvedRefRefinerForRangedPart),
        (key(RefPartType.NAMED, node_class=NamedReferenceableBookNode), ResolvedRefRefinerForNamedNode),
        (key(RefPartType.NUMBERED, node_class=NamedReferenceableBookNode), ResolvedRefRefinerForNamedNode),
        (key(RefPartType.DH), ResolvedRefRefinerForDiburHamatchilPart),
        (key(), ResolvedRefRefinerCatchAll),
    ]
    for k, v in refiners_to_register:
        factory.register_refiner(k, v)
    return factory


resolved_ref_refiner_factory: ResolvedRefRefinerFactory = initialize_resolved_ref_refiner_factory()

```

### sefaria/model/garden.py

```
# coding=utf-8

import copy
from itertools import groupby
from sefaria.system.exceptions import InputError
from sefaria.system.database import db
from . import abstract as abst
from . import text
from . import place
from . import timeperiod
from . import topic
from . import link
from . import user_profile

import structlog
logger = structlog.get_logger(__name__)


class Garden(abst.AbstractMongoRecord):
    """
    https://saravanamudaliar.files.wordpress.com/2014/03/102_5996.jpg
    """
    collection = 'garden'
    track_pkeys = True
    pkeys = ["key"]

    required_attrs = [
        'key',
        'title',
        'heTitle',
        'config'
    ]
    optional_attrs = [
        "subtitle",
        "heSubtitle"
    ]

    default_config = {
        "timeline_scale": "log",  # log / linear
        "timeline_bin_size": None,  # Number of years in a bin.  Defaults to ~20 bins in the extent
        "filter_order": [],
        "filter_rows": 9,  # Number of rows in row filters
        "filters": {
            "default": {
                "en": "Tags",
                "he": "",
                "logic": "AND",  # AND / OR
                "position": "SIDE"  # SIDE / TOP
            }
        },
        "sorts": {
            "start": {
                "en": "Date",
                "he": "",
                "datatype": "Int",  #Int, Str
                "default": "ASC"
            }
        }
    }

    def _set_derived_attributes(self):
        if getattr(self, "config", None) is None:
            self.config = copy.deepcopy(self.default_config)

    def updateConfig(self, config_dict):
        self.config.update(config_dict)

    def updateFilter(self, filterkey, filterdict):
        if self.config["filters"].get(filterkey):
            self.config["filters"][filterkey].update(filterdict)
        else:
            self.config["filters"][filterkey] = filterdict

    def removeFilter(self, filterkey):
        try:
            del self.config["filters"][filterkey]
        except KeyError:
            pass

    def updateSort(self, field, sortdict):
        self.config["sorts"][field] = sortdict

    def removeSort(self, field):
        try:
            del self.config["sorts"][field]
        except KeyError:
            pass

    def stopSet(self, sort=None):
        if not sort:
            sort = [("start", 1)]
        return GardenStopSet({"garden": self.key}, sort=sort)

    def relSet(self, sort=None):
        if not sort:
            sort = [("start", 1)]
        return GardenStopRelationSet({"garden": self.key}, sort=sort)

    def stopsByTime(self):
        res = []
        stops = self.stopSet()
        for k, g in groupby(stops, lambda s: (getattr(s, "start", "unknown"), getattr(s, "end", "unknown"))):
            res.append((k, [s.contents() for s in g]))
        return res

    def stopsByPlace(self):
        res = []
        stops = self.stopSet(sort=[("placeKey", 1)])
        for k, g in groupby(stops, lambda s: getattr(s, "placeKey", "unknown")):
            res.append((k, [s.contents() for s in g]))
        return res

    def placeSet(self):
        pkeys = self.stopSet().distinct("placeKey")
        return place.PlaceSet({"key": {"$in": pkeys}})

    def stopsByAuthor(self):
        res = []
        unknown = []

        stops = self.stopSet(sort=[("start", 1), ("authors", 1)])
        for k, g in groupby(stops, lambda s: getattr(s, "authors", None)):
            if not k:
                unknown.extend([s.contents() for s in g])
            else:
                res.append(([topic.Topic.init(p) for p in k], [s.contents() for s in g]))
        res.append(("unknown", unknown))
        return res

    def stopsByTag(self):
        by_tag = {}
        stops = self.stopSet()

        for stop in stops:
            for typ, tags in getattr(stop, "tags", {}).items():
                if not by_tag.get(typ):
                    by_tag[typ] = {}

                for tag in tags:
                    if by_tag[typ].get(tag):
                        by_tag[typ][tag].append(stop.contents())
                    else:
                        by_tag[typ][tag] = [stop.contents()]

        return by_tag

    def stopData(self):
        return [i.contents() for i in self.stopSet()]

    def add_stop(self, attrs):
        if not attrs.get("tags"):
            attrs["tags"] = {"default": []}

        # Check for existing stop, based on Ref.
        if attrs.get("ref"):
            if attrs.get("enText") is None:
                try:
                    attrs["enText"] = text.TextChunk(text.Ref(attrs["ref"]), "en", attrs.get("enVersionTitle")).as_string()
                except Exception:
                    pass
            if attrs.get("heText") is None:
                try:
                    attrs["heText"] = text.TextChunk(text.Ref(attrs["ref"]), "he", attrs.get("heVersionTitle")).as_string()
                except Exception:
                    pass

            existing = GardenStop().load({"ref": attrs["ref"], "garden": self.key})
            if existing:
                existing.weight += 1

                # Merge tags
                if not getattr(existing, "tags", None):
                    existing.tags = attrs.get("tags")
                else:
                    for typ, tags in attrs.get("tags", {}).items():
                        if not existing.tags.get(typ):
                            existing.tags[typ] = attrs["tags"][typ]
                        else:
                            for tag in tags:
                                if tag not in existing.tags[typ]:
                                    existing.tags[typ].append(tag)

                if attrs.get("enText") and not getattr(existing, "enText", None):
                    existing.enText = attrs.get("enText")
                if attrs.get("heText") and not getattr(existing, "heText", None):
                    existing.enText = attrs.get("heText")

                existing.save()
                return

        # New Stop
        gs = GardenStop(attrs)
        gs.garden = self.key
        gs.weight = 1
        try:
            gs.save()
        except Exception as e:
            logger.warning("Failed to add stop to Garden {}. {}".format(self.title, e))

    def add_relationship(self, attrs):
        gs = GardenStopRelation(attrs)
        gs.garden = self.key
        try:
            gs.save()
        except Exception as e:
            logger.warning("Failed to add relationship to Garden {}. {}".format(self.title, e))

    def import_sheets_by_user(self, user_id):
        self.updateSort("weight", {"type": "Int", "en": "Weight", "he": ""})
        sheet_list = db.sheets.find({"owner": int(user_id), "status": {"$ne": 5}})
        for sheet in sheet_list:
            self.import_sheet(sheet["id"])

    def import_sheets_by_tag(self, tag):
        from sefaria.sheets import get_sheets_by_topic

        self.updateFilter("Sheet Author", {"en": "Sheet Author", "he": " "})
        self.updateSort("weight", {"type": "Int", "en": "Weight", "he": ""})
        sheet_list = get_sheets_by_topic(tag)
        for sheet in sheet_list:
            self.import_sheet(sheet["id"], remove_tags=[tag])

    # todo: this is way too slow.
    def get_links(self):
        """
        Given the current Ref set of the Garden, looks for Links in the core repository, and turns them into GardenStopRelations
        """
        trefs = GardenStopSet({"garden": self.key}).distinct("ref")
        regexes = set()
        refClauses = []
        links = []

        for tref in trefs:
            try:
                ref = text.Ref(tref)
            except:
                continue
            regexes.update(ref.regex(as_list=True))

        for rgx in regexes:
            refClauses += [{"refs.1": {"$regex": rgx}}]

        for rgx in regexes:
            print("Garden.get_links() - {}".format(rgx))
            links += [l for l in link.LinkSet({"$and": [{"refs.0": {"$regex": rgx}}, {"$or": refClauses}]})]

        return links

    def import_search(self, q):
        from sefaria.search import query
        res = query(q)

        self.updateFilter("default", {"en": "Categories", "he": ""})

        for hit in res["hits"]["hits"]:
            tags = {"default": hit["_source"]["path"].split("/")}
            stop = {
                "type": "inside_source",
                "ref": hit["_source"]["ref"],
                "enVersionTitle": hit["_source"]["version"],
                "tags": tags
            }
            if hit["_source"]["lang"] == "en":
                stop["enText"] = " ".join(hit["highlight"]["content"])
            elif hit["_source"]["lang"] == "he":
                stop["heText"] = " ".join(hit["highlight"]["content"])
            self.add_stop(stop)

    def import_ref_list(self, reflist, defaults=None):
        if defaults is None:
            defaults = {}
        self.updateFilter("default", {"en": "Categories", "he": ""})
        for ref in reflist:
            if isinstance(ref, str):
                try:
                    ref = text.Ref(ref)
                except:
                    pass
            if not isinstance(ref, text.Ref):
                continue

            stop_dict = {
                "type": "inside_source",
                "ref": ref.normal(),
                "tags": {"default": ref.index.categories}
            }

            if defaults.get("tags") is not None:
                stop_dict["tags"].update(defaults["tags"])
            stop_dict.update({k:v for k,v in list(defaults.items()) if k != "tags"})

            self.add_stop(stop_dict)
        return self

    def import_sheet(self, sheet_id, remove_tags=None):
        from sefaria.sheets import Sheet, refine_ref_by_text

        sheet = Sheet().load({"id": sheet_id})
        if not sheet:
            logger.warning("Failed to load sheet {}".format(sheet_id))

        def process_sources(sources, tags):
            for source in sources:
                if "ref" in source:
                    text = source.get("text", {}).get("he", None)
                    ref = refine_ref_by_text(source["ref"], text) if text else source["ref"]

                    self.add_stop({
                        "type": "inside_source",
                        "ref": ref,
                        "enText": source['text'].get("en"),
                        "heText": source['text'].get("he"),
                        "tags": tags
                    })
                elif "outsideBiText" in source:
                    self.add_stop({
                        "type": "outside_source",
                        "enText": source['outsideBiText'].get("en"),
                        "heText": source['outsideBiText'].get("he"),
                        "tags": tags
                    })
                elif "outsideText" in source:
                    self.add_stop({
                        "type": "outside_source",
                        "enText": source['outsideText'],
                        "tags": tags
                    })
                elif "comment" in sources:
                    self.add_stop({
                        "type": "blob",
                        "enText": source['comment'],
                        "tags": tags
                    })

                if "subsources" in source:
                    process_sources(source["subsources"], tags)

        tags = getattr(sheet, "tags", [])
        if remove_tags:
            tags = [t for t in tags if t not in remove_tags]
        process_sources(sheet.sources, {"default": tags, "Sheet Author": [user_profile.user_name(sheet.owner)]})
        return self


class GardenSet(abst.AbstractMongoSet):
    recordClass = Garden


class GardenStop(abst.AbstractMongoRecord):
    collection = 'garden_stop'
    track_pkeys = True
    pkeys = ["ref"]

    required_attrs = [
        'garden',
        'type'  # inside_source, outside_source, blob
                # todo: Subclass these?
    ]
    optional_attrs = [
        'ref',
        'heRef',
        'weight',
        'title',
        'heTitle',
        'enVersionTitle',
        'heVersionTitle',  # will we use this?
        'enSubtitle',
        'heSubtitle',
        'enText',
        'heText',
        'tags',  # dictionary of lists
        "start",
        "startIsApprox",
        "end",
        "endIsApprox",
        'placeKey',
        'placeNameEn',
        'placeNameHe',
        'placeGeo',  # keep this here?  Break into point and area?  "area or point"?
        'authors',
        'authorsEn',
        'authorsHe',
        'indexTitle',
        'timePeriodEn',
        'timePeriodHe'
    ]

    def hasCustomText(self, lang):
        assert lang, "hasCustomText() requires a language code"
        if lang == "en":
            return bool(getattr(self, 'enText', False))
        elif lang == "he":
            return bool(getattr(self, 'heText', False))
        else:
            return bool(getattr(self, 'enText', False)) or bool(getattr(self, 'heText', False))

    # on initial value of ref, and change of ref, derive metadata.
    # todo: do we have to support override of this info?

    def _derive_metadata(self):
        # Get index from ref
        if getattr(self, "ref", None):
            oref = text.Ref(self.ref)
            i = oref.index
            assert isinstance(i, text.AbstractIndex)
            self.indexTitle = i.title
            self.heRef = oref.he_normal()

            # Text
            if not getattr(self, "enText", None):
                self.enText = oref.text("en").text
            if not getattr(self, "heText", None):
                self.heText = oref.text("he").text

            # Authors
            if getattr(i, "authors", None):
                self.authors = i.authors
        else:
            i = {}

        # Author
        if getattr(self, "authors", None) and len(self.authors) > 0:
            author = topic.Topic.init(self.authors[0]) or {}
            if author:
                self.authorsEn = author.get_primary_title("en")
                self.authorsHe = author.get_primary_title("he")
        else:
            author = {}

        # Place
        # The "" result is import to have here, for CrossFilter correctness on the frontend
        self.placeKey = getattr(self, "placeKey", "") or getattr(i, "compPlace", "") or getattr(author, "deathPlace", "") or getattr(author, "birthPlace", "")
        if self.placeKey:
            pobj = place.Place().load({"key": self.placeKey})
            if not pobj:
                raise InputError("Failed to find place with key {} while resolving metadata for {}".format(self.placeKey, self.ref))
            self.placeNameEn = pobj.primary_name("en")
            self.placeNameHe = pobj.primary_name("he")
            #self.placeGeo = pobj.get_location()

        # Time
        # This is similar to logic on Index.composition_time_period() refactor
        if getattr(self, "start", None) is None or getattr(self, "end", None) is None:
            years = getattr(i, 'compDate', [])
            if years and len(years) > 0:
                self.startIsApprox = self.endIsApprox = getattr(i, "hasErrorMargin", False)
                if len(years) > 1:
                    self.start = years[0]
                    self.end = years[1]
                else:
                    self.start = self.end = years[0]
            elif author and author.most_accurate_time_period():
                tp = author.most_accurate_time_period()
                self.start = tp.start
                self.end = tp.end
                self.startIsApprox = tp.startIsApprox
                self.endIsApprox = tp.endIsApprox

        tp = self.time_period()
        if tp:
            self.timePeriodEn = tp.period_string("en")
            self.timePeriodHe = tp.period_string("he")
        else:
            self.start = None
            self.end = None

    def _normalize(self):
        if self.is_key_changed("ref"):
            self._derive_metadata()
        if getattr(self, "start", None):
            self.start = int(self.start)
        if getattr(self, "end", None):
            self.end = int(self.end)

    def time_period(self):
        if not getattr(self, "start", False):
            return None
        return timeperiod.TimePeriod({
            "start": self.start,
            "startIsApprox": getattr(self, "startIsApprox", False),
            "end": self.end,
            "endIsApprox": getattr(self, "endIsApprox", False)
        })

    def place(self):
        return place.Place().load({"key": self.placeKey})

    def set_tags(self, tags, type="default"):
        if isinstance(tags, str):
            tags = [tags]
        if self.tags.get(type):
            for tag in tags:
                if tag not in self.tags["type"]:
                    self.tags["type"].append(tag)
        else:
            self.tags["type"] = tags

    def get_tags(self, type="default"):
        return self.tags.get(type)

    def get_all_tags(self):
        return self.tags

class GardenStopSet(abst.AbstractMongoSet):
    recordClass = GardenStop


class GardenStopRelation(abst.AbstractMongoRecord):
    collection = 'garden_rel'
    required_attrs = [
        'garden'
    ]
    optional_attrs = [

    ]

class GardenStopRelationSet(abst.AbstractMongoSet):
    recordClass = GardenStopRelation



"""
def process_index_title_change_in_gardens(indx, **kwargs):
    if indx.is_commentary():
        pattern = r'^{} on '.format(re.escape(kwargs["old"]))
    else:
        commentators = text.IndexSet({"categories.0": "Commentary"}).distinct("title")
        pattern = ur"(^{} \d)|(^({}) on {} \d)".format(re.escape(kwargs["old"]), "|".join(commentators), re.escape(kwargs["old"]))
        #pattern = r'(^{} \d)|( on {} \d)'.format(re.escape(kwargs["old"]), re.escape(kwargs["old"]))
    links = LinkSet({"refs": {"$regex": pattern}})
    for l in links:
        l.refs = [r.replace(kwargs["old"], kwargs["new"], 1) if re.search(pattern, r) else r for r in l.refs]
        try:
            l.save()
        except InputError: #todo: this belongs in a better place - perhaps in abstract
            logger.warning(u"Deleting link that failed to save: {} {}".format(l.refs[0], l.refs[1]))
            l.delete()


def process_index_delete_in_gardens(indx, **kwargs):
    if indx.is_commentary():
        pattern = ur'^{} on '.format(re.escape(indx.title))
    else:
        commentators = text.IndexSet({"categories.0": "Commentary"}).distinct("title")
        pattern = ur"(^{} \d)|^({}) on {} \d".format(re.escape(indx.title), "|".join(commentators), re.escape(indx.title))
    LinkSet({"refs": {"$regex": pattern}}).delete()
"""
```

### sefaria/model/trend.py

```
# -*- coding: utf-8 -*-

"""
trend.py
"""
import json
import time
from datetime import datetime, date, timedelta

from . import abstract as abst
from . import user_profile
from . import text

from sefaria.system.database import db
from sefaria.model import Ref
from sefaria.model.text import library

import structlog
logger = structlog.get_logger(__name__)


def read_in_category_key(c):
    return "ReadInCategory" + c


def reverse_read_in_category_key(k):
    return k[14:]


def get_session_traits(request, uid=None):
    # keys for these traits are duplicated in story editor.  Could be more graceful.

    traits = {
        "inDiaspora": bool(request.diaspora),
        "inIsrael": not request.diaspora,
    }
    if uid is not None:
        traits.update({
            "readsHebrew":                  Trend.get_user_trend_value(uid, "HebrewAbility") >= .5,
            "toleratesEnglish":             Trend.get_user_trend_value(uid, "EnglishTolerance") >= .05,
            "usesSheets":                   Trend.get_user_trend_value(uid, "SheetsRead") >= 2,
        })

        # "createsSheets"
        # "prefersBilingual"
        # "isSephardi"
        # "learnsDafYomi", etc

    return [k for k, v in list(traits.items()) if v]


class DateRange(object):
    new_years_dict = {
        2020: datetime(2020, 9, 18),
        2021: datetime(2021, 9, 6),
        2022: datetime(2022, 9, 25),
        2023: datetime(2023, 9, 15),
        2024: datetime(2024, 10, 2),
        2025: datetime(2025, 9, 22),
        2026: datetime(2026, 9, 11),
        2027: datetime(2027, 10, 1),
        2028: datetime(2028, 9, 20)
    }

    def __init__(self, key, start=None, end=None):
        """
        :param start: datetime or None, meaning open ended
        :param end: datetime or None, meaning open ended
        """
        self.start = start
        self.end = end
        self.key = key

    @classmethod
    def alltime(cls):
        return cls("alltime", None, None)

    @classmethod
    def currently(cls):
        today = datetime.today()
        year_in_days = timedelta(365)
        return cls("currently", today - year_in_days, today)

    @classmethod
    def this_hebrew_year(cls):
        today = date.today()                     
        this_gregorian_year_rh = cls.new_years_dict[today.year]
        try:
            if (this_gregorian_year_rh.date() > today):
                return cls("this_hebrew_year", cls.new_years_dict[today.year-1], this_gregorian_year_rh)
            else:
                return cls("this_hebrew_year", this_gregorian_year_rh, cls.new_years_dict[today.year+1])
        except:
            latest_year = max(cls.new_years_dict.get.keys())
            return cls("this_hebrew_year", cls.new_years_dict[latest_year-1], latest_year)

    @classmethod
    def previous_hebrew_year(cls):
        #todo: add try catch
        today = date.today()
        this_gregorian_year_rh = cls.new_years_dict[today.year]
        try:
            if (this_gregorian_year_rh.date() > today):
                return cls("previous_hebrew_year", cls.new_years_dict[today.year-2], cls.new_years_dict[today.year-1])
            else:
                return cls("previous_hebrew_year", cls.new_years_dict[today.year-1], this_gregorian_year_rh)
        except:
            latest_year = max(cls.new_years_dict.get.keys())
            return cls("this_hebrew_year", cls.new_years_dict[latest_year-2], cls.new_years_dict[latest_year-2])

    def needs_clause(self):
        return self.start or self.end

    def query_clause(self):
        """
        Returns a time range clause, fit for use in a pymongo query
        :return:
        """
        if self.start is None and self.end is None:
            return {}

        timeclause = {}
        if self.start:
            timeclause["$gte"] = self.start
        if self.end:
            timeclause["$lte"] = self.end
        return timeclause

    def update_match(self, match_clause, field="datetime"):
        """
        Update a mongo query dict with a time period query
        :param match_clause: dict
        :param field: the field to match in this query
        :return: dict (though it's been updated in place)
        """
        if self.needs_clause():
            match_clause[field] = self.query_clause()
        return match_clause

    def contains(self, dt):
        """
        Check if the supplied datetime falls in this range
        :param dt:
        :return:
        """
        return ((self.start is None or self.start <= dt)
                and (self.end is None or dt <= self.end))


active_dateranges = [DateRange.alltime(), DateRange.currently()]


class Trend(abst.AbstractMongoRecord):
    '''
    Value
    Timestamp
    Period Covered [week, month, season, alltime, this_hebrew_year]
    Scope [user, user network, comparable, site]
    '''

    collection   = 'trend'
    history_noun = 'trend'

    required_attrs = [
        "name",
        "value",
        "datatype",  # from factory.  needed?
        "timestamp",
        "period",    #
        "scope"
    ]

    optional_attrs = [
        "uid"       # Required when scope is not "site"
    ]

    @classmethod
    def get_user_trend_value(cls, uid, name, period="alltime", default=0):
        trend = cls().load({"uid": uid, "name": name, "period": period})
        if trend:
            return trend.value
        else:
            return default

    def _init_defaults(self):
        self.timestamp = int(time.time())

    def _validate(self):
        assert self.scope == "site" or hasattr(self, "uid")


class TrendSet(abst.AbstractMongoSet):
    recordClass = Trend

def setUserSheetTraits():
    TrendSet({"name": "SheetsRead"}).delete()

    for daterange in active_dateranges:
        all_users = getAllUsersSheetUsage(daterange)
        for uid, data in all_users.items():
            Trend({
                "name":         "SheetsRead",
                "value":        int(data["cnt"]),
                "datatype":     "int",
                "timestamp":    datetime.utcnow(),
                "period":       daterange.key,
                "scope":        "user",
                "uid":          uid
            }).save()

def setCategoryTraits():
    top_categories = library.get_top_categories()

    # User Traits
    for daterange in active_dateranges:
        site_data = {cat: 0 for cat in top_categories}
        for category in top_categories:
            all_users = getAllUsersCategories(daterange, category)
            for uid, data in all_users.items():
                val = data['cnt']
                TrendSet({"period": daterange.key, "uid": uid, "name": read_in_category_key(category)}).delete()

                # for val in list(data["categories"].items()):
                #     if cat not in TOP_CATEGORIES:
                #         continue
                Trend({
                    "name":         read_in_category_key(category),
                    "value":        val,
                    "datatype":     "int",
                    "timestamp":    datetime.utcnow(),
                    "period":       daterange.key,
                    "scope":        "user",
                    "uid":          uid
                }).save()
                site_data[category] += val

        # Site Traits
        TrendSet({"period": daterange.key, "scope": "site", "name": {"$in": list(map(read_in_category_key, top_categories))}}).delete()

        for cat, val in site_data.items():
            Trend({
                "name": read_in_category_key(cat),
                "value": val,
                "datatype": "int",
                "timestamp": datetime.utcnow(),
                "period": daterange.key,
                "scope": "site"
            }).save()


def setSheetTraits():
    TrendSet({"name": "SheetsCreatedPublic"}).delete()
    TrendSet({"name": "SheetsCreated"}).delete()

    for daterange in active_dateranges:
        all_users = getAllUsersSheetCreation(daterange)
        all_users_published = getAllUsersSheetCreation(daterange, publishedOnly=True)
        for uid, data in all_users.items():
            Trend({
                "name":         "SheetsCreated",
                "value":        int(data["cnt"]),
                "datatype":     "int",
                "timestamp":    datetime.utcnow(),
                "period":       daterange.key,
                "scope":        "user",
                "uid":          uid
            }).save()
        for uid, data in all_users_published.items():
            Trend({
                "name":         "SheetsCreatedPublic",
                "value":        int(data["cnt"]),
                "datatype":     "int",
                "timestamp":    datetime.utcnow(),
                "period":       daterange.key,
                "scope":        "user",
                "uid":          uid
            }).save()


def setUserLanguageTraits():
    TrendSet({"name": {"$in": ["EnglishTolerance", "HebrewAbility"]}}).delete()

    for daterange in active_dateranges:
        all_users = getAllUsersLanguageUsage(daterange)
        for uid, data in all_users.items():
            profile = user_profile.UserProfile(id=uid)

            he = float(data["languages"].get("hebrew", 0.0))
            en = float(data["languages"].get("english", 0.0))
            bi = float(data["languages"].get("bilingual", 0.0))
            total = float(data.get("total", 0.0))
            assert total

            # EnglishTolerance
            # If user has English interface conclude English tolerance
            if profile.settings.get("interface_language") == "english" or not he:
                value = 1.0
            else:
                # percentage of visits registered w/ English content
                value = (en + bi) / total

            if value < 0 or value > 1:
                print("UNEXPECTED value for EnglishTolerance: " + str(total))
                print(profile.email + " has en score " + str(en) + ". bi score: " + str(bi) + ". total: " + str(total))
                print(json.dumps(data))

            Trend({
                "name":         "EnglishTolerance",
                "value":        value,
                "datatype":     "float",
                "timestamp":    datetime.utcnow(),
                "period":       daterange.key,
                "scope":        "user",
                "uid":          uid
            }).save()

            # HebrewAbility
            # If user has Hebrew interface conclude Hebrew ability
            if profile.settings.get("interface_language") == "hebrew":
                value = 1.0

            # all bi is .50,  Each he adds a bunch.  Each en takes away a bit.
            else:
                het = he/total
                value = 1.1 * (het / (.1 + het))

            Trend({
                "name":         "HebrewAbility",
                "value":        value,
                "datatype":     "float",
                "timestamp":    datetime.utcnow(),
                "period":       daterange.key,
                "scope":        "user",
                "uid":          uid
            }).save()


def getAllUsersLanguageUsage(daterange):
    '''
    Returns dictionary mapping user ids to dictionaries that look like:
    {u'_id': 62298,
     u'languages': {u'bilingual': 5.0, u'hebrew': 9.0},
     u'total': 14.0}
    {u'_id': 59440, u'languages': {u'bilingual': 10.0}, u'total': 10.0}
    {u'_id': 60586, u'languages': {u'hebrew': 27.0}, u'total': 27.0}

    # https://stackoverflow.com/questions/25843255/mongodb-aggregate-count-on-multiple-fields-simultaneously
    '''

    pipeline = [
        {"$match": daterange.update_match({
            "secondary": False,
            "language": {"$in": ["hebrew", "english", "bilingual"]}
        })},
        {"$group": {
            "_id": {"language": "$language", "uid": "$uid"},
            "cnt": {"$sum": 1}}},
        {"$group": {
            "_id": "$_id.uid",
            "languages": {"$push": {"k": "$_id.language", "v": "$cnt"}},
            "total": {"$sum": "$cnt"}}},
        {"$project": {
            "languages": {"$arrayToObject": "$languages"},
            "total": "$total"}}
    ]
    results = db.user_history.aggregate(pipeline)
    return {d["_id"]: d for d in results}


def getAllUsersSheetUsage(daterange):
    pipeline = [
            {"$match": daterange.update_match({
                "secondary": False,
                "is_sheet": True
            })},
            {"$group": {
                "_id": "$uid",
                "cnt": {"$sum": 1}}}     # Sheet records never have num_times_read greater than 1.
        ]

    results = db.user_history.aggregate(pipeline)
    return {d["_id"]: d for d in results}


def getAllUsersCategories(daterange, category):
    pipeline = [
        {"$match": daterange.update_match({
            "secondary": False,
            "is_sheet": False,
            "categories.0": category
        })},
        {"$group": {
            "_id": "$uid",
             "cnt": { "$sum": {"$max": ["$num_times_read", 1]}}}}]
    results = db.user_history.aggregate(pipeline)
    return {d["_id"]: d for d in results}

def getAllUsersSheetCreation(daterange, publishedOnly=False):
    pipeline = [
        {"$match": daterange.update_match({
            "status": "public"
        }  if publishedOnly else {}, field="dateModified")}, # is this correct
        {"$group": {
            "_id": "$owner",
            "cnt": {"$sum": 1}}}     # Sheet records never have num_times_read greater than 1.
    ]
    results = db.sheets.aggregate(pipeline)
    return {d["_id"]: d for d in results}

def site_stats_data():
    top_categories = library.get_top_categories()

    d = {}
    for daterange in active_dateranges:
        d[daterange.key] = {"categoriesRead": {reverse_read_in_category_key(t.name): t.value
                            for t in TrendSet({"scope": "site", "period": daterange.key,
                                "name": {"$in": list(map(read_in_category_key, top_categories))}
                            })}}
    return d


def user_stats_data(uid):
    """

    :param uid: int or something cast-able to int
    :param start: datetime
    :param end: datetime
    :return:
    """
    from sefaria.model.story import Story
    from sefaria.sheets import user_sheets

    uid = int(uid)
    user_stats_dict = user_profile.public_user_data(uid)

    # All of user's sheets
    usheets = user_sheets(uid)["sheets"]
    usheet_ids = [s["id"] for s in usheets]

    for daterange in active_dateranges:
        # Sheet views in this period
        usheet_views = db.user_history.aggregate([
            {"$match": daterange.update_match({
                "is_sheet": True,
                "sheet_id": {"$in": usheet_ids},
                "uid": {"$ne": uid}
                })},
            {"$group": {
                "_id": "$sheet_id",
                "cnt": {"$sum": 1}}},
        ])

        most_popular_sheet_ids = [s["_id"] for s in sorted(usheet_views, key=lambda o: o["cnt"], reverse=True)[:3]]
        most_popular_sheets = []
        for sheet_id in most_popular_sheet_ids:
            most_popular_sheets += [s for s in usheets if s["id"] == sheet_id]

        sheets_this_period = [s for s in usheets if daterange.contains(datetime.strptime(s["created"], "%Y-%m-%dT%H:%M:%S.%f"))]

        # Refs I viewed
        refs_viewed = db.user_history.aggregate([
            {"$match": daterange.update_match({
                "uid": uid,
                "secondary": False,
                "is_sheet": False
                })},
            {"$group": {
                "_id": "$ref",
                "cnt": {"$sum": 1}}},  # Using $num_times_read isn't reliable.  It counts book views, but not text views.
        ])
        most_viewed_trefs = [s["_id"] for s in sorted(refs_viewed, key=lambda o: o["cnt"], reverse=True) if s["cnt"] > 1 and "Genesis 1" not in s["_id"]][:9]
        most_viewed_refs = [text.Ref(r) for r in most_viewed_trefs]
        most_viewed_ref_dicts = [{"en": r.normal(), "he": r.he_normal(), "book": r.index.title} for r in most_viewed_refs]

        # Sheets I viewed
        sheets_viewed = db.user_history.aggregate([
            {"$match": daterange.update_match({
                "uid": uid,
                "secondary": False,
                "is_sheet": True
                })},
            {"$group": {
                "_id": "$sheet_id",
                "cnt": {"$sum": 1}}},
        ])
        most_viewed_sheets_ids = [s["_id"] for s in sorted(sheets_viewed, key=lambda o: o["cnt"], reverse=True) if s["cnt"] > 1 and s["_id"] not in usheet_ids][:3]


        most_viewed_sheets = [Story.sheet_metadata(i, return_id=True) for i in most_viewed_sheets_ids]
        most_viewed_sheets = [a for a in most_viewed_sheets if a]

        for sheet_dict in most_viewed_sheets:
            sheet_dict.update(Story.publisher_metadata(sheet_dict["publisher_id"]))

        # Construct returned data
        top_categories = library.get_top_categories()
        user_stats_dict[daterange.key] = {
            "sheetsRead": user_profile.UserHistorySet(daterange.update_match({"is_sheet": True, "secondary": False, "uid": uid})).hits(),
            "textsRead": user_profile.UserHistorySet(daterange.update_match({"is_sheet": False, "secondary": False, "uid": uid})).hits(),
            "categoriesRead": {reverse_read_in_category_key(t.name): t.value for t in TrendSet({"uid":uid, "period": daterange.key, "name": {"$in": list(map(read_in_category_key, top_categories))}})},
            "totalSheets": len(usheets),
            "publicSheets": len([s for s in usheets if s["status"] == "public"]),
            "popularSheets": most_popular_sheets,
            "sheetsThisPeriod": len(sheets_this_period),
            "mostViewedRefs": most_viewed_ref_dicts,
            "mostViewedSheets": most_viewed_sheets
        }

    return user_stats_dict

# vv Needs thought / refactor vv
class TrendFactory(object):
    """
    Name
    DataType
    For Users [bool]
    For Network/Site [bool]
    """
    name = ""  # Name of trait / trend
    desc = ""  # Description of trait / trend
    datatype = ""   # int, str, bool, dict
    for_user = False   # bool
    for_group = False  # bool

    
    # to consider: Is a well defined period the way to go with these?
    def process_user(self, user_id, period):
        """
        f(user, period) -> val
        :param period:
        :return:
        """
        pass

    def process_user_network(self, uid, period):
        users = []  # get followed users
        return self._process_users(users, period)
        pass

    def process_comparable(self, period):
        pass

    def process_site(self, period):
        pass

    def _process_users(self, users, period):
        pass

class EnglishToleranceFactory(TrendFactory):
    name = "EnglishTolerance"
    desc = "Value between 0 and 1 - 1 Being clear English appreciation, 0 being clear English intolerance"
    datatype = "float"   # int, float, str, bool, dict
    for_user = True
    for_group = False

class HebrewAbilityFactory(TrendFactory):
    name = "HebrewAbility"
    desc = "Value between 0 and 1 - 1 Being clear Hebrew ability, 0 being clear inability"
    datatype = "float"   # int, float, str, bool, dict
    for_user = True
    for_group = False

class DateRefRange(object):
    def __init__(self, refRangeString, start, end, name):
        """
        hosts ref range and acceptable date parameters to help with determining whether a date/ref combination meets
        criteria for following a schedule
        :param start: datetime
        :param end: datetime
        """
        self.dateRange = DateRange(name, start, end)
        self.ref = Ref(refRangeString)

    def refIsInRange(self, ref, timestamp):
        """
        returns whether the ref criteria is met is filled and whether to continue checking refs against dateRefRange (bucket)
        """
        processedRef = Ref(ref)
        if self.dateRange.start > timestamp:
            return False, True
        elif self.dateRange.end < timestamp:
            return False, False
        elif processedRef.span_size()>=1 and processedRef.overlaps(self.ref): # does this need to be more precise?
                return True, True
        else:
            return False, False # in date range, not in ref range

        # if Ref(ref) in self.ref.all_segment_refs() and self.dateRange.start <= timestamp and self.dateRange.end >= timestamp:
        #     return True
        # else:
        #     return False

class ScheduleManager(object):
    def __init__(self, segmentHits, numberOfSegments, dateRangeEnd, varianceForward, varianceBack, name):
        """
        :param segmentHits: number of segment hits in the correct range that need to be met for user to be a schedule learner
        :param numberOfSegments: number of segments to check
        :param dateRangeEnd: date to work backwards from
        :param varianceForward: number of days after date we consider them as still learning the schedule item
        :param varianceBack: number of days before date we consider them as still learning the schedule item
        """
        self.name = name
        self.segmentHits = segmentHits
        if dateRangeEnd == None:
            self.dateRangeEnd = datetime.today()
        else:
            self.dateRangeEnd = dateRangeEnd
        self.numberOfSegments = numberOfSegments
        self.varianceForward = timedelta(varianceForward)
        self.varianceBack = timedelta(varianceBack)
        pass

    def createDateRefRanges(self):
        """
        retrieves necessary dateRefRanges for particular schedule to meet criteria
        """
        pass

    def getRelevantUserHistories(self):
        pass

    def getUsersWhoAreLearningSchedule(self):
        dateRefRangesTotal = len(self.dateRefRanges)
        # usersWhoAreLearningSchedule = []
        # usersWhoAreNotLearningSchedule = []
        for user in self.getRelevantUserHistories():
            index = 0
            refsInRangeCount = 0
            for history in user["history"]:
                keepCheckingRefAgainstBuckets = True
                while(index < dateRefRangesTotal and keepCheckingRefAgainstBuckets == True):
                    inRange, keepCheckingRefAgainstBuckets = self.dateRefRanges[index].refIsInRange(history["ref"], history["datetime"])
                    if inRange:
                        refsInRangeCount += 1
                        if refsInRangeCount >= self.segmentHits:
                            break
                    if keepCheckingRefAgainstBuckets:
                        index += 1
                if refsInRangeCount >= self.segmentHits:
                    Trend({
                        "name":         self.name,
                        "value":        True,
                        "datatype":     "bool",
                        "timestamp":    datetime.utcnow(),
                        "period":       "currently",
                        "scope":        "user",
                        "uid":          user["_id"]["uid"]
                    }).save()
                    break
                elif self.segmentHits - refsInRangeCount > dateRefRangesTotal - index:
                    break

class ParashaScheduleManager(ScheduleManager):
    #TODO: deal with haftara
    #TODO: make more efficient
    def __init__(self, segmentHits=2, numberOfSegments=4, dateRangeEnd=None, diaspora=True, varianceForward=14, varianceBack=7, name="ParashaLearner"):
        ScheduleManager.__init__(self, segmentHits, numberOfSegments, dateRangeEnd, varianceForward, varianceBack, name)
        self.diaspora = diaspora
        self.daysToAdd = 6
        self.dateRefRanges = self.createDateRefRanges()
        self.overallDateRange = DateRange("overall", self.dateRefRanges[-1].dateRange.start, self.dateRefRanges[0].dateRange.end)

    def createDateRefRanges(self):
        """
        Creates dateRefRanges for ParashaScheduleManager in reverse chronological order
        """
        query = {}
        query["date"] = {
            "$lte": self.dateRangeEnd + timedelta(days=self.daysToAdd)
        }
        query["diaspora"]=self.diaspora
        results = db.parshiot.find(query, limit=self.numberOfSegments).sort("date", -1)
        dateRefRanges = []
        for result in results:
            dateRefRanges.append(DateRefRange(result["ref"], result["date"] - self.varianceBack,  result["date"] + self.varianceBack, result["parasha"]))
        return dateRefRanges

    def getRelevantUserHistories(self):
        query = [{"$match": self.overallDateRange.update_match({
            "is_sheet": False,
            "book": self.dateRefRanges[0].ref.book
        })}, {"$sort": {"datetime": 1}}, {"$group": {"_id": {"uid": "$uid"}, 
        "history": {"$push": {"ref": "$ref", "datetime": "$datetime", "uid": "$uid"}}}}]
        print(query)
        return db.user_history.aggregate(query)

    def getUsersWhoAreLearningSchedule(self):
        ScheduleManager.getUsersWhoAreLearningSchedule(self)

def setScheduleTraits():
    scheduleManagers = [ParashaScheduleManager()]
    for scheduleManager in scheduleManagers:
        scheduleManager.getUsersWhoAreLearningSchedule()

def setAllTrends(skip=[]):
    print("setAllTrends")
    if "userSheet" not in skip:
        print("setUserSheetTraits")
        setUserSheetTraits()
    if "sheet" not in skip:
        print("setSheetTraits")
        setSheetTraits()
    if "userLanguage" not in skip:
        print("setUserLanguageTraits")
        setUserLanguageTraits()
    if "category" not in skip:
        print("setCategoryTraits")
        setCategoryTraits()
    if "schedule" not in skip:
        print("setScheduleTraits")
        setScheduleTraits()

```

### sefaria/model/notification.py

```
# -*- coding: utf-8 -*-

"""
notifications.py - handle user event notifications

Writes to MongoDB Collection: notifications
"""
import re
from datetime import datetime
import json

from django.template.loader import render_to_string

from . import abstract as abst
from . import user_profile
from sefaria.model.collection import Collection
from sefaria.utils.util import strip_tags
from sefaria.system.database import db
from sefaria.system.exceptions import InputError

import structlog
logger = structlog.get_logger(__name__)


class GlobalNotification(abst.AbstractMongoRecord):
    """
    "type" attribute can be: "index", "version", or "general"

    value of "content" attribute for type "general":
        "he"    : hebrew long description (optional)
        "en"    : english long description (optional)
    for type "index":
        "index" : title of index
        "he"    : hebrew long description (optional)
        "en"    : english long description (optional)
    for type "version":
        "index"     : title of index
        "version"   : version title
        "language"  : "en" or "he"
        "he"        : hebrew long description (optional)
        "en"        : english long description (optional)

    """
    collection   = 'global_notification'
    history_noun = 'global notification'

    required_attrs = [
        "type",
        "content",
        "date",
    ]

    optional_attrs = [
    ]

    def _normalize(self):
        from sefaria.model.text import library
        if self.type == "index" or self.type == "version":
            i = library.get_index(self.content.get("index"))
            self.content["index"] = i.title

    def _validate(self):
        from sefaria.model.text import library, Version
        if self.type == "index":
            assert self.content.get("index")
            assert library.get_index(self.content.get("index"))
        elif self.type == "version":
            i = self.content.get("index")
            v = self.content.get("version")
            l = self.content.get("language")

            assert i and v and l

            version = Version().load({
                "title": i,
                "versionTitle": v,
                "language": l,
            })
            assert version, "No Version Found: {}/{}/{}".format(i, v, l)
        elif self.type == "general":
            assert self.content.get("en"), "Please provide an English message."
            assert self.content.get("he"), "Please provide a Hebrew message."
        else:
            raise InputError("Unknown type for GlobalNotification: {}".format(self.type))

    def _init_defaults(self):
        self.content = {}
        self.type    = "general"
        self.date = datetime.now()

    @staticmethod
    def latest_id():
        n = db.global_notification.find_one({}, {"_id": 1}, sort=[["_id", -1]])
        if not n:
            return None
        return n["_id"]

    def make_index(self, index):
        self.type = "index"
        self.content["index"] = index.title
        return self

    def make_version(self, version):
        self.type = "version"
        self.content["version"] = version.versionTitle
        self.content["language"] = version.language
        self.content["index"] = version.title
        return self

    def set_date(self, date):
        self.date = date
        return self

    def set_he(self, msg):
        self.content["he"] = msg
        return self

    def set_en(self, msg):
        self.content["en"] = msg
        return self

    def contents(self):
        d = super(GlobalNotification, self).contents()
        d["_id"] = self.id
        d["date"] = d["date"].isoformat()

        return d

    def client_contents(self):
        n = super(GlobalNotification, self).contents(with_string_id=True)
        n["date"] = n["date"].timestamp()
        return n

    @property
    def id(self):
        return str(self._id)


class GlobalNotificationSet(abst.AbstractMongoSet):
    recordClass = GlobalNotification

    def __init__(self, query=None, page=0, limit=0, sort=[["_id", -1]]):
        super(GlobalNotificationSet, self).__init__(query=query, page=page, limit=limit, sort=sort)

    def register_for_user(self, uid):
        for global_note in self:
            Notification().register_global_notification(global_note, uid).save()

    def contents(self):
        return [n.contents() for n in self]

    def client_contents(self):
        return [n.client_contents() for n in self]


class Notification(abst.AbstractMongoRecord):
    collection   = 'notifications'
    history_noun = 'notification'

    required_attrs = [
        "type",
        "date",
        "uid",
        "content",
        "read",
        "is_global",
    ]
    optional_attrs = [
        "read_via",
        "global_id",
        "suspected_spam"
    ]

    def _init_defaults(self):
        self.read      = False
        self.read_via  = None
        self.type      = "unset"
        self.content   = {}
        self.date      = datetime.now()
        self.is_global = False

    def _set_derived_attributes(self):
        if not self.is_global:
            return

        gnote = GlobalNotification().load({"_id": self.global_id})
        if gnote is None:
            logger.error("Tried to load non-existent global notification: {}".format(self.global_id))
        else:
            self.content = gnote.content
            self.type    = gnote.type

    def register_global_notification(self, global_note, user_id):
        self.is_global  = True
        self.global_id  = global_note._id
        self.uid        = user_id
        self.type       = global_note.type
        self.date       = global_note.date
        return self

    @staticmethod
    def latest_global_for_user(uid):
        n = db.notifications.find_one({"uid": uid, "is_global": True}, {"_id": 1}, sort=[["_id", -1]])
        return n["_id"] if n else None

    def make_sheet_like(self, liker_id=None, sheet_id=None):
        """Make this Notification for a sheet like event"""
        self.type                = "sheet like"
        self.content["liker"]    = liker_id
        self.content["sheet_id"] = sheet_id
        return self

    def make_sheet_publish(self, publisher_id=None, sheet_id=None):
        self.type                 = "sheet publish"
        self.content["publisher"] = publisher_id
        self.content["sheet_id"]  = sheet_id
        return self

    def make_follow(self, follower_id=None):
        """Make this Notification for a new Follow event"""
        self.type                = "follow"
        self.content["follower"] = follower_id
        return self

    def make_collection_add(self, adder_id, collection_slug):
        """Make this Notification for being added to a collection"""
        self.type                       = "collection add"
        self.content["adder"]           = adder_id
        self.content["collection_slug"] = collection_slug
        return self

    def make_discuss(self, adder_id=None, discussion_path=None):
        """Make this Notification for a new note added to a conversation event"""
        self.type                         = "discuss"
        self.content["adder"]             = adder_id
        self.content["discussion_path"]   = discussion_path
        return self

    def mark_read(self, via="site"):
        self.read     = True
        self.read_via = via
        return self

    @property
    def id(self):
        return str(self._id)

    @property
    def actor_id(self):
        """The id of the user who acted in this notification"""
        keys = {
            "sheet like":     "liker",
            "sheet publish":  "publisher",
            "follow":         "follower",
            "collection add": "adder",
            "discuss":        "adder",
        }
        return self.content[keys[self.type]]

    def client_contents(self):
        """
        Returns contents of notification in format usable by client, including needed merged
        data from profiles, sheets, etc
        """
        from sefaria.sheets import get_sheet_metadata
        from sefaria.model.following import FollowRelationship

        n = super(Notification, self).contents(with_string_id=True)
        n["date"] = n["date"].timestamp()
        if "global_id" in n:
            n["global_id"] = str(n["global_id"])

        def annotate_user(n, uid):
            user_data = user_profile.public_user_data(uid)
            n["content"].update({
                "name":       user_data["name"],
                "profileUrl": user_data["profileUrl"],
                "imageUrl":   user_data["imageUrl"],
            })

        def annotate_sheet(n, sheet_id):
            sheet_data = get_sheet_metadata(id=sheet_id)
            n["content"]["sheet_title"] = strip_tags(sheet_data["title"], remove_new_lines=True)
            n["content"]["summary"] = sheet_data["summary"]

        def annotate_collection(n, collection_slug):
            try:
               c = Collection().load({"slug": collection_slug})
               n["content"]["collection_name"] = c.name
            except:
               c = Collection().load({"privateSlug": collection_slug})
               n["content"]["collection_name"] = c.name

        def annotate_follow(n, potential_followee):
            """
            Does `self.uid` follow `potential_followee`
            """
            relationship = FollowRelationship(follower=self.uid, followee=potential_followee)
            n["content"]["is_already_following"] = relationship.exists()

        if n["type"] == "sheet like":
            annotate_sheet(n, n["content"]["sheet_id"])
            annotate_user(n, n["content"]["liker"])

        elif n["type"] == "sheet publish":
            annotate_sheet(n, n["content"]["sheet_id"])
            annotate_user(n, n["content"]["publisher"])

        elif n["type"] == "follow":
            annotate_user(n, n["content"]["follower"])
            annotate_follow(n, n["content"]["follower"])

        elif n["type"] == "collection add":
            annotate_user(n, n["content"]["adder"])
            annotate_collection(n, n["content"]["collection_slug"])

        return n


class NotificationSet(abst.AbstractMongoSet):
    recordClass = Notification

    def __init__(self, query=None, page=0, limit=0, sort=[["date", -1]]):
        super(NotificationSet, self).__init__(query=query, page=page, limit=limit, sort=sort)

    def _add_global_messages(self, uid):
        """
        Add user Notification records for any new GlobalNotifications
        :return:
        """
        latest_id_for_user = Notification.latest_global_for_user(uid)
        latest_global_id = GlobalNotification.latest_id()
        if latest_global_id and (latest_global_id != latest_id_for_user):
            if latest_id_for_user is None:
                GlobalNotificationSet({}, limit=10).register_for_user(uid)
            else:
                GlobalNotificationSet({"_id": {"$gt": latest_id_for_user}}, limit=10).register_for_user(uid)

    def unread_for_user(self, uid):
        """
        Loads the unread notifications for uid.
        """
        # Add globals ...
        self._add_global_messages(uid)
        self.__init__(query={"uid": uid, "read": False})
        return self

    def unread_personal_for_user(self, uid):
        """
        Loads the unread notifications for uid.
        """
        self.__init__(query={"uid": uid, "read": False, "is_global": False, "suspected_spam": {'$in': [False, None]}})
        return self

    def recent_for_user(self, uid, page=0, limit=10):
        """
        Loads recent notifications for uid.
        """
        self._add_global_messages(uid)
        self.__init__(query={"uid": uid, "suspected_spam": {'$in': [False, None]}}, page=page, limit=limit)
        return self

    def mark_read(self, via="site"):
        """Marks all notifications in this set as read"""
        for notification in self:
            notification.mark_read(via=via).save()

    @property
    def unread_count(self):
        return len([n for n in self if not n.read])

    def actors_list(self):
        """Returns a unique list of user ids who acted in this notification set excluding likes"""
        return list(set([n.actor_id for n in self if not n.type == "sheet like"]))

    def actors_string(self):
        """
        Returns a nicely formatted string listing the people who acted in this notifcation set
        Likes are not included in this list (so that like only notifcations can generate a different email subject)
        Returns None if all actions are likes.
        """
        actors = [user_profile.user_name(id) for id in self.actors_list()]
        top, more = actors[:3], actors[3:]
        if len(more) == 1:
            top[2] = "2 others"
        elif len(more) > 1:
            top.append("%d others" % len(more))
        if len(top) > 1:
            top[-1] = "and " + top[-1]
        return ", ".join(top).replace(", and ", " and ")

    def like_count(self):
        """Returns the number of likes in this NotificationSet"""
        return len([n for n in self if n.type == "sheet like"])

    def client_contents(self):
        return [n.client_contents() for n in self]


def process_sheet_deletion_in_notifications(sheet_id):
    """
    When a sheet is deleted remove it from any collections.
    Note: this function is not tied through dependencies.py (since Sheet mongo model isn't generlly used),
    but is called directly from sheet deletion view in sourcesheets/views.py.
    """
    ns = NotificationSet({"content.sheet_id": sheet_id})
    ns.delete()

```

### sefaria/model/tests/index_offsets_by_depth_tests.py

```
import pytest
from sefaria.model import *
from sefaria.system.exceptions import InputError

@pytest.fixture()
def index_offsets_by_depth():
    return {
            '1': 4,
            '2': [0, 3, 6, 9],
            '3': [[0, 2], [2, 6], [8, 10], [12, 14]]
        }

@pytest.fixture()
def node(index_offsets_by_depth):
    node = JaggedArrayNode({'depth': 3,
                            'addressTypes': ['Integer', 'Talmud', 'Integer'],
                            'sectionNames': ['Chapter','Chapter', 'Chapter'],
                            'index_offsets_by_depth': index_offsets_by_depth
                            })
    node.add_primary_titles('test', '')
    return node

@pytest.fixture()
def false_node(node):
    node.index_offsets_by_depth['1'] = [5]
    return node

@pytest.fixture()
def another_false_node(node):
    node.index_offsets_by_depth['3'] = [1, 2, 5]
    return node

def test_node(node):
    node.validate()

def test_false_node(false_node):
    with pytest.raises(AssertionError):
        false_node.validate()

def test_another_false_node(another_false_node):
    with pytest.raises(TypeError):
        another_false_node.validate()

@pytest.fixture()
def index(node):
    index = Index({'title': 'test',
              'categories': ['Talmud'],
              'schema': node.serialize()})
    index.save()
    yield index
    index.delete()

@pytest.fixture()
def text():
    return [[['hello', 'world'], ['my', 'name']], [['is'], ['skippy']], [['and', 'I'], ['am', 'going']], [['to', 'sleep'], ['now', 'bye']]]

@pytest.fixture()
def version(index, text):
    v = Version({
        "language": 'en',
        "title": 'test',
        "versionSource": '',
        "versionTitle": 'test',
        "chapter": text
    })
    return v

def test_version(version):
    version._validate()
    version.chapter.append([['bar']])
    with pytest.raises(AssertionError):
        version._validate()

def test_invalid_version(version):
    version.chapter[0].append(['foo', 'bar'])
    with pytest.raises(AssertionError):
        version._validate()

@pytest.fixture()
def textchunck(text, index):
    t = Ref('test').text('en', 'test')
    t.text = text
    t.save()
    index.versionState().refresh()
    yield
    Version().load({'title': 'Test'}).delete()

def test_refs(textchunck, index, node):
    assert Ref('test 5').sections == [1]
    with pytest.raises(InputError):
        Ref('test 4')
    assert Ref('test 7:4b:15').sections == [3, 2, 5]
    with pytest.raises(InputError):
        Ref('test 10:1')
    assert Ref('test 5:1a:1-7:4b:15').toSections == [3, 2, 5]
    assert Ref('test 5:1a:1-7:4b:15').sections == [1, 1, 1]
    assert Ref(_obj={'index': index,
                 'book': 'Test',
                 'primary_category': 'Talmud',
                 'index_node': node,
                 'sections': [1, 1, 1],
                 'toSections': [3, 2, 5]}) == Ref('test 5:1a:1-7:4b:15')
    assert Ref('Test 5-6').all_segment_refs() == [Ref('Test 5:1a:1'), Ref('Test 5:1a:2'), Ref('Test 5:1b:3'), Ref('Test 5:1b:4'),  Ref('Test 6:2b:3'), Ref('Test 6:3a:7')]

def test_text(textchunck):
    assert Ref('Test 5').text('en').text == [['hello', 'world'], ['my', 'name']]
    assert Ref('Test 5:1b:4-7:4a:10').text('en').text == [[['name']], [['is'], ['skippy']], [['and', 'I']]]

def test_api(textchunck, index_offsets_by_depth):
    tf = TextFamily(Ref('Test'), pad=False)
    assert tf.contents()['index_offsets_by_depth'] == index_offsets_by_depth
    tf = TextFamily(Ref('Test 5:1b:4-7:4a:9'), pad=False)
    assert tf.contents()['index_offsets_by_depth'] == {
            '1': 4,
            '2': [0, 3, 6],
            '3': [[2], [2, 6], [8]]
        }
    tf = TextFamily(Ref('Test 6:2b:7-7:4a:9'), pad=False)
    assert tf.contents()['index_offsets_by_depth'] == {
            '1': 4,
            '2': [3, 6],
            '3': [[2, 6], [8]]
        }
    tf = TextFamily(Ref('Test 6:3a:7-7:4a:9'), pad=False)
    assert tf.contents()['index_offsets_by_depth'] == {
            '1': 4,
            '2': [3, 6],
            '3': [[6], [8]]
        }

```

### sefaria/model/tests/index_test.py

```
# coding=utf-8
import pytest

from sefaria.model import *
from sefaria.model.text import prepare_index_regex_for_dependency_process

def test_index_regex():
    assert Ref('Otzar Midrashim').regex() == prepare_index_regex_for_dependency_process(library.get_index('Otzar Midrashim'))
    assert Ref('Zohar').regex() == prepare_index_regex_for_dependency_process(library.get_index('Zohar'))
    assert Ref('Genesis').regex() == prepare_index_regex_for_dependency_process(library.get_index('Genesis'))
    assert Ref('Rashi on Exodus').regex() == prepare_index_regex_for_dependency_process(library.get_index('Rashi on Exodus'))

    assert Ref('Otzar Midrashim').regex(as_list=True) == prepare_index_regex_for_dependency_process(
        library.get_index('Otzar Midrashim'), as_list=True)
    assert Ref('Zohar').regex(as_list=True) == prepare_index_regex_for_dependency_process(library.get_index('Zohar'), as_list=True)
    assert Ref('Genesis').regex(as_list=True) == prepare_index_regex_for_dependency_process(library.get_index('Genesis'), as_list=True)
    assert Ref('Rashi on Exodus').regex(as_list=True) == prepare_index_regex_for_dependency_process(
        library.get_index('Rashi on Exodus'), as_list=True)

```

### sefaria/model/tests/virtual_text_tests.py

```
# -*- coding: utf-8 -*-

"""
Tests for virtual texts, like dictionries
Handling virtual schema nodes
Refs pointing to virtual schemas, etc

"""

from sefaria.model import *


class Test_VirtualRefs(object):

    def test_identity(self):
        assert Ref('Jastrow,_.1').url() == "Jastrow,_.1"

    def test_parse_of_base(self):
        r = Ref('Jastrow')

    def test_parse_of_section(self):
        r = Ref('Jastrow, ')
        assert r.sections == []
        assert r.index_node.full_title("en") == "Jastrow, "
        assert r.normal() == "Jastrow, "

    def test_section_identity(self):
        r1 = Ref('Jastrow, ')
        r2 = Ref('Jastrow,_')
        assert r1 == r2

    def test_parse_of_segment(self):
        r = Ref('Jastrow,  1')
        assert r.sections == [1]
        assert r.index_node.full_title("en") == "Jastrow, "
        assert r.normal() == 'Jastrow,  1'

    def test_segment_identity(self):
        r1 = Ref('Jastrow,  1')
        r2 = Ref('Jastrow,_.1')
        assert r1 == r2
```

### sefaria/model/tests/note_test.py

```
# -*- coding: utf-8 -*-
import pytest
import sefaria.model as m
from sefaria.system.exceptions import InputError


class Test_Note(object):

    def test_save(self):
        n = m.Note({
            "text": "what a beautiful moment <333",
            "public": False,
            "owner": 1,
            "ref": "Genesis 48:11",
            "type": "note"
        })
        n.text = 'Seemingly ok note... <a href="javascript:alert(8007)">Click me</a>'
        n.save()
        assert n.text == 'Seemingly ok note... <a>Click me</a>'
        n.delete()

```

### sefaria/model/tests/link_test.py

```
# -*- coding: utf-8 -*-
import pytest
from sefaria.model import *
from sefaria.system.exceptions import DuplicateRecordError

class Test_Link_Save(object):

    @classmethod
    def setup_class(cls):
        LinkSet({"generated_by": "link_tester"}).delete()
        LinkSet({"refs": ["Avi Ezer, Deuteronomy 10:16:1", "Deuteronomy 10:16"]}).delete()
        Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "commentary",
                     "refs": ["Avi Ezer, Deuteronomy 10:16:1", "Deuteronomy 10:16"]}).save()

    @classmethod
    def teardown_class(cls):
        LinkSet({"generated_by": "link_tester"}).delete()

    def test_duplicate_links(self):
        link = Link({"auto": True,
                     "generated_by" : "link_tester",
                     "type" : "commentary",
                     "refs": ["Avi Ezer, Deuteronomy 10:16:1", "Deuteronomy 10:16"]})
        with pytest.raises(DuplicateRecordError) as e_info:
            link._pre_save()

    def test_duplicate_links_reversed_order(self):
        link = Link({"auto": True,
                     "generated_by" : "link_tester",
                     "type" : "commentary",
                     "refs": ["Deuteronomy 10:16", "Avi Ezer, Deuteronomy 10:16:1"]})
        with pytest.raises(DuplicateRecordError) as e_info:
            link._pre_save()

    def test_more_precise_links(self):
        link = Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "commentary",
                     "refs": ["Avi Ezer, Deuteronomy 10:16:1", "Deuteronomy 10"]})
        with pytest.raises(DuplicateRecordError) as e_info:
            link._pre_save()
        assert "A more precise link already exists: {} - {}".format("Avi Ezer, Deuteronomy 10:16:1", "Deuteronomy 10:16") in str(e_info.value)

    def test_more_precise_links_reversed_order(self):
        link = Link({"auto": True,
                     "generated_by" : "link_tester",
                     "type" : "commentary",
                     "refs": ["Deuteronomy 10", "Avi Ezer, Deuteronomy 10:16:1"]})
        with pytest.raises(DuplicateRecordError) as e_info:
            link._pre_save()
            assert "A more precise link already exists: {} - {}".format("Avi Ezer, Deuteronomy 10:16:1", "Deuteronomy 10:16") in str(e_info.value)

    def test_override_preciselink(self):
        link1 = Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "quotation_auto_tanakh",
                     "refs": ["Psalms 1:1", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        link1.save()
        l1 = Link().load({"refs": ["Psalms 1:1", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        assert l1
        link2 = Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "quotation_auto_tanakh",
                     "refs": ["Psalms 1:1-6", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        with pytest.raises(DuplicateRecordError) as e_info:
            link2.save()
            assert "A more precise link already exists: {} - {}".format("Psalms 1:1", "Psalms 1") in str(e_info.value)

        link2._override_preciselink = True
        link2.save()
        l2 = Link().load({"refs": ["Psalms 1:1-6", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        assert l2
        l2.delete()
        l2 = Link().load({"refs": ["Psalms 1:1-6", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        assert not l2
        l1 = Link().load({"refs": ["Psalms 1:1", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        assert not l1

    def test_ranged_link_when_section_link_exists(self):
        link1 = Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "quotation_auto_tanakh",
                     "refs": ["Psalms 1", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        link1.save()
        l1 = Link().load({"refs": ["Psalms 1", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        assert l1
        link2 = Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "quotation_auto_tanakh",
                     "refs": ["Psalms 1:1-6", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        with pytest.raises(DuplicateRecordError) as e_info:
            link2.save()
        l1.delete()

    def test_section_link_when_ranged_link_exists(self):
        link1 = Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "quotation_auto_tanakh",
                     "refs": ["Psalms 1:1-6", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        link1.save()
        l1 = Link().load({"refs": ["Psalms 1:1-6", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        assert l1
        link2 = Link({"auto": True,
                     "generated_by": "link_tester",
                     "type": "quotation_auto_tanakh",
                     "refs": ["Psalms 1", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        with pytest.raises(DuplicateRecordError) as e_info:
            link2.save()
        l1.delete()

    def test_section_link_when_ranged_link_exists_reverse(self):
        link1 = Link({"auto": True,
                      "generated_by": "link_tester",
                      "type": "quotation_auto_tanakh",
                      "refs": ['Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1', "Psalms 1:1-6"]})
        link1.save()
        l1 = Link().load({"refs": ["Psalms 1:1-6", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        assert l1
        link2 = Link({"auto": True,
                      "generated_by": "link_tester",
                      "type": "quotation_auto_tanakh",
                      "refs": ["Psalms 1", 'Siddur Ashkenaz, Weekday, Maariv, Vehu Rachum 1']})
        with pytest.raises(DuplicateRecordError) as e_info:
            link2.save()
        l1.delete()

    def test_section_ref_in_default_node(self):
        for ref in ['Genesis 1', 'Zechariah 1']:
            link = Link({"auto": True,
                     "generated_by": "link_tester",
                      "type": "quotation_auto_tanakh",
                      "refs": [ref, "Ramban on Genesis 2:1"]})
            assert link._pre_save() == None

```

### sefaria/model/tests/terms_test.py

```
# -*- coding: utf-8 -*-

import pytest
from sefaria.model import *
from sefaria.system.exceptions import InputError

class Test_Terms_Validation(object):
    @classmethod
    def setup_class(cls):
        TermSet({"scheme": "testing_terms"}).delete()

    @classmethod
    def teardown_class(cls):
        TermSet({"scheme": "testing_terms"}).delete()

    def test_existing_term(self):
        Term().load({"name": 'Bereshit'}).title_group.validate()
        Term().load({"name": 'Rashi'}).title_group.validate()
        Term().load({"name": 'Torah'}).title_group.validate()
        Term().load({"name": 'Verse'}).title_group.validate()

    def test_load_by_non_primary_title(self):
        assert Term().load_by_title('Nachmanides') is not None
        assert Term().load_by_title('  ') is not None

    def test_add_duplicate_primary(self):
        with pytest.raises(InputError):
            term = Term({
                "name": "Test Dup Primary",
                "scheme": "testing_terms",
                "titles": [
                    {
                        "lang": "en",
                        "text": "Test Dup Primary",
                        "primary": True
                    },
                    {
                        "lang": "he",
                        "text": " ",
                        "primary": True
                    },
                    {
                        "lang": "en",
                        "text": "Test Dup Primary",
                    },
                    {
                        "lang": "he",
                        "text": " ",
                    }
                ]
            })
            term.save()

    def test_add_new_term(self):
        term = Term({
            "name"   : "Test One",
            "scheme" : "testing_terms",
            "titles" : [
                {
                    "lang": "en",
                    "text": "Test One",
                    "primary": True
                },
                {
                    "lang": "he",
                    "text": "",
                    "primary": True
                }
            ]
        })
        term.save()

        Term({
            "name": "Test Two",
            "scheme": "testing_terms",
            "titles": [
                {
                    "lang": "en",
                    "text": "Test Two",
                    "primary": True,
                    "presentation" : "alone"
                },
                {
                    "lang": "he",
                    "text": " ",
                    "primary": True,
                    "presentation": "alone"
                }
            ]
        }).save()

    def test_duplicate_terms(self):
        with pytest.raises(InputError):
            Term({
                "scheme": "commentary_works",
                "titles": [
                    {
                        "lang": "en",
                        "text": "Ramban",
                        "primary": True
                    },
                    {
                        "lang": "he",
                        "text": "\"",
                        "primary": True
                    },
                ],
                "name": "Ramban"
            }).save()

        with pytest.raises(InputError):
            Term({
                "scheme": "commentary_works",
                "titles": [
                    {
                        "lang": "en",
                        "text": "New Ramban",
                        "primary": True
                    },
                    {
                        "lang": "en",
                        "text": "Ramban",
                    },
                    {
                        "lang": "he",
                        "text": "\" ",
                        "primary": True
                    },
                ],
                "name": "New Ramban"
            }).save()

        with pytest.raises(InputError):
            Term({"name" : "Parashat Nitzavim",
                "titles" : [
                    {
                        "lang" : "en",
                        "text" : "Parashat Nitzavim",
                        "primary" : True
                    },
                    {
                        "lang" : "he",
                        "text" : "",
                        "primary" : True
                    },
                    {
                        "lang" : "en",
                        "text" : "Nitzavim"
                    },
                    {
                        "lang" : "he",
                        "text" : " "
                    }
                ],
                "scheme" : "Parasha"}).save()

    def test_add_invalid_terms(self):
        with pytest.raises(InputError): # no heb title at all
            Term({
                "name": "Test Fail One",
                "scheme": "testing_terms",
                "titles": [
                    {
                        "lang": "en",
                        "text": "Test Fail One",
                        "primary": True
                    }
                ]
            }).save()

        with pytest.raises(InputError):
            Term({
                "name": "Test Fail Two", # no primaries
                "scheme": "testing_terms",
                "titles": [
                    {"lang": "en", "text": "Test Fail Two"},
                    {"lang": "he", "text": ""}
                ]
            }).save()

        with pytest.raises(InputError):
            Term({
                "name": "Test Fail-Three", # hyphen in 'en' primary
                "scheme": "testing_terms",
                "titles" : [
                    {
                        "lang": "en",
                        "text": "Test Fail-Three",
                        "primary": True
                    },
                    {
                        "lang": "he",
                        "text": "",
                        "primary": True
                    }
                ]
            }).save()

        with pytest.raises(InputError):
            Term({
                "name": "Test Fail Four", # extra attr
                "scheme": "testing_terms",
                "titles" : [
                    {
                        "lang": "en",
                        "text": "Test Fail Four",
                        "primary": True,
                        "junkattr": "great"
                    },
                    {
                        "lang": "he",
                        "text": "",
                        "primary": True
                    }
                ]
            }).save()

        with pytest.raises(InputError):
            Term({
                "name": "Test Fail Five", # name not the same as primary
                "scheme": "testing_terms",
                "titles" : [
                    {
                        "lang": "en",
                        "text": "alalalalalala",
                        "primary": True
                    },
                    {
                        "lang": "he",
                        "text": "",
                        "primary": True
                    }
                ]
            }).save()

        # for ascii validation
        with pytest.raises(InputError):
            Term({
                "name": "Test Fail Six\u2019", # primary contains non ascii
                "scheme": "testing_terms",
                "titles" : [
                    {
                        "lang": "en",
                        "text": "Test Fail Six\u2019",
                        "primary": True
                    },
                    {
                        "lang": "he",
                        "text": "",
                        "primary": True
                    }
                ]
            }).save()


```

### sefaria/model/tests/marked_up_text_chunk.py

```
from copy import deepcopy

import pytest
from sefaria.system.database import db as mongo_db
from sefaria.model.marked_up_text_chunk import MarkedUpTextChunk
from sefaria.system.exceptions import DuplicateRecordError, InputError
from sefaria.model.text import Ref
pytestmark = pytest.mark.django_db

MARKED_UP_TEXT_CHUNKS_DATA = [
    {
        "ref": "Rashi on Genesis 1:6:1",
        "versionTitle": "Pentateuch with Rashi's commentary by M. Rosenbaum and A.M. Silbermann, 1929-1934",
        "language": "en",
        "spans": [
            {
                "charRange": [319, 337],
                "text": "Genesis Rabbah 4:2",
                "type": "citation",
                "ref": "Bereshit Rabbah 4:2",
            },
            {
                "charRange": [399, 408],
                "text": "Job 26:11",
                "type": "citation",
                "ref": "Job 26:11",
            },
            {
                "charRange": [543, 552],
                "text": "Job 26:11",
                "type": "citation",
                "ref": "Job 26:11",
            }
        ],
    },
    {
        "ref": "Rashi on Genesis 1:1:1",
        "versionTitle": "Pentateuch with Rashi's commentary by M. Rosenbaum and A.M. Silbermann, 1929-1934",
        "language": "en",
        "spans": [
            {
                "charRange": [912, 939],
                "text": "Yalkut Shimoni on Torah 187",
                "type": "citation",
                "ref": "Yalkut Shimoni on Torah 187",
            }
        ],
    },
    {
        "ref": "Rashi on Genesis 2:7:1",
        "versionTitle": "Pentateuch with Rashi's commentary by M. Rosenbaum and A.M. Silbermann, 1929-1934",
        "language": "en",
        "spans": [
            {
                "charRange": [361, 387],
                "text": "Midrash Tanchuma, Tazria 1",
                "type": "citation",
                "ref": "Midrash Tanchuma, Tazria 1",
            }
        ],
    },
]


def _aggregate_chunks(chunks: list[dict]) -> list[dict]:
    """
    Aggregates MarkedUpTextChunk payloads by their primary key fields.
    If multiple chunks share the same key,
    their 'spans' lists are merged.
    The key fields are dynamically read from the model's pkeys list.
    """
    pkeys = MarkedUpTextChunk.pkeys
    merged: dict[tuple, dict] = {}

    for chunk in chunks:
        key = tuple(chunk[field] for field in pkeys)
        if key not in merged:
            merged[key] = deepcopy(chunk)
        else:
            merged[key]["spans"].extend(deepcopy(chunk["spans"]))

    return list(merged.values())


# ---------------------------------------------------------------------------#
# Fixture: load  yield  cleanup (identical pattern to Topic graph tests)   #
# ---------------------------------------------------------------------------#
@pytest.fixture(scope="module")
def marked_up_chunks():
    """
    Prepare a clean set of MarkedUpTextChunk records in Mongo,
    then yield them for the tests, then delete them afterwards.
    """
    # 1) Start with a clean slate for the PKs we care about
    for c in MARKED_UP_TEXT_CHUNKS_DATA:
        mongo_db.marked_up_text_chunks.delete_many(
            {"ref": c["ref"], "versionTitle": c["versionTitle"], "language": c["language"]}
        )

    # 2) Insert merged (PK-unique) payloads
    objs = []
    payloads = _aggregate_chunks(MARKED_UP_TEXT_CHUNKS_DATA)
    for data in payloads:
        obj = MarkedUpTextChunk(data)
        obj.save()  # validation & normalisation happen inside .save()
        objs.append(obj)

    yield {
        "objects": objs,     # the live objects we saved
        "payloads": payloads # the canonical input they were built from
    }

    # 3) Tear-down  remove every object we created
    for o in objs:
        o.delete()

class TestMarkedUpTextChunk:
    def test_inserted_records_match_input(self, marked_up_chunks):
        objs  = marked_up_chunks["objects"]
        input = { (p["ref"], p["versionTitle"], p["language"]): p for p in marked_up_chunks["payloads"] }

        for obj in objs:
            k = (obj.ref, obj.versionTitle, obj.language)
            p = input[k]

            assert obj.ref == p["ref"]
            assert obj.versionTitle == p["versionTitle"]
            assert obj.language == p["language"]
            # normalisation: .ref and every span['ref'] are .normal()d
            assert obj.ref == Ref(p["ref"]).normal()
            assert {s["ref"] for s in obj.spans} == {Ref(s["ref"]).normal() for s in p["spans"]}
            # spans preserved (order-agnostic)
            assert len(obj.spans) == len(p["spans"])

    def test_primary_key_uniqueness(self, marked_up_chunks):
        dup_payload = deepcopy(marked_up_chunks["payloads"][0])
        with pytest.raises(DuplicateRecordError):
            MarkedUpTextChunk(dup_payload).save()

    def test_incorrect_text_span(self, marked_up_chunks):
        marked_up_chunk = marked_up_chunks["objects"][0]
        for span in marked_up_chunk.spans:
            span["text"] = "incorrect text"
        with pytest.raises(InputError):
            marked_up_chunk.save()

    def test_empty_spans(self, marked_up_chunks):
        marked_up_chunk = marked_up_chunks["objects"][0]
        marked_up_chunk.spans = []
        with pytest.raises(InputError):
            marked_up_chunk.save()

    def test_validation_failure(self, marked_up_chunks):
        """
        Invalid language  InputError
        """
        invalid_language_payload = deepcopy(marked_up_chunks["payloads"][0])
        invalid_language_payload["language"] = "fr"
        with pytest.raises(InputError):
            MarkedUpTextChunk(invalid_language_payload).save()

```

### sefaria/model/tests/schema_test.py

```
# -*- coding: utf-8 -*-

import pytest
from sefaria.model import *
import re
from sefaria.system.exceptions import InputError, BookNameError



def setup_module(module):
    global root
    root = SchemaNode()
    root.add_title("Orot", "en", primary=True)
    root.add_title("", "he", primary=True)
    root.key = "Orot"

    part1 = SchemaNode()
    part1.add_title(" ", "he", primary=True)
    part1.add_title("Lights from Darkness", "en", primary=True)
    part1.key = "from darkness"

    part2 = JaggedArrayNode()
    part2.add_title("  ", "he", primary=True)
    part2.add_title("The Process of Ideals in Israel", "en", primary=True)
    part2.depth = 1
    part2.lengths = [6]
    part2.sectionNames = ["Chapter"]
    part2.addressTypes = ["Integer"]
    part2.key = "ideals"

    part3 = JaggedArrayNode()
    part3.add_title("", "he", primary=True)
    part3.add_title("Seeds", "en", primary=True)
    part3.depth = 1
    part3.lengths = [8]
    part3.sectionNames = ["Chapter"]
    part3.addressTypes = ["Integer"]
    part3.key = "seeds"

    part4 = JaggedArrayNode()
    part4.add_title(" ", "he", primary=True)
    part4.add_title("Lights of Israel", "en", primary=True)
    part4.depth = 1
    part4.lengths = [9]
    part4.sectionNames = ["Chapter"]
    part4.addressTypes = ["Integer"]
    part4.key = "israel"

    root.append(part1)
    root.append(part2)
    root.append(part3)
    root.append(part4)

    # Part 1

    part1_subsections = [
        [" ","Land of Israel", 8],
        ["", "War", 10],
        [" ", "Israel and its Rebirth", 32],
        [" ", "Lights of Rebirth", 72],
    ]

    for sub in part1_subsections:
        n = JaggedArrayNode()
        n.key = sub[1]
        n.add_title(sub[1], "en", primary=True)
        n.add_title(sub[0], "he", primary=True)
        n.depth = 2
        n.lengths = [sub[2]]
        n.sectionNames = ["Chapter", "Paragraph"]
        n.addressTypes = ["Integer", "Integer"]
        n.append_to(part1)
    n = JaggedArrayNode()
    n.key = "Great Calling"
    n.add_title("Great Calling", "en", primary=True)
    n.add_title(" ", "he", primary=True)
    n.depth = 1
    n.sectionNames = ["Paragraph"]
    n.addressTypes = ["Integer"]
    n.append_to(part1)


    # Part 2
    part2_subsections = [
        ["    ", "The Godly and the National Ideal in the Individual"],
        ["    ", "The Godly and the National Ideal in Israel"],
        ["    ", "Dissolution of Ideals"],
        [" ", "The Situation in Exile"],
        ["   .  .    ", "The First and Second Temples; Religion"],
        ["     ,   ", "Unification of Ideals"]
    ]
    for sub in part2_subsections:
        n = JaggedArrayNode()
        n.key = sub[1]
        n.add_title(sub[1], "en", primary=True)
        n.add_title(sub[0], "he", primary=True)
        n.depth = 1
        n.sectionNames = ["Paragraph"]
        n.addressTypes = ["Integer"]
        n.append_to(part2)

    # Part 3 -
    part3_subsections = [
        ["  ", "Thirst for the Living God"],
        ["  ", "The Wise is Preferable to Prophet"],
        ["   ", "The Souls of the World of Chaos"],
        [" ", "Acts of Creation"],
        [" ", "Suffering Cleanses"],
        ["  ", "The War of Ideas"],
        ["  ", "National Soul and Body"],
        [" ", "The Value of Rebirth"]
    ]

    for sub in part3_subsections:
        n = JaggedArrayNode()
        n.key = sub[1]
        n.add_title(sub[1], "en", primary=True)
        n.add_title(sub[0], "he", primary=True)
        n.depth = 1
        n.sectionNames = ["Paragraph"]
        n.addressTypes = ["Integer"]
        n.append_to(part3)

    # Part 4
    part4_subsections = [
        ["    ", "The Essence of Israel", 16],
        ["   ", "The Individual and the Collective", 8],
        ["   ", "Connection to the Collective", 11],
        [" ", "Love of Israel", 10],
        ["  ", "Israel and the Nations", 16],
        [" ", "Nationhood of Israel", 9],
        ["   ", "Israel's Soul and its Rebirth", 19],
        [" ", "Preciousness of Israel", 9],
        [" ", "Holiness of Israel", 9]
    ]

    for sub in part4_subsections:
        n = JaggedArrayNode()
        n.key = sub[1]
        n.add_title(sub[1], "en", primary=True)
        n.add_title(sub[0], "he", primary=True)
        n.depth = 1
        n.lengths = [sub[2]]
        n.sectionNames = ["Subsection"]
        n.addressTypes = ["Integer"]
        n.append_to(part4)

    root.validate()


def test_relationships():
    assert root.first_child().first_child() is root.first_leaf()
    assert root.last_child().last_child() is root.last_leaf()
    assert root.first_child().next_sibling().prev_sibling() is root.first_child()
    assert root.first_child().last_child().next_leaf() is root.first_child().next_sibling().first_child()
    assert root.first_child().next_sibling().first_child().prev_leaf() is root.first_child().last_child()

    assert root.first_child().prev_sibling() is None
    assert root.last_child().next_sibling() is None
    assert root.first_leaf().prev_sibling() is None
    assert root.last_leaf().next_sibling() is None


def test_ancestors():
    assert root.last_leaf().ancestors() == [root, root.last_child()]


def test_text_index_map():
    def tokenizer(s):
        s = re.sub(r'<.+?>','',s).strip()
        return re.split(r'\s+', s)


    nodes = library.get_index("Megillat Taanit").nodes
    index_list, ref_list = nodes.text_index_map(tokenizer=tokenizer)
    assert index_list[1] == 9
    assert index_list[2] == 20
    assert index_list[5] == 423

    #now let's get serious. run text_index_map and check for rand_inds that each ref at that ind matches the corresponding indices in index_list
    index = library.get_index("Otzar Midrashim")
    nodes = index.nodes
    index_list, ref_list = nodes.text_index_map(tokenizer=tokenizer)
    mes_list = index.nodes.traverse_to_list(
        lambda n, _: TextChunk(n.ref(), "he").ja().flatten_to_array() if not n.children else [])
    mes_str_array = [w for seg in mes_list for w in tokenizer(seg)]

    rand_inds = [1,20,45,1046,len(index_list)-2]
    for ri in rand_inds:
        assert ' '.join(tokenizer(ref_list[ri].text("he").text)) == ' '.join(mes_str_array[index_list[ri]:index_list[ri+1]])

    index = library.get_index("Genesis")
    nodes = index.nodes
    index_list, ref_list = nodes.text_index_map(tokenizer=tokenizer, lang="he", vtitle="Tanach with Text Only")
    mes_list = index.nodes.traverse_to_list(
        lambda n, _: TextChunk(n.ref(), lang="he", vtitle="Tanach with Text Only").ja().flatten_to_array() if not n.children else [])
    mes_str_array = [w for seg in mes_list for w in tokenizer(seg)]

    rand_inds = [1, 20, 245, len(index_list)-2]
    for ri in rand_inds:
        assert ' '.join(tokenizer(ref_list[ri].text(lang="he",vtitle="Tanach with Text Only").text)) == ' '.join(mes_str_array[index_list[ri]:index_list[ri+1]])


def test_ja_node_with_hyphens():
    node = JaggedArrayNode()
    node.add_primary_titles('Title with-this', '')
    node.add_structure(['Something'])
    with pytest.raises(InputError):
        node.validate()

def test_ja_node_without_primary():
    node = JaggedArrayNode()
    node.add_title('Title with this', 'en')
    node.add_title('', 'he')
    node.add_structure(['Something'])
    with pytest.raises(InputError):
        node.validate()

def test_non_ascii():
    node = JaggedArrayNode()
    node.add_primary_titles('Title with this\u2019', '')
    node.add_structure(['Something'])
    with pytest.raises(InputError):
        node.validate()


@pytest.mark.deep
def test_nodes_missing_content():
    # check a known simple text and complex text
    assert library.get_index("Job").nodes.nodes_missing_content() == (False, [])
    assert library.get_index("Pesach Haggadah").nodes.nodes_missing_content() == (False, [])

    # construct a more complex schema. First, ensure that the index does not exist in the system
    try:
        test_index = library.get_index("test text")
        test_index.delete()
    except BookNameError:
        pass

    root_node = SchemaNode()
    root_node.add_primary_titles('test text', '')
    middle1 = SchemaNode()
    middle1.add_primary_titles('mid1', '1')
    for i in range(1, 4):
        leaf = JaggedArrayNode()
        leaf.add_primary_titles('leaf{}'.format(i), '{}'.format(i))
        leaf.add_structure(["Verse"])
        middle1.append(leaf)
    root_node.append(middle1)
    middle2 = SchemaNode()
    middle2.add_primary_titles('mid2', '2')
    for i in range(4, 6):
        leaf = JaggedArrayNode()
        leaf.add_primary_titles('leaf{}'.format(i), '{}'.format(i))
        leaf.add_structure(["Verse"])
        middle2.append(leaf)
    root_node.append(middle2)
    root_node.validate()
    test_index = Index({
        'title': 'test text',
        'categories': ['Other'],
        'schema': root_node.serialize()
    })
    test_index.save()

    # add text
    chunk = Ref('test text, mid1, leaf1').text('en', 'test version')
    chunk.text = ['Lorem ipsum']
    chunk.save()

    result = test_index.nodes.nodes_missing_content()
    assert result[0] is False
    assert len(result[1]) == 3
    test_index.delete()

# Todo parametrize for all address types
def test_folio_type():
    folio = schema.AddressFolio(1)
    for i in [1,2,3,4,5,6,7,15,23,64,128]:
        assert folio.toNumber("en", folio.toStr("en", i)) == i


class TestDefaultNodeWithChildren:
    @classmethod
    def setup_class(cls):
        root_node = SchemaNode()
        root_node.add_primary_titles('test text', '')
        leaf = JaggedArrayNode()
        leaf.add_primary_titles('leaf', '')
        leaf.add_structure(["Verse"])
        root_node.append(leaf)
        cls.test_index = Index({
            'title': 'test text',
            'categories': ['Tanakh'],
            'schema': root_node.serialize()
        })
        cls.test_index.save()
        cls.test_version = Version(
            {
                "chapter": cls.test_index.nodes.create_skeleton(),
                "versionTitle": "Version TEST",
                "versionSource": "blabla",
                "language": "en",
                "title": cls.test_index.title
            }
        )
        cls.test_version.save()

    @classmethod
    def teardown_class(cls):
        cls.test_index.delete()
        cls.test_version.delete()

    def test_default_node_with_children(self):
        from sefaria.helper.schema import insert_last_child, prepare_ja_for_children
        ja_parent = JaggedArrayNode()
        ja_parent.key = "default"
        ja_parent.default = True
        ja_parent.add_structure(["Part", "Perek"])
        insert_last_child(ja_parent, self.test_index.nodes)
        prepare_ja_for_children(ja_parent)

        # make sure prepare_ja_for_children worked
        v = Version().load({"title": self.test_index.title, "versionTitle": self.test_version.versionTitle})
        assert v.chapter['default'] == {}

        ja_leaf = JaggedArrayNode()
        ja_leaf.add_primary_titles('leaf2', '2')
        ja_leaf.add_structure(["Perek"])
        insert_last_child(ja_leaf, ja_parent)

        i = Index().load({"title": self.test_index.title})
        assert i.nodes.children[-1].children[-1].nodeType == "JaggedArrayNode"

        # verify that you can refer to leaf2 by either name or number
        assert Ref(f"{self.test_index.title}, leaf2") == Ref(f"{self.test_index.title} 1")


```

### sefaria/model/tests/story_test.py

```
"""
Adding as a stub for future tests, kayn yirbu.
"""
```

### sefaria/model/tests/autospell_test.py

```
# -*- coding: utf-8 -*-


# This is failing because of the Postgres access happening in build_full_auto_completer()
# E   RuntimeError: Database access not allowed, use the "django_db" mark, or the "db" or "transactional_db" fixtures to enable it.
# todo: Add the right mark to get the module allowed

"""

import pytest
from sefaria.model import *


def setup_module(module):
    library.get_toc_tree()
    library.build_full_auto_completer()
    library.build_lexicon_auto_completers()
    library.build_cross_lexicon_auto_completer()


class Test_Complete_Method(object):
    # Does limit return exactly the right number of results?
    @pytest.mark.parametrize("ac,search", [
        (library.full_auto_completer("en"), "cor"),
        (library.full_auto_completer("he"), ""),
        (library.cross_lexicon_auto_completer(), "")
    ])
    @pytest.mark.parametrize("limit", [5, 10, 15])
    def test_limits(self, ac, search, limit):
        assert len(ac.complete(search, limit)[0]) == limit

    # Are all string titles accounted for in objects?
    # Are all object titles accounted for in string titles?
    @pytest.mark.parametrize("ac,search", [
        (library.full_auto_completer("en"), "cor"),
        (library.full_auto_completer("he"), ""),
        (library.cross_lexicon_auto_completer(), "")
    ])
    @pytest.mark.parametrize("limit", [10, 0])
    def test_object_completness(self,  ac, search, limit):
        [strs, objs] = ac.complete(search, limit)
        o_set = set([o["title"] for o in objs])
        s_set = set(strs)
        assert o_set == s_set

    # Do we swap languages for non matches?
    @pytest.mark.parametrize("he_str,en_str", [
        ("", "daniel"),
        ("", "songs"),
        ("", "eckv"),
        ("", "hux;")
    ])
    def test_language_flip(self,he_str,en_str):
        assert library.full_auto_completer("he").complete(he_str,10)[0] == library.full_auto_completer("en").complete(en_str, 10)[0]

"""




    # Does 0 limit work to have no limits?
    # How do we test that?
    # Do individual dictionary autompleters return results?
    # Do cross dictionary ac return results from all dicts?
    # Are all refs noted as such in name api?
    # Do dictionary entries resolve in name api?

```

### sefaria/model/tests/abstract_test.py

```
# -*- coding: utf-8 -*-

import pytest

from sefaria.system.database import db
from sefaria.system.exceptions import InputError
import sefaria.model as model
import sefaria.model.abstract as abstract

# cascade functions are tested in person_test.py


def setup_module(module):
    global record_classes
    global set_classes
    global record_to_set
    record_classes = abstract.get_record_classes()
    set_classes = abstract.get_set_classes()
    record_to_set = {
        set_class.recordClass.__name__: set_class for set_class in set_classes
    }
    print(record_classes)


def get_record_classes_with_slugs():
    classes = abstract.get_record_classes()
    return filter(lambda x: getattr(x, 'slug_fields', None) is not None and x.__name__ != "Portal", classes)


class TestMongoRecordModels(object):

    def test_class_attribute_collection(self):
        for sub in record_classes:
            assert sub.collection
            assert isinstance(sub.collection, str)

    def test_class_attribute_required_attrs(self):
        for sub in record_classes:
            assert isinstance(sub.required_attrs, (list, tuple))
            assert len(sub.required_attrs)
            assert "_id" not in sub.required_attrs

    @pytest.mark.parametrize("sub", abstract.get_record_classes())
    def test_instanciation_load_and_validity(self, sub):
        m = sub()
        if m.collection == "term": #remove this line once terms are normalized
            return
        res = m.load({})
        if not res:  # Collection may be empty
            return
        assert m._id
        m._validate()

    def test_normalize_slug(self):
        a = abstract.SluggedAbstractMongoRecord

        def test_slug(slug, final_slug):
            new_slug = a.normalize_slug(slug)
            assert new_slug == final_slug

        test_slug('blah', 'blah')
        test_slug('blah1', 'blah1')
        test_slug('bla-h', 'bla-h')
        test_slug('blah and blah', 'blah-and-blah')
        test_slug('blah/blah', 'blah-blah')
        test_slug('blah == ', 'blah-')

    @pytest.mark.django_db
    @pytest.mark.parametrize("sub", get_record_classes_with_slugs())
    def test_normalize_slug_field(self, sub):
        """

        :return:
        """
        test_slug = 'test'

        def get_slug(base, slug_field):
            return abstract.SluggedAbstractMongoRecord.normalize_slug('{}{}'.format(base, slug_field))
        attrs = {  # fill in requirements
            attr: None for attr in sub.required_attrs
        }
        attrs.update({
            slug_field: get_slug(test_slug, slug_field) for slug_field in sub.slug_fields
        })
        inst = sub(attrs)
        for slug_field in sub.slug_fields:
            temp_slug = get_slug(test_slug, slug_field)
            num_records = 1
            dup_str = ''
            count = 0
            sub_set = record_to_set[sub.__name__]({slug_field: temp_slug})
            sub_set.delete()
            while num_records > 0:
                sub_set = record_to_set[sub.__name__]({slug_field: temp_slug + dup_str})  # delete all
                count += 1
                dup_str = str(count)
                num_records = sub_set.count()
                sub_set.delete()
            new_slug = inst.normalize_slug_field(slug_field)
            assert new_slug == temp_slug

        # check that save doesn't alter slug
        inst.save()
        for slug_field in sub.slug_fields:
            temp_slug = get_slug(test_slug, slug_field)
            assert getattr(inst, slug_field) == temp_slug

        # check that duplicate slugs are handled correctly
        inst2 = sub(attrs)
        inst2.save()
        for slug_field in sub.slug_fields:
            temp_slug = get_slug(test_slug, slug_field) + '1'
            assert getattr(inst2, slug_field) == temp_slug

        # cleanup
        inst.delete()
        inst2.delete()

    @pytest.mark.deep
    @pytest.mark.parametrize("record_class", abstract.get_record_classes())
    def test_attr_definitions(self, record_class):
        """
        As currently written, this examines every record in the mongo db.
        If this test fails, use the validate_model_attr_definitions.py script to diagnose.
        """
        class_keys = set(record_class.required_attrs + record_class.optional_attrs + [record_class.id_field])
        req_class_keys = set(record_class.required_attrs)
        records = getattr(db, record_class.collection).find()
        for rec in records:
            record_keys = set(rec.keys())
            assert record_keys <= class_keys, "{} - unhandled keys {}".format(record_class, record_keys - class_keys)
            assert req_class_keys <= record_keys, "{} - required keys missing: {}".format(record_class, req_class_keys - record_keys)


class Test_Mongo_Set_Models(object):

    def test_record_class(self):
        for sub in set_classes:
            assert sub.recordClass != abstract.AbstractMongoRecord
            assert issubclass(sub.recordClass, abstract.AbstractMongoRecord)


class Test_Mongo_Record_Methods(object):
    """ Tests of the methods on the abstract models.
    They often need instanciation, but are not designed to test the subclasses specifically.
    """
    def test_equality_and_identity(self):
        attrs = {
            "ref": "Psalms 145:22",
            "text": "Not a part of this Psalm, but tradition to read it at the end of recitation in Psukei d'Zimrah",
            "anchorText": "Psalm 115:18",
            "owner": 7934,
            "type": "note",
            "public": True
        }
        model.Note(attrs).save() # added to make sure there is a note in the db, if the table is truncated or not extant.
        n1 = model.Note(attrs)
        n2 = model.Note(attrs)
        n3 = model.Note()
        assert n1 is not n2
        assert n1 == n2
        assert not n1.same_record(n2)
        assert n1 != n3
        assert not n1.same_record(n3)


        n4 = model.Note().load({"ref": "Psalms 145:22", "owner": 7934})
        n5 = model.Note().load({"ref": "Psalms 145:22", "owner": 7934})
        assert n4 is not n5
        assert n4.same_record(n5)
        assert not n1.same_record(n5)

    def test_attribute_exception(self):
        attrs = {
            "ref": "Psalms 150:1",
            "text": "blah",
            "anchorText": "blue",
            "owner": 28,
            "type": "note",
            "public": True,
            "foobar": "blaz"  # should raise an exception when loaded
        }
        db.notes.delete_one({"ref": "Psalms 150:1", "owner": 28})
        db.notes.insert_one(attrs)
        with pytest.raises(Exception):
            model.Note().load({"ref": "Psalms 150:1", "owner": 28})
        db.notes.delete_one({"ref": "Psalms 150:1", "owner": 28})

    def test_copy(self):
        for sub in record_classes:
            if sub is model.VersionState: #VersionState is derived - this test doesn't work well with it
                continue
            m = sub()
            res = m.load({})
            if not res:  # Collection may be empty
                return
            c = res.copy()
            del res._id
            assert res == c



```

### sefaria/model/tests/lock_test.py

```
import datetime

import sefaria.model as model


def test_locks():
    ref = "Mishnah Oktzin 1:3"
    lang = "en"
    version = "Sefaria Community Translation"
    user = 0

    model.release_lock(ref, lang, version)
    model.set_lock(ref, lang, version, user)
    assert model.check_lock(ref, lang, version)
    model.release_lock(ref, lang, version)
    assert not model.check_lock(ref, lang, version)

    # test expiring locks
    twice_cutoff_ago = datetime.datetime.now() - datetime.timedelta(seconds=(model.lock.LOCK_TIMEOUT * 2))
    model.Lock({
        "ref": ref,
        "lang": lang,
        "version": version,
        "user": user,
        "time": twice_cutoff_ago,
    }).save()
    assert model.check_lock(ref, lang, version)
    model.expire_locks()
    assert not model.check_lock(ref, lang, version)

```

### sefaria/model/tests/user_history_test.py

```
import pytest
from sefaria.model.user_profile import UserHistory, UserHistorySet
from sefaria.system.exceptions import InputError

def make_uh(uid=0, ref="Genesis 1:1", he_ref=" :", versions=None, time_stamp=0, server_time_stamp=0, last_place=False, book="Genesis", saved=False, secondary=False):
    versions = versions or {
        "he": "blah",
        "en": "blah"
    }
    uh = UserHistory({
        "uid": uid,
        "ref": ref,
        "he_ref": he_ref,
        "versions": versions,
        "time_stamp": time_stamp,
        "server_time_stamp": server_time_stamp,
        "last_place": last_place,
        "book": book,
        "saved": saved,
        "secondary": secondary        
    })
    uh.save()
    return uh

@pytest.fixture(scope='module')
def uh_secondary_item():
    uh = make_uh(secondary=True)
    yield uh
    uh.delete()

def test_saved_with_secondary_item(uh_secondary_item):
    """
    Ephraim found bug where saved flag can get attached to secondary item that was created immediately before saving
    This tests for this case
    """
    uh = UserHistory.save_history_item(0, {
        "ref": "Genesis 1:1",
        "versions": {
            "he": "blah",
            "en": "blah"
        },
        "saved": True,
        "last_place": True,
        "secondary": False,
        "time_stamp": 1,
        "server_time_stamp": 1,
        "action": "add_saved"
    })
    assert uh_secondary_item._id != uh._id
    assert not uh.secondary
    uh.delete()

def test_validation_for_saved_and_secondary():
    with pytest.raises(InputError):
        make_uh(saved=True, secondary=True)
```

### sefaria/model/tests/text_test.py

```
# -*- coding: utf-8 -*-
import regex as re
from copy import deepcopy
import pytest

import sefaria.model as model
from sefaria.system.exceptions import InputError
from sefaria.system.testing import test_uid


def teardown_module(module):
    titles = ['Test Commentator Name',
              'Bartenura (The Next Generation)',
              'Test Index Name',
              "Changed Test Index",
              "Third Attempt",
              "Test Iu",
              "Test Del"]

    for title in titles:
        try:
            model.IndexSet({"title": title}).delete()
        except Exception:
            pass
        try:
            model.VersionSet({"title": title}).delete()
        except Exception:
            pass


def test_dup_index_save():
    title = 'Test Commentator Name'
    model.IndexSet({"title": title}).delete()
    d = {
         "categories" : [
            "Liturgy"
        ],
        "title" : title,
        "schema" : {
            "titles" : [
                {
                    "lang" : "en",
                    "text" : title,
                    "primary" : True
                },
                {
                    "lang" : "he",
                    "text" : "",
                    "primary" : True
                }
            ],
            "nodeType" : "JaggedArrayNode",
            "depth" : 2,
            "sectionNames" : [
                "Section",
                "Line"
            ],
            "addressTypes" : [
                "Integer",
                "Integer"
            ],
            "key": title
        },
    }
    idx = model.Index(d)
    idx.save()
    assert model.IndexSet({"title": title}).count() == 1
    with pytest.raises(InputError) as e_info:
        d2 = {
            "title": title,
            "heTitle": " ",
            "titleVariants": [title],
            "sectionNames": ["Chapter", "Paragraph"],
            "categories": ["Commentary"],
        }
        idx2 = model.Index(d2).save()

    assert model.IndexSet({"title": title}).count() == 1


def test_invalid_index_save_no_existing_base_text():
    title = 'Bartenura (The Next Generation)'
    model.IndexSet({"title": title}).delete()
    d = {
         "categories" : [
            "Mishnah",
            "Commentary",
            "Bartenura",
            "Seder Zeraim"
        ],
        "base_text_titles": ["Gargamel"],
        "title" : title,
        "schema" : {
            "titles" : [
                {
                    "lang" : "en",
                    "text" : title,
                    "primary" : True
                },
                {
                    "lang" : "he",
                    "text" : "",
                    "primary" : True
                }
            ],
            "nodeType" : "JaggedArrayNode",
            "depth" : 2,
            "sectionNames" : [
                "Section",
                "Line"
            ],
            "addressTypes" : [
                "Integer",
                "Integer"
            ],
            "key": title
        },
    }
    idx = model.Index(d)
    with pytest.raises(InputError) as e_info:
        idx.save()
    assert "Base Text Titles must point to existing texts in the system." in str(e_info.value)
    assert model.IndexSet({"title": title}).count() == 0


def test_invalid_index_save_no_category():
    title = 'Bartenura (The Next Generation)'
    model.IndexSet({"title": title}).delete()
    d = {
         "categories" : [
            "Mishnah",
            "Commentary",
            "Bartenura",
            "Gargamel"
        ],
        "title" : title,
        "schema" : {
            "titles" : [
                {
                    "lang" : "en",
                    "text" : title,
                    "primary" : True
                },
                {
                    "lang" : "he",
                    "text" : "",
                    "primary" : True
                }
            ],
            "nodeType" : "JaggedArrayNode",
            "depth" : 2,
            "sectionNames" : [
                "Section",
                "Line"
            ],
            "addressTypes" : [
                "Integer",
                "Integer"
            ],
            "key": title
        },
    }
    idx = model.Index(d)
    with pytest.raises(InputError) as e_info:
        idx.save()
    assert "You must create category Mishnah/Commentary/Bartenura/Gargamel before adding texts to it." in str(e_info.value)
    assert model.IndexSet({"title": title}).count() == 0

def test_best_time_period():
    i = model.library.get_index("Rashi on Genesis")
    assert i.best_time_period().period_string('en') == ' (c.1075   c.1105 CE)'
    i.compDate = None
    assert i.best_time_period().period_string('en') == ' (1040   1105 CE)'  # now that compDate is None, period_string should return Rashi's birth to death years

def test_invalid_index_save_no_hebrew_collective_title():
    title = 'Bartenura (The Next Generation)'
    model.IndexSet({"title": title}).delete()
    d = {
         "categories" : [
            "Mishnah",
            "Rishonim on Mishnah",
            "Bartenura"
        ],
        "collective_title": 'Gargamel',
        "title" : title,
        "schema" : {
            "titles" : [
                {
                    "lang" : "en",
                    "text" : title,
                    "primary" : True
                },
                {
                    "lang" : "he",
                    "text" : "",
                    "primary" : True
                }
            ],
            "nodeType" : "JaggedArrayNode",
            "depth" : 2,
            "sectionNames" : [
                "Section",
                "Line"
            ],
            "addressTypes" : [
                "Integer",
                "Integer"
            ],
            "key": title
        },
    }
    idx = model.Index(d)
    with pytest.raises(InputError) as e_info:
        idx.save()
    assert "You must add a hebrew translation Term for any new Collective Title: Gargamel." in str(e_info.value)
    assert model.IndexSet({"title": title}).count() == 0



"""def test_add_old_commentator():
    title = "Old Commentator Record"
    commentator = {
        "title": title,
        "heTitle": u" ",
        "titleVariants": [title],
        "sectionNames": ["", ""],
        "categories": ["Commentary"],
    }
    commentator_idx = model.Index(commentator).save()
    assert getattr(commentator_idx, "nodes", None) is not None"""


def test_index_title_setter():
    title = 'Test Index Name'
    he_title = ""
    d = {
         "categories" : [
            "Liturgy"
        ],
        "title" : title,
        "schema" : {
            "titles" : [
                {
                    "lang" : "en",
                    "text" : title,
                    "primary" : True
                },
                {
                    "lang" : "he",
                    "text" : he_title,
                    "primary" : True
                }
            ],
            "nodeType" : "JaggedArrayNode",
            "depth" : 2,
            "sectionNames" : [
                "Section",
                "Line"
            ],
            "addressTypes" : [
                "Integer",
                "Integer"
            ],
            "key": title
        },
    }
    idx = model.Index(d)
    assert idx.title == title
    assert idx.nodes.key == title
    assert idx.nodes.primary_title("en") == title
    assert getattr(idx, 'title') == title
    idx.save()

    new_title = "Changed Test Index"
    new_heb_title = "  "
    idx.title = new_title

    assert idx.title == new_title
    assert idx.nodes.key == new_title
    assert idx.nodes.primary_title("en") == new_title
    assert getattr(idx, 'title') == new_title

    idx.set_title(new_heb_title, 'he')
    assert idx.nodes.primary_title('he') == new_heb_title


    third_title = "Third Attempt"
    setattr(idx, 'title', third_title)
    assert idx.title == third_title
    assert idx.nodes.key == third_title
    assert idx.nodes.primary_title("en") == third_title
    assert getattr(idx, 'title') == third_title
    idx.save()
    # make sure all caches pointing to this index are cleaned up
    for t in [("en",title),("en",new_title),("he",he_title),("en",new_heb_title)]:
        assert t[1] not in model.library._index_title_maps[t[0]]
    assert title not in model.library._index_map
    assert new_title not in model.library._index_map
    idx.delete()
    assert title not in model.library._index_map
    assert new_title not in model.library._index_map
    assert third_title not in model.library._index_map
    for t in [("en",title),("en",new_title),("en", third_title),("he",he_title),("en",new_heb_title)]:
        assert t[1] not in model.library._index_title_maps[t[0]]


def test_get_index():
    r = model.library.get_index("Rashi on Exodus")
    assert isinstance(r, model.Index)
    assert 'Rashi on Exodus' == r.title

    r = model.library.get_index("Exodus")
    assert isinstance(r, model.Index)
    assert r.title == 'Exodus'


def test_merge():
    assert model.merge_texts([["a", ""], ["", "b", "c"]], ["first", "second"]) == [["a", "b", "c"], ["first","second","second"]]
    # This fails because the source field isn't nested on return
    # assert model.merge_texts([[["a", ""],["p","","q"]], [["", "b", "c"],["p","d",""]]], ["first", "second"]) == [[["a", "b", "c"],["p","d","q"]], [["first","second","second"],["first","second","first"]]]

    # depth 2
    assert model.merge_texts([[["a", ""],["p","","q"]], [["", "b", "c"],["p","d",""]]], ["first", "second"])[0] == [["a", "b", "c"],["p","d","q"]]

    # three texts, depth 2
    assert model.merge_texts([[["a", ""],["p","",""]], [["", "b", ""],["p","d",""]], [["","","c"],["","","q"]]], ["first", "second", "third"])[0] == [["a", "b", "c"],["p","d","q"]]


def test_text_helpers():
    res = model.library.get_indices_by_collective_title("Rashi")
    assert 'Rashi on Bava Batra' in res
    assert 'Rashi on Genesis' in res
    assert 'Rashbam on Genesis' not in res

    res = model.library.get_indices_by_collective_title("Bartenura")
    assert 'Bartenura on Mishnah Shabbat' in res
    assert 'Bartenura on Mishnah Oholot' in res
    assert 'Rashbam on Genesis' not in res

    cats = model.library.get_text_categories()
    assert 'Tanakh' in cats
    assert 'Torah' in cats
    assert 'Prophets' in cats
    assert 'Commentary' in cats

@pytest.mark.parametrize(('book_title', 'dependence_type', 'structure_match', 'expected_titles', 'not_expected_titles'), [
    [None, None, False, [
        'Rashbam on Genesis',
        'Rashi on Bava Batra',
        'Bartenura on Mishnah Oholot',
        'Onkelos Leviticus',
        'Chizkuni',
    ], [
        'Akeidat Yitzchak',
        'Berakhot']
     ],
    ['Exodus', None, False, ['Ibn Ezra on Exodus',
                             'Ramban on Exodus',
                             'Abarbanel on Torah',
                             'Meshekh Chokhmah',
                             'Targum Jonathan on Exodus',
                             'Onkelos Exodus',
                             'Harchev Davar on Exodus'
                             ], ['Exodus',
                                 'Rashi on Genesis']
     ],
    ['Exodus', 'Commentary', False, ['Ibn Ezra on Exodus',
                             'Ramban on Exodus',
                             'Abarbanel on Torah',
                             'Meshekh Chokhmah',
                             'Harchev Davar on Exodus'
                             ], ['Targum Jonathan on Exodus',
                                 'Onkelos Exodus',
                                 'Exodus',
                                 'Rashi on Genesis']
     ],
    ['Exodus', 'Commentary', True, ['Ibn Ezra on Exodus',
                                    'Ramban on Exodus'
                                    ], ['Abarbanel on Torah',
                                        'Meshekh Chokhmah',
                                        'Targum Jonathan on Exodus',
                                        'Onkelos Exodus',
                                        'Harchev Davar on Exodus',
                                        'Exodus',
                                        'Rashi on Genesis']
     ],
])
def test_get_dependent_indices(book_title, dependence_type, structure_match, expected_titles, not_expected_titles):
    res = model.library.get_dependant_indices(book_title=book_title, dependence_type=dependence_type, structure_match=structure_match)
    for title in expected_titles:
        assert title in res
    for title in not_expected_titles:
        assert title not in res


def test_index_update():
    '''
    :return: Test:
        index creation from legacy form
        update() function
        update of Index, like what happens on the frontend, doesn't whack hidden attrs
    '''
    ti = "Test Iu"

    i = model.Index({
        "title": ti,
        "heTitle": "",
        "titleVariants": [ti],
        "sectionNames": ["Chapter", "Paragraph"],
        "categories": ["Talmud", "Bavli"],
    }).save()
    i = model.Index().load({"title": ti})
    assert "Bavli" in i.categories
    assert i.schema["addressTypes"] == ["Talmud", "Integer"]

    i = model.Index().update({"title": ti}, {
        "title": ti,
        "heTitle": "",
        "titleVariants": [ti],
        "sectionNames": ["Chapter", "Paragraph"],
        "addressTypes": ["Integer", "Integer"],  # this change will not go through as addressTypes cannot be changed this way for an existing Index. instead, it's necessary to edit the JaggedArrayNode
        "categories": ["Musar"]
    })
    i = model.Index().load({"title": ti})
    assert "Musar" in i.categories
    assert "Bavli" not in i.categories

    i = model.Index().load({"title": ti})
    assert i.schema["addressTypes"] == ["Talmud", "Integer"]
    model.IndexSet({"title": ti}).delete()


def test_index_delete():
    #Simple Text
    ti = "Test Del"

    i = model.Index({
        "title": ti,
        "heTitle": "",
        "titleVariants": [ti],
        "sectionNames": ["Chapter", "Paragraph"],
        "categories": ["Musar"],
    }).save()
    new_version1 = model.Version(
                {
                    "chapter": i.nodes.create_skeleton(),
                    "versionTitle": "Version 1 TEST",
                    "versionSource": "blabla",
                    "language": "he",
                    "title": i.title
                }
    )
    new_version1.chapter = [[''],[''],["      "]]
    new_version1.save()
    new_version2 = model.Version(
                {
                    "chapter": i.nodes.create_skeleton(),
                    "versionTitle": "Version 2 TEST",
                    "versionSource": "blabla",
                    "language": "en",
                    "title": i.title
                }
    )
    new_version2.chapter = [[],["Hello goodbye bla bla blah"],[]]
    new_version2.save()

    i.delete()
    assert model.Index().load({'title': ti}) is None
    assert model.VersionSet({'title':ti}).count() == 0





@pytest.mark.deep
def test_index_name_change():

    #Simple Text
    tests = [
        ("The Book of Maccabees I", "Movement of Ja People"),  # Simple Text
        # (u"Rashi", u"The Vintner")              # Commentator Invalid after commentary refactor?
    ]

    for old, new in tests:
        index = model.Index().load({"title": old})

        # Make sure that the test isn't passing just because we've been comparing 0 to 0
        assert all([cnt > 0 for cnt in dep_counts(old, index)])

        for cnt in list(dep_counts(new, index).values()):
            assert cnt == 0

        old_counts = dep_counts(old, index)

        old_index = deepcopy(index)
        #new_in_alt = new in index.titleVariants
        index.title = new
        index.save()
        assert old_counts == dep_counts(new, index)

        index.title = old
        #if not new_in_alt:
        if getattr(index, "titleVariants", None):
            index.titleVariants.remove(new)
        index.save()
        #assert old_index == index   #needs redo of titling, above, i suspect
        assert old_counts == dep_counts(old, index)
        for cnt in list(dep_counts(new, index).values()):
            assert cnt == 0


def dep_counts(name, indx):

    def construct_query(attribute, queries):
        query_list = [{attribute: {'$regex': query}} for query in queries]
        return {'$or': query_list}

    from sefaria.model.text import prepare_index_regex_for_dependency_process
    patterns = prepare_index_regex_for_dependency_process(indx, as_list=True)
    patterns = [pattern.replace(re.escape(indx.title), re.escape(name)) for pattern in patterns]

    ret = {
        'version title exact match': model.VersionSet({"title": name}, sort=[('title', 1)]).count(),
        'history title exact match': model.HistorySet({"title": name}, sort=[('title', 1)]).count(),
        'note match ': model.NoteSet(construct_query("ref", patterns), sort=[('ref', 1)]).count(),
        'link match ': model.LinkSet(construct_query("refs", patterns)).count(),
        'history refs match ': model.HistorySet(construct_query("ref", patterns), sort=[('ref', 1)]).count(),
        'history new refs match ': model.HistorySet(construct_query("new.refs", patterns), sort=[('new.refs', 1)]).count()
    }

    return ret


def test_version_word_count():
    #simple
    assert model.Version().load({"title": "Genesis", "language": "he", "versionTitle": "Tanach with Ta'amei Hamikra"}).word_count() == 20813
    assert model.Version().load({"title": "Rashi on Shabbat", "language": "he"}).word_count() > 0
    #complex
    assert model.Version().load({"title": "Pesach Haggadah", "language": "he"}).word_count() > 0
    assert model.Version().load({"title": "Orot", "language": "he"}).word_count() > 0
    assert model.Version().load({"title": "Ephod Bad on Pesach Haggadah"}).word_count() > 0

    #sets
    assert model.VersionSet({"title": {"$regex": "Haggadah"}}).word_count() > 200000


def test_version_walk_thru_contents():
    def action(segment_str, tref, heTref, version):
        r = model.Ref(tref)
        tc = model.TextChunk(r, lang=version.language, vtitle=version.versionTitle)
        assert tc.text == segment_str
        assert tref == r.normal()
        assert heTref == r.he_normal()

    test_index_titles = ["Genesis", "Rashi on Shabbat", "Pesach Haggadah", "Orot", "Ramban on Deuteronomy", "Zohar"]
    for t in test_index_titles:
        ind = model.library.get_index(t)
        version = ind.versionSet()[0]
        version.walk_thru_contents(action)


class TestModifyVersion:
    simpleIndexTitle = "Test ModifyVersion Simple " + test_uid
    complexIndexTitle = "Test ModifyVersion Complex " + test_uid
    vtitle = "Version TEST"
    vlang = "he"

    @classmethod
    def setup_class(cls):
        cls.simpleIndex = model.Index({
            "title": cls.simpleIndexTitle,
            "heTitle": "1",
            "titleVariants": [cls.simpleIndexTitle],
            "sectionNames": ["Chapter", "Paragraph"],
            "categories": ["Musar"],
        }).save()
        cls.simpleVersion = model.Version(
            {
                "chapter": cls.simpleIndex.nodes.create_skeleton(),
                "versionTitle": "Version 1 TEST",
                "versionSource": "blabla",
                "language": "he",
                "title": cls.simpleIndexTitle
            }
        )
        cls.simpleVersion.chapter = [['1'], ['2'], ["original text", "2nd"]]
        cls.simpleVersion.save()
        cls.complexIndex = model.Index({
            "title": cls.complexIndexTitle,
            "heTitle": "2",
            "titleVariants": [cls.complexIndexTitle],
            "schema": {
                "nodes": [
                    {
                        "nodes": [
                            {
                                "nodeType": "JaggedArrayNode",
                                "depth": 2,
                                "sectionNames": ["Chapter", "Paragraph"],
                                "addressTypes": ["Integer", "Integer"],
                                "titles": [{"text": "Node 2", "lang": "en", "primary": True}, {"text": "Node 2 he", "lang": "he", "primary": True}],
                                "key": "Node 2"
                            }
                        ],
                        "titles": [{"text": "Node 1", "lang": "en", "primary": True}, {"text": "Node 1 he", "lang": "he", "primary": True}],
                        "key": "Node 1"
                    },
                    {
                        "nodeType": "JaggedArrayNode",
                        "depth": 1,
                        "sectionNames": ["Paragraph"],
                        "addressTypes": ["Integer"],
                        "titles": [{"text": "Node 3", "lang": "en", "primary": True}, {"text": "Node 3 he", "lang": "he", "primary": True}],
                        "key": "Node 3"
                    }
                ],
                "titles": [{"text": cls.complexIndexTitle, "lang": "en", "primary": True},
                           {"text": cls.complexIndexTitle + "he", "lang": "he", "primary": True}],
                "key": cls.complexIndexTitle
            },
            "categories": ["Musar"]
        }).save()
        cls.complexVersion = model.Version(
                    {
                        "chapter": cls.complexIndex.nodes.create_skeleton(),
                        "versionTitle": "Version 2 TEST",
                        "versionSource": "blabla",
                        "language": "en",
                        "title": cls.complexIndexTitle
                    }
        )
        cls.complexVersion.chapter = {"Node 1": {"Node 2": [['yo'],['', 'blah'],["original text", "2nd"]]}, "Node 3": ['1', '2', '3', '4']}
        cls.complexVersion.save()

    @classmethod
    def teardown_class(cls):
        for c in [cls.simpleIndex, cls.complexIndex, cls.simpleVersion, cls.complexVersion]:
            try:
                c.delete()
            except Exception:
                pass

    def test_sub_content_with_ref(self):
        self.simpleVersion.sub_content_with_ref(model.Ref(f"{self.simpleIndexTitle} 3:2"), "new text")
        assert self.simpleVersion.chapter[2][1] == "new text"

        self.complexVersion.sub_content_with_ref(model.Ref(f"{self.complexIndexTitle}, Node 1, Node 2 3:2"), "new text")
        assert self.complexVersion.chapter["Node 1"]["Node 2"][2][1] == "new text"

        self.complexVersion.sub_content_with_ref(model.Ref(f"{self.complexIndexTitle}, Node 1, Node 2 3"), ["blah", "blarg"])
        assert self.complexVersion.chapter["Node 1"]["Node 2"][2] == ["blah", "blarg"]
        self.complexVersion.sub_content_with_ref(model.Ref(f"{self.complexIndexTitle}, Node 1, Node 2 3"), ["original text", "2nd"])  # set back to original content for other tests

        self.complexVersion.sub_content_with_ref(model.Ref(f"{self.complexIndexTitle}, Node 1, Node 2"), [["blah", "blarg"], ['more content']])
        assert self.complexVersion.chapter["Node 1"]["Node 2"] == [["blah", "blarg"], ['more content']]
        self.complexVersion.sub_content_with_ref(model.Ref(f"{self.complexIndexTitle}, Node 1, Node 2"), [['yo'],['', 'blah'],["original text", "2nd"]])  # set back to original content for other tests

    def test_sub_content_with_ref_padding(self):
        self.simpleVersion.sub_content_with_ref(model.Ref(f"{self.simpleIndexTitle} 3:5"), "new text")
        assert self.simpleVersion.chapter[2][2] == ""
        assert self.simpleVersion.chapter[2][3] == ""
        assert self.simpleVersion.chapter[2][4] == "new text"

        self.simpleVersion.sub_content_with_ref(model.Ref(f"{self.simpleIndexTitle} 5:1"), "new text2")
        assert self.simpleVersion.chapter[3] == []
        assert self.simpleVersion.chapter[4][0] == "new text2"

        # reset
        self.simpleVersion.sub_content_with_ref(model.Ref(f"{self.simpleIndexTitle}"), [['1'], ['2'], ["original text", "2nd"]])

    def test_sub_content_simple_setter(self):
        self.simpleVersion.sub_content(self.simpleIndex.nodes.version_address(), value=[[], [], []])
        for i in range(3):
            assert self.simpleVersion.chapter[i] == []

        self.simpleVersion.sub_content(self.simpleIndex.nodes.version_address(), [1], value=['yo1', 'yo2', 'yo3'])
        assert self.simpleVersion.chapter[1] == ['yo1', 'yo2', 'yo3']

        self.simpleVersion.sub_content(self.simpleIndex.nodes.version_address(), [0, 1], value='yo')
        assert self.simpleVersion.chapter[0][1] == 'yo'

        # reset
        self.simpleVersion.sub_content_with_ref(model.Ref(f"{self.simpleIndexTitle}"), [['1'], ['2'], ["original text", "2nd"]])

    def test_sub_content_complex_setter(self):
        self.complexVersion.sub_content([], value={"Node 1": {"Node 2": [['wadup']], "Node 3": []}})
        assert self.complexVersion.chapter['Node 1']['Node 2'] == [['wadup']]

        self.complexVersion.sub_content(["Node 1"], value={"Node 2": [['yoyoyo']]})
        assert self.complexVersion.chapter['Node 1']['Node 2'] == [['yoyoyo']]

        self.complexVersion.sub_content_with_ref(model.Ref(f"{self.complexIndexTitle}"), {"Node 1": {"Node 2": [['yo'],['', 'blah'],["original text", "2nd"]]}, "Node 3": ['1', '2', '3', '4']})

    def test_get_top_level_jas_text_chunk(self):
        tc = model.Ref(self.simpleIndexTitle).text('he')
        jas, parent_key_list = tc.get_top_level_jas()
        assert len(jas) == 1 == len(parent_key_list)
        assert jas[0][1][0] == '2'
        assert parent_key_list[0][0] is None
        assert parent_key_list[0][1] is None

    def test_get_top_level_jas_version_simple(self):
        jas, parent_key_list = self.simpleVersion.get_top_level_jas()
        assert len(jas) == 1 == len(parent_key_list)
        assert jas[0][1][0] == '2'
        assert parent_key_list[0][0] is None
        assert parent_key_list[0][1] is None

    def test_get_top_level_jas_version_complex(self):
        jas, parent_key_list = self.complexVersion.get_top_level_jas()
        assert len(jas) == 2 == len(parent_key_list)
        assert jas[0][1][1] == 'blah'
        assert jas[1][3] == '4'
        for ja, (parent, key) in zip(jas, parent_key_list):
            assert parent[key] == ja

    def test_trim_ending_whitespace_text_chunk(self):
        tc = model.Ref(self.simpleIndexTitle).text('he')
        original_len = len(tc.text[0])
        tc.text[0] += ['', '', '   ', None]
        tc._trim_ending_whitespace()
        assert len(tc.text[0]) == original_len

    def test_trim_ending_whitespace_version_simple(self):
        original_len = len(self.simpleVersion.chapter[0])
        self.simpleVersion.chapter[0] += ['', '', '   ', None]
        self.simpleVersion._trim_ending_whitespace()
        assert len(self.simpleVersion.chapter[0]) == original_len

    def test_trim_ending_whitespace_version_complex(self):
        node = self.complexVersion.chapter['Node 1']['Node 2']
        original_len = len(node[0])
        node[0] += ['', '', '   ', None]
        self.complexVersion._trim_ending_whitespace()
        assert (len(self.complexVersion.chapter['Node 1']['Node 2'][0]) == original_len)


class TestVersionActualLanguage:
    myIndexTitle = "Test VersionActualLanguage " + test_uid
    vtitle = "Version TEST"
    vlang = "he"

    @classmethod
    def setup_class(cls):
        cls.myIndex = model.Index({
            "title": cls.myIndexTitle,
            "heTitle": "2",
            "titleVariants": [cls.myIndexTitle],
            "sectionNames": ["Chapter", "Paragraph"],
            "categories": ["Musar"],
        }).save()
        cls.firstTranslationVersion = model.Version(
            {
                "chapter": [['1'], ['2'], ["original text", "2nd"]],
                "versionTitle": "Version 1 TEST [fr]",
                "versionSource": "blabla",
                "language": "en",
                "title": cls.myIndexTitle
            }
        ).save()
        cls.sourceVersion = model.Version(
            {
                "chapter":cls.myIndex.nodes.create_skeleton(),
                "versionTitle": "Version 1 TEST",
                "versionSource": "blabla",
                "language": "he",
                "title": cls.myIndexTitle
            }
        )
        cls.sourceVersion.chapter = [['1'], ['2'], ["original text", "2nd"]]
        cls.sourceVersion.save()
        cls.versionWithLangCodeMismatch = model.Version(
            {
                "chapter": cls.myIndex.nodes.create_skeleton(),
                "versionTitle": "Version 1 TEST [ar]",
                "versionSource": "blabla",
                "language": "en",
                'actualLanguage': 'fr',
                "title": cls.myIndexTitle
            }
        )
        
    @classmethod
    def teardown_class(cls):
        for c in [cls.myIndex, cls.sourceVersion, cls.firstTranslationVersion, cls.versionWithLangCodeMismatch]:
            try:
                c.delete()
            except Exception:
                pass
    
    def test_normalize(self):
        expected_attrs = {
            'firstTranslationVersion': {
                'actualLanguage': 'fr',
                'direction': 'ltr',
                'languageFamilyName': 'french',
                'isPrimary': True,
                'isSource': False,
            },
            'sourceVersion': {
                'actualLanguage': 'he',
                'direction': 'rtl',
                'languageFamilyName': 'hebrew',
                'isPrimary': True,
                'isSource': True,
            },
            'versionWithLangCodeMismatch': {
                'actualLanguage': 'fr',
                'direction': 'ltr',
                'languageFamilyName': 'french',
                'isPrimary': False,
                'isSource': False,
            },
        }
        self.versionWithLangCodeMismatch._normalize()
        for version_key in expected_attrs:
            version = getattr(self, version_key)
            for attr in expected_attrs[version_key]:
                assert getattr(version, attr) == expected_attrs[version_key][attr]

@pytest.mark.parametrize(('text_with_html', 'text_without_html'),
                         [
                         ["</big><big>      ",
                          "      "],
                         [
                             "Happy is the <big>man</big> who has not followed the counsel of the wicked,<br/>or taken the path of sinners,<br>or joined the company of the insolent;",
                             "Happy is the man who has not followed the counsel of the wicked, or taken the path of sinners, or joined the company of the insolent;"]
                         ])

def test_remove_html(text_with_html, text_without_html):
    assert model.TextChunk.remove_html(text_with_html) == text_without_html



```

### sefaria/model/tests/portal_test.py

```
import pytest
from sefaria.model.portal import Portal  # Replace with your actual validation function
from sefaria.model.topic import Topic
from sefaria.system.exceptions import SluggedMongoRecordMissingError
from sefaria.system.database import db

valids = [
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "title_url": "https://example.com",
            "image_uri": "gs://your-bucket/image.jpg",
            "description": {
                "en": "English Description",
                "he": "Hebrew Description"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com",
            "ios_link": "https://ios-link.com"
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "description": {
                "en": "Newsletter English Description",
                "he": "Newsletter Hebrew Description"
            },
            "api_schema": {
                "http_method": "POST",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "description": {
                "en": "English Description",
                "he": "Hebrew Description"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            }
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "api_schema": {
                "http_method": "GET"
            }
        }
    },
{
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "title_url": "https://example.com",
            "image_uri": "gs://your-bucket/image.jpg",
            "description": {
                "en": "English Description",
                "he": "Hebrew Description"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com",
            "ios_link": "https://ios-link.com"
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "description": {
                "en": "Newsletter English Description",
                "he": "Newsletter Hebrew Description"
            },
            "api_schema": {
                "http_method": "POST",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            }
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "api_schema": {
                "http_method": "GET"
            }
        }
    },
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com"
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            }
        }
    },
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "title_url": "https://example.com",
            "image_uri": "gs://your-bucket/image.jpg",
            "description": {
                "en": "English Description",
                "he": "Hebrew Description"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com",
            "ios_link": "https://ios-link.com"
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "description": {
                "en": "Newsletter English Description",
                "he": "Newsletter Hebrew Description"
            },
            "api_schema": {
                "http_method": "POST",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            }
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "api_schema": {
                "http_method": "GET"
            }
        }
    },
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "image_uri": "gs://your-bucket/image.jpg"
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            }
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "api_schema": {
                "http_method": "POST",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "About Us",
                "he": ""
            }
        }
    }
]

invalids = [
    # Missing "about" key
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com",
            "ios_link": "https://ios-link.com"
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "api_schema": {
                "http_method": "POST",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    # Invalid "about.title_url" (not a URL)
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "title_url": "invalid-url",
            "image_uri": "gs://your-bucket/image.jpg",
            "description": {
                "en": "English Description",
                "he": "Hebrew Description"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com",
            "ios_link": "https://ios-link.com"
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "description": {
                "en": "Newsletter English Description",
                "he": "Newsletter Hebrew Description"
            },
            "api_schema": {
                "http_method": "POST",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    # Including invalid field "newsletter.description.fr"
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "title_url": "https://example.com",
            "image_uri": "gs://your-bucket/image.jpg",
            "description": {
                "en": "English Description",
                "he": "Hebrew Description"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com"
        },
        "newsletter": {
            "title": {
                "fr": "Titre de la newsletter",
                "he": "Newsletter Hebrew Title"
            },
            "description": {
                "en": "Newsletter English Description",
                "he": "Newsletter Hebrew Description"
            },
            "api_schema": {
                "http_method": "POST",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    # Invalid "newsletter.api_schema.http_method" (not a valid HTTP method)
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "English Title",
                "he": "Hebrew Title"
            },
            "image_uri": "gs://your-bucket/image.jpg",
            "description": {
                "en": "English Description",
                "he": "Hebrew Description"
            }
        },
        "mobile": {
            "title": {
                "en": "Mobile Title",
                "he": "Mobile Hebrew Title"
            },
            "android_link": "https://android-link.com",
            "ios_link": "https://ios-link.com"
        },
        "newsletter": {
            "title": {
                "en": "Newsletter Title",
                "he": "Newsletter Hebrew Title"
            },
            "description": {
                "en": "Newsletter English Description",
                "he": "Newsletter Hebrew Description"
            },
            "api_schema": {
                "http_method": "INVALID_METHOD",
                "payload": {
                    "first_name_key": "fname",
                    "last_name_key": "lname",
                    "email_key": "email"
                }
            }
        }
    },
    # Invalid data types:
    {
        "slug": "English Title",
        "name": {"en": "a", "he": "b"},
        "about": {
            "title": {
                "en": "About Us",
                "he": ""
            },
            "image_uri": 67890,
            "description": {
                "en": "Description in English",
                "he": " "
            }
        }
    },
{
    # Incorrect field names
    "slug": "English Title",
    "name": {"en": "a", "he": "b"},
    "about": {
        "title": {
            "en": "About Us",
            "he": ""
        },
        "image_uri": "gs://bucket/image.jpg",
        "description": {
            "en": "Description in English",
            "he": " "
        }
    },
    "mobile": {
        "title": {
            "en": "Mobile App",
            "he": " "
        },
        "android_link": "https://play.google.com/store/apps/details?id=com.example.app",
        "ios_link": "https://apps.apple.com/us/app/example-app/id1234567890",
        "invalid_field": "This field should not be here"
    }
}


]
@pytest.mark.parametrize("data", valids)
def test_valid_schema(data):
    p = Portal(data)
    assert p._validate() == True

@pytest.mark.parametrize("invalid_case", invalids)
def test_invalid_schema(invalid_case):
    with pytest.raises(Exception):
        p = Portal(invalid_case)
        p._validate()


@pytest.fixture()
def simple_portal():
    raw_portal = valids[0]
    portal = Portal(raw_portal)
    portal.save()

    yield portal

    portal.delete()


@pytest.fixture()
def simple_portal_saved_directly_to_mongo():
    raw_portal = valids[0]
    inserted_result = db.portals.insert_one(raw_portal)

    yield Portal(raw_portal)

    db.portals.delete_one({"_id": inserted_result.inserted_id})


@pytest.fixture()
def simple_topic(simple_portal, django_db_setup, django_db_blocker):
    with django_db_blocker.unblock():
        topic = Topic({
            "slug": "blah",
            "titles": [{"text": "Blah", "lang": "en", "primary": True}],
            "portal_slug": simple_portal.slug,
        })
        topic.save()

        yield topic

        topic.delete()


def test_save_simple_portal(simple_portal):
    """
    Tests that simple_portal was saved properly and has a normalized slug
    """
    assert simple_portal.slug == "english-title"


def test_topic_validates_portal_exists(simple_topic):
    assert simple_topic is not None


def test_topic_validation_fails_for_non_existent_portal():
    with pytest.raises(SluggedMongoRecordMissingError):
        topic = Topic({
            "slug": "blah",
            "titles": [{"text": "Blah", "lang": "en", "primary": True}],
            "portal_slug": "non-existent-portal",
        })
        topic.save()


def test_load_portal(simple_portal_saved_directly_to_mongo):
    portal = Portal().load({"slug": simple_portal_saved_directly_to_mongo.slug})
    assert portal is not None

```

### sefaria/model/tests/__init__.py

```

```

### sefaria/model/tests/ref_test.py

```
# -*- coding: utf-8 -*-
import pytest
from sefaria.model import *
from sefaria.system.exceptions import InputError

class Test_Ref(object):

    def test_short_names(self):
        ref = Ref("Exo. 3:1")
        assert ref.book == "Exodus"
        assert Ref("Prov. 3.19") == Ref("Proverbs 3:19")
        assert Ref("Exo. 3.19")
        assert Ref("Prov 3.20")
        assert Ref("Exo 3.20")
        assert Ref("Prov.3.21")
        assert Ref("Exo.3.21")
        assert Ref("1Ch.") == Ref("1 Chronicles")

    def test_normal_form_is_identifcal(self):
        assert Ref("Genesis 2:5").normal() == "Genesis 2:5"
        assert Ref("Shabbat 32b").normal() == "Shabbat 32b"
        assert Ref("Mishnah Peah 4:2-4").normal() == "Mishnah Peah 4:2-4"

    def test_bible_range(self):
        ref = Ref("Job.2:3-3:1")
        assert ref.toSections == [3, 1]
        ref = Ref("Jeremiah 7:17\u201318")  # test with unicode dash
        assert ref.toSections == [7, 18]
        ref = Ref("Jeremiah 7:17\u201118")  # test with unicode dash
        assert ref.toSections == [7, 18]
        ref = Ref("I Chronicles 1:2 - I Chronicles 1:3")  # test with unicode dash
        assert ref.toSections == [1, 3]

    def test_short_bible_refs(self):
        assert Ref("Exodus") != Ref("Exodus 1")
        assert Ref("Exodus").padded_ref() == Ref("Exodus 1")

    def test_short_talmud_refs(self):
        assert Ref("Sanhedrin 2a") != Ref("Sanhedrin")

    def test_talmud_refs_without_amud(self):
        assert Ref("Sanhedrin 2") == Ref("Sanhedrin 2a-2b")
        assert Ref("Shabbat 7") == Ref("Shabbat 7a-7b")

    def test_talmud_refs_short_range(self):
        assert Ref("Shabbat 7a-b") == Ref("Shabbat 7a-7b")

    def test_refs_beyond_end_of_book(self):
        assert Ref("Yoma 88") == Ref("Yoma 88a")
        assert Ref("Yoma 87-90") == Ref("Yoma 87a-88a")

    # This test runs for 90% of this suite's time, and passes.  Seems pretty trivial.  Can we trim it?
    @pytest.mark.deep
    def test_each_title(object):
        for lang in ["en", "he"]:
            for t in library.full_title_list(lang, False):
                assert library.all_titles_regex(lang).match(t), "'{}' doesn't resolve".format(t)

    def test_comma(self):
        assert Ref("Me'or Einayim, Chayei Sara 24") == Ref("Me'or Einayim, Chayei Sara, 24")
        assert Ref("Genesis 18:24") == Ref("Genesis, 18:24")

    def test_padded_ref(self):
        assert Ref("Exodus").padded_ref().normal() == "Exodus 1"
        assert Ref("Exodus 1").padded_ref().normal() == "Exodus 1"
        assert Ref("Exodus 1:1").padded_ref().normal() == "Exodus 1:1"
        assert Ref("Rashi on Genesis 2:3:1").padded_ref().normal() == "Rashi on Genesis 2:3:1"
        assert Ref("Shabbat").padded_ref().normal() == "Shabbat 2a"
        assert Ref("Shabbat 2a").padded_ref().normal() == "Shabbat 2a"
        assert Ref("Shabbat 2a:1").padded_ref().normal() == "Shabbat 2a:1"
        assert Ref("Rashi on Shabbat 2a:1:1").padded_ref().normal() == "Rashi on Shabbat 2a:1:1"

    def test_starting_and_ending(self):
        assert Ref("Leviticus 15:3 - 17:12").starting_ref() == Ref("Leviticus 15:3")
        assert Ref("Leviticus 15:3 - 17:12").ending_ref() == Ref("Leviticus 17:12")
        assert Ref("Leviticus 15-17").starting_ref() == Ref("Leviticus 15")
        assert Ref("Leviticus 15-17").ending_ref() == Ref("Leviticus 17")
        assert Ref("Leviticus 15:17-21").starting_ref() == Ref("Leviticus 15:17")
        assert Ref("Leviticus 15:17-21").ending_ref() == Ref("Leviticus 15:21")

        assert Ref("Leviticus 15:17").starting_ref() == Ref("Leviticus 15:17")
        assert Ref("Leviticus 15:17").ending_ref() == Ref("Leviticus 15:17")

        assert Ref("Leviticus 15").starting_ref() == Ref("Leviticus 15")
        assert Ref("Leviticus 15").ending_ref() == Ref("Leviticus 15")

        assert Ref("Leviticus").starting_ref() == Ref("Leviticus")
        assert Ref("Leviticus").ending_ref() == Ref("Leviticus")

        assert Ref("Shabbat 15a-16b").starting_ref() == Ref("Shabbat 15a")
        assert Ref("Shabbat 15a-16b").ending_ref() == Ref("Shabbat 16b")
        assert Ref("Shabbat 15a").starting_ref() == Ref("Shabbat 15a")
        assert Ref("Shabbat 15a").ending_ref() == Ref("Shabbat 15a")
        assert Ref("Shabbat 15a:15-15b:13").starting_ref() == Ref("Shabbat 15a:15")
        assert Ref("Shabbat 15a:15-15b:13").ending_ref() == Ref("Shabbat 15b:13")

        assert Ref("Rashi on Leviticus 15:3-17:12").starting_ref() == Ref("Rashi on Leviticus 15:3")
        assert Ref("Rashi on Leviticus 15:3-17:12").ending_ref() == Ref("Rashi on Leviticus 17:12")

        assert Ref("Rashi on Leviticus 15-17").starting_ref() == Ref("Rashi on Leviticus 15")
        assert Ref("Rashi on Leviticus 15-17").ending_ref() == Ref("Rashi on Leviticus 17")

        assert Ref("Rashi on Leviticus 15:17-21").starting_ref() == Ref("Rashi on Leviticus 15:17")
        assert Ref("Rashi on Leviticus 15:17-21").ending_ref() == Ref("Rashi on Leviticus 15:21")

        assert Ref("Rashi on Leviticus 15:17").starting_ref() == Ref("Rashi on Leviticus 15:17")
        assert Ref("Rashi on Leviticus 15:17").ending_ref() == Ref("Rashi on Leviticus 15:17")

        assert Ref("Rashi on Shabbat 15a-16b").starting_ref() == Ref("Rashi on Shabbat 15a")
        assert Ref("Rashi on Shabbat 15a-16b").ending_ref() == Ref("Rashi on Shabbat 16b")

        assert Ref("Rashi on Shabbat 15a").starting_ref() == Ref("Rashi on Shabbat 15a")
        assert Ref("Rashi on Shabbat 15a").ending_ref() == Ref("Rashi on Shabbat 15a")

        assert Ref("Rashi on Shabbat 15a:15-15b:13").starting_ref() == Ref("Rashi on Shabbat 15a:15")
        assert Ref("Rashi on Shabbat 15a:15-15b:13").ending_ref() == Ref("Rashi on Shabbat 15b:13")

        assert Ref("Rashi on Exodus 3:1-4:1").starting_ref() == Ref("Rashi on Exodus 3:1")
        assert Ref("Rashi on Exodus 3:1-4:1").ending_ref() == Ref("Rashi on Exodus 4:1")

        assert Ref("Rashi on Exodus 3:1-4:10").starting_ref() == Ref("Rashi on Exodus 3:1")
        assert Ref("Rashi on Exodus 3:1-4:10").ending_ref() == Ref("Rashi on Exodus 4:10")

        assert Ref("Rashi on Exodus 3:1-3:10").starting_ref() == Ref("Rashi on Exodus 3:1")
        assert Ref("Rashi on Exodus 3:1-3:10").ending_ref() == Ref("Rashi on Exodus 3:10")

        assert Ref("Rashi on Exodus 3:1:1-3:1:3").starting_ref() == Ref("Rashi on Exodus 3:1:1")
        assert Ref("Rashi on Exodus 3:1:1-3:1:3").ending_ref() == Ref("Rashi on Exodus 3:1:3")

    def test_is_talmud(self):
        assert not Ref("Exodus").is_talmud()
        assert not Ref("Exodus 1:3").is_talmud()
        assert not Ref("Rashi on Genesis 2:3:1").is_talmud()
        assert Ref("Shabbat").is_talmud()
        assert Ref("Shabbat 7b").is_talmud()
        assert Ref("Rashi on Shabbat 2a:1:1").is_talmud()

    def test_all_context_refs(self):
        assert Ref('Rashi on Genesis 2:3:4').all_context_refs() == [Ref('Rashi on Genesis 2:3:4'), Ref('Rashi on Genesis 2:3'), Ref('Rashi on Genesis 2')]
        assert Ref('Rashi on Genesis 2:3:4').all_context_refs(include_self = False, include_book = True) == [Ref('Rashi on Genesis 2:3'), Ref('Rashi on Genesis 2'), Ref('Rashi on Genesis')]
        assert Ref('Rashi on Genesis 2:3:4').all_context_refs(include_self = False, include_book = False) == [Ref('Rashi on Genesis 2:3'), Ref('Rashi on Genesis 2')]
        assert Ref('Rashi on Genesis 2:3:4').all_context_refs(include_self = True, include_book = True) == [Ref('Rashi on Genesis 2:3:4'), Ref('Rashi on Genesis 2:3'), Ref('Rashi on Genesis 2'), Ref('Rashi on Genesis')]

        assert Ref("Pesach Haggadah, Magid, First Fruits Declaration 2") .all_context_refs() == [Ref('Pesach Haggadah, Magid, First Fruits Declaration 2'), Ref('Pesach Haggadah, Magid, First Fruits Declaration'), Ref('Pesach Haggadah, Magid')]
        assert Ref("Pesach Haggadah, Magid, First Fruits Declaration 2") .all_context_refs(include_self = True, include_book = True) == [Ref('Pesach Haggadah, Magid, First Fruits Declaration 2'), Ref('Pesach Haggadah, Magid, First Fruits Declaration'), Ref('Pesach Haggadah, Magid'), Ref('Pesach Haggadah')]
        assert Ref("Pesach Haggadah, Magid, First Fruits Declaration 2") .all_context_refs(include_self = False, include_book = True) == [Ref('Pesach Haggadah, Magid, First Fruits Declaration'), Ref('Pesach Haggadah, Magid'), Ref('Pesach Haggadah')]
        assert Ref("Pesach Haggadah, Magid, First Fruits Declaration 2") .all_context_refs(include_self = False, include_book = False) == [Ref('Pesach Haggadah, Magid, First Fruits Declaration'), Ref('Pesach Haggadah, Magid')]

        # Don't choke on Schema nodes.
        assert Ref("Pesach Haggadah, Magid").all_context_refs() == [Ref("Pesach Haggadah, Magid")]

        # Don't choke on Virtual nodes
        assert Ref("Jastrow, ").all_context_refs() == [Ref("Jastrow, "), Ref('Jastrow<d>')]

    # These won't work unless the sheet is present in the db
    @pytest.mark.deep
    def test_sheet_refs(self):
        assert Ref("Sheet 4:3").all_context_refs() == [Ref('Sheet 4:3'), Ref('Sheet 4')]

    def test_context_ref(self):
        assert Ref("Genesis 2:3").context_ref().normal() == "Genesis 2"
        assert Ref("Rashi on Genesis 2:3:1").context_ref().normal() == "Rashi on Genesis 2:3"
        assert Ref("Rashi on Genesis 2:3:1").context_ref(2).normal() == "Rashi on Genesis 2"

    def test_section_ref(self):
        assert Ref("Rashi on Genesis 2:3:1").section_ref().normal() == "Rashi on Genesis 2:3"
        assert Ref("Genesis 2:3").section_ref().normal() == "Genesis 2"
        assert Ref("Shabbat 4a").section_ref().normal() == "Shabbat 4a"

    def test_top_section_ref(self):
        assert Ref("Job 4:5").top_section_ref().normal() == "Job 4"
        assert Ref("Rashi on Genesis 1:2:3").top_section_ref().normal() == "Rashi on Genesis 1"
        assert Ref("Genesis").top_section_ref().normal() == "Genesis 1"

    def test_next_ref(self):
        assert Ref("Job 4:5").next_section_ref().normal() == "Job 5"
        assert Ref("Shabbat 4b").next_section_ref().normal() == "Shabbat 5a"
        assert Ref("Shabbat 5a").next_section_ref().normal() == "Shabbat 5b"
        assert Ref("Rashi on Genesis 5:32:2").next_section_ref().normal() == "Rashi on Genesis 6:2"
        assert Ref("Berakhot 64a").next_section_ref() is None
        assert Ref("Rif Chullin 43a").next_section_ref().normal() == "Rif Chullin 44b"

    def test_complex_next_ref(self): #at time of test we only had complex commentaries stable to test with
        assert Ref('Pesach Haggadah, Kadesh').next_section_ref().normal() == 'Pesach Haggadah, Urchatz'
        assert Ref('Orot, Lights from Darkness, Lights of Rebirth 72').next_section_ref().normal() == 'Orot, Lights from Darkness, Great Calling'
        assert Ref('Orot, Lights from Darkness, Great Calling').next_section_ref().normal() == 'Orot, The Process of Ideals in Israel, The Godly and the National Ideal in the Individual'
        assert Ref('Ephod Bad on Pesach Haggadah, Magid, The Four Sons 1').next_section_ref().normal() == 'Ephod Bad on Pesach Haggadah, Magid, The Four Sons 2'
        assert Ref('Ephod Bad on Pesach Haggadah, Magid, In the Beginning Our Fathers Were Idol Worshipers 5').next_section_ref().normal() == 'Ephod Bad on Pesach Haggadah, Magid, First Fruits Declaration 2'
        assert Ref("Naftali Seva Ratzon on Pesach Haggadah, Kadesh 2").next_section_ref().normal() == "Naftali Seva Ratzon on Pesach Haggadah, Karpas 1"
        assert Ref("Naftali Seva Ratzon on Pesach Haggadah, Magid, Ha Lachma Anya 2").next_section_ref().normal() == "Naftali Seva Ratzon on Pesach Haggadah, Magid, We Were Slaves in Egypt 2"
        assert Ref("Ephod Bad on Pesach Haggadah, Magid, First Half of Hallel 4").next_section_ref().normal() == "Ephod Bad on Pesach Haggadah, Barech, Pour Out Thy Wrath 2"
        assert Ref("Kos Shel Eliyahu on Pesach Haggadah, Magid, Second Cup of Wine 2").next_section_ref() is Ref('Kos Eliyahu on Pesach Haggadah, Barech, Pour Out Thy Wrath 2')

    def test_prev_ref(self):
        assert Ref("Job 4:5").prev_section_ref().normal() == "Job 3"
        assert Ref("Shabbat 4b").prev_section_ref().normal() == "Shabbat 4a"
        assert Ref("Shabbat 5a").prev_section_ref().normal() == "Shabbat 4b"
        assert Ref("Rashi on Genesis 6:2:1").prev_section_ref().normal() == "Rashi on Genesis 5:32"
        assert Ref("Berakhot 2a").prev_section_ref() is None
        assert Ref("Rif Chullin 44b").prev_section_ref().normal() == "Rif Chullin 43a"

    def test_complex_prev_ref(self):
        assert Ref('Pesach Haggadah, Urchatz').prev_section_ref().normal() == 'Pesach Haggadah, Kadesh'
        assert Ref('Orot, Lights from Darkness, Great Calling').prev_section_ref().normal() == 'Orot, Lights from Darkness, Lights of Rebirth 72'
        assert Ref('Orot, The Process of Ideals in Israel, The Godly and the National Ideal in the Individual').prev_section_ref().normal() == 'Orot, Lights from Darkness, Great Calling'
        assert Ref('Ephod Bad on Pesach Haggadah, Magid, The Four Sons 2').prev_section_ref().normal() == 'Ephod Bad on Pesach Haggadah, Magid, The Four Sons 1'
        assert Ref('Ephod Bad on Pesach Haggadah, Magid, First Fruits Declaration 2').prev_section_ref().normal() == 'Ephod Bad on Pesach Haggadah, Magid, In the Beginning Our Fathers Were Idol Worshipers 5'
        assert Ref("Naftali Seva Ratzon on Pesach Haggadah, Karpas 1").prev_section_ref().normal() == "Naftali Seva Ratzon on Pesach Haggadah, Kadesh 2"
        assert Ref("Naftali Seva Ratzon on Pesach Haggadah, Magid, We Were Slaves in Egypt 2").prev_section_ref().normal() == "Naftali Seva Ratzon on Pesach Haggadah, Magid, Ha Lachma Anya 2"
        assert Ref("Ephod Bad on Pesach Haggadah, Hallel, Second Half of Hallel 2").prev_section_ref().normal() == "Ephod Bad on Pesach Haggadah, Barech, Pour Out Thy Wrath 2"
        assert Ref("Kos Shel Eliyahu on Pesach Haggadah, Magid, Ha Lachma Anya 3").prev_section_ref() is None

    def test_next_segment_ref(self):
        assert Ref("Exodus 4:1").next_segment_ref() == Ref("Exodus 4:2")
        assert Ref("Exodus 3:22").next_segment_ref() == Ref("Exodus 4:1")
        assert Ref("Rashi on Exodus 3:1:1").next_segment_ref() == Ref("Rashi on Exodus 3:1:2")
        assert Ref("Rashi on Exodus 2:25:1").next_segment_ref() == Ref("Rashi on Exodus 3:1:1")
        assert Ref("Rashi on Exodus 3:19:2").next_segment_ref() == Ref("Rashi on Exodus 3:22:1")
        assert Ref("Shabbat 5b:9").next_segment_ref() == Ref("Shabbat 5b:10")
        assert Ref("Shabbat 5b:11").next_segment_ref() == Ref("Shabbat 6a:1")
        assert Ref("Rashi on Shabbat 5b:5:4").next_segment_ref() == Ref("Rashi on Shabbat 5b:5:5")
        assert Ref("Rashi on Shabbat 6a:1:1").next_segment_ref() == Ref("Rashi on Shabbat 6a:3:1")
        assert Ref("Rashi on Shabbat 5b:10:1").next_segment_ref() == Ref("Rashi on Shabbat 6a:1:1")

    def test_prev_segment_ref(self):
        assert Ref("Exodus 4:3").prev_segment_ref() == Ref("Exodus 4:2")
        assert Ref("Exodus 4:1").prev_segment_ref() == Ref("Exodus 3:22")
        assert Ref("Rashi on Exodus 3:1:2").prev_segment_ref() == Ref("Rashi on Exodus 3:1:1")
        assert Ref("Rashi on Exodus 3:1:1").prev_segment_ref() == Ref("Rashi on Exodus 2:25:1")
        assert Ref("Rashi on Exodus 3:22:1").prev_segment_ref() == Ref("Rashi on Exodus 3:19:2")
        assert Ref("Shabbat 5b:10").prev_segment_ref() == Ref("Shabbat 5b:9")
        assert Ref("Shabbat 6a:1").prev_segment_ref() == Ref("Shabbat 5b:11")
        assert Ref("Rashi on Shabbat 5b:5:5").prev_segment_ref() == Ref("Rashi on Shabbat 5b:5:4")
        assert Ref("Rashi on Shabbat 6a:3:1").prev_segment_ref() == Ref("Rashi on Shabbat 6a:1:1")
        assert Ref("Rashi on Shabbat 6a:1:1").prev_segment_ref() == Ref("Rashi on Shabbat 5b:10:1")

    def test_last_segment_ref(self):
        assert Ref("Exodus").last_segment_ref() == Ref('Exodus 40:38')
        assert Ref("Rashi on Exodus").last_segment_ref() == Ref('Rashi on Exodus 40:38:1')
        assert Ref("Shabbat").last_segment_ref() == Ref('Shabbat 157b:3')
        assert Ref("Rashi on Shabbat").last_segment_ref() == Ref("Rashi on Shabbat 157b:2:2")

    def test_range_depth(self):
        assert Ref("Leviticus 15:3 - 17:12").range_depth() == 2
        assert Ref("Leviticus 15-17").range_depth() == 2
        assert Ref("Leviticus 15:17-21").range_depth() == 1
        assert Ref("Leviticus 15:17").range_depth() == 0
        assert Ref("Shabbat 15a-16b").range_depth() == 2
        assert Ref("Shabbat 15a").range_depth() == 0
        assert Ref("Shabbat 15a:15-15b:13").range_depth() == 2

        assert Ref("Rashi on Leviticus 15:3-17:12").range_depth() == 3
        assert Ref("Rashi on Leviticus 15-17").range_depth() == 3
        assert Ref("Rashi on Leviticus 15:17-21").range_depth() == 2
        assert Ref("Rashi on Leviticus 15:17").range_depth() == 0
        assert Ref("Rashi on Shabbat 15a-16b").range_depth() == 3
        assert Ref("Rashi on Shabbat 15a").range_depth() == 0
        assert Ref("Rashi on Shabbat 15a:15-15b:13").range_depth() == 3
        assert Ref("Rashi on Exodus 3:1-4:1").range_depth() == 3
        assert Ref("Rashi on Exodus 3:1-4:10").range_depth() == 3
        assert Ref("Rashi on Exodus 3:1-3:10").range_depth() == 2
        assert Ref("Rashi on Exodus 3:1:1-3:1:3").range_depth() == 1

    def test_range_index(self):
        assert Ref("Leviticus 15:3 - 17:12").range_index() == 0
        assert Ref("Leviticus 15-17").range_index() == 0
        assert Ref("Leviticus 15:17-21").range_index() == 1
        assert Ref("Leviticus 15:17").range_index() == 2
        assert Ref("Shabbat 15a-16b").range_index() == 0
        assert Ref("Shabbat 15a").range_index() == 2
        assert Ref("Shabbat 15a:15-15b:13").range_index() == 0

        assert Ref("Rashi on Leviticus 15:3-17:12").range_index() == 0
        assert Ref("Rashi on Leviticus 15-17").range_index() == 0
        assert Ref("Rashi on Leviticus 15:17-21").range_index() == 1
        assert Ref("Rashi on Leviticus 15:17").range_index() == 3
        assert Ref("Rashi on Shabbat 15a-16b").range_index() == 0
        assert Ref("Rashi on Shabbat 15a").range_index() == 3
        assert Ref("Rashi on Shabbat 15a:15-15b:13").range_index() == 0
        assert Ref("Rashi on Exodus 3:1-4:1").range_index() == 0
        assert Ref("Rashi on Exodus 3:1-4:10").range_index() == 0
        assert Ref("Rashi on Exodus 3:1-3:10").range_index() == 1
        assert Ref("Rashi on Exodus 3:1:1-3:1:3").range_index() == 2

    def test_out_of_order_range(self):
        with pytest.raises(InputError):
            r = Ref("Leviticus 15 - 13")
        with pytest.raises(InputError):
            r = Ref("Leviticus 15:3 - 15:1")

    def test_to_section_segment(self):
        r = Ref("Leviticus 15")
        s = Ref("Leviticus 16:3")
        t = r.to(s)
        assert t.sections == [15,1]
        assert t.toSections == [16,3]

        r = Ref("Leviticus 15:3")
        s = Ref("Leviticus 16")
        t = r.to(s)
        assert t.sections == [15,3]
        assert t.toSections == [16, 34]

    def test_pad_to_last_segment_ref(self):
        r = Ref("Leviticus 16")
        assert r.pad_to_last_segment_ref().sections == [16,34]

        r = Ref("Leviticus")
        assert r.pad_to_last_segment_ref() == r.last_segment_ref()

    def test_span_size(self):
        assert Ref("Leviticus 15:3 - 17:12").span_size() == 3
        assert Ref("Leviticus 15-17").span_size() == 3
        assert Ref("Leviticus 15:17-21").span_size() == 1
        assert Ref("Leviticus 15:17").span_size() == 1
        assert Ref("Shabbat 15a-16b").span_size() == 4
        assert Ref("Shabbat 15a").span_size() == 1
        assert Ref("Shabbat 15a:15-15b:13").span_size() == 2

        assert Ref("Rashi on Leviticus 15:3-17:12").span_size() == 3
        assert Ref("Rashi on Leviticus 15-17").span_size() == 3
        assert Ref("Rashi on Leviticus 15:17-21").span_size() == 5
        assert Ref("Rashi on Leviticus 15:17").span_size() == 1
        assert Ref("Rashi on Shabbat 15a-16b").span_size() == 4
        assert Ref("Rashi on Shabbat 15a").span_size() == 1
        assert Ref("Rashi on Shabbat 15a:15-15b:13").span_size() == 2
        assert Ref("Rashi on Exodus 3:1-4:1").span_size() == 2
        assert Ref("Rashi on Exodus 3:1-4:10").span_size() == 2

    def test_split_spanning_ref(self):
        assert Ref("Leviticus 15:3 - 17:12").split_spanning_ref() == [Ref('Leviticus 15:3-33'), Ref('Leviticus 16'), Ref('Leviticus 17:1-12')]
        assert Ref("Leviticus 15-17").split_spanning_ref() == [Ref('Leviticus 15'), Ref('Leviticus 16'), Ref('Leviticus 17')]
        assert Ref("Leviticus 15:17-21").split_spanning_ref() == [Ref('Leviticus 15:17-21')]
        assert Ref("Leviticus 15:17").split_spanning_ref() == [Ref('Leviticus 15:17')]
        assert Ref("Shabbat 15a-16b").split_spanning_ref() == [Ref('Shabbat 15a'), Ref('Shabbat 15b'), Ref('Shabbat 16a'), Ref('Shabbat 16b')]
        assert Ref("Shabbat 15a").split_spanning_ref() == [Ref('Shabbat 15a')]
        assert Ref("Shabbat 15a:8-15b:8").split_spanning_ref() == [Ref('Shabbat 15a:8-10'), Ref('Shabbat 15b:1-8')]
        assert Ref("Rashi on Exodus 5:3-6:7").split_spanning_ref() == [Ref('Rashi on Exodus 5:3'), Ref('Rashi on Exodus 5:4'), Ref('Rashi on Exodus 5:5'), Ref('Rashi on Exodus 5:6'), Ref('Rashi on Exodus 5:7'), Ref('Rashi on Exodus 5:8'), Ref('Rashi on Exodus 5:9'), Ref('Rashi on Exodus 5:10'), Ref('Rashi on Exodus 5:11'), Ref('Rashi on Exodus 5:12'), Ref('Rashi on Exodus 5:13'), Ref('Rashi on Exodus 5:14'), Ref('Rashi on Exodus 5:15'), Ref('Rashi on Exodus 5:16'), Ref('Rashi on Exodus 5:17'), Ref('Rashi on Exodus 5:18'), Ref('Rashi on Exodus 5:19'), Ref('Rashi on Exodus 5:20'), Ref('Rashi on Exodus 5:21'), Ref('Rashi on Exodus 5:22'), Ref('Rashi on Exodus 5:23'), Ref('Rashi on Exodus 6:1'), Ref('Rashi on Exodus 6:2'), Ref('Rashi on Exodus 6:3'), Ref('Rashi on Exodus 6:4'), Ref('Rashi on Exodus 6:5'), Ref('Rashi on Exodus 6:6'), Ref('Rashi on Exodus 6:7')]

    def test_spanning_with_empty_first_ref(self):
        r = Ref("Rashi on Genesis 21:2:3-7:3")
        refs = r.split_spanning_ref()
        assert refs[0] == Ref("Rashi on Genesis 21:3")

    def test_first_spanned_ref(self):
        tests = [
            Ref("Exodus 15:3 - 17:12"),
            Ref("Rashi on Genesis 5:3-6:7"),
            Ref("Gittin 15a:8-15b:8"),
            Ref("Rashi on Gittin 2b:1-7a:3"),
            Ref("Shabbat 6b-9a")
        ]
        for ref in tests:
            first = ref.first_spanned_ref()
            assert first == ref.split_spanning_ref()[0]

    @pytest.mark.xfail(reason="cause")
    def test_split_spanning_ref_expanded(self):
        assert Ref("Leviticus 15:3 - 17:12").split_spanning_ref() == [Ref('Leviticus 15:3-33'), Ref('Leviticus 16:1-34'), Ref('Leviticus 17:1-12')]

    def test_range_list(self):
        assert Ref("Leviticus 15:12-17").range_list() ==  [Ref('Leviticus 15:12'), Ref('Leviticus 15:13'), Ref('Leviticus 15:14'), Ref('Leviticus 15:15'), Ref('Leviticus 15:16'), Ref('Leviticus 15:17')]
        assert Ref("Shabbat 15b:5-8").range_list() ==  [Ref('Shabbat 15b:5'), Ref('Shabbat 15b:6'), Ref('Shabbat 15b:7'), Ref('Shabbat 15b:8')]

        assert Ref("Exodus 15:25-16:2").range_list() == [
                             Ref('Exodus 15:25'),
                             Ref('Exodus 15:26'),
                             Ref('Exodus 15:27'),
                             Ref('Exodus 16:1'),
                             Ref('Exodus 16:2')]

        assert Ref("Shabbat 15a:9-15b:2").range_list() == [Ref('Shabbat 15a:9'),
                                                        Ref('Shabbat 15a:10'),
                                                        Ref('Shabbat 15b:1'),
                                                        Ref('Shabbat 15b:2')]

    def test_range_list_first_and_last_segment(self):
        assert Ref("Shabbat 15a:9-15b:1").range_list() == [Ref('Shabbat 15a:9'),
                                                            Ref('Shabbat 15a:10'),
                                                            Ref('Shabbat 15b:1')]
        assert Ref("Shabbat 15a:10-15b:1").range_list() == [Ref('Shabbat 15a:10'),
                                                            Ref('Shabbat 15b:1')]
        assert Ref("Shabbat 15a:10-15b:2").range_list() == [Ref('Shabbat 15a:10'),
                                                            Ref('Shabbat 15b:1'), Ref('Shabbat 15b:2')]
        assert Ref("Exodus 15:25-16:1").range_list() == [Ref('Exodus 15:25'), Ref('Exodus 15:26'), Ref('Exodus 15:27'),
                                                         Ref('Exodus 16:1')]

    def test_stating_refs_of_span(self):
        assert Ref("Rashi on Berakhot 3a:2:1-4a:3:1").starting_refs_of_span() == [Ref("Rashi on Berakhot 3a:2:1"), Ref("Rashi on Berakhot 3b"), Ref("Rashi on Berakhot 4a")]
        assert Ref("Genesis 12:1-14:3").starting_refs_of_span() == [Ref("Genesis 12:1"), Ref("Genesis 13"), Ref("Genesis 14")]
        assert Ref("Rashi on Berakhot 3a:2:1-5:1").starting_refs_of_span() == [Ref("Rashi on Berakhot 3a:2:1")]
        assert Ref("Rashi on Berakhot 3a:4:1-6:1").starting_refs_of_span(True) == [Ref("Rashi on Berakhot 3a:4:1"), Ref("Rashi on Berakhot 3a:5"), Ref("Rashi on Berakhot 3a:6")]

    def test_as_ranged_segment_ref(self):
        assert Ref("Rashi on Berakhot").as_ranged_segment_ref() == Ref("Rashi on Berakhot 2a:1:1-64a:15:1")
        assert Ref("Berakhot").as_ranged_segment_ref() == Ref("Berakhot 2a:1-64a:15")
        assert Ref('Genesis').as_ranged_segment_ref() == Ref('Genesis.1.1-50.26')
        assert Ref('Shabbat.3a.1').as_ranged_segment_ref() == Ref('Shabbat.3a.1')
        assert Ref('Rashi on Shabbat.3b').as_ranged_segment_ref() == Ref('Rashi on Shabbat.3b.1.1-3b.13.1')
        assert Ref('Tur, Orach Chaim.57-59').as_ranged_segment_ref() == Ref('Tur, Orach Chaim.57.1-59.1')
        # empty at the end
        assert Ref('Tosafot on Bava Metzia.2a').as_ranged_segment_ref() == Ref('Tosafot on Bava Metzia.2a.1.1-2a.12.1')
        # empty at the beginning
        assert Ref('Tosafot on Bava Metzia.3a').as_ranged_segment_ref() == Ref('Tosafot on Bava Metzia.3a.1.1-3a.18.1')
        assert Ref('Genesis.1-14').as_ranged_segment_ref() == Ref('Genesis.1.1-14.24')
        #assert Ref('Pesach Haggadah, Karpas').as_ranged_segment_ref() == Ref('Pesach Haggadah, Karpas.1-4')

        # This begins at 2.1, but as_ranged_segment_ref returns 1.1
        #assert Ref('Marbeh_Lesaper_on_Pesach_Haggadah,_Kadesh').as_ranged_segment_ref() == Ref('Marbeh_Lesaper_on_Pesach_Haggadah,_Kadesh.2.1-12.1')

    def test_subref(self):
        assert Ref("Exodus").subref(5) == Ref("Exodus 5")
        assert Ref("Exodus 5").subref(5) == Ref("Exodus 5:5")
        assert Ref("Rashi on Exodus").subref(5) == Ref("Rashi on Exodus 5")
        assert Ref("Rashi on Exodus 5").subref(5) == Ref("Rashi on Exodus 5:5")
        assert Ref("Rashi on Exodus 5:5").subref(5) == Ref("Rashi on Exodus 5:5:5")
        assert Ref("Shabbat").subref(10) == Ref("Shabbat 5b")
        assert Ref("Shabbat 5b").subref(10) == Ref("Shabbat 5b:10")
        assert Ref("Rashi on Shabbat").subref(10) == Ref("Rashi on Shabbat 5b")
        assert Ref("Rashi on Shabbat 5b").subref(10) == Ref("Rashi on Shabbat 5b:10")

        assert Ref("Exodus").subref([5, 8]) == Ref("Exodus 5:8")
        assert Ref("Rashi on Exodus 5").subref([5,5]) == Ref("Rashi on Exodus 5:5:5")
        assert Ref("Rashi on Exodus").subref([5,5,5]) == Ref("Rashi on Exodus 5:5:5")

    def test_negative_subref(self):
        assert Ref("Exodus").subref(-1) == Ref("Exodus 40")
        assert Ref("Exodus").subref(-3).subref(-4) == Ref("Exodus 38:28")
        assert Ref("Rashi on Exodus").subref(-5) == Ref("Rashi on Exodus 36")
        assert Ref("Rashi on Exodus 5").subref(-1) == Ref("Rashi on Exodus 5:23")
        assert Ref("Rashi on Exodus 5:7").subref(-2) == Ref("Rashi on Exodus 5:7:3")

        assert Ref("Exodus").subref([5, -1]) == Ref("Exodus 5:23")
        assert Ref("Rashi on Exodus 5").subref([5, -1]) == Ref("Rashi on Exodus 5:5:1")

    def test_all_subrefs(self):
        assert Ref("Genesis").all_subrefs()[49] == Ref("Genesis 50")
        assert Ref("Genesis 40").all_subrefs()[22] == Ref("Genesis 40:23")

    def test_ref_regex(self):
        assert Ref("Exodus 15").regex() == '^Exodus( 15$| 15:| 15 \\d)'
        assert Ref("Exodus 15:15-17").regex() == '^Exodus( 15:15$| 15:15:| 15:15 \\d| 15:16$| 15:16:| 15:16 \\d| 15:17$| 15:17:| 15:17 \\d)'
        assert Ref("Yoma 14a").regex() == '^Yoma( 14a$| 14a:| 14a \\d)'
        assert Ref("Yoma 14a:12-15").regex() == '^Yoma( 14a:12$| 14a:12:| 14a:12 \\d| 14a:13$| 14a:13:| 14a:13 \\d| 14a:14$| 14a:14:| 14a:14 \\d| 14a:15$| 14a:15:| 14a:15 \\d)'
        assert Ref("Yoma").regex() == '^Yoma($|:| \\d)'  # This is as legacy had it

    def test_spanning_ref_regex(self):
        assert Ref("Exodus 4:30-6:2").regex() == '^Exodus( 4:30$| 4:30:| 4:30 \\d| 4:31$| 4:31:| 4:31 \\d| 5$| 5:| 5 \\d| 6:1$| 6:1:| 6:1 \\d| 6:2$| 6:2:| 6:2 \\d)'

    #todo: devise a better test of version_list()
    def test_version_list(self):
        assert len(Ref("Exodus").version_list()) > 3
        assert len(Ref("Exodus").version_list()) > len(Ref("Exodus 5").version_list())
        assert len(Ref("Shabbat").version_list()) > 3
        assert len(Ref("Shabbat").version_list()) > len(Ref("Shabbat 5b").version_list())

    def test_in_terms_of(self):
        Ref("Genesis 6:3").in_terms_of(Ref("Genesis 6")) == [3]
        Ref("Genesis 6:3").in_terms_of(Ref("Genesis")) == [6, 3]
        Ref("Genesis 6:3").in_terms_of(Ref("Genesis 6-7")) == [1, 3]
        Ref("Genesis 6").in_terms_of(Ref("Genesis 6-7")) == [1]
        Ref("Genesis 6").in_terms_of(Ref("Genesis 6")) == []

        Ref("Genesis 6:8").in_terms_of(Ref("Genesis 6:3-7:3")) == [1, 6]
        Ref("Genesis 7").in_terms_of(Ref("Genesis 6-8")) == [2]
        Ref("Genesis 7").in_terms_of(Ref("Genesis 6:5-8:5")) == [2]

        Ref("Genesis 21:5").in_terms_of(Ref("Genesis 19-21")) == [3, 5]
        Ref("Numbers 14:8").in_terms_of(Ref("Numbers 14")) == [8]

    def test_out_of_range(self):
        """
        Test exactly on the cut-off line, for each type of text that has a different algorithmic path
        """
        Ref("Genesis 50")
        Ref("Zevachim 120b")
        Ref("Jerusalem Talmud Nazir 9:6")

        with pytest.raises(InputError):
            Ref("Genesis 51")
        with pytest.raises(InputError):
            Ref("Zevachim 121a")
        # TODO currently doesn't raise error because new Yerushalmi doesn't have lengths on Index record
        # with pytest.raises(InputError):
        #     Ref("Jerusalem Talmud Nazir 10:1")

    def test_tamid(self):
        Ref("Tamid 25b")  # First amud
        Ref("Tamid 33b")  # Last amud

    def test_surrounding_ref(self):
        assert Ref("Genesis 3.3").surrounding_ref() == Ref("Genesis 3.2-4")
        assert Ref("Genesis 3.3").surrounding_ref(2) == Ref("Genesis 3.1-5")
        assert Ref("Genesis 3.3").surrounding_ref(3) == Ref("Genesis 3.1-6")

        assert Ref('Genesis 1:3-2:23').surrounding_ref() == Ref("Genesis 1:2-2:24")
        assert Ref('Genesis 1:3-2:23').surrounding_ref(2) == Ref("Genesis 1:1-2:25")
        assert Ref('Genesis 1:3-2:23').surrounding_ref(3) == Ref("Genesis 1:1-2:25")  # Chapter ends on both sides

    def test_malbim(self):
        # Used to short circuit, fail to resolve to Malachi, and fail
        assert Ref("Malbim Beur Hamilot on Ezekiel")

    def test_distance(self):
        r1 = Ref("Genesis 1:3")
        r2 = Ref("Genesis 3:4")
        assert r1.distance(r2) == 57

        r1 = Ref("Shir HaShirim Rabbah 2:12:1")
        r2 = Ref("Shir HaShirim Rabbah 2:9:5")
        assert r1.distance(r2) == 2

    def test_is_segment_level(self):
        assert Ref("Leviticus 15:3").is_segment_level()
        assert not Ref("Leviticus 15").is_segment_level()
        assert not Ref("Rashi on Leviticus 15:3").is_segment_level()
        assert Ref("Rashi on Leviticus 15:3:1").is_segment_level()
        assert not Ref("Leviticus").is_segment_level() # JA root
        assert not Ref("Orot").is_segment_level() # schema node
        assert not Ref("Orot,_Lights_from_Darkness,_Land_of_Israel").is_segment_level() # JA root in complex text
        assert not Ref("Orot,_Lights_from_Darkness,_Land_of_Israel.4").is_segment_level()
        assert Ref("Orot,_Lights_from_Darkness,_Land_of_Israel.4.1").is_segment_level()

    def test_is_section_level(self):
        assert not Ref("Leviticus 15:3").is_section_level()
        assert Ref("Leviticus 15").is_section_level()
        assert Ref("Rashi on Leviticus 15:3").is_section_level()
        assert not Ref("Rashi on Leviticus 15:3:1").is_section_level()
        assert not Ref("Leviticus").is_section_level()  # JA root
        assert not Ref("Orot").is_section_level()  # schema node
        assert not Ref("Orot,_Lights_from_Darkness,_Land_of_Israel").is_section_level()  # JA root in complex text
        assert Ref("Orot,_Lights_from_Darkness,_Land_of_Israel.4").is_section_level()
        assert not Ref("Orot,_Lights_from_Darkness,_Land_of_Israel.4.1").is_section_level()

    def test_word_to(self):
        assert Ref("Kohelet Rabbah to 6:9") is Ref("Kohelet Rabbah 6.9")

class Test_Cache(object):
    def test_index_flush_from_cache(self):
        r1 = Ref("Genesis 1")
        r2 = Ref("Exodus 3")
        Ref.remove_index_from_cache("Genesis")
        assert r1 is not Ref("Genesis 1")
        assert r2 is Ref("Exodus 3")
        Ref.remove_index_from_cache("Genesis")

        r1 = Ref("Rashi on Genesis 1")
        r2 = Ref("Rashi on Exodus 3")
        Ref.remove_index_from_cache("Rashi on Genesis")
        assert r1 is not Ref("Rashi on Genesis 1")
        assert r2 is Ref("Rashi on Exodus 3")

    def test_flush_index_not_found(self):
        Ref("Genesis 1")
        Ref.remove_index_from_cache("Genesis")
        Ref.remove_index_from_cache("Genesis")

    def test_cache_identity(self):
        assert Ref("Ramban on Genesis 1") is Ref("Ramban on Genesis 1")
        assert Ref(" ' .") is Ref(" ' .")

    def test_obj_created_cache_identity(self):
        assert Ref("Job 4") is Ref("Job 4:5").top_section_ref()
        assert Ref("Rashi on Genesis 2:3:1").context_ref() is Ref("Rashi on Genesis 2:3")

    def test_different_tref_cache_identity(self):
        assert Ref("Genesis 27:3") is Ref("Gen. 27:3")
        assert Ref("Gen. 27:3") is Ref(" .")

    def test_cache_clearing(self):
        r1 = Ref("Ramban on Genesis 1")
        Ref.clear_cache()
        r2 = Ref("Ramban on Genesis 1")
        assert r1 is not r2

    '''
    # Retired.  Since we're dealing with objects, tref will either bleed one way or the other.
    # Removed last dependencies on tref outside of object init. 
    def test_tref_bleed(self):
        # Insure that instanciating trefs are correct for this instance, and don't bleed through the cache.
        Ref(u' ')
        r = Ref("Shabbat 31a")
        assert r.tref == "Shabbat 31a"
    '''

class Test_normal_forms(object):
    def test_normal(self):
        assert Ref("Genesis 2:5").normal() == "Genesis 2:5"
        assert Ref("Shabbat 32b").normal() == "Shabbat 32b"
        assert Ref("Mishnah Peah 4:2-4").normal() == "Mishnah Peah 4:2-4"

    def test_url_form(self):
        assert Ref("Genesis 2:5").url() == "Genesis.2.5"
        assert Ref("Genesis 2:5-10").url() == "Genesis.2.5-10"
        assert Ref("Rashi on Shabbat 12a.10").url() == "Rashi_on_Shabbat.12a.10"


    def test_talmud_range_short(self):
        oref = Ref("Berakhot 2a-2b")
        assert oref.normal() == "Berakhot 2"
        assert oref.he_normal() == " "

    def test_talmud_range_long(self):
        oref = Ref("Berakhot 2a-3b")
        assert oref.normal() == "Berakhot 2-3"
        assert oref.he_normal() == " -"

    def test_talmud_range_a_to_a(self):
        oref = Ref("Berakhot 2a-3a")
        assert oref.normal() == "Berakhot 2a-3a"
        assert oref.he_normal() == "  - "

    def test_talmud_range_b_to_b(self):
        oref = Ref("Bava Metzia 20b-21b")
        oref_capitalized = Ref("Bava Metzia 20B-21B")
        assert oref.normal() == "Bava Metzia 20b-21b" == oref_capitalized.normal()
        assert oref.he_normal() == "   - " == oref_capitalized.he_normal()

    def test_talmud_segment_range(self):
        oref = Ref("Bava Metzia 20a:1-20b:1")
        assert oref.normal() == "Bava Metzia 20a:1-20b:1"
        assert oref.he_normal() == "   :- :"

    def test_talmud_aA_bB(self):
        assert Ref("Berakhot 2a") == Ref("Berakhot 2A")
        assert Ref("Berakhot 2B") == Ref("Berakhot 2B")

    @pytest.mark.skip(reason='Zohar structure has been changed. We currently have no index with talmud at second place')
    def test_zohar_volume_range(self):
        oref = Ref("Zohar 1-2")
        assert oref.normal() == "Zohar 1-2"
        assert oref.he_normal() == "  -"

    @pytest.mark.skip(reason='Zohar structure has been changed. We currently have no index with talmud at second place')
    def test_zohar_daf_range(self):
        oref = Ref("Zohar 1:25a-27b")
        assert oref.normal() == "Zohar 1:25-27"
        assert oref.he_normal() == "  :-"

    @pytest.mark.skip(reason='Zohar structure has been changed. We currently have no index with talmud at second place')
    def test_zohar_volume_daf_range(self):
        oref = Ref("Zohar 1:25a-2:27b")
        assert oref.normal() == "Zohar 1:25-2:27"
        assert oref.he_normal() == "  :-:"

    def test_first_available_section_ref(self):
        assert Ref('Genesis').first_available_section_ref() == Ref('Genesis 1')
        assert Ref('Siddur Ashkenaz').first_available_section_ref() == Ref('Siddur Ashkenaz, Weekday, Shacharit, Preparatory Prayers, Modeh Ani')
        assert Ref('Penei Moshe on Jerusalem Talmud Shabbat 2').first_available_section_ref() == Ref('Penei Moshe on Jerusalem Talmud Shabbat 2:1:1')
        assert Ref('Animadversions by Elias Levita on Sefer HaShorashim').first_available_section_ref() == Ref('Animadversions by Elias Levita on Sefer HaShorashim, ')
        assert Ref('Jastrow,  I 1').first_available_section_ref() == Ref('Jastrow,  I 1')






class Test_term_refs(object):
    def test_ref_resolution(self):
        assert Ref("bo") ==  Ref('Exodus 10:1-13:16')
        assert Ref("") == Ref("Exodus 21:1-24:18")
        assert Ref("Shemot") == Ref("Exodus")  # This behavior may change, if we spec it more carefully

    def test_term_only(self):
        with pytest.raises(InputError):
            Ref("bo and then something")
        with pytest.raises(InputError):
            assert not Ref("botox")
        with pytest.raises(InputError):
            assert not Ref(" ")


class Test_Ambiguous_Forms(object):
    def test_mishnah_check_first(self):
        assert Ref("Shabbat 8:7") == Ref('Mishnah Shabbat 8:7')
        assert Ref("Shabbat 28:7").normal() == 'Shabbat 28a:7'
        assert Ref("Shabbat 7") == Ref("Shabbat 7a-7b")
        assert Ref("Shabbat 7a:1") != Ref("Shabbat 7:1")


class Test_comparisons(object):
    def test_overlaps(self):
        assert Ref("Genesis 5:10-20").overlaps(Ref("Genesis 5:18-25"))
        assert Ref("Genesis 5:10-20").overlaps(Ref("Genesis 5:13-28"))
        assert Ref("Genesis 5:13-28").overlaps(Ref("Genesis 5:10-20"))
        assert not Ref("Genesis 5:10-20").overlaps(Ref("Genesis 5:21-25"))

        assert not Ref("Genesis 1").overlaps(Ref("Genesis 2"))
        assert not Ref("Genesis 2").overlaps(Ref("Genesis 1"))
        assert Ref("Genesis 1").overlaps(Ref("Genesis 1"))

        assert Ref("Genesis 5:10-6:20").overlaps(Ref("Genesis 6:18-25"))
        assert Ref("Genesis 5:10-6:20").overlaps(Ref("Genesis 5:18-25"))
        assert Ref("Genesis 5:18-25").overlaps(Ref("Genesis 5:10-6:20"))
        assert not Ref("Genesis 5:10-6:20").overlaps(Ref("Genesis 6:21-25"))

        assert Ref("Genesis 5").overlaps(Ref("Genesis"))
        assert Ref("Genesis").overlaps(Ref("Genesis 5"))

        assert Ref("Rashi on Genesis 5:10-20").overlaps(Ref("Rashi on Genesis 5:18-25"))
        assert not Ref("Rashi on Genesis 5:10-20").overlaps(Ref("Rashi on Genesis 5:21-25"))

        assert Ref("Rashi on Genesis 5:10-6:20").overlaps(Ref("Rashi on Genesis 6:18-25"))
        assert not Ref("Rashi on Genesis 5:10-6:20").overlaps(Ref("Rashi on Genesis 6:21-25"))

        assert not Ref("Genesis 5:10-6:20").overlaps(Ref("Rashi on Genesis 5:10-6:20"))

        assert Ref("Shabbat 5b-7a").overlaps(Ref("Shabbat 6b-9a"))
        assert not Ref("Shabbat 5b-7a").overlaps(Ref("Shabbat 15b-17a"))

        assert Ref("Shabbat 5b:10-20").overlaps(Ref("Shabbat 5b:18-20"))
        assert not Ref("Shabbat 5b:10-20").overlaps(Ref("Shabbat 5b:23-29"))

        assert Ref("Genesis 1:10-4:10").overlaps(Ref("Genesis 3:15-5:5"))


    def test_contains(self):
        assert Ref("Genesis 5:10-20").contains(Ref("Genesis 5:10-20"))
        assert Ref("Genesis 5:10-20").contains(Ref("Genesis 5:13-18"))
        assert not Ref("Genesis 5:10-20").contains(Ref("Genesis 5:21-25"))
        assert not Ref("Genesis 5:10-20").contains(Ref("Genesis 5:18-25"))

        assert Ref("Genesis 5:10-6:20").contains(Ref("Genesis 5:18-25"))
        assert Ref("Genesis 5:10-6:20").contains(Ref("Genesis 5:18-6:10"))
        assert not Ref("Genesis 5:10-6:20").contains(Ref("Genesis 6:21-25"))
        assert not Ref("Genesis 5:10-6:20").contains(Ref("Genesis 6:5-25"))

        assert Ref("Exodus 6").contains(Ref("Exodus 6:2"))
        assert Ref("Exodus 6").contains(Ref("Exodus 6:2-12"))

        assert Ref("Genesis 1:1-31").contains(Ref("Genesis 1"))
        assert Ref("Genesis 1").contains(Ref("Genesis 1:1-31"))

        assert Ref("Exodus").contains(Ref("Exodus 6"))
        assert Ref("Exodus").contains(Ref("Exodus 6:2"))
        assert Ref("Exodus").contains(Ref("Exodus 6:2-12"))

        assert not Ref("Exodus 6:2").contains(Ref("Exodus 6"))
        assert not Ref("Exodus 6:2-12").contains(Ref("Exodus 6"))

        assert not Ref("Exodus 6").contains(Ref("Exodus"))
        assert not Ref("Exodus 6:2").contains(Ref("Exodus"))
        assert not Ref("Exodus 6:2-12").contains(Ref("Exodus"))

        assert Ref("Leviticus").contains(Ref("Leviticus"))
        assert Ref("Leviticus").contains(Ref("Leviticus 1:1-27.34"))
        assert Ref("Leviticus").contains(Ref("Leviticus 1-27"))
        assert Ref("Leviticus 1:1-27.34").contains(Ref("Leviticus"))
        assert not Ref("Leviticus 1:1-27.30").contains(Ref("Leviticus"))
        assert not Ref("Leviticus 1:2-27.30").contains(Ref("Leviticus"))
        assert not Ref("Leviticus 2:2-27.30").contains(Ref("Leviticus"))

        # These fail, and always did
        # assert not Ref("Leviticus").contains(Ref("Leviticus 1:1-27.35"))
        # assert not Ref("Leviticus").contains(Ref("Leviticus 1-28"))

        assert Ref("Rashi on Genesis 5:10-20").contains(Ref("Rashi on Genesis 5:18-20"))
        assert not Ref("Rashi on Genesis 5:10-20").contains(Ref("Rashi on Genesis 5:21-25"))
        assert not Ref("Rashi on Genesis 5:10-20").contains(Ref("Rashi on Genesis 5:15-25"))

        assert Ref("Rashi on Genesis 5:10-6:20").contains(Ref("Rashi on Genesis 6:18-19"))
        assert not Ref("Rashi on Genesis 5:10-6:20").contains(Ref("Rashi on Genesis 6:21-25"))
        assert not Ref("Rashi on Genesis 5:10-6:20").contains(Ref("Rashi on Genesis 6:5-25"))

        assert not Ref("Genesis 5:10-6:20").contains(Ref("Rashi on Genesis 5:10-6:20"))
        assert not Ref("Rashi on Genesis 5:10-6:20").contains(Ref("Genesis 5:10-6:20"))

        assert Ref("Shabbat 5b-7a").contains(Ref("Shabbat 6b-7a"))
        assert not Ref("Shabbat 5b-7a").contains(Ref("Shabbat 15b-17a"))
        assert not Ref("Shabbat 5b-7a").contains(Ref("Shabbat 6b-17a"))

        assert Ref("Shabbat 5b:10-20").contains(Ref("Shabbat 5b:18-20"))
        assert not Ref("Shabbat 5b:10-20").contains(Ref("Shabbat 5b:23-29"))
        assert not Ref("Shabbat 5b:10-20").contains(Ref("Shabbat 5b:15-29"))

        assert not Ref("Steinsaltz_on_Jerusalem_Talmud_Shekalim.4.4.42-5.1.10").contains(Ref("Steinsaltz on Jerusalem Talmud Shekalim 4:4:1"))

        assert Ref("Jastrow").contains(Ref("Jastrow,  1"))
        assert not Ref("Jastrow,  1").contains(Ref("Jastrow"))


    def test_precedes(self):
        assert Ref("Genesis 5:10-20").precedes(Ref("Genesis 5:21-25"))
        assert Ref("Genesis 5:10-20").precedes(Ref("Genesis 7:21-25"))
        assert Ref("Genesis 5:10-20").precedes(Ref("Genesis 7"))
        assert Ref("Genesis 5:10-20").precedes(Ref("Genesis 7:21"))

        assert not Ref("Genesis").precedes(Ref("Genesis 5"))
        assert not Ref("Genesis").precedes(Ref("Genesis 5:16"))
        assert not Ref("Genesis").precedes(Ref("Genesis 5:16-25"))

        assert not Ref("Genesis 4").precedes(Ref("Genesis"))
        assert not Ref("Genesis 4:3").precedes(Ref("Genesis"))
        assert not Ref("Genesis 4:3-5").precedes(Ref("Genesis"))

        assert not Ref("Genesis 5:10-20").precedes(Ref("Genesis 5:16-25"))
        assert not Ref("Genesis 5:10-20").precedes(Ref("Genesis 4:18-25"))

        assert Ref("Genesis 5:10-6:20").precedes(Ref("Genesis 6:23-25"))
        assert Ref("Genesis 5:10-6:20").precedes(Ref("Genesis 6:21-8:10"))
        assert not Ref("Genesis 5:10-6:20").precedes(Ref("Genesis 6:5-25"))
        assert not Ref("Genesis 5:10-6:20").precedes(Ref("Genesis 6:5"))
        assert not Ref("Genesis 5:10-6:20").precedes(Ref("Genesis 4:12"))
        assert not Ref("Genesis 5:10-6:20").precedes(Ref("Genesis 5:5"))
        assert not Ref("Genesis 5:10-6:20").precedes(Ref("Genesis 5"))

        assert not Ref("Rashi on Genesis 5:10-20").precedes(Ref("Rashi on Genesis 5:18-20"))
        assert Ref("Rashi on Genesis 5:10-20").precedes(Ref("Rashi on Genesis 5:21-25"))
        assert not Ref("Rashi on Genesis 5:10-20").precedes(Ref("Rashi on Genesis 5:15-25"))

        assert not Ref("Rashi on Genesis 5:10-6:20").precedes(Ref("Rashi on Genesis 6:18-19"))
        assert Ref("Rashi on Genesis 5:10-6:20").precedes(Ref("Rashi on Genesis 6:21-25"))
        assert not Ref("Rashi on Genesis 5:10-6:20").precedes(Ref("Rashi on Genesis 6:5-25"))

        assert not Ref("Genesis 5:10-6:20").precedes(Ref("Rashi on Genesis 5:10-6:20"))
        assert not Ref("Rashi on Genesis 5:10-6:20").precedes(Ref("Genesis 5:10-6:20"))

        assert not Ref("Shabbat 5b-7a").precedes(Ref("Shabbat 6b-7a"))
        assert Ref("Shabbat 5b-7a").precedes(Ref("Shabbat 15b-17a"))
        assert not Ref("Shabbat 5b-7a").precedes(Ref("Shabbat 6b-17a"))

        assert not Ref("Shabbat 5b:10-20").precedes(Ref("Shabbat 5b:18-20"))
        assert Ref("Shabbat 5b:10-20").precedes(Ref("Shabbat 5b:23-29"))
        assert not Ref("Shabbat 5b:10-20").precedes(Ref("Shabbat 5b:15-29"))


    def test_follows(self):
        assert Ref("Genesis 5:21-25").follows(Ref("Genesis 5:10-20"))
        assert Ref("Genesis 7:21-25").follows(Ref("Genesis 5:10-20"))
        assert Ref("Genesis 7").follows(Ref("Genesis 5:10-20"))
        assert Ref("Genesis 7:21").follows(Ref("Genesis 5:10-20"))

        assert not Ref("Genesis").follows(Ref("Genesis 5"))
        assert not Ref("Genesis").follows(Ref("Genesis 5:16"))
        assert not Ref("Genesis").follows(Ref("Genesis 5:16-25"))

        assert not Ref("Genesis 4").follows(Ref("Genesis"))
        assert not Ref("Genesis 4:3").follows(Ref("Genesis"))
        assert not Ref("Genesis 4:3-5").follows(Ref("Genesis"))

        assert not Ref("Genesis 5:16-25").follows(Ref("Genesis 5:10-20"))
        assert not Ref("Genesis 4:18-25").follows(Ref("Genesis 5:10-20"))

        assert Ref("Genesis 6:23-25").follows(Ref("Genesis 5:10-6:20"))
        assert Ref("Genesis 6:21-8:10").follows(Ref("Genesis 5:10-6:20"))
        assert not Ref("Genesis 6:5-25").follows(Ref("Genesis 5:10-6:20"))
        assert not Ref("Genesis 6:5").follows(Ref("Genesis 5:10-6:20"))
        assert not Ref("Genesis 4:12").follows(Ref("Genesis 5:10-6:20"))
        assert not Ref("Genesis 5:5").follows(Ref("Genesis 5:10-6:20"))
        assert not Ref("Genesis 5").follows(Ref("Genesis 5:10-6:20"))

        assert not Ref("Rashi on Genesis 5:18-20").follows(Ref("Rashi on Genesis 5:10-20"))
        assert Ref("Rashi on Genesis 5:21-25").follows(Ref("Rashi on Genesis 5:10-20"))
        assert not Ref("Rashi on Genesis 5:15-25").follows(Ref("Rashi on Genesis 5:10-20"))

        assert not Ref("Rashi on Genesis 6:18-19").follows(Ref("Rashi on Genesis 5:10-6:20"))
        assert Ref("Rashi on Genesis 6:21-25").follows(Ref("Rashi on Genesis 5:10-6:20"))
        assert not Ref("Rashi on Genesis 6:5-25").follows(Ref("Rashi on Genesis 5:10-6:20"))

        assert not Ref("Rashi on Genesis 5:10-6:20").follows(Ref("Genesis 5:10-6:20"))
        assert not Ref("Genesis 5:10-6:20").follows(Ref("Rashi on Genesis 5:10-6:20"))

        assert not Ref("Shabbat 6b-7a").follows(Ref("Shabbat 5b-7a"))
        assert Ref("Shabbat 15b-17a").follows(Ref("Shabbat 5b-7a"))
        assert not Ref("Shabbat 6b-17a").follows(Ref("Shabbat 5b-7a"))

        assert not Ref("Shabbat 5b:18-20").follows(Ref("Shabbat 5b:10-20"))
        assert Ref("Shabbat 5b:23-29").follows(Ref("Shabbat 5b:10-20"))
        assert not Ref("Shabbat 5b:15-29").follows(Ref("Shabbat 5b:10-20"))

@pytest.mark.skip(reason='Zohar structure has been changed. We currently have no index with talmud at second place')
class Test_Talmud_at_Second_Place(object):
    def test_simple_ref(self):
        assert Ref("Zohar 1.15b.3").sections[1] == 30
        assert Ref("Zohar 1.15a.3").sections[1] == 29
        assert Ref("Zohar 2.15b.3").sections[1] == 30
        assert Ref("Zohar 2.15a.3").sections[1] == 29
        assert Ref("Zohar 3.15b.3").sections[1] == 30
        assert Ref("Zohar 3.15a.3").sections[1] == 29

        assert Ref("Zohar 1.15b").sections[1] == 30
        assert Ref("Zohar 1.15a").sections[1] == 29
        assert Ref("Zohar 2.15b").sections[1] == 30
        assert Ref("Zohar 2.15a").sections[1] == 29
        assert Ref("Zohar 3.15b").sections[1] == 30
        assert Ref("Zohar 3.15a").sections[1] == 29

        assert Ref("Zohar 1.15b.3").sections[2] == 3
        assert Ref("Zohar 2.15b.3").sections[2] == 3
        assert Ref("Zohar 3.15b.3").sections[2] == 3

    def test_range(self):
        assert Ref("Zohar 1.10a:1 - 15b.3").toSections[1] == 30
        assert Ref("Zohar 1.10a:1 - 15a.3").toSections[1] == 29
        assert Ref("Zohar 2.10a:1 - 15b.3").toSections[1] == 30
        assert Ref("Zohar 2.10a:1 - 15a.3").toSections[1] == 29
        assert Ref("Zohar 3.10a:1 - 15b.3").toSections[1] == 30
        assert Ref("Zohar 3.10a:1 - 15a.3").toSections[1] == 29

        assert Ref("Zohar 1.10a - 15b").toSections[1] == 30
        assert Ref("Zohar 1.10a - 15a").toSections[1] == 29
        assert Ref("Zohar 2.10a - 15b").toSections[1] == 30
        assert Ref("Zohar 2.10a - 15a").toSections[1] == 29
        assert Ref("Zohar 3.10a - 15b").toSections[1] == 30
        assert Ref("Zohar 3.10a - 15a").toSections[1] == 29

    def test_cross_volume_range(self):
        assert Ref("Zohar 1.50a - 2.15b").toSections[1] == 30
        assert Ref("Zohar 1.50a - 2.15a").toSections[1] == 29
        assert Ref("Zohar 2.50a - 3.15b").toSections[1] == 30
        assert Ref("Zohar 2.50a - 3.15a").toSections[1] == 29
        assert Ref("Zohar 1.50a - 3.15b").toSections[1] == 30
        assert Ref("Zohar 1.50a - 3.15a").toSections[1] == 29

    def test_Zohar_Parsha_ref(self):
        assert Ref("Zohar, Lech Lecha")
        assert Ref("Zohar, Bo")

    def test_range_short_form(self):
        assert Ref("Zohar 2.15a - 15b").sections[1] == 29
        assert Ref("Zohar 2.15a - 15b").toSections[1] == 30
        assert Ref("Zohar 2.15a - b").sections[1] == 29
        assert Ref("Zohar 2.15a - b").toSections[1] == 30

class Test_condition_and_projection(object):
    def test_condition(self):
        #many variations
        pass

    def test_projection_simple_section(self):
        r = Ref("Exodus")
        p = r.part_projection()
        assert all([k in p for k in Version.required_attrs + Version.optional_attrs if k != Version.content_attr])
        assert Version.content_attr in p
        assert p[Version.content_attr] == 1

        # Todo: test Version objects returned
        """
        vs = VersionSet(r.condition_query(), p)
        assert vs.count() > 0
        for v in vs:
            assert ...
        """

    def test_projection_complex_section(self):
        r = Ref('Shelah, Torah Shebikhtav, Bereshit, Torah Ohr')
        p = r.part_projection()
        assert all([k in p for k in Version.required_attrs + Version.optional_attrs if k != Version.content_attr])
        assert Version.content_attr not in p
        assert 'chapter.Torah Shebikhtav.Bereshit.Torah Ohr' in p
        assert p['chapter.Torah Shebikhtav.Bereshit.Torah Ohr'] == 1

    def test_projection_simple_segment_slice(self):
        r = Ref("Exodus 4")
        p = r.part_projection()
        assert all([k in p for k in Version.required_attrs + Version.optional_attrs if k != Version.content_attr])
        assert Version.content_attr in p
        assert p[Version.content_attr] == {"$slice": [3, 1]}

    def test_projection_simple_segment_range_slice(self):
        r = Ref("Exodus 4-7")
        p = r.part_projection()
        assert all([k in p for k in Version.required_attrs + Version.optional_attrs if k != Version.content_attr])
        assert Version.content_attr in p
        assert p[Version.content_attr] == {"$slice": [3, 4]}

        r = Ref("Exodus 4:3-7:1")
        p = r.part_projection()
        assert all([k in p for k in Version.required_attrs + Version.optional_attrs if k != Version.content_attr])
        assert Version.content_attr in p
        assert p[Version.content_attr] == {"$slice": [3, 4]}


    def test_projection_complex_segment_slice(self):
        r = Ref('Shelah, Torah Shebikhtav, Bereshit, Torah Ohr 52')
        p = r.part_projection()
        assert all([k in p for k in Version.required_attrs + Version.optional_attrs if k != Version.content_attr])
        assert Version.content_attr not in p
        assert 'chapter.Torah Shebikhtav.Bereshit.Torah Ohr' in p
        assert p['chapter.Torah Shebikhtav.Bereshit.Torah Ohr'] == {"$slice": [51, 1]}

    def test_projection_complex_segment_range_slice(self):
        r = Ref('Shelah, Torah Shebikhtav, Bereshit, Torah Ohr 50-52')
        p = r.part_projection()
        assert all([k in p for k in Version.required_attrs + Version.optional_attrs if k != Version.content_attr])
        assert Version.content_attr not in p
        assert 'chapter.Torah Shebikhtav.Bereshit.Torah Ohr' in p
        assert p['chapter.Torah Shebikhtav.Bereshit.Torah Ohr'] == {"$slice": [49, 3]}


class Test_set_construction_from_ref(object):
    def test_ref_noteset(self):
        pass

    def test_ref_linkset(self):
        pass


class Test_Order_Id(object):
    def test_order_id_processes(self):
        assert Ref("Klein Dictionary, ").order_id()
        assert Ref("Shabbat 17b").order_id()
        assert Ref("Job 15:13").order_id()
        assert Ref("Shabbat 12a:14").order_id()
        assert Ref("Rashi on Shabbat 17b:12").order_id()
        assert Ref("Tosafot on Yoma 25a:24").order_id()

    def test_ordering_of_order_id(self):
        assert Ref("Job 15:13").order_id() < Ref("Shabbat 17b").order_id()
        assert Ref("Shabbat 12b").order_id() < Ref("Shabbat 17b").order_id()
        assert Ref("Shabbat 12b").order_id() < Ref("Bava Kamma 17b").order_id()

    def test_ordering_of_complex_texts(self):
        assert Ref("Meshekh Chokhmah, Vaera 2").order_id() > Ref("Meshekh Chokhmah, Shemot 6").order_id()

    def test_ordering_of_dictionary(self):
        i = library.get_index("Klein Dictionary")
        first = i.nodes.get_default_child().first_child()
        second = first.next_sibling()
        third = second.next_sibling()

        assert first.ref().order_id() < second.ref().order_id()
        assert second.ref().order_id() < third.ref().order_id()

'''
class Test_ref_manipulations():

    def test_section_level_ref(self):
        assert t.section_level_ref("Rashi on Genesis 2:3:1") == "Rashi on Genesis 2:3"
        assert t.section_level_ref("Genesis 2:3") == "Genesis 2"
        assert t.section_level_ref("Shabbat 4a") == "Shabbat 4a"

    def test_list_refs_in_range(self):
        assert t.list_refs_in_range("Job 4:5-9") == ["Job 4:5","Job 4:6","Job 4:7","Job 4:8","Job 4:9"]
        assert t.list_refs_in_range("Genesis 2:3") == ["Genesis 2:3"]
'''

```

### sefaria/model/tests/ref_catching_test.py

```
# -*- coding: utf-8 -*-

import django
django.setup()
import pytest
import regex as re
from sefaria.utils.hebrew import has_hebrew
import sefaria.model as m



class In(object):
    """
    Support test assertions for `library.get_regex_string`

    Examples:
        assert In('Ruth 1 1').looking_for('Ruth').finds("Ruth 1:1")
        assert In('( ")').looking_for('').with_parenthesis().finds("Leviticus 25")
        assert In(' "').looking_for('').with_parenthesis().finds_nothing()

    """
    def __init__(self, haystack):
        self._haystack = haystack
        self._needle = ""
        self._with_parenthesis = False

    def looking_for(self, needle):
        self._needle = needle
        return self

    def with_parenthesis(self):
        self._with_parenthesis = True
        return self

    def finds(self, result):
        match = self._do_search(self._needle, self._haystack)
        if not match:
            return False
        if m.Ref(match.group(1)).normal() == result:
            return True
        else:
            print("Mismatched.  Found: {}, which normalizes to: {}, not {}".format(match.group(1), m.Ref(match.group(1)).normal(), result))
            return False

    def finds_multiple(self, result):
        lang = "he" if has_hebrew(self._needle) else "en"
        for title_match in m.library.all_titles_regex(lang, citing_only=False).finditer(self._haystack):
            match = self._do_search(self._needle, self._haystack[title_match.start():])
            if not match:
                return False
            if m.Ref(match.group(1)).normal() in result:
                return True
            else:
                print("Mismatched.  Found: {}, which normalizes to: {}, which is not in {}".format(match.group(1),
                                                                                       m.Ref(match.group(1)).normal(),
                                                                                       result))
                return False

    def finds_nothing(self):
        return not self._do_search(self._needle, self._haystack)

    def _do_search(self, needle, haystack):
        lang = "he" if has_hebrew(needle) else "en"
        reg_str = m.library.get_regex_string(
            needle, lang, for_js=True, anchored=False, capture_title=False, parentheses=self._with_parenthesis)
        reg = re.compile(reg_str, re.VERBOSE)
        match = reg.search(haystack)
        return match


class Test_find_citation_in_text(object):

    def test_regex_string_en_js(self):
        assert In('Ruth 1 1').looking_for('Ruth').finds("Ruth 1:1")
        assert In("Genesis 1:2-3").looking_for("Genesis").finds("Genesis 1:2-3")
        assert In("Genesis 1-3").looking_for("Genesis").finds("Genesis 1-3")

    def test_regex_string_en_array(self):
        assert In("Genesis 2:1-Genesis 2:3").looking_for("Genesis").finds_multiple(["Genesis 2:1", "Genesis 2:3"])
        assert In("Genesis 2:1/Bereshit 2:3").looking_for("Genesis").finds_multiple(["Genesis 2:1", "Genesis 2:3"])

    def test_regex_string_he_js_with_prefix(self):
        assert In(' "').looking_for('').finds("Leviticus 25")

    def test_regex_string_he_in_parentheses_only(self):
        assert In('( ")').looking_for('').with_parenthesis().finds("Leviticus 25")
        assert In(' "').looking_for('').with_parenthesis().finds_nothing()

    def test_regex_string_he_in_parentheses(self):
        assert In("(    )").looking_for('').with_parenthesis().finds("Deuteronomy 32")
        assert In("(    )").looking_for('').with_parenthesis().finds_nothing()

    def test_regex_string_he_in_parentheses_3(self):
        assert In('<p>[     (   )  ]')\
            .looking_for(' ').with_parenthesis().finds("Jerusalem Talmud Ketubot 1:2")

        assert In('<p>[     ( :)  ]')\
            .looking_for(' ').with_parenthesis().finds("Song of Songs 1")

    def test_check_first(self):
        assert In('  " ",  ').looking_for(' ').finds("Mishnah Bava Metzia 4:6")

```

### sefaria/model/tests/library_test.py

```
# -*- coding: utf-8 -*-

import pytest
from sefaria.model import *
from functools import reduce


def setup_module(module):
    global texts
    global refs
    texts = {}
    refs = {}
    texts['bible_mid'] = "Here we have Genesis 3:5 may it be blessed"
    texts['bible_begin'] = "Genesis 3:5 in the house"
    texts['bible_end'] = "Let there be Genesis 3:5"
    texts['2ref'] = "This is a test of a Brachot 7b and also of an Isaiah 12:13."
    texts['barenum'] = "In this text, there is no reference but there is 1 bare number."

    texts['false_pos'] = "  ( , )   "
    texts['bible_ref'] = "        ( , )   "
    texts['he_bible_begin'] = "( , )     "  # These work, even though the presentation of the parens may be confusing.
    texts['he_bible_mid'] = " ( , )    "
    texts['he_bible_end'] = "  ( , )"
    texts['he_2ref'] = "  (    ),  (  ):   "
    texts['neg327'] = '    ,   " ,  " ( ,;  ,;  ,;  ,).'
    texts['2talmud'] = "        (  ).       (''  ).   "
    texts['bk-abbrev'] = "        (  ).       (\"  ).   "
    texts['dq_talmud'] = '( ")'
    texts['sq_talmud'] = ""  # Need to find one in the wild
    texts['3dig'] = '( ")'
    texts['2with_lead'] = '(  ,;  ,)'
    texts['ignored_middle'] = '( , )           ,                                       ( , )'
    texts['weird_ref'] = "In this string The Book of Susanna 1.2 should match the long regex only, but Leviticus 12.4 should match both of them"
    texts['weird_ref_he'] = " (   )     ,   (  )   "


class Test_get_refs_in_text(object):

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_bare_digits(self, citing_only):
        assert set() == set(library.get_refs_in_string(texts['barenum'], citing_only=citing_only)) # Fixed in 5a4b813819ef652def8360da2ac1b7539896c732

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_positions(self, citing_only):
        for a in ['bible_mid','bible_begin', 'bible_end']:
            ref = library.get_refs_in_string(texts[a], citing_only=citing_only)
            assert 1 == len(ref)
            assert ref[0] == Ref("Genesis 3:5")

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_multiple(self, citing_only):
        ref = library.get_refs_in_string(texts['2ref'], citing_only=citing_only)
        assert 2 == len(ref)
        assert {Ref('Brachot 7b'), Ref('Isaiah 12:13')} == set(library.get_refs_in_string(texts['2ref'], citing_only=True))

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_inner_parenthesis(self, citing_only):

        ref = library.get_refs_in_string("Bereishit Rabbah (55:7)", "en", citing_only=citing_only)
        assert 1 == len(ref)
        assert ref[0] == Ref('Bereshit Rabbah 55:7')

        ''' Ranges not yet supported
        ref = library.get_refs_in_string(u"Yishayahu (64:9-10)", "en")
        assert 1 == len(ref)
        assert ref[0] == Ref(u'Isiah 64:9-10')
        '''

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_commentary(self, citing_only):
        s = "Here's one with Rashi on Genesis 2:5:3"
        s2 = "Here's one with both Rashi on Genesis 3:4 and Exodus 5:2. yeah"
        s3 = "Here's one with Genesis 2:3"
        s4 = "Here's a tricky one. Rashi on Shabbat 25a:5. Bet you'll never get it"
        assert library.get_refs_in_string(s, "en", citing_only=citing_only) == [Ref("Rashi on Genesis 2:5:3")]
        assert library.get_refs_in_string(s2, "en", citing_only=citing_only) == [Ref("Rashi on Genesis 3:4"), Ref("Exodus 5:2")]
        assert library.get_refs_in_string(s3, "en", citing_only=citing_only) == [Ref("Genesis 2:3")]
        assert library.get_refs_in_string(s4, "en", citing_only=False) == [Ref("Rashi on Shabbat 25a:5")] # Rashi on Shabbat has `is_citing=False`

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_citing_only(self, citing_only):
        matched_refs = library.get_refs_in_string(texts['weird_ref'], lang='en', citing_only=citing_only)
        if citing_only:
            assert matched_refs == [Ref("Leviticus 12.4")]
        else:
            assert matched_refs == [Ref("The Book of Susanna 1.2"), Ref("Leviticus 12.4")]


    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_supposed_ref_graceful_fail(self, citing_only):
        matched_refs = library.get_refs_in_string("What's important is that you get the Job done.", lang='en', citing_only=citing_only)
        assert matched_refs == []


    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_ranged_ref(self, citing_only):
        trefs = ["Deuteronomy 23:8-9", "Job.2:3-3:1", "Leviticus 15:3 - 17:12", "Shabbat 15a-16b",
                 "Shabbat 15a:15-15b:13", "Shabbat 15a:10-13", "Rashi on Exodus 3:1-3:10", "Rashi on Exodus 3:1:1-3:1:3",
                 "Rashi on Exodus 3:1:1-1:3", "Rashi on Exodus 3:1:1-3", "Berakhot 3a-b"]
        test_strings = [
            "I am going to quote a range. hopefully you can parse it. ({}) plus some other stuff.".format(temp_tref) for
            temp_tref in trefs
        ]
        for i, test_string in enumerate(test_strings):
            matched_refs = library.get_refs_in_string(test_string, lang='en', citing_only=citing_only)
            assert matched_refs == [Ref(trefs[i])]

    def test_ranged_ref_not_cited(self):
        trefs = ["Berakhot 2a-b", "Rashi on Shabbat 15a:10-13", "Shulchan Arukh, Orach Chayim 444:46"] # NOTE the m-dash in the Shulchan Arukh ref
        test_strings = [
            "I am going to quote a range. hopefully you can parse it. ({}) plus some other stuff.".format(temp_tref) for
            temp_tref in trefs
        ]
        for i, test_string in enumerate(test_strings):
            matched_refs = library.get_refs_in_string(test_string, lang='en', citing_only=False)
            assert matched_refs == [Ref(trefs[i])]

    def test_bad_ranged_refs(self):
        trefs = ["Rashi on Shabbat 15a:4-16a", "Rashi on Shabbat 2a:2-2b", "Rashi on Shabbat 2b:1:1-2a:2:1",
                 "Rashi on Shabbat 2b-2a", "Genesis 3:1-2:5", "Genesis 3-4:2"]
        test_strings = [
            "I am going to quote a range. hopefully you can NOT parse it. ({}) plus some other stuff.".format(temp_tref)
            for temp_tref in trefs
        ]
        for i, test_string in enumerate(test_strings):
            matched_refs = library.get_refs_in_string(test_string, lang='en', citing_only=False)
            assert matched_refs == []

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_wrap_refs(self, citing_only):
        trefs = ["Deuteronomy 23:8-9", "Job.2:3-3:1", "Leviticus 15:3 - 17:12", "Shabbat 15a-16b",
                 "Shabbat 15a:15-15b:13", "Shabbat 15a:10-13", "Rashi on Exodus 3:1-3:10", "Rashi on Exodus 3:1:1-3:1:3",
                 "Rashi on Exodus 3:1:1-1:3", "Rashi on Exodus 3:1:1-3"]
        orefs = [Ref(tref) for tref in trefs]
        st = reduce(lambda a, b: a + b + " blah blah ", trefs, "")
        res = reduce(lambda a, b: a + '<a class ="refLink" href="/{}" data-ref="{}">{}</a> blah blah '.format(b[0].url(), b[0].normal(), b[1]), list(zip(orefs, trefs)), "")
        wrapped = library.get_wrapped_refs_string(st, lang="en", citing_only=citing_only)
        assert wrapped == res

class Test_he_get_refs_in_text(object):
    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_positions(self, citing_only):
        for a in ['he_bible_mid', 'he_bible_begin', 'he_bible_end']:
            ref = library.get_refs_in_string(texts[a], citing_only=citing_only)
            assert 1 == len(ref)
            assert ref[0] == Ref(" , ")

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_false_positive(self, citing_only):
        ref = library.get_refs_in_string(texts['false_pos'], citing_only=citing_only)
        assert 1 == len(ref)
        assert ref[0] == Ref(" , ")

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_divrei_hayamim(self, citing_only):
        ref = library.get_refs_in_string("(   , )", citing_only=citing_only)
        assert 1 == len(ref)

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_double_ref_alt(self, citing_only):
        ref = library.get_refs_in_string("  (   ,),  (  ):   ", citing_only=citing_only)
        assert 2 == len(ref)

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_double_ref(self, citing_only):
        ref = library.get_refs_in_string(texts['he_2ref'], citing_only=citing_only)
        assert 2 == len(ref)
        assert {Ref('  '), Ref('    ')} == set(ref)

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_double_talmud(self, citing_only):
        ''' includes  '' - why would that work?'''
        #ref = lib.get_refs_in_string(texts['2talmud'])
        #assert 2 == len(ref)
        ''' includes  " '''
        ref = library.get_refs_in_string(texts['bk-abbrev'], citing_only=citing_only)
        assert 2 == len(ref)

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_out_of_brackets(self, citing_only):
        ref = library.get_refs_in_string(texts['ignored_middle'], citing_only=citing_only)
        assert 2 == len(ref)

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_double_quote_talmud(self, citing_only):
        ref = library.get_refs_in_string(texts['dq_talmud'], citing_only=citing_only)
        assert 1 == len(ref)
        assert Ref(' "') == ref[0]

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_sefer_mitzvot(self, citing_only):
        ref = library.get_refs_in_string(texts['neg327'], citing_only=citing_only)
        assert 4 == len(ref)
        assert {Ref(' ,'), Ref(' ,'), Ref(' ,'), Ref(' ,')} == set(ref)

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_three_digit_chapter(self, citing_only):
        ref = library.get_refs_in_string(texts['3dig'], citing_only=citing_only)
        assert 1 == len(ref)
        assert Ref(' "') == ref[0]

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_with_lead(self, citing_only):
        ref = library.get_refs_in_string(texts['2with_lead'], citing_only=citing_only)
        assert 2 == len(ref)
        assert {Ref(' ,'), Ref(' ,')} == set(ref)

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_two_single_quotes(self, citing_only):
        ref = library.get_refs_in_string("   ( '')  ", citing_only=citing_only)
        assert 1 == len(ref)
        assert ref[0] == Ref(" ''")

        ref = library.get_refs_in_string("  (  '')   ", citing_only=True)
        assert 1 == len(ref)
        assert ref[0] == Ref("  ''")

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_spelled_mishnah(self, citing_only):
        ref = library.get_refs_in_string(' ( "  )  ', citing_only=citing_only)
        assert 1 == len(ref)
        assert ref[0] == Ref(' "  ')

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_beyond_length(self, citing_only):
        ref = library.get_refs_in_string(' ( )  ( , ) ', citing_only=citing_only)
        assert 1 == len(ref)
        assert ref[0] == Ref(' , ')

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_word_boundary(self, citing_only):
        st = '  ,  (  , )    '
        ref = library.get_refs_in_string(st, citing_only=citing_only)
        assert len(ref) == 0

        #Assumes that Yalkut Shimoni Esther is not a text
        st = """ (   , ' ")   """
        ref = library.get_refs_in_string(st, citing_only=True)
        assert len(ref) == 1
        assert ref[0].sections[0] == 1
        assert len(ref[0].sections) == 1

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_nikkud_stripping(self, citing_only):
        st = '     ( "): "  '
        ref = library.get_refs_in_string(st, citing_only=citing_only)
        assert len(ref) == 1

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_citing_only_he(self, citing_only):
        matched_refs = library.get_refs_in_string(texts['weird_ref_he'], lang='he', citing_only=citing_only)
        if citing_only:
            assert matched_refs == [Ref("Leviticus 12.4")]
        else:
            assert set(matched_refs) == {Ref("The Book of Susanna 1.2"), Ref("Leviticus 12.4")}

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_supposed_ref_graceful_fail(self, citing_only):
        matched_refs = library.get_refs_in_string("   ", lang='he', citing_only=citing_only)
        assert matched_refs == []

    @pytest.mark.xfail(reason="unknown")
    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_huge_second_addr(self, citing_only):
        st = """  " (   , ) "   """
        ref = library.get_refs_in_string(st, citing_only=citing_only)[0]
        assert ref.sections[0] == 1
        assert len(ref.sections) == 1

        ''' These only work in the js
        ref = library.get_refs_in_string(u'  (, ") -  ', "he")
        assert 1 == len(ref)
        assert ref[0] == Ref(u'Shavuot 30a')

        ref = library.get_refs_in_string(u",   ( ,  ), ", "he")
        assert 1 == len(ref)
        assert ref[0] == Ref(u'Mishnah Nidah 6:4')
        '''

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_ranged_ref_plain(self, citing_only):
        trefs = [" :-", ', ", -', ', ",  - ', ', ",  - ']
        test_strings = [
            "   ({}) ".format(temp_tref)
            for temp_tref in trefs
        ]
        for i, test_string in enumerate(test_strings):
            matched_refs = library.get_refs_in_string(test_string, lang='he', citing_only=citing_only)
            assert matched_refs == [Ref(trefs[i])]

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_wrap_refs(self, citing_only):
        trefs = [" :-", ', ", -', ', ",  - ', ', ",  - ', "  -"]
        orefs = [Ref(tref) for tref in trefs]
        st = reduce(lambda a, b: a + "({})   ".format(b), trefs, "")
        res = reduce(lambda a, b: a + '(<a class ="refLink" href="/{}" data-ref="{}">{}</a>)   '.format(b[0].url(), b[0].normal(), b[1]), list(zip(orefs, trefs)), "")
        wrapped = library.get_wrapped_refs_string(st, lang="he", citing_only=citing_only)
        assert wrapped == res

    def test_ranged_talmud_wrap_refs(self):
        st = "( \' \'  \')"
        wrapped = library.get_wrapped_refs_string(st, lang="he", citing_only=True)

    def test_bad_refs(self):
        # We want to make sure that bad refs don't get wrapped in a tags nor error out
        trefs = [" ,  -, "]
        orefs = [Ref(tref) for tref in trefs]
        st = reduce(lambda a, b: a + "({})   ".format(b), trefs, "")
        res = reduce(lambda a, b: a + '(<a class ="refLink" href="/{}" data-ref="{}">{}</a>)   '.format(b[0].url(), b[0].normal(), b[1]), list(zip(orefs, trefs)), "")
        wrapped = library.get_wrapped_refs_string(st, lang="he", citing_only=True)
        assert wrapped != res and wrapped == st

    def test_already_wrapped_refs(self):
        st = 'wick (<a class="refLink" data-ref="Jerusalem Talmud Shabbat 2:3:2" href="/Jerusalem_Talmud_Shabbat.2.3.2">Note 16</a>) and'
        wrapped = library.get_wrapped_refs_string(st, lang="en")
        assert wrapped == st


class Test_get_titles_in_text(object):

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_no_bare_number(self, citing_only):
        barenum = "In this text, there is no reference but there is 1 bare number."
        res = library.get_titles_in_string(barenum, citing_only=citing_only)
        assert set(res) == set()

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_positions(self, citing_only):
        bible_mid = "Here we have Genesis 3:5 may it be blessed"
        bible_begin = "Genesis 3:5 in the house"
        bible_end = "Let there be Genesis 3:5"
        for a in [bible_mid, bible_begin, bible_end]:
            assert {'Genesis'} <= set(library.get_titles_in_string(a, citing_only=citing_only))

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_multi_titles(self, citing_only):
        two_ref = "This is a test of a Brachot 7b and also of an Isaiah 12:13."
        res = library.get_titles_in_string(two_ref, citing_only=citing_only)
        assert set(res) >= {'Brachot', 'Isaiah'}

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_he_bible_ref(self, citing_only):
        bible_ref = "        ( , )   "
        false_pos = "  ( , )   "

        res = library.get_titles_in_string(bible_ref, "he", citing_only=citing_only)
        assert set(res) >= {""}

        res = library.get_titles_in_string(false_pos, "he", citing_only=citing_only)
        assert set(res) >= {"", ""}

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_he_positions(self, citing_only):
        bible_begin = "( , )     "  # These work, even though the presentation of the parens may be confusing.
        bible_mid = " ( , )    "
        bible_end = "  ( , )"
        for a in [bible_mid, bible_begin, bible_end]:
            assert {""} <= set(library.get_titles_in_string(a, "he", citing_only=citing_only))

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_citing_only_en(self, citing_only):
        titles = library.get_titles_in_string(texts['weird_ref'], lang='en', citing_only=citing_only)
        if citing_only:
            assert set(titles) == {'Leviticus'}
        else:
            assert set(titles) == {'Leviticus', 'The Book of Susanna'}

    @pytest.mark.parametrize(('citing_only'), (True, False))
    def test_citing_only_he(self, citing_only):
        titles = library.get_titles_in_string(texts['weird_ref_he'], lang='he', citing_only=citing_only)
        if citing_only:
            assert set(titles) == {''}
        else:
            assert set(titles) == {'', ' '}


class Test_Library(object):
    def test_schema_validity(self):
        for i in library.all_index_records():
            assert isinstance(i, Index)
            i.nodes.validate()
            for name, obj in list(i.get_alt_structures().items()):
                obj.validate()


    def test_cache_populated_on_instanciation(self):
        assert library._index_map
        assert "en" in library.langs
        assert "he" in library.langs
        for lang in library.langs:
            assert library._index_title_maps[lang]
            with pytest.raises(Exception):
                assert library._index_title_commentary_maps[lang] #should not exist anymore
            assert library._title_node_maps[lang]
            with pytest.raises(Exception):
                assert library._title_node_with_commentary_maps[lang]

    def test_all_index_caches_removed_and_added_simple(self):

        assert "Genesis" in library._index_map
        assert "Bereishit" in library._index_title_maps["en"]["Genesis"]
        assert "Bereishit" in library._title_node_maps["en"]
        assert "" in library._index_title_maps["he"]["Genesis"]
        assert "" in library._title_node_maps["he"]

        library.remove_index_record_from_cache(library.get_index("Genesis"))

        assert "Genesis" not in library._index_map
        assert "Genesis" not in  library._index_title_maps["en"]
        assert "Bereishit" not in library._title_node_maps["en"]
        assert "Genesis" not in library._index_title_maps["he"]
        assert "" not in library._title_node_maps["he"]

        library.add_index_record_to_cache(Index().load({"title": "Genesis"}))

        assert "Genesis" in library._index_map
        assert "Bereishit" in library._index_title_maps["en"]["Genesis"]
        assert "Bereishit" in library._title_node_maps["en"]
        assert "" in library._index_title_maps["he"]["Genesis"]
        assert "" in library._title_node_maps["he"]


    def test_all_index_caches_removed_and_added_commentary(self):
        assert "Rashi on Genesis" in library._index_map
        assert "Rashi on Bereishit" in library._title_node_maps["en"]
        assert "Rashi on Bereishit" in library._index_title_maps["en"]["Rashi on Genesis"]
        assert '"  ' in library._index_title_maps["he"]["Rashi on Genesis"]
        assert '"  ' in library._title_node_maps["he"]

        library.remove_index_record_from_cache(library.get_index("Rashi on Genesis"))

        assert "Rashi on Genesis" not in library._index_map
        assert "Rashi on Bereishit" not in library._title_node_maps["en"]
        assert "Rashi on Genesis" not in library._index_title_maps["en"]
        assert "Rashi on Genesis" not in library._index_title_maps["he"]
        assert '"  ' not in library._title_node_maps["he"]

        library.add_index_record_to_cache(Index().load({"title": "Rashi on Genesis"}))

        assert "Rashi on Genesis" in library._index_map
        assert "Rashi on Bereishit" in library._title_node_maps["en"]
        assert "Rashi on Bereishit" in library._index_title_maps["en"]["Rashi on Genesis"]
        assert '"  ' in library._index_title_maps["he"]["Rashi on Genesis"]
        assert '"  ' in library._title_node_maps["he"]

    def test_get_title_node(self):
        node = library.get_schema_node("Exodus")
        assert node.is_flat()
        assert node.primary_title() == "Exodus"
        assert node.primary_title("he") == ""
        n2 = library.get_schema_node("", "he")
        assert node == n2

    def test_get_indexes_in_corpus(self):
        for corpus, count in [('Tanakh', 39), ('Mishnah', 63), ('Bavli', 37), ('Yerushalmi', 39)]:
            assert len(library.get_indexes_in_corpus(corpus)) == count


class Test_Term_Map(object):
    @classmethod
    def teardown_class(cls):
        CategorySet({'path': ["Tanakh", "Torah", "New Category"]}).delete()
        TermSet({"name": 'New Term'}).delete()


    def test_terms_in_map(self):
        assert "Siman" in library.get_simple_term_mapping()
        assert "Chapter" in library.get_simple_term_mapping()

    def test_cats_in_map(self):
        assert "Tanakh" in library.get_simple_term_mapping()
        assert "Commentary" in library.get_simple_term_mapping()
 
    @pytest.mark.deep
    def test_cache_and_reset_of_term_map(self):
        # Check that cache works
        old = library.get_simple_term_mapping()
        assert old == library.get_simple_term_mapping()

        # Add category causes cache refresh
        c = Category()
        c.add_primary_titles("New Category", "")
        c.path = ["Tanakh", "Torah", "New Category"]
        c.save()

        assert old != library.get_simple_term_mapping()
        old = library.get_simple_term_mapping()

        # Delete category causes cache refresh
        CategorySet({'path': ["Tanakh", "Torah", "New Category"]}).delete()
        assert old != library.get_simple_term_mapping()
        old = library.get_simple_term_mapping()

        # Add term causes cache refresh
        t = Term()
        t.name = "New Term"
        t.scheme = "Parasha"
        t.add_primary_titles("New Term", "")
        t.save()

        assert old != library.get_simple_term_mapping()
        old = library.get_simple_term_mapping()

        # Delete term causes cache refresh
        Term().load({"name": 'New Term'}).delete()
        assert old != library.get_simple_term_mapping()


class TestNamedEntityWrapping:
    @staticmethod
    def make_ne_link(slug, ref, start, end, vtitle, lang, text):
        link = RefTopicLink({
            "toTopic": slug,
            "dataSource": "sefaria",
            "ref": ref,
            "linkType": "mention",
            "class": "refTopic",
            "is_sheet": False,
            "expandedRefs": [ref],  # assuming all ne links are to segment ref
            "charLevelData": {
                "startChar": start,
                "endChar": end,
                "versionTitle": vtitle,
                "language": lang,
                "text": text
            }
        })
        return link

    def test_get_wrapped_named_entities_string(self):
        import re
        text = "A blah. BBB yoyo and C"
        links = [self.make_ne_link(m.group().lower(), 'Genesis 1:1', m.start(), m.end(), '1', 'en', m.group()) for m in re.finditer(r'[A-Z]+', text)]
        wrapped = library.get_wrapped_named_entities_string(links, text)
        wrapped_comp = """<a href="/topics/a" class="namedEntityLink" data-slug="a">A</a> blah. <a href="/topics/bbb" class="namedEntityLink" data-slug="bbb">BBB</a> yoyo and <a href="/topics/c" class="namedEntityLink" data-slug="c">C</a>"""
        assert wrapped == wrapped_comp

    def test_get_wrapped_named_entities_string_text_mismatch(self):
        import re
        text = "A blah. BBB yoyo and C"
        links = [self.make_ne_link(m.group().lower(), 'Genesis 1:1', m.start(), m.end(), '1', 'en', m.group()) for m in re.finditer(r'[A-Z]+', text)]
        links[0].charLevelData['startChar'] += 1  # manual offset to make text mismatch
        links[0].charLevelData['endChar'] += 1
        wrapped = library.get_wrapped_named_entities_string(links, text)
        wrapped_comp = """A blah. <a href="/topics/bbb" class="namedEntityLink" data-slug="bbb">BBB</a> yoyo and <a href="/topics/c" class="namedEntityLink" data-slug="c">C</a>"""
        assert wrapped == wrapped_comp


def test_get_en_text_titles():
    txts = ['Avot', 'Avoth', 'Daniel', 'Dan', 'Dan.'] # u"Me'or Einayim, Vayera"
    ctxts = ['Rashi on Exodus', 'Ramban on Genesis', 'Tosafot on Shabbat', 'Rashi on Gen.', 'Nachmanides on Exodus', 'Nachmanides on Ex.']
    titles = library.full_title_list()
    for txt in txts:
        assert txt in titles
    for txt in ctxts:
        assert txt in titles



def test_get_he_text_titles():
    txts = ['\u05d1\u05e8\u05d0\u05e9\u05d9\u05ea', '\u05e9\u05de\u05d5\u05ea', '\u05d5\u05d9\u05e7\u05e8\u05d0', '"  ']
    titles = library.full_title_list(lang="he")
    for txt in txts:
        assert txt in titles


```

### sefaria/model/tests/chunk_test.py

```
# coding=utf-8
import pytest

from sefaria.model import *
from sefaria.system.exceptions import InputError
import re
from sefaria.model.text import AbstractTextRecord
from sefaria.utils.util import list_depth


def test_text_index_map():
    r = Ref("Shabbat 8b")
    tc = TextChunk(r,"he")

    def tokenizer(str):
        return re.split(r"\s+",str)

    ind_list,ref_list, total_len = tc.text_index_map(tokenizer)
    #print len(ind_list), len(ref_list)
    #make sure the last element in ind_last (start index of last segment) + the last of the last segment == len of the whole string
    assert ind_list[-1]+len(tokenizer(TextChunk(r.all_subrefs()[-1],"he").as_string())) == len(tokenizer(tc.as_string()))

    # Test Range
    g = Ref('Genesis 1:31-2:2')
    chunk = g.text('en', 'The Holy Scriptures: A New Translation (JPS 1917)')
    ind_list, ref_list, total_len = chunk.text_index_map(lambda x: x.split(' '))
    assert (ind_list, ref_list) == ([0, 26, 40], [Ref('Genesis 1:31'), Ref('Genesis 2:1'), Ref('Genesis 2:2')])

    #test depth 3 with empty sections
    r = Ref("Rashi on Joshua")
    tc = TextChunk(r,"he")
    ind_list, ref_list, total_len = tc.text_index_map()
    for sub_ref in ref_list:
        assert sub_ref.is_segment_level()
    assert ref_list[5] == Ref('Rashi on Joshua 1:4:3')
    assert ref_list[8] == Ref('Rashi on Joshua 1:7:1')

    #test depth 2 range
    r = Ref("Rashi on Joshua 1:4-1:7")
    tc = TextChunk(r,"he")
    ind_list, ref_list, total_len = tc.text_index_map()
    assert ref_list[5] == Ref('Rashi on Joshua 1:7:1')

    #test depth 3 range with missing super-section (Ramban Chapter 50 is missing)
    r = Ref("Ramban on Genesis 48-50")
    tc = TextChunk(r,"he")
    ind_list, ref_list, total_len = tc.text_index_map()
    assert ref_list[-1] == Ref('Ramban on Genesis 49:33:3')


    #test depth 2 with empty segments
    #r = Ref("Targum Jerusalem, Genesis")

def test_verse_chunk():
    chunks = [
        TextChunk(Ref("Daniel 2:3"), "en", "The Holy Scriptures: A New Translation (JPS 1917)"),
        TextChunk(Ref("Daniel 2:3"), "he", "Tanach with Nikkud"),
        TextChunk(Ref("Daniel 2:3"), "en"),
        TextChunk(Ref("Daniel 2:3"), "he")
    ]
    for c in chunks:
        assert isinstance(c.text, str)
        assert len(c.text)


def test_chapter_chunk():
    chunks = [
        TextChunk(Ref("Daniel 2"), "en", "The Holy Scriptures: A New Translation (JPS 1917)"),
        TextChunk(Ref("Daniel 2"), "he", "Tanach with Nikkud"),
        TextChunk(Ref("Daniel 2"), "en"),
        TextChunk(Ref("Daniel 2"), "he")
    ]
    for c in chunks:
        assert isinstance(c.text, list)
        assert len(c.text)


def test_depth_1_chunk():
    c = TextChunk(Ref("Hadran"), "he")
    assert isinstance(c.text, list)
    c = TextChunk(Ref("Hadran 3"), "he")
    assert isinstance(c.text, str)


def test_out_of_range_chunks():
    # test out of range where text has length
    with pytest.raises(InputError):
        TextChunk(Ref("Job 80"), "he")

    with pytest.raises(InputError):
        TextChunk(Ref("Shabbat 180"), "he")


def test_range_chunk():
    chunks = [
        TextChunk(Ref("Daniel 2:3-5"), "en", "The Holy Scriptures: A New Translation (JPS 1917)"),
        TextChunk(Ref("Daniel 2:3-5"), "he", "Tanach with Nikkud"),
        TextChunk(Ref("Daniel 2:3-5"), "en"),
        TextChunk(Ref("Daniel 2:3-5"), "he"),
    ]

    for c in chunks:
        assert isinstance(c.text, list)
        assert len(c.text) == 3


def test_spanning_chunk():
    chunks = [
        TextChunk(Ref("Daniel 2:3-4:5"), "en", "The Holy Scriptures: A New Translation (JPS 1917)"),
        TextChunk(Ref("Daniel 2:3-4:5"), "he", "Tanach with Nikkud"),
        TextChunk(Ref("Daniel 2:3-4:5"), "en"),
        TextChunk(Ref("Daniel 2:3-4:5"), "he")
    ]

    for c in chunks:
        assert isinstance(c.text, list)
        assert isinstance(c.text[0], list)
        assert len(c.text) == 3
        assert len(c.text[2]) == 5


def test_commentary_chunks():
    verse = TextChunk(Ref("Rashi on Exodus 3:1"), lang="he")
    rang = TextChunk(Ref("Rashi on Exodus 3:1-10"), lang="he")
    span = TextChunk(Ref("Rashi on Exodus 3:1-4:10"), lang="he")
    assert verse.text == rang.text[0]
    assert verse.text == span.text[0][0]

    verse = TextChunk(Ref("Rashi on Exodus 4:10"), lang="he")
    rang = TextChunk(Ref("Rashi on Exodus 4:1-10"), lang="he")
    assert rang.text[-1] == verse.text
    assert span.text[-1][-1] == verse.text


def test_default_in_family():
    r = Ref('Shulchan Arukh, Even HaEzer')
    f = TextFamily(r)
    assert isinstance(f.text, list)
    assert isinstance(f.he, list)
    assert len(f.text) > 0
    assert len(f.he) > 0


def test_spanning_family():
    f = TextFamily(Ref("Daniel 2:3-4:5"), context=0)

    assert isinstance(f.text, list)
    assert isinstance(f.he, list)
    assert len(f.text) == 3
    assert len(f.text[2]) == 5
    assert len(f.he) == 3
    assert len(f.he[2]) == 5
    assert isinstance(f.commentary[0], list)

    f = TextFamily(Ref("Daniel 2:3-4:5"))  # context = 1
    assert isinstance(f.text, list)
    assert isinstance(f.he, list)
    assert len(f.text) == 3
    assert len(f.text[2]) == 34
    assert len(f.he) == 3
    assert len(f.he[2]) == 34
    assert isinstance(f.commentary[0], list)


def test_family_chapter_result_no_merge():
    families = [
        TextFamily(Ref("Onkelos Exodus 12")),  # this is supposed to get a version with exactly 1 en and 1 he.  The data may change.
        TextFamily(Ref("Daniel 2")),
        TextFamily(Ref("Daniel 4"), lang="en", version="The Holy Scriptures: A New Translation (JPS 1917)"),
        TextFamily(Ref("Daniel 4"), lang="he", version="Tanach with Nikkud")
    ]

    for v in families:
        assert isinstance(v.text, list)
        assert isinstance(v.he, list)

        c = v.contents()
        for key in ["text", "ref", "he", "book", "commentary"]:  # todo: etc.
            assert key in c

# Yoma.1 is no longer merged.
# todo: find a merged text to test with
@pytest.mark.xfail(reason="unknown")
def test_chapter_result_merge():
    v = TextFamily(Ref("Mishnah_Yoma.1"))

    assert isinstance(v.text, list)
    assert isinstance(v.he, list)
    c = v.contents()
    for key in ["text", "ref", "he", "book", "sources", "commentary"]:  # todo: etc.
        assert key in c


def test_text_family_alts():
    tf = TextFamily(Ref("Exodus 6"), commentary=False, alts=True)
    c = tf.contents()
    assert c.get("alts")

def test_text_family_version_with_underscores():
    with_spaces = TextFamily(
        Ref("Amos 1"), lang="he", lang2="en", commentary=False,
        version="Miqra according to the Masorah",
        version2="Tanakh: The Holy Scriptures, published by JPS")
    with_underscores = TextFamily(
        Ref("Amos 1"), lang="he", lang2="en", commentary=False,
        version="Miqra_according_to_the_Masorah",
        version2="Tanakh:_The_Holy_Scriptures,_published_by_JPS")
    assert with_spaces.he == with_underscores.he
    assert with_spaces.text == with_underscores.text

def test_validate():
    passing_refs = [
        Ref("Exodus"),
        Ref("Exodus 3"),
        Ref("Exodus 3:4"),
        Ref("Exodus 3-5"),
        Ref("Exodus 3:4-5:7"),
        Ref("Exodus 3:4-7"),
        Ref("Rashi on Exodus"),
        Ref("Rashi on Exodus 3"),
        Ref("Rashi on Exodus 3:2"),
        Ref("Rashi on Exodus 3-5"),
        Ref("Rashi on Exodus 3:2-5:7"),
        Ref("Rashi on Exodus 3:2-7"),
        Ref("Rashi on Exodus 3:2:1"),
        Ref("Rashi on Exodus 3:2:1-3"),
        Ref("Rashi on Exodus 3:2:1-3:5:1"),
        Ref("Shabbat"),
        Ref("Shabbat 7a"),
        Ref("Shabbat 7a-8b"),
        Ref("Shabbat 7a:9"),
        Ref("Shabbat 7a:2-9"),
        Ref("Shabbat 7a:2-7b:3"),
        Ref("Rashi on Shabbat 7a"),
        Ref("Rashi on Shabbat 7a-8b"),
        Ref("Rashi on Shabbat 7a:9"),
        Ref("Rashi on Shabbat 7a:2-9"),
        Ref("Rashi on Shabbat 7a:2-7b:3")
    ]
    for ref in passing_refs:
        TextChunk(ref, lang="he")._validate()


def test_save():
    # Delete any old ghost
    vs = ["Hadran Test", "Pirkei Avot Test", "Rashi on Exodus Test"]
    for vt in vs:
        try:
            Version().load({"versionTitle": vt}).delete()
        except:
            pass

    # create new version, depth 1
    v = Version({
        "language": "en",
        "title": "Hadran",
        "versionSource": "http://foobar.com",
        "versionTitle": "Hadran Test",
        "chapter": []
    }).save()
    # write to blank version
    c = TextChunk(Ref("Hadran 3"), "en", "Hadran Test")
    c.text = "Here's a translation for the eras"
    c.save()

    # write beyond current extent
    c = TextChunk(Ref("Hadran 5"), "en", "Hadran Test")
    c.text = "Here's another translation for the eras"
    c.save()

    # write within current extent
    c = TextChunk(Ref("Hadran 4"), "en", "Hadran Test")
    c.text = "Here's yet another translation for the eras"
    c.save()

    # insert some nefarious code
    c = TextChunk(Ref("Hadran 6"), "en", "Hadran Test")
    c.text = 'Here\'s yet another translation for the eras <a href="javascript:alert(8007)">Click me</a>'
    c.save()

    # verify
    c = TextChunk(Ref("Hadran"), "en", "Hadran Test")
    assert c.text[2] == "Here's a translation for the eras"
    assert c.text[3] == "Here's yet another translation for the eras"
    assert c.text[4] == "Here's another translation for the eras"
    assert c.text[5] == "Here's yet another translation for the eras <a>Click me</a>"

    # delete version
    v.delete()

    # create new version, depth 2
    v = Version({
        "language": "en",
        "title": "Pirkei Avot",
        "versionSource": "http://foobar.com",
        "versionTitle": "Pirkei Avot Test",
        "chapter": []
    }).save()

    # write to new verse of new chapter
    c = TextChunk(Ref("Pirkei Avot 2:3"), "en", "Pirkei Avot Test")
    c.text = "Text for 2:3"
    c.save()

    # extend to new verse of later chapter
    c = TextChunk(Ref("Pirkei Avot 3:4"), "en", "Pirkei Avot Test")
    c.text = "Text for 3:4"
    c.save()

    # write new chapter beyond created range
    # also test that blank space isn't saved
    c = TextChunk(Ref("Pirkei Avot 5"), "en", "Pirkei Avot Test")
    c.text = ["Text for 5:1", "Text for 5:2", "Text for 5:3", "Text for 5:4", "", " "]
    c.save()

    # write new chapter within created range
    c = TextChunk(Ref("Pirkei Avot 4"), "en", "Pirkei Avot Test")
    c.text = ["Text for 4:1", "Text for 4:2", "Text for 4:3", "Text for 4:4"]
    c.save()

    # write within explicitly created chapter
    c = TextChunk(Ref("Pirkei Avot 3:5"), "en", "Pirkei Avot Test")
    c.text = "Text for 3:5"
    c.save()
    c = TextChunk(Ref("Pirkei Avot 3:3"), "en", "Pirkei Avot Test")
    c.text = "Text for 3:3"
    c.save()

    # write within implicitly created chapter
    c = TextChunk(Ref("Pirkei Avot 1:5"), "en", "Pirkei Avot Test")
    c.text = "Text for 1:5"
    c.save()

    # Rewrite
    c = TextChunk(Ref("Pirkei Avot 4:2"), "en", "Pirkei Avot Test")
    c.text = "New Text for 4:2"
    c.save()

    # verify
    c = TextChunk(Ref("Pirkei Avot"), "en", "Pirkei Avot Test")
    assert c.text == [
        ["", "", "", "", "Text for 1:5"],
        ["", "", "Text for 2:3"],
        ["", "", "Text for 3:3", "Text for 3:4", "Text for 3:5"],
        ["Text for 4:1", "New Text for 4:2", "Text for 4:3", "Text for 4:4"],
        ["Text for 5:1", "Text for 5:2", "Text for 5:3", "Text for 5:4"]
    ]

    # Test overwrite of whole text
    # also test that blank space isn't saved
    c.text = [
        ["Fee", "", "Fi", ""],
        ["", "", "Fo"],
        ["", "Fum", "Text for 3:3", "Text for 3:4"],
        ["Text for 4:1", "New Text for 4:2","", "Text for 4:4",""]
    ]
    c.save()
    c = TextChunk(Ref("Pirkei Avot"), "en", "Pirkei Avot Test")
    assert c.text == [
        ["Fee", "", "Fi"],
        ["", "", "Fo"],
        ["", "Fum", "Text for 3:3", "Text for 3:4"],
        ["Text for 4:1", "New Text for 4:2","", "Text for 4:4"]
    ]

    v.delete()

    with pytest.raises(Exception) as e_info:
        # create new version for a non existing commentary, depth 3 - should fail
        v = Version({
            "language": "en",
            "title": "Rashi on Pirkei Avot",
            "versionSource": "http://foobar.com",
            "versionTitle": "Rashi on Pirkei Avot Test",
            "chapter": []
        }).save()

    v = Version({
        "language": "en",
        "title": "Rashi on Exodus",
        "versionSource": "http://foobar.com",
        "versionTitle": "Rashi on Exodus Test",
        "chapter": []
    }).save()
    # write to new verse of new chapter
    c = TextChunk(Ref("Rashi on Exodus 2:3"), "en", "Rashi on Exodus Test")
    c.text = ["Text for 2:3:1", "Text for 2:3:2"]
    c.save()

    # extend to new verse of later chapter
    c = TextChunk(Ref("Rashi on Exodus 3:4:3"), "en", "Rashi on Exodus Test")
    c.text = "Text for 3:4:3"
    c.save()

    # write new chapter beyond created range
    # test that blank space isn't saved
    c = TextChunk(Ref("Rashi on Exodus 5"), "en", "Rashi on Exodus Test")
    c.text = [["Text for 5:1:1"], ["Text for 5:2:1", "", ""], ["Text for 5:3:1","Text for 5:3:2", "     ", "", " "],["Text for 5:4:1", "", "  "]]
    c.save()

    # write new chapter within created range
    c = TextChunk(Ref("Rashi on Exodus 4"), "en", "Rashi on Exodus Test")
    c.text = [["Text for 4:1:1", "Text for 4:1:2", "Text for 4:1:3", "Text for 4:1:4"]]
    c.save()

    # write within explicitly created chapter
    c = TextChunk(Ref("Rashi on Exodus 3:5:1"), "en", "Rashi on Exodus Test")
    c.text = "Text for 3:5:1"
    c.save()
    c = TextChunk(Ref("Rashi on Exodus 3:3:3"), "en", "Rashi on Exodus Test")
    c.text = "Text for 3:3:3"
    c.save()

    # write within implicitly created chapter
    c = TextChunk(Ref("Rashi on Exodus 1:5"), "en", "Rashi on Exodus Test")
    c.text = ["Text for 1:5", "Text for 1:5:2"]
    c.save()

    # Rewrite
    c = TextChunk(Ref("Rashi on Exodus 4:1:2"), "en", "Rashi on Exodus Test")
    c.text = "New Text for 4:1:2"
    c.save()

    # verify
    c = TextChunk(Ref("Rashi on Exodus"), "en", "Rashi on Exodus Test")
    assert c.text == [
        [[], [], [], [], ["Text for 1:5", "Text for 1:5:2"]],
        [[], [], ["Text for 2:3:1", "Text for 2:3:2"]],
        [[], [], ["", "", "Text for 3:3:3"], ["", "", "Text for 3:4:3"], ["Text for 3:5:1"]],
        [["Text for 4:1:1", "New Text for 4:1:2", "Text for 4:1:3", "Text for 4:1:4"]],
        [["Text for 5:1:1"], ["Text for 5:2:1"], ["Text for 5:3:1", "Text for 5:3:2"], ["Text for 5:4:1"]]
    ]

    v.delete()

    # write


def test_complex_with_depth_1():
    # There was a bug that chunks of complex texts always returned the first element of the array, even for deeper chunks
    r = Ref('Pesach Haggadah, Kadesh 1')
    c = TextChunk(r, "he")
    assert " " in c.text

    r = Ref('Pesach Haggadah, Kadesh 2')
    c = TextChunk(r, "he")
    assert "" in c.text

    r = Ref('Pesach Haggadah, Kadesh 2-4')
    c = TextChunk(r, "he")
    assert len(c.text) == 3
    assert "" in c.text[0]

    #Comparing Hebrew is hard.
    #assert u" " in c.text[1]
    #assert u"" in c.text[2]

    c = TextChunk(r, "en")
    assert len(c.text) == 3
    assert "kiddush" in c.text[0].lower()
    assert "seventh day" in c.text[2]


def test_complex_with_depth_2():
    pass


def test_strip_imgs():
    text = "text with an image"
    image = "<img src='src.jpg' alt='image caption'>"
    assert AbstractTextRecord.strip_imgs(f"{text}{image}") == text
    assert AbstractTextRecord.strip_imgs(text) == text


@pytest.mark.xfail(reason="<br/> tags become <br>, so don't match exactly.")
def test_strip_itags():
    vs = ["Hadran Test"]
    for vt in vs:
        try:
            Version().load({"versionTitle": vt}).delete()
        except:
            pass

    r = Ref("Genesis 1:1")
    c = TextChunk(r, "he")
    text = c._get_text_after_modifications([c.strip_itags])
    assert text == TextChunk(r, "he").text

    r = Ref("Genesis 1")
    c = TextChunk(r, "he")
    modified_text = c._get_text_after_modifications([c.strip_itags])
    original_text = TextChunk(r, "he").text
    for mod, ori in zip(modified_text, original_text):
        assert mod == ori

    # create new version, depth 1
    v = Version({
        "language": "en",
        "title": "Hadran",
        "versionSource": "http://foobar.com",
        "versionTitle": "Hadran Test",
        "chapter": ['Cool text <sup>1</sup><i class="footnote yo">well, not that cool</i>',
                    'Silly text <sup>1</sup><i class="footnote">See <i>cool text</i></i>',
                    'More text <i data-commentator="Boring comment" data-order="1"></i> and yet more',
                    'Where the <i data-overlay="Other system" data-value=""></i>']
    }).save()
    modified_text = ['Cool text', 'Silly text', 'More text and yet more']
    c = TextChunk(Ref("Hadran"), "en", "Hadran Test")
    test_modified_text = c._get_text_after_modifications([c.strip_itags, lambda x, _: ' '.join(x.split()).strip()])
    for m, t in zip(modified_text, test_modified_text):
        assert m == t

    test_modified_text = v._get_text_after_modifications([v.strip_itags, lambda x, _: ' '.join(x.split()).strip()])
    for m, t in zip(modified_text, test_modified_text):
        assert m == t

    # test without any modification functions
    test_modified_text = c._get_text_after_modifications([])
    for m, t in zip(c.text, test_modified_text):
        assert m == t

    test_modified_text = v._get_text_after_modifications([])
    for m, t in zip(v.chapter, test_modified_text):
        assert m == t

    text = '<i></i>Lo, his spirit.'
    assert TextChunk.strip_itags(text) == text


```

### sefaria/model/tests/lexicon_tests.py

```
# -*- coding: utf-8 -*-
import pytest
from sefaria.model import *


class Test_Lexicon_Lookup(object):

    def test_bible_lookup(self):
        word = ""
        lookup_ref = "Leviticus 19.3"
        #["lookup_ref", "never_split", "always_split"]
        results = LexiconLookupAggregator.lexicon_lookup(word)
        results2 = LexiconLookupAggregator.lexicon_lookup(word, **{"lookup_ref": lookup_ref})
        results3 = LexiconLookupAggregator.lexicon_lookup(word, **{"always_split": 1})
        assert results.count() == 1
        assert results[0].headword == ""
        assert results2.count() == 1
        assert results3.count() == 1

        word2 = ""
        results = LexiconLookupAggregator.lexicon_lookup(word2)
        assert results.count() == 2

    def test_hts_lookup(self):
        word = "Ma'aser Sheni"
        word2 = "Am Ha'aretz"
        word3 = "Bikurim"
        lookup_ref = "Mishnah Maaser Sheni 3"
        # ["lookup_ref", "never_split", "always_split"]
        results = LexiconLookupAggregator.lexicon_lookup(word)
        results2 = LexiconLookupAggregator.lexicon_lookup(word, **{"lookup_ref": lookup_ref})
        results3 = LexiconLookupAggregator.lexicon_lookup(word, **{"always_split": 1})
        assert results.count() == 1
        assert results[0].headword == " "
        assert results2.count() == 1
        assert results3.count() == 1

        results = LexiconLookupAggregator.lexicon_lookup(word2)
        assert results.count() == 1

        results = LexiconLookupAggregator.lexicon_lookup(word3)
        assert results.count() == 1


class Test_Lexicon_Save(object):

    def test_sanitize(self):
        entry = {
            "plural_form": [

            ],
            "prev_hw": " ",
            "alt_headwords": [

            ],
            "next_hw": "",
            "parent_lexicon": "Jastrow Dictionary",
            "refs": [
                "Shabbat 104a"
            ],
            "content": {
                "senses": [
                    {
                        "definition": " as numeral letter, <i>one</i>, as <span dir=\"rtl\"> </span> = <span dir=\"rtl\"> </span> one letter. <a class=\"refLink\" href=\"/Shabbat.104a\" data-ref=\"Shabbat 104a\">Sabb. 104</a>; a. fr. [Editions and Mss. vary, according to space, between the full numeral and the numeral letter, <a dir=\"rtl\" class=\"refLink\" href=\"/Jastrow,_.1\" data-ref=\"Jastrow,  1\"></a> for <span dir=\"rtl\"></span>, <span dir=\"rtl\"></span>; <a dir=\"rtl\" class=\"refLink\" href=\"/Jastrow,_.1\" data-ref=\"Jastrow,  1\"></a> for <a dir=\"rtl\" class=\"refLink\" href=\"/Jastrow,_.1\" data-ref=\"Jastrow,  1\"></a>, <span dir=\"rtl\"></span>, <a dir=\"rtl\" class=\"refLink\" href=\"/Jastrow,_.1\" data-ref=\"Jastrow,  1\"></a> &c.]"
                    },
                    {
                        "definition": 'Seemingly ok definition... <a href="javascript:alert(8007)">Click me</a>'
                    }
                ]
            },
            "quotes": [

            ],
            "headword": "",
            "rid": "A00006"
        }
        l = JastrowDictionaryEntry(entry)
        l.save()
        print(l.content["senses"][0]["definition"])
        assert l.content["senses"][0]["definition"] == """ as numeral letter, <i>one</i>, as <span dir="rtl"> </span> = <span dir="rtl"> </span> one letter. <a class="refLink" data-ref="Shabbat 104a" href="/Shabbat.104a">Sabb. 104</a>; a. fr. [Editions and Mss. vary, according to space, between the full numeral and the numeral letter, <a class="refLink" data-ref="Jastrow,  1" dir="rtl" href="/Jastrow,_.1"></a> for <span dir="rtl"></span>, <span dir="rtl"></span>; <a class="refLink" data-ref="Jastrow,  1" dir="rtl" href="/Jastrow,_.1"></a> for <a class="refLink" data-ref="Jastrow,  1" dir="rtl" href="/Jastrow,_.1"></a>, <span dir="rtl"></span>, <a class="refLink" data-ref="Jastrow,  1" dir="rtl" href="/Jastrow,_.1"></a> &amp;c.]"""
        assert l.content["senses"][1]["definition"] == 'Seemingly ok definition... <a>Click me</a>'
        l.delete()



```

### sefaria/model/tests/index_schema_test.py

```
# -*- coding: utf-8 -*-

import pytest
import pprint
from sefaria.model import *
from sefaria.system.exceptions import InputError

class Test_Schema(object):
    # This needs a bunch of hebrew titles to validate

    @classmethod
    def setup_class(cls):
        pass

    @classmethod
    def teardown_class(cls):
        pass

    @pytest.mark.xfail(reason="unknown")
    def test_schema_load(self):
        i = Index().load({"title": "Mishnah Torah Test"})
        if i:
            i.delete()
        schema = {
            "key": "Mishnah Torah Test",
            "titles": [
                {
                    "lang": "en",
                    "text": "Mishnah Torah Test",
                    "primary": True
                },
                {
                    "lang": "en",
                    "text": "Rambam Test"
                },
                {
                    "lang": "he",
                    "text": "  ",
                    "primary": True
                }
            ],
            "nodes": [
                {
                    "key": "Introduction",
                    "titles": [
                        {
                            "lang": "en",
                            "text": "Introduction",
                            "primary": True
                        },
                        {
                            "lang": "he",
                            "text": "",
                            "primary": True
                        }
                    ],
                    "nodes": [
                        {
                            "key": "Transmission",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "Transmission",
                                "primary": True
                                }
                            ],
                            "nodeType": "JaggedArrayNode",
                            "depth": 1,
                            "addressTypes": ["Integer"],
                            "sectionNames": ["Paragraph"]
                        },
                        {
                            "key": "List of Positive Mitzvot",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "List of Positive Mitzvot",
                                "primary": True
                                }
                            ],
                            "nodeType": "JaggedArrayNode",
                            "depth": 1,
                            "addressTypes": ["Integer"],
                            "sectionNames": ["Mitzvah"]
                        },
                        {
                            "key": "List of Negative Mitzvot",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "List of Negative Mitzvot",
                                "primary": True
                                }
                            ],
                            "nodeType": "JaggedArrayNode",
                            "depth": 1,
                            "addressTypes": ["Integer"],
                            "sectionNames": ["Mitzvah"]
                        }
                    ]

                },
                {
                    "key": "Sefer Mada",
                    "titles": [
                        {
                        "lang": "en",
                        "text": "Sefer Mada",
                        "primary": True
                        }
                    ],
                    "nodes": [
                        {
                            "key": "Foundations of the Torah",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "Foundations of the Torah",
                                "primary": True
                                }
                            ],
                            "nodes": [
                                {
                                    "key": "Introduction",
                                    "titles": [
                                        {
                                        "lang": "en",
                                        "text": "Introduction",
                                        "primary": True
                                        }
                                    ],
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 0,
                                    "addressTypes": [],
                                    "sectionNames": []
                                },
                                {
                                    "key": "default",
                                    "default": True,
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 2,
                                    "addressTypes": ["Integer", "Integer"],
                                    "sectionNames": ["Chapter", "Law"]
                                }
                            ]
                        },
                        {
                            "key": "Human Dispositions",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "Human Dispositions",
                                "primary": True
                                }
                            ],
                            "nodes": [
                                {
                                    "key": "Introduction",
                                    "titles": [
                                        {
                                        "lang": "en",
                                        "text": "Introduction",
                                        "primary": True
                                        }
                                    ],
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 0,
                                    "addressTypes": [],
                                    "sectionNames": []
                                },
                                {
                                    "key": "default",
                                    "default": True,
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 2,
                                    "addressTypes": ["Integer", "Integer"],
                                    "sectionNames": ["Chapter", "Law"]
                                }
                            ]
                        },
                        {
                            "key": "Torah Study",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "Torah Study",
                                "primary": True
                                }
                            ],
                            "nodes": [
                                {
                                    "key": "Introduction",
                                    "titles": [
                                        {
                                        "lang": "en",
                                        "text": "Introduction",
                                        "primary": True
                                        }
                                    ],
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 0,
                                    "addressTypes": [],
                                    "sectionNames": []
                                },
                                {
                                    "key": "default",
                                    "default": True,
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 2,
                                    "addressTypes": ["Integer", "Integer"],
                                    "sectionNames": ["Chapter", "Law"]
                                }
                            ]
                        },
                        {
                            "key": "Foreign Worship and Customs of the Nations",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "Foreign Worship and Customs of the Nations",
                                "primary": True
                                }
                            ],
                            "nodes": [
                                {
                                    "key": "Introduction",
                                    "titles": [
                                        {
                                        "lang": "en",
                                        "text": "Introduction",
                                        "primary": True
                                        }
                                    ],
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 0,
                                    "addressTypes": [],
                                    "sectionNames": []
                                },
                                {
                                    "key": "default",
                                    "default": True,
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 2,
                                    "addressTypes": ["Integer", "Integer"],
                                    "sectionNames": ["Chapter", "Law"]
                                }
                            ]
                        },
                        {
                            "key": "Repentance",
                            "titles": [
                                {
                                "lang": "en",
                                "text": "Repentance",
                                "primary": True
                                },
                                {
                                "lang": "en",
                                "text": "Hilchot TeshuvaX",
                                "presentation": "alone"
                                }
                            ],
                            "nodes": [
                                {
                                    "key": "Introduction",
                                    "titles": [
                                        {
                                        "lang": "en",
                                        "text": "Introduction",
                                        "primary": True
                                        }
                                    ],
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 0,
                                    "addressTypes": [],
                                    "sectionNames": []
                                },
                                {
                                    "key": "default",
                                    "default": True,
                                    "nodeType": "JaggedArrayNode",
                                    "depth": 2,
                                    "addressTypes": ["Integer", "Integer"],
                                    "sectionNames": ["Chapter", "Law"]
                                }
                            ]
                        }
                    ]
                }
            ]
        }

        i = Index({
            "schema": schema,
            "title": "Mishnah Torah Test",
            "categories": ["Halakhah"]
        })
        i.save()
        i.nodes.all_tree_titles("en")
        i.nodes.title_dict("en")
        schema['titles'] = sorted(schema['titles'], key=lambda x: x['text'])
        serialized = i.nodes.serialize()
        serialized['titles'] = sorted(serialized['titles'], key=lambda x: x['text'])
        assert schema == serialized

        Ref("Mishnah Torah Test, Introduction, Transmission")

        with pytest.raises(InputError):
            Ref("Mishnah Torah Test, Introduction, TransmisXsion")  # Mispelled last piece

        i.delete()

    def test_schema_load_2(self):
        i = Index().load({"title": "Lekutei Moharan"})
        if i:
            i.delete()
        lm_schema = {
            "key": "Lekutei Moharan",
            "titles": [
                {
                    "lang": "en",
                    "text": "Lekutei Moharan",
                    "primary": True
                },
                {
                    "lang": "en",
                    "text": "Likutey Moharan"
                },
                {
                    "lang": "en",
                    "text": "Likkutei Moharan"
                },
                {
                    "lang": "he",
                    "text": ' ',  # took the " out from before final nun to avoid name conflict
                    "primary": True
                }
            ],
            "nodes": [
                {
                    "key": "Approbations",
                    "titles": [
                        {
                            "lang": "en",
                            "text": "Approbations",
                            "primary": True
                        },
                        {
                            "lang": "he",
                            "text": '',
                            "primary": True
                        }
                    ],
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Approbation"]
                },
                {
                    "key": "Introduction",
                    "titles": [
                        {
                            "lang": "en",
                            "text": "Introduction",
                            "primary": True
                        },
                        {
                            "lang": "he",
                            "text": "",
                            "primary": True
                        }
                    ],
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Paragraph"]
                },
                {
                    "key": "default",
                    "default": True,
                    "nodeType": "JaggedArrayNode",
                    "depth": 3,
                    "addressTypes": ["Integer", "Integer", "Integer"],
                    "sectionNames": ["Torah", "Section", "Paragraph"]
                },
                {
                    "key": "Tanina",
                    "titles": [
                        {
                            "lang": "en",
                            "text": "Tanina",
                            "primary": True
                        },
                        {
                            "lang": "he",
                            "text": '',
                            "primary": True
                        }
                    ],
                    "nodes": [
                        {
                            "key": "default",
                            "default": True,
                            "nodeType": "JaggedArrayNode",
                            "depth": 3,
                            "addressTypes": ["Integer", "Integer", "Integer"],
                            "sectionNames": ["Torah", "Section", "Paragraph"]
                        },
                        {
                            "key": "Letters",
                            "titles" : [
                                {
                                    "lang": "en",
                                    "text": "Letters",
                                    "primary": True
                                },
                                {
                                    "lang": "he",
                                    "text": ' ',
                                    "primary": True
                                }
                            ],
                            "nodeType": "JaggedArrayNode",
                            "depth": 2,
                            "addressTypes": ["Integer", "Integer"],
                            "sectionNames": ["Letter", "Paragraph"]
                        }
                    ]
                }
            ]
        }
        i = Index({
            "schema": lm_schema,
            "title": "Lekutei Moharan",
            "categories": ["Chasidut"]
        })
        i.save()
        i.nodes.all_tree_titles("en")
        i.nodes.title_dict("en")

        assert len(i.nodes.children) == 4
        assert library.get_schema_node("Lekutei Moharan, Introduction").next_leaf() == library.get_schema_node("Lekutei Moharan")
        assert library.get_schema_node("Lekutei Moharan").next_leaf() == library.get_schema_node("Lekutei Moharan, Tanina")
        assert library.get_schema_node("Lekutei Moharan, Tanina").next_leaf() == library.get_schema_node("Lekutei Moharan, Tanina, Letters")

        assert library.get_schema_node("Lekutei Moharan, Tanina, Letters").prev_leaf() == library.get_schema_node("Lekutei Moharan, Tanina")
        assert library.get_schema_node("Lekutei Moharan, Tanina").prev_leaf() == library.get_schema_node("Lekutei Moharan")
        assert library.get_schema_node("Lekutei Moharan").prev_leaf() == library.get_schema_node("Lekutei Moharan, Introduction")

        lm_schema['titles'] = sorted(lm_schema['titles'], key=lambda x: x['text'])
        serialized = i.nodes.serialize()
        serialized['titles'] = sorted(serialized['titles'], key=lambda x: x['text'])
        assert lm_schema == serialized

        i.delete()

    def test_sharedTitles(self):
        i = Index().load({"title": "Parshanut Test"})
        if i:
            i.delete()
        schema = {
            "key": "Parshanut Test",
            "titles": [
                {
                    "lang": "en",
                    "text": "Parshanut Test",
                    "primary": True
                },
                {
                    "lang": "he",
                    "text": '',
                    "primary": True
                }
            ],
            "nodes": [
                {
                    "key": "Bereshit",
                    "sharedTitle": "Bereshit",
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Torah"]
                },
                {
                    "key": "Noach",
                    "sharedTitle": "Noach",
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Torah"]
                },
                {
                    "key": "Lech Lecha",
                    "sharedTitle": "Lech Lecha",
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Torah"]
                }
            ]
        }
        i = Index({
            "schema": schema,
            "title": "Parshanut Test",
            "categories": ["Chasidut"]
        })
        i.save()
        i.nodes.all_tree_titles("en")
        i.nodes.title_dict("en")
        schema['titles'] = sorted(schema['titles'], key=lambda x: x['text'])
        serialized = i.nodes.serialize()
        serialized['titles'] = sorted(serialized['titles'], key=lambda x: x['text'])
        assert schema == serialized
        i.delete()

    def test_alt_struct(self):
        i = Index().load({"title": "Altstest"})
        if i:
            i.delete()
        schema = {
            "key": "Altstest",
            "titles": [
                {
                    "lang": "en",
                    "text": "Altstest",
                    "primary": True
                },
                {
                    "lang": "he",
                    "text": '',
                    "primary": True
                }
            ],
            "nodeType": "JaggedArrayNode",
            "depth": 2,
            "addressTypes": ["Integer", "Integer"],
            "sectionNames": ["Chapter","Verse"]
        }

        structs = {
            "parasha": {
                "nodes": [
                    {
                        'sharedTitle': 'Shemot',
                        "nodeType": "ArrayMapNode",
                        "depth": 1,
                        "addressTypes": ["Integer"],
                        "sectionNames": ["Aliyah"],
                        'wholeRef': 'Altstest 1:1-6:1',
                        'refs': [
                                "Altstest 1:1-1:17",
                                "Altstest 1:18-2:10",
                                "Altstest 2:11-2:25",
                                "Altstest 3:1-3:15",
                                "Altstest 3:16-4:17",
                                "Altstest 4:18-4:31",
                                "Altstest 5:1-6:1",
                        ]
                    },
                    {
                        'sharedTitle': 'Vaera',
                        "nodeType": "ArrayMapNode",
                        "depth": 1,
                        "addressTypes": ["Integer"],
                        "sectionNames": ["Aliyah"],
                        'wholeRef': 'Altstest 6:2-9:35',
                        'refs': [
                            "Altstest 10:1-10:11",
                            "Altstest 10:12-10:23",
                            "Altstest 10:24-11:3",
                            "Altstest 11:4-12:20",
                            "Altstest 12:21-12:28",
                            "Altstest 12:29-12:51",
                            "Altstest 13:1-13:16",
                        ]
                    },
                ]
            }
        }

        creating_dict = {
            "schema": schema,
            "title": "Altstest",
            "categories": ["Chasidut"],
            "alt_structs": structs
        }
        i = Index(creating_dict)
        i.save()
        i.nodes.all_tree_titles("en")
        i.nodes.title_dict("en")
        schema['titles'] = sorted(schema['titles'], key=lambda x: x['text'])
        serialized = i.nodes.serialize()
        serialized['titles'] = sorted(serialized['titles'], key=lambda x: x['text'])
        assert schema == serialized

        contents =  i.contents(raw=True)
        contents['schema']['titles'] = sorted(contents['schema']['titles'], key=lambda x: x['text'])
        creating_dict['schema']['titles'] = sorted(creating_dict['schema']['titles'], key=lambda x: x['text'])
        assert contents == creating_dict

        assert Ref("Altstest, Vaera 3") == Ref("Altstest 10:24-11:3")
        assert Ref("Altstest, Vaera") == Ref("Altstest 6:2-9:35")

        with pytest.raises(InputError):
            Ref("Altstest, Foobar")

        with pytest.raises(InputError):
            Ref("Altstest, Foobar 3")

        with pytest.raises(InputError):
            Ref("Altstest, Vaera 12")

        i.delete()

    def test_complex_with_alt_struct(self):
        i = Index().load({"title": "CAtest"})
        if i:
            i.delete()
        schema = {
            "key": "CAtest",
            "titles": [
                {
                    "lang": "en",
                    "text": "CAtest",
                    "primary": True
                },
                {
                    "lang": "he",
                    "text": '',
                    "primary": True
                }
            ],
            "nodes": [
                {
                    "key": "child_one",
                    "nodeType": "JaggedArrayNode",
                    "depth": 2,
                    "addressTypes": ["Integer", "Integer"],
                    "sectionNames": ["Chapter", "Verse"],
                    "titles": [
                        {
                            "lang": "en",
                            "text": "Kid",
                            "primary": True
                        },
                        {
                            "lang": "he",
                            "text": '',
                            "primary": True
                        }
                    ],
                },
                {
                    "key": "child_two",
                    "nodeType": "JaggedArrayNode",
                    "depth": 2,
                    "addressTypes": ["Integer", "Integer"],
                    "sectionNames": ["Chapter", "Verse"],
                    "titles": [
                        {
                            "lang": "en",
                            "text": "Other Kid",
                            "primary": True
                        },
                        {
                            "lang": "he",
                            "text": ' ',
                            "primary": True
                        }
                    ],
                }
            ]
        }

        structs = {
            "parasha": {
                "nodes": [
                    {
                        'sharedTitle': 'Shemot',
                        "nodeType": "ArrayMapNode",
                        "depth": 1,
                        "addressTypes": ["Integer"],
                        "sectionNames": ["Aliyah"],
                        'wholeRef': 'CAtest, Kid 1:1-6:1',
                        'refs': [
                                "CAtest, Kid 1:1-1:17",
                                "CAtest, Kid 1:18-2:10",
                                "CAtest, Kid 2:11-2:25",
                                "CAtest, Kid 3:1-3:15",
                                "CAtest, Kid 3:16-4:17",
                                "CAtest, Kid 4:18-4:31",
                                "CAtest, Kid 5:1-6:1",
                        ]
                    },
                    {
                        'sharedTitle': 'Vaera',
                        "nodeType": "ArrayMapNode",
                        "depth": 1,
                        "addressTypes": ["Integer"],
                        "sectionNames": ["Aliyah"],
                        'wholeRef': 'CAtest, Kid 6:2-9:35',
                        'refs': [
                            "CAtest, Kid 10:1-10:11",
                            "CAtest, Kid 10:12-10:23",
                            "CAtest, Kid 10:24-11:3",
                            "CAtest, Kid 11:4-12:20",
                            "CAtest, Kid 12:21-12:28",
                            "CAtest, Kid 12:29-12:51",
                            "CAtest, Kid 13:1-13:16",
                        ]
                    },
                ]
            }
        }

        creating_dict = {
            "schema": schema,
            "title": "CAtest",
            "categories": ["Chasidut"],
            "alt_structs": structs
        }
        i = Index(creating_dict)
        i.save()
        i.nodes.all_tree_titles("en")
        i.nodes.title_dict("en")
        schema['titles'] = sorted(schema['titles'], key=lambda x: x['text'])
        serialized = i.nodes.serialize()
        serialized['titles'] = sorted(serialized['titles'], key=lambda x: x['text'])
        assert schema == serialized

        contents = i.contents(raw=True)
        contents['schema']['titles'] = sorted(contents['schema']['titles'], key=lambda x: x['text'])
        creating_dict['schema']['titles'] = sorted(creating_dict['schema']['titles'], key=lambda x: x['text'])
        assert contents == creating_dict

        assert Ref("CAtest, Vaera 3") == Ref("CAtest, Kid 10:24-11:3")
        assert Ref("CAtest, Vaera") == Ref("CAtest, Kid 6:2-9:35")

        with pytest.raises(InputError):
            Ref("CAtest, Foobar")

        with pytest.raises(InputError):
            Ref("CAtest, Foobar 3")

        with pytest.raises(InputError):
            Ref("CAtest, Vaera 12")

        i.delete()

    def test_numbered_primary_struct(self):
        i = Index().load({"title": "NumbPrimeTest"})
        if i:
            i.delete()
        schema = {
            "key": "NumbPrimeTest",
            "titles": [
                {
                    "lang": "en",
                    "text": "NumbPrimeTest",
                    "primary": True
                },
                {
                    "lang": "he",
                    "text": '',
                    "primary": True
                }
            ],
            "nodeType": "JaggedArrayNode",
            "sectionNames": ["Parasha"],
            "addressTypes": ["Integer"],
            "depth": 1,
            "nodes": [
                {
                    "key": "s1",
                    'sharedTitle': 'Shemot',
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Vort"],
                },
                {
                    "key": "s2",
                    'sharedTitle': 'Vaera',
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Vort"],
                },
                {
                    "key": "s3",
                    'sharedTitle': 'Bo',
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Vort"],
                },
                {
                    "key": "s4",
                    'sharedTitle': 'Beshalach',
                    "nodeType": "JaggedArrayNode",
                    "depth": 1,
                    "addressTypes": ["Integer"],
                    "sectionNames": ["Vort"],
                },
            ]
        }

        creating_dict = {
            "schema": schema,
            "title": "NumbPrimeTest",
            "categories": ["Chasidut"],
        }
        i = Index(creating_dict)
        i.save()
        i.nodes.all_tree_titles("en")
        i.nodes.title_dict("en")
        schema['titles'] = sorted(schema['titles'], key=lambda x: x['text'])
        serialized = i.nodes.serialize()
        serialized['titles'] = sorted(serialized['titles'], key=lambda x: x['text'])
        assert schema == serialized
        contents = i.contents(raw=True)
        contents['schema']['titles'] = sorted(contents['schema']['titles'], key=lambda x: x['text'])
        creating_dict['schema']['titles'] = sorted(creating_dict['schema']['titles'], key=lambda x: x['text'])
        assert contents == creating_dict

        assert Ref("NumbPrimeTest 3:5") == Ref("NumbPrimeTest, Bo 5")
        assert Ref("NumbPrimeTest 3") == Ref("NumbPrimeTest, Bo")

        with pytest.raises(InputError):
            Ref("NumbPrimeTest, Foobar")

        with pytest.raises(InputError):
            Ref("NumbPrimeTest, Foobar 3")

        i.delete()

    def test_numbered_alt_struct(self):
        i = Index().load({"title": "Stest"})
        if i:
            i.delete()
        schema = {
            "key": "Stest",
            "titles": [
                {
                    "lang": "en",
                    "text": "Stest",
                    "primary": True
                },
                {
                    "lang": "he",
                    "text": '',
                    "primary": True
                }
            ],
            "nodeType": "JaggedArrayNode",
            "depth": 2,
            "addressTypes": ["Integer", "Integer"],
            "sectionNames": ["Chapter", "Verse"]
        }

        structs = {
            "parasha": {
                "nodeType": "NumberedTitledTreeNode",
                "sectionNames": ["Chapter"],
                "addressTypes": ["Perek"],
                "depth": 1,
                "nodes": [
                    {
                        'sharedTitle': 'Shemot',
                        "nodeType": "ArrayMapNode",
                        "depth": 1,
                        "addressTypes": ["Integer"],
                        "sectionNames": ["Aliyah"],
                        'wholeRef': 'Stest 1:1-6:1',
                        'refs': [
                                "Stest 1:1-1:17",
                                "Stest 1:18-2:10",
                                "Stest 2:11-2:25",
                                "Stest 3:1-3:15",
                                "Stest 3:16-4:17",
                                "Stest 4:18-4:31",
                                "Stest 5:1-6:1",
                        ]
                    },
                    {
                        'sharedTitle': 'Vaera',
                        "nodeType": "ArrayMapNode",
                        "depth": 1,
                        "addressTypes": ["Integer"],
                        "sectionNames": ["Aliyah"],
                        'wholeRef': 'Stest 6:2-9:35',
                        'refs': [
                            "Stest 10:1-10:11",
                            "Stest 10:12-10:23",
                            "Stest 10:24-11:3",
                            "Stest 11:4-12:20",
                            "Stest 12:21-12:28",
                            "Stest 12:29-12:51",
                            "Stest 13:1-13:16",
                        ]
                    },
                ]
            }
        }

        creating_dict = {
            "schema": schema,
            "title": "Stest",
            "categories": ["Chasidut"],
            "alt_structs": structs
        }
        i = Index(creating_dict)
        i.save()
        i.nodes.all_tree_titles("en")
        i.nodes.title_dict("en")
        schema['titles'] = sorted(schema['titles'], key=lambda x: x['text'])
        serialized = i.nodes.serialize()
        serialized['titles'] = sorted(serialized['titles'], key=lambda x: x['text'])
        assert schema == serialized
        contents =  i.contents(raw=True)
        contents['schema']['titles'] = sorted(contents['schema']['titles'], key=lambda x: x['text'])
        creating_dict['schema']['titles'] = sorted(creating_dict['schema']['titles'], key=lambda x: x['text'])
        assert contents == creating_dict

        assert Ref("Stest Perek 2:3") == Ref("Stest, Vaera 3")
        assert Ref("Stest Perek 2:3") == Ref("Stest 10:24-11:3")

        i.delete()

    def test_quick_initialization(self):
        old_style = JaggedArrayNode()
        old_style.add_title('Title', 'en', primary=True)
        old_style.add_title('', 'he', primary=True)
        old_style.key = 'Title'
        old_style.sectionNames = ['Chapter', 'Verse']
        old_style.addressTypes = ['Integer', 'Integer']
        old_style.depth = 2

        quick_way = JaggedArrayNode()
        quick_way.add_primary_titles('Title', '')
        quick_way.add_structure(['Chapter', 'Verse'])

        assert quick_way.serialize() == old_style.serialize()

class Test_Default_Nodes(object):
    @classmethod
    def setup_class(cls):
        pass

    @classmethod
    def teardown_class(cls):
        v = Version().load({"title":"Chofetz Chaim", "versionTitle": "test_default_node"})
        if v:
            v.delete()

    def test_derivations(self):
        ref = Ref("Chofetz_Chaim,_Part_One,_The_Prohibition_Against_Lashon_Hara,_Principle_1")
        assert ref.has_default_child()
        child_ref = ref.default_child_ref()
        assert ref != child_ref

        sn = ref.index_node
        assert isinstance(sn, SchemaNode)
        assert not sn.is_default()
        assert sn.has_default_child()
        dnode = sn.get_default_child()
        assert dnode.is_default()


    def test_default_in_leaf_nodes(self):
        sn = Ref("Chofetz_Chaim,_Part_One,_The_Prohibition_Against_Lashon_Hara,_Principle_1").index_node
        assert isinstance(sn, SchemaNode)
        dnode = sn.get_default_child()
        root = sn.root()
        leaves = root.get_leaf_nodes()
        assert dnode in leaves
        assert sn not in leaves

    def test_load_default_text_chunk(self):
        ref = Ref("Chofetz_Chaim,_Part_One,_The_Prohibition_Against_Lashon_Hara,_Principle_1")
        TextChunk(ref)

    def test_load_default_text_chunk(self):
        ref = Ref("Chofetz_Chaim,_Part_One,_The_Prohibition_Against_Lashon_Hara,_Principle_1")
        tc = TextChunk(ref, "en", "test_default_node")
        tc.text = [["Foo", "Bar", "Blitz"],["Glam", "Blam", "Flam"]]
        tc.save()
        subref = Ref("Chofetz_Chaim,_Part_One,_The_Prohibition_Against_Lashon_Hara,_Principle_1.2.3")
        assert TextChunk(subref, "en", "test_default_node").text == "Flam"
```

### sefaria/model/tests/he_ref_test.py

```
# -*- coding: utf-8 -*-

import django
django.setup()
import pytest
from sefaria.system.exceptions import InputError

import sefaria.model as m

#todo: simplify this file

def setup_module(module):
    global texts
    global refs
    texts = {}
    refs = {}
    texts['false_pos'] = "  ( , )   "
    texts['bible_ref'] = "        ( , )   "
    texts['bible_begin'] = "( , )     "  # These work, even though the presentation of the parens may be confusing.
    texts['bible_mid'] = " ( , )    "
    texts['bible_end'] = "  ( , )"
    texts['2ref'] = "  (    ),  (  ):   "
    texts['neg327'] = '    ,   " ,  " ( ,;  ,;  ,;  ,).'
    texts['2talmud'] = "        (  ).       (''  ).   "
    texts['bk-abbrev'] = "        (  ).       (\"  ).   "
    texts['dq_talmud'] = '( ")'
    texts['sq_talmud'] = ""  # Need to find one in the wild
    texts['3dig'] = '( ")'
    texts['2with_lead'] = '(  ,;  ,)'
    texts['ignored_middle'] = '( , )           ,                                       ( , )'


class Test_parse_he_ref(object):
    def test_simple_bible(self):
        r = m.Ref(" , ")
        assert r.book == 'Exodus'
        assert r.sections[0] == 21
        assert r.sections[1] == 4

        r = m.Ref(" , ")
        assert r.book == 'Deuteronomy'
        assert r.sections[0] == 16
        assert r.sections[1] == 18

        r = m.Ref(' "')
        assert r.book == 'Psalms'
        assert r.sections[0] == 119
        assert len(r.sections) == 1

        r = m.Ref(' .')
        assert r.book == 'Genesis'
        assert r.sections[0] == 27
        assert r.sections[1] == 3

    def test_divrei_hayamim(self):
        r = m.Ref('    ')
        assert r.book == 'II Chronicles'
        assert r.sections[0] == 32
        assert r.sections[1] == 19

        r = m.Ref('   ')
        assert r.book == 'II Chronicles'
        assert r.sections[0] == 32
        assert len(r.sections) == 1


    def test_talmud(self):
        r = m.Ref(' "')
        assert r.book == 'Yevamot'
        assert r.sections[0] == 129
        assert len(r.sections) == 1

        r = m.Ref(" ' .")
        assert r.book == 'Shabbat'
        assert r.sections[0] == 43
        assert len(r.sections) == 1

        r = m.Ref("  :")
        assert r.book == 'Bava Metzia'
        assert r.sections[0] == 116
        assert len(r.sections) == 1

        r = m.Ref(" ' :")
        assert r.book == 'Pesachim'
        assert r.sections[0] == 116
        assert len(r.sections) == 1

        r = m.Ref(" ' .")
        assert r.book == 'Menachot'
        assert r.sections[0] == 97
        assert len(r.sections) == 1

        r = m.Ref("  ")
        assert r.book == 'Menachot'
        assert r.sections[0] == 57
        assert len(r.sections) == 1

        r = m.Ref("  ")
        assert r.book == 'Menachot'
        assert r.sections[0] == 58
        assert len(r.sections) == 1

        r = m.Ref("  :")
        assert r.book == 'Sotah'
        assert r.sections == [69, 11]

    def test_length_catching(self):
        with pytest.raises(InputError):
            r = m.Ref(' ')

        with pytest.raises(InputError):
            r = m.Ref(' , ')

    def test_talmud_ayin_amud_form(self):
        r = m.Ref('  " "')
        assert r.sections[0] == 90
        assert len(r.sections) == 1
        r = m.Ref("  '' ''")
        assert r.sections[0] == 90
        assert len(r.sections) == 1


    def test_talmud_refs_with_amud(self):
        assert m.Ref(" .") == m.Ref("Berakhot 8a")
        assert m.Ref(" :") == m.Ref("Berakhot 8b")
        assert m.Ref(" , ") == m.Ref("Berakhot 8a")
        assert m.Ref("""  """") == m.Ref("Berakhot 8b")
        assert m.Ref(""" "  '""") == m.Ref("Berakhot 11b")
        assert m.Ref("""  "  '""") == m.Ref("Berakhot 11b")


    def test_talmud_refs_without_amud(self):
        assert m.Ref(" ") == m.Ref("Berakhot 8a-8b")
        assert m.Ref(" ") == m.Ref("Berakhot 2a-2b")

    def test_talmud_range(self):
        assert m.Ref(" , ") == m.Ref("Shabbat 33")
        assert m.Ref(' -') == m.Ref("Eruvin 82-83")
        assert m.Ref(""" "   '""") == m.Ref("Berakhot 11")

    def test_bible_word_end(self):
        with pytest.raises(InputError):
            r = m.Ref(' ')

        with pytest.raises(InputError):
            r = m.Ref('  ')

        with pytest.raises(InputError):
            r = m.Ref(" , ")

    def test_talmud_word_end(self):
        with pytest.raises(InputError):
            r = m.Ref("  ")

        with pytest.raises(InputError):
            r = m.Ref(" ")

    def test_midrash_word_end(self):
        # Assumes that Esther Rabbah Petichta
        with pytest.raises(InputError):
            r = m.Ref("  ")

    def test_pehmem_form(self):
        r = m.Ref(' " "')
        assert r.book == 'Mishnah Parah'
        assert r.sections[0] == 8
        assert r.sections[1] == 7
        assert len(r.sections) == 2

        r = m.Ref(' "')
        assert r.book == 'Menachot'
        assert r.sections[0] == 175
        assert len(r.sections) == 1

        r = m.Ref(' " "')
        assert r.book == 'Mishnah Menachot'
        assert r.sections[0] == 8
        assert r.sections[1] == 7
        assert len(r.sections) == 2


    def test_perek_form(self):
        r = m.Ref('  ')
        assert r.book == 'Pirkei Avot'
        assert r.sections[0] == 4
        assert len(r.sections) == 1

    def test_peh_form(self):
        r = m.Ref(' "')

        assert r.book == 'Pirkei Avot'
        assert r.sections[0] == 4
        assert len(r.sections) == 1

        assert m.Ref(' :') == m.Ref("Psalms 84:3")  # dont strip peh when no quotation

    def test_volume_address(self):
        assert m.Ref(" , , , ") == m.Ref("Orot HaKodesh 1:1:1")
        assert m.Ref(" , , , ") == m.Ref("Orot HaKodesh 1:1:1")
        assert m.Ref("    ") == m.Ref("Orot HaKodesh 1:1:1")
        assert m.Ref("  , , ") == m.Ref("Orot HaKodesh 1:1:1")
        assert m.Ref("   , , ") == m.Ref("Orot HaKodesh 1:1:1")
        assert m.Ref('  ", , ') == m.Ref("Orot HaKodesh 3:1:1")

        assert m.Ref("Orot HaKodesh, Volume 1, 1 1") == m.Ref("Orot HaKodesh 1:1:1")

    def test_two_single_quotes(self):
        r = m.Ref(" ''")
        assert r.book == 'Exodus'
        assert len(r.sections) == 1
        assert r.sections[0] == 22

        r = m.Ref(" ''")
        assert r.book == 'Numbers'
        assert len(r.sections) == 1
        assert r.sections[0] == 35

        r = m.Ref("  ''")
        assert r.book == 'Judges'
        assert len(r.sections) == 2
        assert r.sections[0] == 20
        assert r.sections[1] == 11


    def test_peh_and_spelled_mishnah(self):
        r = m.Ref(' "  ')
        assert r.book == 'Mishnah Tahorot'
        assert len(r.sections) == 2
        assert r.sections[0] == 3
        assert r.sections[1] == 2

    def test_spelled_perek_and_mem(self):
        r = m.Ref('   ')
        assert r.book == 'Mishnah Tahorot'
        assert len(r.sections) == 2
        assert r.sections[0] == 3
        assert r.sections[1] == 2

    def test_spelled_perek_and_mishnah(self):
        r = m.Ref('    ')
        assert r.book == 'Mishnah Tahorot'
        assert len(r.sections) == 2
        assert r.sections[0] == 3
        assert r.sections[1] == 2

    def test_mishnah_form_equality(self):
        assert m.Ref('    ') == m.Ref('   ')
        assert m.Ref('   ') == m.Ref(' "  ')
        assert m.Ref('  ') == m.Ref(' "  ')

    def test_hebrew_english_equality(self):
        assert m.Ref('    ') == m.Ref("Mishnah Tahorot 3:2")
        assert m.Ref("  ''") == m.Ref("Judges 20:11")
        assert m.Ref(" ' :") == m.Ref("Pesachim 58b")
        assert m.Ref(' "') == m.Ref("Yevamot 65a-65b")
        assert m.Ref(' "') == m.Ref("Psalms 119")
        assert m.Ref(" , ") == m.Ref("Exodus 21:4")
        assert m.Ref(" ' .") == m.Ref("Shabbat 22a")

    def test_repr_on_hebrew(self):
        repr(m.Ref('    '))


class Test_Hebrew_Quoting_Styles(object):
    def test_leading_geresh(self):
        assert m.Ref("  ") == m.Ref('Exodus 10:12')
        assert m.Ref("  ") == m.Ref('Exodus 10:12')
        assert m.Ref("  ") == m.Ref('Exodus 10:12')

    def test_no_punctuation(self):
        assert m.Ref("  ") == m.Ref('Exodus 10:12')
        assert m.Ref("  ") == m.Ref('Exodus 12:10')

    def test_leading_gershaim(self):
        assert m.Ref("  ") == m.Ref('Exodus 12:10')
        assert m.Ref("  ") == m.Ref('Exodus 12:10')

    def test_trailing_geresh(self):
        assert m.Ref("  ") == m.Ref('Exodus 12:10')
        assert m.Ref("  ") == m.Ref('Exodus 12:10')

    def test_trailing_gershaim(self):
        assert m.Ref("  ") == m.Ref('Exodus 10:12')
        assert m.Ref("  ") == m.Ref('Exodus 10:12')


#todo: surprised this works. Had been marked as failing.  What's the coverage of these kinds of refs?
class Test_parse_he_commentary(object):
    def test_hebrew_commentary(self):
        assert m.Ref('"   :') == m.Ref("Rashi on Leviticus 15:3")


class Test_parse_he_ref_range(object):
    def test_hebrew_range_simple(self):
        assert m.Ref(', ", -') == m.Ref('Exodus 24:13-14')
        assert m.Ref(', ",  - ') == m.Ref("Numbers 27:15-23")
        assert m.Ref(', ",  - ') == m.Ref("Numbers 27:15-29:23")
        assert m.Ref('   : -    :') == m.Ref('I Chronicles 15:15-16:17')

    def test_hebrew_range_with_colons(self):
        assert m.Ref(' :-:') == m.Ref("Ruth 3:18-4:1")

    def test_hebrew_range_commentary(self):
        assert m.Ref('"   :-:') == m.Ref("Rashi on Leviticus 15:3-17:12")
        assert m.Ref('"   ::-') == m.Ref("Rashi on Exodus 3:1:1-3")

    def test_hebrew_range_talmud(self):
        assert m.Ref(' . - :') == m.Ref("Shabbat 15a-16b")
        assert m.Ref('   -  ') == m.Ref("Shabbat 15a-16b")
        assert m.Ref('  "-"') == m.Ref("Yevamot 61a-b")

    @pytest.mark.xfail(reason="unknown")
    def test_hebrew_range_talmud_commentary(self):
        assert m.Ref('') == m.Ref("Rashi on Shabbat 15a:15-15b:13")


class Test_Hebrew_Normal(object):

    def test_simple(self):
        assert m.Ref("Exodus").he_normal() == ''
        assert m.Ref("Exodus 4").he_normal() == ' '
        assert m.Ref("Exodus 4:3").he_normal() == ' :'

    def test_talmud(self):
        assert m.Ref("Shabbat").he_normal() == ''
        assert m.Ref("Shabbat 3b").he_normal() == '  '
        # assert m.Ref("Shabbat 3b:23").he_normal() == u'   23'
        assert m.Ref("Shabbat 3b:23").he_normal() == '  :'

    def test_simple_range(self):
        assert m.Ref("Exodus 4-5").he_normal() == ' -'
        assert m.Ref("Exodus 4:3-8").he_normal() == ' :-'
        assert m.Ref("Exodus 4:3-5:8").he_normal() == ' :-:'

    def test_talmud_normal_range(self):
        assert m.Ref("Shabbat 3b-5a").he_normal() == '  - '
        # assert m.Ref("Shabbat 3b:3-24").he_normal() == u'   3-24'
        assert m.Ref("Shabbat 3b:3-24").he_normal() == '  :-'
        # assert m.Ref("Shabbat 3b:3-5a:24").he_normal() == u' : 3-. 24'

    def test_complex(self):
        pass


class Test_parse_he_Data_Types(object):

    def test_perek_pasuk(self):
        pass
        # assert m.Ref(u'    ') == m.Ref('Genesis 1:3')
        # assert m.Ref(u'   -') == m.Ref('Exodus 4:3-6')

        ## this test fails since 2015 because Perek looks for "
        # assert m.Ref(u' "') == m.Ref('Psalms 86')
        ## these tests fail because ranges doesn't use DataTypes after the hyphen

        # assert m.Ref(u'    -    ') == m.Ref('Exodus 4:3-6:2')
        # assert m.Ref(u'    -  ') == m.Ref('Exodus 4:3-6:30')
        # assert m.Ref(u'    -  ') == m.Ref('Exodus 4:3-6')
        # assert m.Ref(u'   -    ') == m.Ref('Exodus 4:1-5:6')


#todo: convert to all_titles_regex
class Test_get_titles_in_string(object):
    def test_bible_ref(self):
        res = m.library.get_titles_in_string(texts['bible_ref'], "he")
        assert set(res) >= set([""])

        res = m.library.get_titles_in_string(texts['false_pos'], "he")
        assert set(res) >= set(["", ""])

    def test_positions(self):
        for a in ['bible_mid', 'bible_begin', 'bible_end']:
            assert set([""]) <= set(m.library.get_titles_in_string(texts[a], "he"))


    def test_abbreviations(self):
        t = "        (  ).       (\"  ).   "
        res = m.library.get_refs_in_string(t)
        assert len(res) == 2

        t = '       (  )     (  )             '
        res = m.library.get_refs_in_string(t)
        assert len(res) == 2

        t = ',    ("  ),   '
        res = m.library.get_refs_in_string(t)
        assert len(res) == 1





```

### sefaria/model/tests/webpage_test.py

```
import pytest
from sefaria.model import *
from sefaria.model.webpage import WebPage, get_webpages_for_ref

title_good_url = "Dvar Torah"


@pytest.fixture(scope='module')
def create_good_web_page():
	data_good_url = {'url': 'http://blogs.timesofisrael.com/dvar-torah2',
									 'title': title_good_url,
									 'description': 'A Dvar Torah on Times of Israel',
									 'refs': ["Haamek Davar on Genesis, Kidmat Ha'Emek 1", 'Shulchan Aruch, Orach Chaim 7:1']}

	result, webpage = WebPage().add_or_update_from_linker(data_good_url)
	yield {"result": result, "webpage": webpage, "data": data_good_url}
	WebPage().load(data_good_url["url"]).delete()


@pytest.fixture(scope='module')
def create_web_page_wout_desc():
	data_good_url = {'url': 'http://blogs.timesofisrael.com/dvar-torah4',
									 'title': title_good_url+" without description",
									 'refs': ["Haamek Davar on Genesis, Kidmat Ha'Emek 2", 'Genesis 3']}

	result, webpage = WebPage().add_or_update_from_linker(data_good_url)
	yield {"result": result, "webpage": webpage, "data": data_good_url}
	webpage.delete()


@pytest.fixture(scope='module')
def create_web_page_wout_site():
	data = {'url': 'http://notarealsite.org/123', 'title': "This is a good title", "refs": ["Genesis 2"]}
	result, webpage = WebPage().add_or_update_from_linker(data)
	yield {'result': result, 'webpage': webpage, 'data': data}
	webpage.delete()

def test_add_bad_domain_from_linker():
	#localhost:8000 should not be added to the linker, so make sure attempting to do so fails

	data = {'url': 'http://localhost:8000/static/test/linker_test.html',
					'title': 'Linker Test Page',
					'description': 'A Page We Do Not Want',
					'refs': ["Haamek Davar on Genesis, Kidmat Ha'Emek 1", 'Shulchan Aruch, Orach Chaim 7:1']}

	result, _ = WebPage().add_or_update_from_linker(data)
	assert result == "excluded"

def test_add_no_refs_from_linker():
	# blogs.timesofisrael.com/random should not be added to the linker, because it contains no refs
	# even though it's a good URL

	data = {'url': 'http://blogs.timesofisrael.com/random',
					'title': 'Random Blog',
					'description': 'A Page We Do Not Want',
					'refs': []}

	result, _ = WebPage().add_or_update_from_linker(data)
	assert result == "excluded"


def test_add_bad_title_from_linker():
	# http://rabbisacks.com/archives should not be added because it has a title we do not want

	data = {'url': 'http://rabbisacks.org/archives',
					'title': 'Page 1 of 2',
					'description': 'A Page We Do Not Want',
					'refs': ["Genesis 1:1"]}

	result, _ = WebPage().add_or_update_from_linker(data)
	assert result == "excluded"


def test_add_webpage_without_website(create_web_page_wout_site):
	# this should be possible even though no corresponding domain exists in a WebSite object
	result, webpage, data = create_web_page_wout_site["result"], create_web_page_wout_site["webpage"], create_web_page_wout_site["data"]
	assert result == "saved"


def test_add_and_update_good_url_from_linker(create_good_web_page):
	#blogs.timesofisrael.com is a whitelisted site with refs and good title so it should be added to the linker,
	#so make sure attempting to do so succeeds
	result, webpage, data = create_good_web_page["result"], create_good_web_page["webpage"], create_good_web_page["data"]
	assert result == "saved"
	linker_hits = webpage.linkerHits

	# now, we simply update an existing site with different refs and make sure it updated
	data['refs'] = ["Genesis 3:3", 'Exodus 3:10']

	result, webpage = WebPage().add_or_update_from_linker(data, add_hit=True)
	assert result == "saved"
	assert set(WebPage().load(data["url"]).refs) == set(["Genesis 3:3", 'Exodus 3:10'])
	assert WebPage().load(data["url"]).linkerHits == linker_hits + 1


def test_add_and_update_with_same_data(create_good_web_page):
	# create a page and then try to add_or_update it with same data and assert it fails
	result, webpage, data = create_good_web_page["result"], create_good_web_page["webpage"], create_good_web_page["data"]
	assert result == "saved"
	result, _ = WebPage().add_or_update_from_linker(data)
	assert result == "excluded"


def test_update_blank_title_from_linker(create_good_web_page):
	result, webpage, data = create_good_web_page["result"], create_good_web_page["webpage"], create_good_web_page["data"]
	print(webpage.contents())
	assert result == "saved"

	data["title"] = ""

	result, _ = WebPage().add_or_update_from_linker(data)
	assert result == "saved"
	print(WebPage().load(data["url"]).contents())
	assert WebPage().load(data["url"]).title == title_good_url



def test_add_search_URL():
	urls = ["https://opensiddur.org/search/", "https://opensiddur.org/search?q=prayer", "https://opensiddur.org/search"]
	for url in urls:
		linker_data = {'url': url,
		 'title': '"On Prayer," by Abraham Joshua Heschel (1969)  the Open Siddur Project    ',
		 'description': 'Rabbi Dr. Abraham Joshua Heschel\'s speech, "On Prayer," delivered at an inter-religious convocation held under the auspices of the U.S. Liturgical Conference in Milwaukee, Wisconsin, on August 28, 1969. His talk was printed in the journal Conservative Judaism v.25:1 Fall 1970, p.1-12.   . . .',
		 'refs': ['Psalms 141', 'Psalms 4272', 'Psalms 7389', 'Psalms 90106', 'Psalms 107150', 'Psalms 130:1',
				  'Psalms 63:2-4', 'Psalms 42:2-4']}

		result, _ = WebPage.add_or_update_from_linker(linker_data)
		assert result == "excluded"


def test_page_wout_description(create_web_page_wout_desc):
	result, webpage, data = create_web_page_wout_desc["result"], create_web_page_wout_desc["webpage"], create_web_page_wout_desc["data"]
	assert result == "saved"

	data["description"] = "here is a desc"
	assert WebPage().add_or_update_from_linker(data)[0] == "saved"

	assert WebPage().add_or_update_from_linker(data)[0] == "excluded"


def test_get_webpages_for_ref():
	results = get_webpages_for_ref("Rashi on Genesis 1:1")
	assert "title" in results[0]
	assert "url" in results[0]
	assert "refs" in results[0]

```

### sefaria/model/tests/category_test.py

```
# -*- coding: utf-8 -*-
import pytest
import json
from deepdiff import DeepDiff
import copy
from sefaria.system.exceptions import InputError
from sefaria.model import *
from sefaria import tracker
import sefaria.model.category as c
from sefaria.helper.category import update_order_of_category_children
import datetime
class Test_Category_Editor(object):
    @pytest.fixture(autouse=True, scope='module')
    def create_new_terms(self):
        terms = []
        for title in ["New Fake Category 1", "New Fake Category 2"]:
            t = Term()
            t.add_primary_titles(title, title[::-1])
            t.name = title
            t.save()
            terms.append(t)
        yield terms
        for t in terms:
            t.delete()

    @pytest.fixture(autouse=True, scope='module')
    def create_new_cats(self, create_new_terms):
        titles = {"New Fake Category 1": ["New Fake Category 1"],
                  "New Fake Category 2": ["New Fake Category 1", "New Fake Category 2"]}
        cats = []
        for title, path in titles.items():
            c = Category()
            c.path = path
            c.add_shared_term(title)
            c.save()
            cats.append(c)
        yield cats
        for c in cats[::-1]:
            c.delete()
            library.rebuild_toc()

    @pytest.fixture(autouse=True, scope='module')
    def create_fake_indices(self, create_new_cats):
        books = []
        paths_for_books = [create_new_cats[0].path, create_new_cats[0].path, create_new_cats[1].path]
        for i, title in enumerate(['Fake Book One', 'Fake Book Two', 'Fake Book Three']):
            data = {
                "categories": list(paths_for_books[i]),
                "title": title,
                "order": [i*5],
                "schema": {
                    "titles": [
                        {
                            "lang": "en",
                            "text": title,
                            "primary": True
                        },
                        {
                            "lang": "he",
                            "text": title[::-1],
                            "primary": True
                        }
                    ],
                    "nodeType": "JaggedArrayNode",
                    "depth": 2,
                    "sectionNames": [
                        "Section",
                        "Line"
                    ],
                    "addressTypes": [
                        "Integer",
                        "Integer"
                    ],
                    "key": title
                },
            }
            book = Index(data)
            book.save()
            books.append(book)

        yield books
        for b in books:
            library.get_index(b.title).delete() # need to reload this due to caching

    @pytest.fixture(scope='module', autouse=True)
    def create_new_main_cat_shared_title(self):
        t = Term()
        t.name = "New Shared Title for Main Cat"
        t.add_primary_titles(t.name, t.name[::-1])
        t.save()
        yield t
        t.delete()

    @pytest.fixture(scope='module', autouse=True)
    def create_new_collection(self, create_new_cats):
        c = Collection({"name": "Fake Collection 123",
                    "sheets": 1,
                    "slug": "fake-collection",
                    "lastModified": datetime.datetime(2021, 2, 5, 14, 57, 32, 336000),
                    "admins": [1],
                    "members": [1]})
        c.toc = {"categories": create_new_cats[0].path, "description": "", "heDescription": ""}
        c.save()
        yield c
        c.delete()

    @staticmethod
    def change_cat(term, orig_categories, new_categories):
        # simulate category editor: change title, path and desc all at once
        main_cat_new_dict = {"path": new_categories, "sharedTitle": term.get_primary_title('en'),
                             "heSharedTitle": term.get_primary_title('he'), "origPath": orig_categories}
        return tracker.update(1, Category, main_cat_new_dict).contents()

    @staticmethod
    def get_thin_toc(path):
        return library.get_toc_tree().lookup(path).serialize(thin=True)

    def test_reorder_editor(self, create_new_cats, create_fake_indices):
        first_book = create_fake_indices[0]
        second_book = create_fake_indices[1]
        assert second_book.order[0] > first_book.order[0]   # confirm that second book follows first

        reorderedBooks = [second_book.title, first_book.title]
        update_order_of_category_children(create_new_cats[0], 1, reorderedBooks)
        assert library.get_index(second_book.title).order[0] < library.get_index(first_book.title).order[0]   # confirm that order has been reversed

    @staticmethod
    def modify_and_reverse(new_categories, create_new_main_cat_shared_title, create_new_terms, create_new_cats, create_new_collection, create_fake_indices):
        # modify a category and then reverse the changes and confirm that everything is back in place
        main_cat_term = create_new_terms[0]
        main_cat = create_new_cats[0]
        orig_contents = copy.deepcopy(main_cat.contents())
        orig_categories = orig_contents["path"]
        orig_toc = Test_Category_Editor.get_thin_toc(orig_categories)

        first_run = {"term": create_new_main_cat_shared_title, "orig": orig_categories, "new": new_categories,
                     "deep_diff": lambda orig, new: DeepDiff(orig, new, ignore_order=True)}
        second_run = {"term": main_cat_term, "orig": new_categories, "new": orig_categories,
                      "deep_diff": lambda orig, new: not DeepDiff(orig, new, ignore_order=True)}
        for run in [first_run, second_run]:
            Test_Category_Editor.change_cat(run["term"], run["orig"], run["new"])
            assert Collection().load({'name': create_new_collection.name}).toc["categories"] == run["new"]  # need to reload this due to caching

            index_cats = library.get_index(create_fake_indices[0].title).categories  # need to reload this due to caching
            assert run["new"] == index_cats, f"{index_cats} should be the same as {new_categories}"

            new_toc = Test_Category_Editor.get_thin_toc(run["new"])
            assert run["deep_diff"](orig_toc, new_toc), f"Deep Diff test failed for {run['term'].get_primary_title('en')}"


    def test_title_change_and_parent_change(self, create_new_main_cat_shared_title, create_new_terms, create_new_cats, create_new_collection, create_fake_indices):
        new_categories = ["Midrash", create_new_main_cat_shared_title.name]
        Test_Category_Editor.modify_and_reverse(new_categories, create_new_main_cat_shared_title, create_new_terms, create_new_cats, create_new_collection, create_fake_indices)

    def test_title_change_only(self, create_new_main_cat_shared_title, create_new_terms, create_new_cats, create_new_collection, create_fake_indices):
        main_cat = create_new_cats[0]
        orig_contents = copy.deepcopy(main_cat.contents())
        orig_categories = orig_contents["path"]
        new_categories = orig_categories[:-1] + [create_new_main_cat_shared_title.name]
        Test_Category_Editor.modify_and_reverse(new_categories, create_new_main_cat_shared_title, create_new_terms, create_new_cats, create_new_collection, create_fake_indices)


class Test_Categories(object):
    def test_index_save_with_bad_categories(self):
        title = 'Test Bad Cat'
        d = {
            "categories": [
                "Liturgy", "Bobby McGee"
            ],
            "title": title,
            "schema": {
                "titles": [
                    {
                        "lang": "en",
                        "text": title,
                        "primary": True
                    },
                    {
                        "lang": "he",
                        "text": " ",
                        "primary": True
                    }
                ],
                "nodeType": "JaggedArrayNode",
                "depth": 2,
                "sectionNames": [
                    "Section",
                    "Line"
                ],
                "addressTypes": [
                    "Integer",
                    "Integer"
                ],
                "key": title
            },
        }
        i = Index(d)
        with pytest.raises(InputError):
            i.save()

    @pytest.mark.deep
    def test_cat_name_change(self):
        base_toc = library.get_toc()
        base_json = json.dumps(base_toc, sort_keys=True)

        toc_tree = library.get_toc_tree()
        cat = toc_tree.lookup(["Tanakh", "Torah"]).get_category_object()
        cat.change_key_name("Seder Moed")
        cat.save()

        toc_tree = library.get_toc_tree()
        cat = toc_tree.lookup(["Tanakh", "Torah"])
        assert cat is None

        toc_cat = toc_tree.lookup(["Tanakh", "Seder Moed"])
        assert toc_cat
        for child in toc_cat.all_children():
            if isinstance(child, c.TocCategory):
                cobj = child.get_category_object()
                assert cobj.path[1] == "Seder Moed"
            elif isinstance(child, c.TocTextIndex):
                i = child.get_index_object()
                assert i.categories[1] == "Seder Moed"
            else:
                raise Exception()

        # Now unwind it
        cat = toc_cat.get_category_object()
        cat.change_key_name("Torah")
        cat.save()

        new_toc = library.get_toc()
        new_json = json.dumps(new_toc, sort_keys=True)

        # Deep test of toc lists
        assert not DeepDiff(base_toc, new_toc)

        # Check that the json is identical -
        # that the round-trip didn't change anything by reference that would poison the deep test
        assert len(base_json) == len(new_json)


"""
Are these tests necessary anymore?
They were useful in validating 1st class categories against older forms. 
"""
class Test_OO_Toc(object):
    def test_round_trip(self):
        base_toc = library.get_toc()
        base_json = json.dumps(base_toc, sort_keys=True)
        oo_toc = c.toc_serial_to_objects(base_toc)
        rt_toc = oo_toc.serialize()["contents"]

        # Deep test of toc lists
        assert not DeepDiff(base_toc, rt_toc)

        # Check that the json is identical -
        # that the round-trip didn't change anything by reference that would poison the deep test
        new_json = json.dumps(rt_toc, sort_keys=True)
        assert len(base_json) == len(new_json)


```

### sefaria/model/tests/manuscript_test.py

```
# encoding=utf-8

import pytest

from sefaria.model import *
from sefaria.system.exceptions import *


def make_testing_manuscript_page(manuscript_slug, page_id, **kwargs) -> ManuscriptPage:
    """
    Convenience method, sets the required url attributes to filler values, and giving a valid ManuscriptPage instance.
    Other parameters can be set with a keyword argument.
    :param manuscript_slug:
    :param page_id:
    :return:
    """
    data_dict = {
        'image_url': 'foo',
        'thumbnail_url': 'foo.thumb',
        'page_id': page_id,
        'manuscript_slug': manuscript_slug
    }
    for attr in ManuscriptPage.required_attrs:
        if attr in kwargs:
            data_dict[attr] = kwargs[attr]

    return ManuscriptPage().load_from_dict(data_dict)


def make_testing_manuscript(title, **kwargs) -> Manuscript:
    """
    Convenience method, sets all the required attributes except the title and return a valid Manuscript instance.
    Other parameters can be set with a keyword argument.
    :param title:
    :return:
    """
    data_dict = {  # slug gets defined from the title, we do not need to set it manually
        'title': title,
        'he_title': '',
        'source': 'This manuscript is a forgery',
        'description': 'testing manuscript, delete this',
        'he_description': ' '
    }
    for attr in Manuscript.required_attrs:
        if attr in kwargs:
            data_dict[attr] = kwargs[attr]

    return Manuscript().load_from_dict(data_dict)


def setup_module():
    teardown_module()
    m = make_testing_manuscript('Delete Me')
    m.save()
    for i in range(5):
        page = make_testing_manuscript_page(m.slug, i)
        page.save()


def teardown_module():
    possible_titles = ['Delete Me', 'Remove Me']
    for title in possible_titles:
        m = Manuscript().load({'slug': Manuscript.normalize_slug(title)})
        if m:
            m.delete()
        ManuscriptPageSet({'manuscript_slug': Manuscript.normalize_slug(title)}).delete()


class TestDataValidation:

    def test_check_for_parent(self):
        foo = make_testing_manuscript_page('no_parent', 1)
        assert not foo.validate()

        with pytest.raises(ManuscriptError):
            foo.save()

    def test_duplicate_manuscript(self):
        with pytest.raises(DuplicateRecordError):
            make_testing_manuscript('Delete Me').save()

        num_test_manuscripts = ManuscriptSet({'title': 'Delete Me'})
        assert num_test_manuscripts.count() == 1


class TestPageRefs:

    @classmethod
    def setup_class(cls):
        refs = [
            'Job 3',
            'Job 4',
            'Job 5-6',
        ]
        slug = Manuscript.normalize_slug('Delete Me')
        for i, r in enumerate(refs):
            mp = ManuscriptPage().load({'manuscript_slug': slug, 'page_id': i})
            if r not in mp.contained_refs:
                mp.add_ref(r)
                mp.save()

    def test_new_ref_overlap(self):
        mp = ManuscriptPage().load({'expanded_refs': 'Job 3:1'})
        assert mp is not None
        with pytest.raises(ManuscriptError):
            mp.add_ref('Job 3-4')

    def test_load_by_segment(self):
        mps = ManuscriptPageSet.load_by_ref(Ref('Job 3:1'))
        test_specific = [m for m in mps if m.manuscript_slug == Manuscript.normalize_slug('Delete Me')]
        assert len(test_specific) == 1

    def test_load_by_section(self):
        mps = ManuscriptPageSet.load_by_ref(Ref('Job 3'))
        test_specific = [m for m in mps if m.manuscript_slug == Manuscript.normalize_slug('Delete Me')]
        assert len(test_specific) == 1

    def test_load_range(self):
        mps = ManuscriptPageSet.load_by_ref(Ref("Job 4:2-6:3"))
        test_specific = [m for m in mps if m.manuscript_slug == Manuscript.normalize_slug('Delete Me')]
        assert len(test_specific) == 2

    def test_load_for_client(self):
        slug = Manuscript.normalize_slug('Delete Me')
        data = ManuscriptPageSet.load_set_for_client("Job 4")
        data = [d for d in data if d['manuscript_slug'] == slug][0]  # this is here to limit us to just the testing data

        mock_page = make_testing_manuscript_page(slug, 1)
        mock_page.add_ref("Job 4")
        mock_page = mock_page.contents()
        mock_page['manuscript'] = make_testing_manuscript('Delete Me').contents()
        mock_page['manuscript']['slug'] = slug
        mock_page['anchorRef'] = "Job 4"
        mock_page['anchorRefExpanded'] = mock_page['expanded_refs']
        del mock_page['expanded_refs']
        del mock_page['contained_refs']
        for key, value in mock_page.items():
            if isinstance(value, list):
                assert sorted(data[key]) == sorted(value)
            else:
                assert data[key] == value


class TestDependencies:

    def test_rename_manuscript(self):
        original_title, new_title = 'Delete Me', 'Remove Me'
        m = Manuscript().load({'slug': Manuscript.normalize_slug(original_title)})
        m.title = new_title
        m.save()
        assert m.slug == Manuscript.normalize_slug(new_title)

        mps = ManuscriptPageSet({'manuscript_slug': Manuscript.normalize_slug(original_title)})
        assert mps.count() == 0
        mps = ManuscriptPageSet({'manuscript_slug': Manuscript.normalize_slug(new_title)})
        assert mps.count() == 5

        m.title = original_title
        m.save()

        mps = ManuscriptPageSet({'manuscript_slug': Manuscript.normalize_slug(original_title)})
        assert mps.count() == 5
        mps = ManuscriptPageSet({'manuscript_slug': Manuscript.normalize_slug(new_title)})
        assert mps.count() == 0

    def test_delete_manuscript(self):
        slug = Manuscript.normalize_slug('Delete Me')
        m = Manuscript().load({'slug': slug})
        m.delete()

        mps = ManuscriptPageSet({'manuscript_slug': slug})
        assert mps.count() == 0

        setup_module()

```

### sefaria/model/tests/topic_test.py

```
import pytest

from sefaria.model.topic import Topic, TopicSet, IntraTopicLink, RefTopicLink, TopicLinkHelper, IntraTopicLinkSet, RefTopicLinkSet
from sefaria.model.text import Ref
from sefaria.system.database import db as mongo_db
from sefaria.system.exceptions import SluggedMongoRecordMissingError
from django_topics.models import Topic as DjangoTopic, TopicPool
from django_topics.models.pool import PoolType


def _ms(slug_suffix):
    """
    ms = make slug. makes full test slug.
    @param slug_suffix:
    @return:
    """
    return 'this-is-a-test-slug-'+slug_suffix


def make_topic(slug_suffix):
    slug = _ms(slug_suffix)
    ts = TopicSet({'slug': slug})
    if ts.count() > 0:
        ts.delete()
    t = Topic({'slug': slug, 'titles': [{'text': slug, 'primary': True, 'lang': 'en'}]})
    t.save()
    return t


def make_it_link(a, b, type):
    l = IntraTopicLink({'fromTopic': _ms(a), 'toTopic': _ms(b), 'linkType': type, 'dataSource': 'sefaria'})
    l.save()
    return l


def make_rt_link(a, tref):
    l = RefTopicLink({'toTopic': _ms(a), 'ref': tref, 'linkType': 'about', 'dataSource': 'sefaria'})
    l.save()
    return l


def clean_links(a):
    """
    Remove any existing links to `a` in the db
    :param a:
    :return:
    """
    ls = RefTopicLinkSet({'toTopic': _ms(a)})
    if ls.count() > 0:
        ls.delete()

    ls = IntraTopicLinkSet({"$or": [{"fromTopic": _ms(a)}, {"toTopic": _ms(a)}]})
    if ls.count() > 0:
        ls.delete()


@pytest.fixture(scope='module', autouse=True)
def library_and_sheets_topic_pools(django_db_setup, django_db_blocker):
    with django_db_blocker.unblock():
        TopicPool.objects.get_or_create(name=PoolType.LIBRARY.value)
        TopicPool.objects.get_or_create(name=PoolType.SHEETS.value)


@pytest.fixture(scope='module')
def topic_graph(django_db_setup, django_db_blocker):
    with django_db_blocker.unblock():
        isa_links = [
            (1, 2),
            (2, 3),
            (2, 4),
            (4, 5),
            (6, 5),
        ]
        trefs = [r.normal() for r in Ref('Genesis 1:1-10').range_list()]
        for a, b in isa_links:
            clean_links(str(a))
            clean_links(str(b))
        graph = {
            'topics': {
                str(i): make_topic(str(i)) for i in range(1, 10)
            },
            'links': [make_it_link(str(a), str(b), 'is-a') for a, b in isa_links] + [make_rt_link('1', r) for r in trefs]
        }
        yield graph
        for k, v in graph['topics'].items():
            v.delete()
        for v in graph['links']:
            v.delete()


@pytest.fixture(scope='module')
def topic_graph_to_merge(django_db_setup, django_db_blocker):
    with django_db_blocker.unblock():
        isa_links = [
            (10, 20),
            (20, 30),
            (20, 40),
            (40, 50),
            (60, 50),
        ]
        trefs = [r.normal() for r in Ref('Genesis 1:1-10').range_list()]
        trefs1 = [r.normal() for r in Ref('Exodus 1:1-10').range_list()]
        trefs2 = [r.normal() for r in Ref('Leviticus 1:1-10').range_list()]

        graph = {
            'topics': {
                str(i): make_topic(str(i)) for i in range(10, 100, 10)
            },
            'links': [make_it_link(str(a), str(b), 'is-a') for a, b in isa_links] + [make_rt_link('10', r) for r in trefs] + [make_rt_link('20', r) for r in trefs1] + [make_rt_link('40', r) for r in trefs2]
        }
        mongo_db.sheets.insert_one({
            "id": 1234567890,
            "topics": [
                {"slug": _ms('20'), 'asTyped': 'twenty'},
                {"slug": _ms('40'), 'asTyped': '4d'},
                {"slug": _ms('20'), 'asTyped': 'twent-e'},
                {"slug": _ms('30'), 'asTyped': 'thirty'}
            ]
        })

        yield graph
        for k, v in graph['topics'].items():
            v.delete()
        for v in graph['links']:
            v.delete()
        mongo_db.sheets.delete_one({"id": 1234567890})


@pytest.fixture(scope='module')
def topic_pool(django_db_setup, django_db_blocker):
    with django_db_blocker.unblock():
        pool = TopicPool.objects.create(name='test-pool')
        yield pool
        pool.delete()

@pytest.mark.django_db
class TestTopics(object):

    def test_graph_funcs(self, topic_graph):
        ts = topic_graph['topics']
        assert ts['1'].get_types() == {_ms(x) for x in {'1', '2', '3', '4', '5'}}
        assert ts['2'].get_types() == {_ms(x) for x in {'2', '3', '4', '5'}}
        assert ts['5'].get_types() == {_ms('5')}

        assert ts['1'].has_types({_ms('3'), _ms('8')})
        assert not ts['2'].has_types({_ms('6')})

        assert {t.slug for t in ts['5'].topics_by_link_type_recursively(linkType='is-a', only_leaves=True)} == {_ms('1'), _ms('6')}
        assert ts['1'].topics_by_link_type_recursively(linkType='is-a', only_leaves=True) == [ts['1']]

    def test_link_set(self, topic_graph):
        ts = topic_graph['topics']
        ls = ts['1'].link_set(_class='intraTopic')
        assert list(ls)[0].topic == _ms('2')
        assert ls.count() == 1

        ls = ts['4'].link_set(_class='intraTopic')
        assert {l.topic for l in ls} == {_ms('2'), _ms('5')}

        trefs = {r.normal() for r in Ref('Genesis 1:1-10').range_list()}

        ls = ts['1'].link_set(_class='refTopic')
        assert {l.ref for l in ls} == trefs

        ls = ts['1'].link_set(_class=None)
        assert {getattr(l, 'ref', getattr(l, 'topic', None)) for l in ls} == (trefs | {_ms('2')})

    def test_merge(self, topic_graph_to_merge):
        ts = topic_graph_to_merge['topics']
        ts['20'].merge(ts['40'])

        t20 = Topic.init(_ms('20'))
        assert t20.slug == _ms('20')
        assert len(t20.titles) == 2
        assert t20.get_primary_title('en') == _ms('20')
        ls = t20.link_set(_class='intraTopic')
        assert {l.topic for l in ls} == {_ms(x) for x in {'10', '30', '50'}}

        s = mongo_db.sheets.find_one({"id": 1234567890})
        assert s['topics'] == [
            {"slug": _ms('20'), 'asTyped': 'twenty'},
            {"slug": _ms('20'), 'asTyped': '4d'},
            {"slug": _ms('20'), 'asTyped': 'twent-e'},
            {"slug": _ms('30'), 'asTyped': 'thirty'}
        ]

        t40 = Topic.init(_ms('40'))
        assert t40 is None
        DjangoTopic.objects.get(slug=_ms('20'))
        with pytest.raises(DjangoTopic.DoesNotExist):
            DjangoTopic.objects.get(slug=_ms('40'))

    def test_change_title(self, topic_graph):
        ts = topic_graph['topics']
        dt1 = DjangoTopic.objects.get(slug=ts['1'].slug)
        assert dt1.en_title == ts['1'].get_primary_title('en')
        ts['1'].title_group.add_title('new title', 'en', True, True)
        ts['1'].save()
        dt1 = DjangoTopic.objects.get(slug=ts['1'].slug)
        assert dt1.en_title == ts['1'].get_primary_title('en')

    @pytest.mark.django_db
    def test_pools(self, topic_graph, topic_pool):
        ts = topic_graph['topics']
        t1 = ts['1']
        t1.add_pool(topic_pool.name)
        assert topic_pool.name in t1.get_pools()

        # dont add duplicates
        t1.add_pool(topic_pool.name)
        assert t1.get_pools().count(topic_pool.name) == 1

        assert t1.has_pool(topic_pool.name)
        t1.remove_pool(topic_pool.name)
        assert topic_pool.name not in t1.get_pools()
        # dont error when removing non-existent pool
        t1.remove_pool(topic_pool.name)

    def test_sanitize(self):
        t = Topic()
        t.slug = "sdfsdg<script/>"
        t.description={"en":"<b>Foo</b> <script>balrg</script>", "he": "snurg <script> gdgf </script>"}
        t._sanitize()
        assert "<b>" not in t.description["en"]
        assert "<script>" not in t.description["en"]
        assert "<script>" not in t.description["he"]
        assert "<script>" not in t.slug


class TestTopicLinkHelper(object):

    def test_init_by_class(self, topic_graph):
        l1 = mongo_db.topic_links.find_one({'fromTopic': _ms('1'), 'toTopic': _ms('2'), 'linkType': 'is-a'})
        l2 = mongo_db.topic_links.find_one({'toTopic': _ms('1'), 'ref': 'Genesis 1:1'})

        obj = TopicLinkHelper.init_by_class(l1)
        assert isinstance(obj, IntraTopicLink)

        obj = TopicLinkHelper.init_by_class(l2)
        assert isinstance(obj, RefTopicLink)

        obj = TopicLinkHelper.init_by_class(l1, context_slug=_ms('2'))
        assert obj.topic == _ms('1')
        assert obj.is_inverse

        obj = TopicLinkHelper.init_by_class(l1, context_slug=_ms('1'))
        assert obj.topic == _ms('2')


class TestIntraTopicLink(object):

    def test_validate(self, topic_graph):
        from sefaria.system.exceptions import DuplicateRecordError, InputError

        attrs = {
            'fromTopic': _ms('1'),
            'toTopic': _ms('6'),
            'linkType': 'is-a',
            'dataSource': 'sefaria'
        }
        l = IntraTopicLink(attrs)
        l.save()
        assert getattr(l, 'class') == 'intraTopic'
        l.delete()

        attrs = {
            'fromTopic': _ms('1'),
            'toTopic': _ms('2'),
            'linkType': 'is-a',
            'dataSource': 'sefaria'
        }
        l = IntraTopicLink(attrs)
        with pytest.raises(DuplicateRecordError):
            l.save()

        # non-existent datasource
        attrs = {
            'fromTopic': _ms('1'),
            'toTopic': _ms('2'),
            'linkType': 'is-a',
            'dataSource': 'blahblah'
        }
        l = IntraTopicLink(attrs)
        with pytest.raises(SluggedMongoRecordMissingError):
            l.save()

        # non-existent toTopic
        attrs = {
            'fromTopic': _ms('1'),
            'toTopic': _ms('2222'),
            'linkType': 'is-a',
            'dataSource': 'sefaria'
        }
        l = IntraTopicLink(attrs)
        with pytest.raises(SluggedMongoRecordMissingError):
            l.save()

        # non-existent fromTopic
        attrs = {
            'fromTopic': _ms('11111'),
            'toTopic': _ms('2'),
            'linkType': 'is-a',
            'dataSource': 'sefaria'
        }
        l = IntraTopicLink(attrs)
        with pytest.raises(SluggedMongoRecordMissingError):
            l.save()

        # non-existent linkType
        attrs = {
            'fromTopic': _ms('11111'),
            'toTopic': _ms('2'),
            'linkType': 'is-aaaaaa',
            'dataSource': 'sefaria'
        }
        l = IntraTopicLink(attrs)
        with pytest.raises(SluggedMongoRecordMissingError):
            l.save()

        # duplicate for symmetric linkType
        attrs = {
            'fromTopic': _ms('1'),
            'toTopic': _ms('2'),
            'linkType': 'related-to',
            'dataSource': 'sefaria'
        }
        l1 = IntraTopicLink(attrs)
        l1.save()
        attrs['fromTopic'] = _ms('2')
        attrs['toTopic'] = _ms('1')
        l2 = IntraTopicLink(attrs)
        with pytest.raises(DuplicateRecordError):
            l2.save()
        l1.delete()


class TestRefTopicLink(object):

    def test_add_expanded_refs(self, topic_graph):
        attrs = {
            'ref': 'Genesis 1:1',
            'toTopic': _ms('6'),
            'linkType': 'about',
            'dataSource': 'sefaria'
        }
        l = RefTopicLink(attrs)
        l.save()
        assert getattr(l, 'class') == 'refTopic'
        assert l.expandedRefs == ['Genesis 1:1']
        l.delete()

        attrs = {
            'ref': 'Genesis 1:1-3',
            'toTopic': _ms('6'),
            'linkType': 'about',
            'dataSource': 'sefaria'
        }
        l = RefTopicLink(attrs)
        l.save()
        assert l.expandedRefs == ['Genesis 1:1', 'Genesis 1:2', 'Genesis 1:3']
        l.delete()

        attrs = {
            'ref': 'Genesis 1-2',
            'toTopic': _ms('6'),
            'linkType': 'about',
            'dataSource': 'sefaria'
        }
        l = RefTopicLink(attrs)
        l.save()
        test_refs = [r.normal() for r in Ref('Genesis 1-2').all_segment_refs()]
        assert l.expandedRefs == test_refs
        l.delete()

    def test_duplicate(self, topic_graph):
        from sefaria.system.exceptions import DuplicateRecordError

        attrs = {
            'ref': 'Genesis 1:1',
            'toTopic': _ms('6'),
            'linkType': 'about',
            'dataSource': 'sefaria'
        }
        l1 = RefTopicLink(attrs)
        l1.save()
        l2 = RefTopicLink(attrs)
        with pytest.raises(DuplicateRecordError):
            l2.save()
        l1.delete()

```

### sefaria/model/tests/vstate_test.py

```
# -*- coding: utf-8 -*-

from sefaria.model import *


class Test_VState(object):

    def test_integrity(self):
        titles = ["Exodus", "Shabbat", "Rashi on Exodus", "Rashi on Genesis", "Rashi on Shabbat"]
        for title in titles:
            index = library.get_index(title)
            vs = VersionState(index)
            assert getattr(vs, "title")
            assert getattr(vs, "content")


class Test_VSNode(object):
    def test_section_counts(self):
        sn = StateNode("Exodus")
        cd = sn.get_available_counts_dict("he")
        assert "Chapter" in cd
        assert "Verse" in cd
        assert sn.get_available_counts_dict("en") == cd

        sn = StateNode("Shabbat")
        cd = sn.get_available_counts_dict("he")
        assert "Daf" in cd
        assert "Amud" in cd
        assert "Line" in cd

        sn = StateNode("Rashi on Shabbat")
        cd = sn.get_available_counts_dict("he")
        assert "Daf" in cd
        assert "Amud" in cd
        assert "Line" in cd
        assert "Comment" in cd

        sn = StateNode("Rashi on Exodus")
        cd = sn.get_available_counts_dict("he")
        assert "Chapter" in cd
        assert "Verse" in cd
        assert "Comment" in cd


```

### sefaria/model/tests/node_test.py

```
# -*- coding: utf-8 -*-
import copy

import pytest

from sefaria.model import *
from sefaria.model.schema import JaggedArrayNode, SchemaNode
from sefaria.system.exceptions import IndexSchemaError


class Test_Validate(object):
    def test_jaggedarray_fields(self):

        j = JaggedArrayNode()
        j.add_title("title1", "en", primary=True)\
         .add_title("", "he", primary=True)\
         .add_title("title2", "en")\
         .add_title("", "he")
        j.depth = 1
        j.sectionNames = ["Foo"]
        j.addressTypes = ["Integer"]
        j.key = "bob"

        j.validate()

        for f in ["depth", "sectionNames", "addressTypes", "key"]:
            t = copy.deepcopy(j)
            delattr(t, f)

            with pytest.raises(IndexSchemaError):
                t.validate()

        t = copy.deepcopy(j)
        t.sectionNames += ["foob"]
        with pytest.raises(IndexSchemaError):
            t.validate()

        t = copy.deepcopy(j)
        t.addressTypes += ["Integer"]
        with pytest.raises(IndexSchemaError):
            t.validate()


    def test_validate_children(self):
        """
        Does validate fall through to children?
        """
        s = SchemaNode()
        s.key = "root"
        s.add_title("root", "en", primary=True)
        j = JaggedArrayNode()
        j.add_title("child", "en", primary=True)
        j.key = "child"
        j.depth = 1
        j.sectionNames = ["Foo"]
        j.append_to(s)

        with pytest.raises(IndexSchemaError):
            s.validate()


class Test_Titles(object):

    def test_add(self):
        j = JaggedArrayNode()
        j.add_title("title1", "en", primary=True)
        j.add_title("", "he", primary=True)
        j.add_title("title2", "en")
        j.add_title("", "he")
        assert len(j.all_node_titles("he")) == 2
        assert len(j.all_node_titles("en")) == 2

        assert j.primary_title("en") == "title1"
        j.add_title("title3", "en", primary=True, replace_primary=True)
        assert len(j.all_node_titles("en")) == 3
        assert len(j.all_node_titles("he")) == 2
        assert j.primary_title("en") == "title3"

    def test_remove(self):
        j = JaggedArrayNode()
        j.add_title("title1", "en", primary=True)\
         .add_title("", "he", primary=True)\
         .add_title("title2", "en")\
         .add_title("", "he")
        j.remove_title("title1", "en")
        j.depth = 1
        j.sectionNames = ["Foo"]
        j.addressTypes = ["Integer"]
        j.key = "bob"

        with pytest.raises(IndexSchemaError):
            j.validate()

    #todo: why failing?
    @pytest.mark.xfail(reason="unknown")
    def test_terms_and_he(self):
        s = SchemaNode()
        s.key = "root"
        s.add_title("root", "en", primary=True)
        s.add_title("", "he", primary=True)

        j = JaggedArrayNode()
        j.key = "bereshit"
        j.depth = 1
        j.sectionNames = ["Foo"]
        j.addressTypes = ["Integer"]
        j.add_shared_term("Bereshit")
        j.append_to(s)

        j2 = JaggedArrayNode()
        j2.key = "noah"
        j2.depth = 1
        j2.sectionNames = ["Foo"]
        j2.addressTypes = ["Integer"]
        j2.add_shared_term("Noach")
        j2.append_to(s)

        s.validate()

        td = s.title_dict("he")
        assert len(td) == 5

        target = {
            '': s,
            ', ': j,
            ', ': j2,
            ' ': j,
            ' ': j2,
        }

        assert td == target

    def test_bad_term(self):
        with pytest.raises(IndexError):
            j = JaggedArrayNode()
            j.add_shared_term("BadTermName")

    #todo: why failing?
    @pytest.mark.xfail(reason="unknown")
    def test_presentation_and_default(self):
        s = SchemaNode()
        s.key = "root"
        s.add_title("root", "en", primary=True)

        j2 = JaggedArrayNode()
        j2.key = "default"
        j2.default = True
        j2.depth = 1
        j2.sectionNames = ["Foo"]
        j2.addressTypes = ["Integer"]
        s.append(j2)

        assert not s.has_titled_continuation()

        j = JaggedArrayNode()
        j.key = "child1"
        j.depth = 1
        j.sectionNames = ["Foo"]
        j.addressTypes = ["Integer"]
        j.add_title("Child 1", "en", primary=True)
        j.add_title("Sweet Child", "en", presentation="alone")
        j.add_title("Sweet Child of Mine", "en", presentation="both")
        s.append(j)

        s.validate()

        assert s.has_titled_continuation()
        assert s.has_numeric_continuation()
        assert not j.has_titled_continuation()
        assert not j2.has_titled_continuation()
        assert j2.has_numeric_continuation()
        assert j.has_numeric_continuation()

        td = s.title_dict()
        assert len(td) == 7

        target = {
            'root': j2,
            'root, Child 1': j,
            'root, Sweet Child of Mine': j,
            'root Child 1': j,
            'root Sweet Child of Mine': j,
            'Sweet Child of Mine': j,
            'Sweet Child': j,
        }

        assert td == target

    #todo: why failing?
    @pytest.mark.xfail(reason="unknown")
    def test_grandchild_presentation(self):
        s = SchemaNode()
        s.key = "root"
        s.add_title("root", "en", primary=True)
        s.add_title("alt root", "en")

        s2 = SchemaNode()
        s2.key = "l2"
        s2.add_title("Level 2", "en", primary=True)
        s2.add_title("Level 2 Alone", "en", presentation="alone")
        s2.add_title("Level 2 Both", "en", presentation="both")
        s2.append_to(s)

        j = JaggedArrayNode()
        j.key = "child1"
        j.depth = 1
        j.sectionNames = ["Foo"]
        j.addressTypes = ["Integer"]
        j.add_title("Level 3a", "en", primary=True)
        j.add_title("Level 3a alone", "en", presentation="alone")
        j.add_title("Level 3a both", "en", presentation="both")
        j.append_to(s2)

        j2 = JaggedArrayNode()
        j2.key = "child2"
        j2.depth = 1
        j2.sectionNames = ["Foo"]
        j2.addressTypes = ["Integer"]
        j2.add_title("Level 3b", "en", primary=True)
        j2.add_title("Level 3b alone", "en", presentation="alone")
        j2.add_title("Level 3b both", "en", presentation="both")
        j2.append_to(s2)

        s.validate()

        assert not s.has_numeric_continuation()
        assert not s2.has_numeric_continuation()

        td = s.title_dict()
        assert len(td) == 96

        target = {
            "root": s,
            "alt root": s,
            "Level 2 Alone": s2,
            "Level 3b alone": j2,
            "Level 3a alone": j,
            "Level 2 Both": s2,
            "Level 3a both": j,
            "Level 3b both": j2,

            # combined, with comma separator
            "root, Level 2 Both": s2,
            "root, Level 2": s2,
            "alt root, Level 2 Both": s2,
            "alt root, Level 2": s2,

            "root, Level 2 Both, Level 3a": j,
            "root, Level 2, Level 3a": j,
            "alt root, Level 2 Both, Level 3a": j,
            "alt root, Level 2, Level 3a": j,
            "Level 2 Alone, Level 3a": j,
            "Level 2 Both, Level 3a": j,

            "root, Level 2 Both, Level 3a both": j,
            "root, Level 2, Level 3a both": j,
            "alt root, Level 2 Both, Level 3a both": j,
            "alt root, Level 2, Level 3a both": j,
            "Level 2 Alone, Level 3a both": j,
            "Level 2 Both, Level 3a both": j,

            "root, Level 2 Both, Level 3b": j2,
            "root, Level 2, Level 3b": j2,
            "alt root, Level 2 Both, Level 3b": j2,
            "alt root, Level 2, Level 3b": j2,
            "Level 2 Alone, Level 3b": j2,
            "Level 2 Both, Level 3b": j2,

            "root, Level 2 Both, Level 3b both": j2,
            "root, Level 2, Level 3b both": j2,
            "alt root, Level 2 Both, Level 3b both": j2,
            "alt root, Level 2, Level 3b both": j2,
            "Level 2 Alone, Level 3b both": j2,
            "Level 2 Both, Level 3b both": j2,

            # combined, with space separator
            "root Level 2 Both": s2,
            "root Level 2": s2,
            "alt root Level 2 Both": s2,
            "alt root Level 2": s2,

            "root Level 2 Both Level 3a": j,
            "root Level 2 Level 3a": j,
            "alt root Level 2 Both Level 3a": j,
            "alt root Level 2 Level 3a": j,
            "Level 2 Alone Level 3a": j,
            "Level 2 Both Level 3a": j,

            "root Level 2 Both Level 3a both": j,
            "root Level 2 Level 3a both": j,
            "alt root Level 2 Both Level 3a both": j,
            "alt root Level 2 Level 3a both": j,
            "Level 2 Alone Level 3a both": j,
            "Level 2 Both Level 3a both": j,

            "root Level 2 Both Level 3b": j2,
            "root Level 2 Level 3b": j2,
            "alt root Level 2 Both Level 3b": j2,
            "alt root Level 2 Level 3b": j2,
            "Level 2 Alone Level 3b": j2,
            "Level 2 Both Level 3b": j2,

            "root Level 2 Both Level 3b both": j2,
            "root Level 2 Level 3b both": j2,
            "alt root Level 2 Both Level 3b both": j2,
            "alt root Level 2 Level 3b both": j2,
            "Level 2 Alone Level 3b both": j2,
            "Level 2 Both Level 3b both": j2,

            # combined, space, comma
            "root Level 2 Both, Level 3a": j,
            "root Level 2, Level 3a": j,
            "alt root Level 2 Both, Level 3a": j,
            "alt root Level 2, Level 3a": j,
            "root Level 2 Both, Level 3a both": j,
            "root Level 2, Level 3a both": j,
            "alt root Level 2 Both, Level 3a both": j,
            "alt root Level 2, Level 3a both": j,
            "root Level 2 Both, Level 3b": j2,
            "root Level 2, Level 3b": j2,
            "alt root Level 2 Both, Level 3b": j2,
            "alt root Level 2, Level 3b": j2,
            "root Level 2 Both, Level 3b both": j2,
            "root Level 2, Level 3b both": j2,
            "alt root Level 2 Both, Level 3b both": j2,
            "alt root Level 2, Level 3b both": j2,

            # combined, comma, space
            "root, Level 2 Both Level 3a": j,
            "root, Level 2 Level 3a": j,
            "alt root, Level 2 Both Level 3a": j,
            "alt root, Level 2 Level 3a": j,
            "root, Level 2 Both Level 3a both": j,
            "root, Level 2 Level 3a both": j,
            "alt root, Level 2 Both Level 3a both": j,
            "alt root, Level 2 Level 3a both": j,
            "root, Level 2 Both Level 3b": j2,
            "root, Level 2 Level 3b": j2,
            "alt root, Level 2 Both Level 3b": j2,
            "alt root, Level 2 Level 3b": j2,
            "root, Level 2 Both Level 3b both": j2,
            "root, Level 2 Level 3b both": j2,
            "alt root, Level 2 Both Level 3b both": j2,
            "alt root, Level 2 Level 3b both": j2,

        }

        assert td == target

    def test_default_chain(self):
        s = SchemaNode()
        s.key = "root"
        s.add_title("root", "en", primary=True)
        s.add_title("", "he", primary=True)
        s.add_title("alt root", "en")

        s2 = SchemaNode()
        s2.key = "default"
        s2.default = True
        s2.append_to(s)

        j = JaggedArrayNode()
        j.key = "default"
        j.depth = 1
        j.default = True
        j.sectionNames = ["Foo"]
        j.addressTypes = ["Integer"]
        j.append_to(s2)

        s.validate()

        assert s.has_numeric_continuation()
        assert s2.has_numeric_continuation()
        assert j.has_numeric_continuation()
        assert not s.has_titled_continuation()
        assert not s2.has_titled_continuation()
        assert not j.has_titled_continuation()


    def test_duplicate_primary(self):
        with pytest.raises(IndexSchemaError):
            j = JaggedArrayNode()
            j.add_title("title1", "en", primary=True)
            j.add_title("title2", "en", primary=True)

        with pytest.raises(IndexSchemaError):
            j = JaggedArrayNode()
            j.add_title("", "he", primary=True)
            j.add_title("", "he", primary=True)
```

### sefaria/model/tests/collection_test.py

```
# -*- coding: utf-8 -*-
import pytest
import sefaria.model as m
from sefaria.system.exceptions import InputError


class Test_Collection(object):

    def test_save(self):
        g = m.Collection({
            "name": "Test Collection!",
            "headerUrl": "",
            "imageUrl": "",
            "coverUrl": "",
            "description": "innocent description",
            "admins": [ 1 ],
            "members": []
        })
        
        # Slug is assigned
        g.save()
        assert hasattr(g, "slug")

        # Clean Description HTML
        g.description = 'Seemingly ok description... <a href="javascript:alert(8007)">Click me</a>'
        g.save()
        assert g.description == 'Seemingly ok description... <a>Click me</a>'

        # Can't save with empty title
        g.name = ""
        with pytest.raises(InputError):
            g.save()
        g.name = "Text Collection"

        # Can't be public without image, public sheets
        g.listed = True
        with pytest.raises(InputError):
            g.save()

        g.delete()

```

### sefaria/model/version_state.py

```
"""
version_state.py
Writes to MongoDB Collection:
"""
import structlog
from functools import reduce


logger = structlog.get_logger(__name__)

from . import abstract as abst
from . import text
from . import link
from .text import VersionSet, AbstractIndex, AbstractSchemaContent, IndexSet, library, Ref
from sefaria.datatype.jagged_array import JaggedTextArray, JaggedIntArray
from sefaria.system.exceptions import InputError, BookNameError
from sefaria.system.cache import delete_template_cache
try:
    from sefaria.settings import USE_VARNISH
except ImportError:
    USE_VARNISH = False
'''
'''


class VersionState(abst.AbstractMongoRecord, AbstractSchemaContent):
    """
    This model overrides default init/load/save behavior, since there is one and only one VersionState record for each Index record.

    The `content` attribute is a dictionary which is the root of a tree, mirroring the shape of a Version, where the leaf nodes of the tree are dictionaries with a shape like the following:
        {
            "_en": {
                "availableTexts":  Mask of what texts are available in this language.  Boolean values (0 or 1) in the shape of the JaggedArray
                "availableCounts":  Array, with length == depth of the node.  Each element is the number of available elements at that depth.  e.g [chapters, verses]
                "percentAvailable":  Percent of this text available in this language TODO: Only used on the dashboard. Remove?
                'percentAvailableInvalid':  Boolean. Whether the value of "percentAvailable" can be trusted.  TODO: Only used on the dashboard. Remove?
                "textComplete":  Boolean. Whether the text is complete in this language. TODO: Not used outside of this file. Should be removed.
                'completenessPercent':  Percent of this text complete in this language TODO: Not used outside of this file. Should be removed.
                'sparseness': Legacy - present on some records, but no longer in code TODO: remove
            }
            "_he": {...} # same keys as _en
            "_all" {
                "availableTexts": Mask what texts are available in this text overall.  Boolean values (0 or 1) in the shape of the JaggedArray
                "shape":
                    For depth 1: Integer -length
                    For depth 2: List of section lengths
                    For depth 3: List of list of section lengths
            }
        }

    For example:
    - the `content` attribute for a simple text like `Genesis` will be a dictionary with keys "_en", "_he", and "_all", as above.
    - the `content` attribute for `Pesach Haggadah` will be a dictionary with keys: "Kadesh", "Urchatz", "Karpas" ... each with a value of a dictionary like the above.
        The key "Magid" has a value of a dictionary, where each key is a different sub-section of Magid.
        The value for each key is a dictionary as detailed above, specific to each sub-section.
        So for example, one key will be "Ha Lachma Anya" and the value will be a dictionary, like the above, specific to the details of "Ha Lachma Anya".

    Every JaggedArrayNode has a corresponding vstate dictionary. So for complex texts, each leaf node (and leaf nodes by definition must be JaggedArrayNodes) has this corresponding dictionary.
    """
    collection = 'vstate'

    required_attrs = [
        "title",  # Index title
        "content"  # tree of data about nodes.  See above.
    ]
    optional_attrs = [
        "flags",  # "heComplete" : Bool, "enComplete" : Bool
        "linksCount",  # Integer
        "first_section_ref"  # Normal text Ref
    ]

    langs = ["en", "he"]
    lang_map = {lang: "_" + lang for lang in langs}
    lang_keys = list(lang_map.values())

    def __init__(self, index=None, attrs=None, proj=None):
        """
        :param index: Index record or name of Index
        :type index: text.Index|string
        :return:
        """
        super(VersionState, self).__init__(attrs)

        if not index:  # so that basic model tests can run
            if getattr(self, "title", None):
                try:
                    self.index = library.get_index(self.title)
                except BookNameError as e:
                    logger.warning("Failed to load Index for VersionState - {}: {} (Normal on Index name change)".format(self.title, e))
            return

        if not isinstance(index, AbstractIndex):
            try:
                index = library.get_index(index)
            except BookNameError as e:
                logger.warning("Failed to load Index for VersionState {}: {}".format(index, e))
                raise

        self.index = index
        self._versions = {}
        self.is_new_state = False

        if not self.load({"title": index.title}, proj=proj):
            if not getattr(self, "flags", None):  # allow flags to be set in initial attrs
                self.flags = {}
            self.content = self.index.nodes.create_content(lambda n: {})
            self.title = index.title
            self.refresh()
            self.is_new_state = True  # variable naming: don't override 'is_new' - a method of the superclass

    def contents(self, **kwargs):
        c = super(VersionState, self).contents()
        c.update(self.index.contents())
        return c

    def _load_versions(self):
        for lang in self.langs:
            self._versions[lang] = [v for v in VersionSet({"title": self.index.title, "language": lang})]

    def versions(self, lang):
        if not self._versions.get(lang):
            self._load_versions()
        return self._versions.get(lang)

    def _first_section_ref(self):
        if not getattr(self, "index", False):
            return None

        current_leaf = self.index.nodes.first_leaf()
        new_section = None

        while current_leaf:
            if not current_leaf.is_virtual:    # todo: handle first entries of virtual nodes
                r = current_leaf.ref()
                c = self.state_node(current_leaf).ja("all")
                new_section = c.next_index([])
                if new_section:
                    break
            current_leaf = current_leaf.next_leaf()

        if not new_section:
            return None

        depth_up = 0 if current_leaf.depth == 1 else 1

        d = r._core_dict()
        d["toSections"] = d["sections"] = [(s + 1) for s in new_section[:-depth_up]]
        return Ref(_obj=d)

    def refresh(self):
        if self.is_new_state:  # refresh done on init
            return
        self.content = self.index.nodes.visit_content(self._content_node_visitor, self.content)
        self.index.nodes.visit_structure(self._aggregate_structure_state, self)
        self.linksCount = link.LinkSet(Ref(self.index.title)).count()
        fsr = self._first_section_ref()
        self.first_section_ref = fsr.normal() if fsr else None
        self.save()

        if USE_VARNISH:
            from sefaria.system.varnish.wrapper import invalidate_counts
            invalidate_counts(self.index)

    def get_flag(self, flag):
        return self.flags.get(flag, False) # consider all flags False until set True
        
    def set_flag(self, flag, value):
        self.flags[flag] = value  # could use mongo level $set to avoid doc load, for speedup
        delete_template_cache("texts_dashboard")
        return self

    def state_node(self, snode):
        sn = StateNode(_obj=self.content_node(snode))
        sn.snode = snode
        sn.versionState = self
        return sn

    def _aggregate_structure_state(self, snode, contents, **kwargs):
        """
        :param snode: SchemaStructureNode
        :param contents: A subtree of a VersionState tree, to be modified in place
        :param kwargs:
        :return:
        """
        #This does not account for relative importance/size of children
        #todo: revisit this algorithm when there are texts in the system.

        ckeys = [child.key for child in snode.concrete_children()]
        for lkey in self.lang_keys:
            contents[lkey] = {
                "percentAvailable": sum([contents[ckey][lkey]["percentAvailable"] for ckey in ckeys]) / len(ckeys),
                "textComplete": all([contents[ckey][lkey]["textComplete"] for ckey in ckeys]),
                'completenessPercent': sum([contents[ckey][lkey]["completenessPercent"] for ckey in ckeys]) / len(ckeys),
                'percentAvailableInvalid': any([contents[ckey][lkey]["percentAvailableInvalid"] for ckey in ckeys]),
            }

    #todo: do we want to use an object here?
    def _content_node_visitor(self, snode, *contents, **kwargs):
        """
        :param snode: SchemaContentNode
        :param contents: Array of one node - the self.counts node
        :param kwargs:
        :return:
        """
        assert len(contents) == 1
        current = contents[0]  # some information is manually set - don't wipe and re-create it.   todo: just copy flags?
        depth = snode.depth  # This also acts as an assertion that we have a SchemaContentNode
        ja = {}  # JaggedIntArrays for each language and 'all'
        padded_ja = {}  # Padded JaggedIntArrays for each language

        # Get base counts for each language
        for lang, lkey in list(self.lang_map.items()):
            if not current.get(lkey):
                current[lkey] = {}

            ja[lkey] = self._node_count(snode, lang)

        # Sum all of the languages
        ja['_all'] = reduce(lambda x, y: x + y, [ja[lkey] for lkey in self.lang_keys])
        zero_mask = ja['_all'].zero_mask()
        current["_all"] = {
            "availableTexts": ja['_all'].array(),
            "shape": ja['_all'].shape()
        }
        # Get derived data for all languages
        for lang, lkey in list(self.lang_map.items()):
            # build zero-padded count ("availableTexts")
            padded_ja[lkey] = ja[lkey] + zero_mask
            current[lkey]["availableTexts"] = padded_ja[lkey].array()

            # number of units at each level ("availableCounts") from raw counts
            # depth_sum() reduces anything greater than 1 to 1,
            # so that the count returned is an accurate measure of how much material is there
            current[lkey]["availableCounts"] = [ja[lkey].depth_sum(d) for d in range(depth)]

            # Percent of text available, versus its metadata count ("percentAvailable")
            # and if it's a valid measure ('percentAvailableInvalid')
            if getattr(snode, "lengths", None):
                if len(snode.lengths) == depth:
                    langtotal = reduce(lambda x, y: x + y, current[lkey]["availableCounts"])
                    schematotal = reduce(lambda x, y: x + y, snode.lengths)
                    try:
                        current[lkey]["percentAvailable"] = langtotal / float(schematotal) * 100
                    except ZeroDivisionError:
                        current[lkey]["percentAvailable"] = 0
                elif len(snode.lengths) < depth:
                    current[lkey]["percentAvailable"] = current[lkey]["availableCounts"][0] / float(snode.lengths[0]) * 100
                else:
                    raise Exception("Text has less sections than node.lengths for {}".format(snode.full_title()))
                current[lkey]['percentAvailableInvalid'] = current[lkey]["percentAvailable"] > 100
            else:
                current[lkey]["percentAvailable"] = 0
                current[lkey]['percentAvailableInvalid'] = True

            # Is this text complete? ("textComplete")
            current[lkey]["textComplete"] = current[lkey]["percentAvailable"] > 99.9

            # What percent complete? ('completenessPercent')
            # are we doing this with the zero-padded array on purpose?
            current[lkey]['completenessPercent'] = self._calc_text_structure_completeness(depth, current[lkey]["availableTexts"])

        return current

    def _node_count(self, snode, lang="en"):
        """
        Count available versions of a text in the db, segment by segment.
        :return counts:
        :type return: JaggedIntArray
        """
        counts = JaggedIntArray()

        versions = self.versions(lang)
        for version in versions:
            raw_text_ja = version.content_node(snode)
            ja = JaggedTextArray(raw_text_ja)
            mask = ja.mask()
            counts = counts + mask

        return counts


    @classmethod
    def _calc_text_structure_completeness(cls, text_depth, structure):
        """
        This function calculates the percentage of how full an array is compared to it's structre
        i.e how many elements are not null or zero
        :param text_depth: the depth of the array
        :param structure: a counts structure from count_texts()
        :return: a precentage of the array fullness
        """
        result = {'full': 0, 'total':0}
        cls._rec_calc_text_structure_completeness(text_depth, structure, result)
        return float(result['full']) / result['total'] * 100

    @classmethod
    def _rec_calc_text_structure_completeness(cls, depth, text, result):
        """
        Recursive sub-utility function of the above function. Carries out the actual calculation recursively.
        :param depth: the depth of the current structure
        :param text: the structure to count
        :param result: the result obj to update
        :return: the result obj
        """
        if isinstance(text, list):
            #empty array
            if not text:
                #an empty array element may represent a lot of missing text
                #TODO: maybe find a better estimate (average of text lengths at a certain depth?)
                result['total'] += 3**depth
            else:
                for t in text:
                    cls._rec_calc_text_structure_completeness(depth - 1, t, result)
        else:
            result['total'] += 1
            if text is not None and text != "" and text > 0:
                result['full'] += 1


class VersionStateSet(abst.AbstractMongoSet):
    recordClass = VersionState


class StateNode(object):
    lang_map = {lang: "_" + lang for lang in ["he", "en", "all"]}
    lang_keys = list(lang_map.values())
    meta_proj = {'content._all.completenessPercent': 1,
         'content._all.percentAvailable': 1,
         'content._all.percentAvailableInvalid': 1,
         'content._all.textComplete': 1,
         'content._en.completenessPercent': 1,
         'content._en.percentAvailable': 1,
         'content._en.percentAvailableInvalid': 1,
         'content._en.textComplete': 1,
         'content._he.completenessPercent': 1,
         'content._he.percentAvailable': 1,
         'content._he.percentAvailableInvalid': 1,
         'content._he.textComplete': 1,
         'flags': 1,
         'linksCount': 1,
         'title': 1,
         'first_section_ref': 1}
    #todo: self.snode could be a SchemaNode, but get_available_counts_dict() assumes JaggedArrayNode
    def __init__(self, title=None, snode=None, _obj=None, meta=False, hint=None):
        """
        :param title:
        :param snode:
        :param _obj:
        :param meta: If true, returns only the overview information, and not the detailed counts
        :param hint: hint - a list of (lang, key) tuples of pieces of VersionState to return
        :return:
        """
        if title:
            snode = library.get_schema_node(title)
            if not snode:
                snode = library.get_schema_node(title)
            if not snode:
                raise InputError("Can not resolve name: {}".format(title))
            if snode.is_default():
                snode = snode.parent
        if snode:
            proj = None
            if meta:
                if snode.parent:
                    raise Exception("StateNode.meta() only supported for Index roots.  Called with {} / {}".format(title, snode.primary_title("en")))
                proj = self.meta_proj
            if hint:
                hint_proj = {}
                base = [VersionState.content_attr] + snode.version_address()
                for l, k in hint:
                    hint_proj[".".join(base + [self.lang_map[l]] + [k])] = 1
                if proj:
                    proj.update(hint_proj)
                else:
                    proj = hint_proj
            self.snode = snode
            self.versionState = VersionState(snode.index.title, proj=proj)
            self.d = self.versionState.content_node(snode)
        elif _obj:
            self.d = _obj

    def get_percent_available(self, lang):
        return self.var(lang, "percentAvailable")

    def get_available_counts(self, lang):
        return self.var(lang, "availableCounts")

    def get_flag(self, flag):
        return self.versionState.get_flag(flag)

    def get_available_counts_dict(self, lang):
        """
        return a dictionary
        which zips together section names and available counts.
        """
        d = {}
        for i in range(self.snode.depth):
            d.update(
                self.snode.address_class(i).format_count(
                    self.snode.sectionNames[i],
                    self.get_available_counts(lang)[i]
                )
            )
        return d

    def var(self, lang, key):
        try:
            return self.d[self.lang_map[lang]][key]
        except Exception as e:
            raise e.__class__("Failed in StateNode.var(), in node: {}, language: {}, key: {}".format(self.snode.primary_title("en"), lang, key))

    def ja(self, lang, key="availableTexts"):
        """
        :param lang: "he", "en", or "all"
        :param addr:
        :return:
        """
        return JaggedIntArray(self.var(lang, key))

    def contents(self):
        #mix in Index?
        return self.d


    def get_untranslated_count_by_unit(self, unit):
        """
        Returns the (approximate) number of untranslated units of text

        Counts are approximate because they do not adjust for an English section
        that may have no corresponding Hebrew.
        """
        he = self.get_available_counts_dict("he")
        en = self.get_available_counts_dict("en")

        return he[unit] - en[unit]


    def get_translated_count_by_unit(self, unit):
        """
        Return the (approximate) number of translated units in text,

        Counts are approximate because they do not adjust for an English section
        that may have no corresponding Hebrew.
        """
        en = self.get_available_counts_dict("en")

        return en[unit]


def refresh_all_states():
    indices = IndexSet()

    for index in indices:
        logger.debug("Rebuilding state for {}".format(index.title))
        try:
            VersionState(index).refresh()
        except Exception as e:
            logger.warning("Got exception rebuilding state for {}: {}".format(index.title, e))

    library.rebuild_toc()


def process_index_delete_in_version_state(indx, **kwargs):
    from sefaria.system.database import db
    db.vstate.delete_one({"title": indx.title})

def process_index_title_change_in_version_state(indx, **kwargs):
    VersionStateSet({"title": kwargs["old"]}).update({"title": kwargs["new"]})


def create_version_state_on_index_creation(indx, **kwargs):
    vs = VersionState(indx.title)
    if vs.is_new_state:
        vs.save()

```

### sefaria/model/__init__.py

```
"""
This works as:
    from sefaria.model import *
symbols are then accessed directly as, e.g.:
    get_index("Genesis")
      or
    Version()
      or
    library
"""

from . import abstract

# not sure why we have to do this now - it wasn't previously required
from . import history, schema, text, link, note, layer, notification, queue, lock, following, blocking, user_profile, \
    version_state, lexicon, place, timeperiod, garden, collection, topic, manuscript, guide

from .history import History, HistorySet, log_add, log_delete, log_update, log_text
from .schema import deserialize_tree, Term, TermSet, TermScheme, TermSchemeSet, TitledTreeNode, SchemaNode, \
    ArrayMapNode, JaggedArrayNode, NumberedTitledTreeNode, NonUniqueTerm, NonUniqueTermSet
from .text import library, Index, IndexSet, Version, VersionSet, TextChunk, TextRange, TextFamily, Ref, merge_texts
from .link import Link, LinkSet, get_link_counts, get_book_link_collection, get_book_category_linkset
from .note import Note, NoteSet
from .layer import Layer, LayerSet
from .notification import Notification, NotificationSet, GlobalNotification, GlobalNotificationSet
from .trend import get_session_traits
from .queue import IndexQueue, IndexQueueSet
from .lock import Lock, LockSet, set_lock, release_lock, check_lock, expire_locks
from .following import FollowRelationship, FollowersSet, FolloweesSet
from .blocking import BlockRelationship, BlockersSet, BlockeesSet
from .user_profile import UserWrapper, UserProfile, UserHistory, UserHistorySet, annotate_user_list
from .collection import Collection, CollectionSet
from .version_state import VersionState, VersionStateSet, StateNode, refresh_all_states
from .timeperiod import TimePeriod, TimePeriodSet
from .lexicon import Lexicon, LexiconEntry, LexiconEntrySet, Dictionary, DictionaryEntry, StrongsDictionaryEntry, RashiDictionaryEntry, JastrowDictionaryEntry, KleinDictionaryEntry, WordForm, WordFormSet, LexiconLookupAggregator
from .place import Place, PlaceSet
from .garden import Garden, GardenStop, GardenStopRelation, GardenSet, GardenStopSet, GardenStopRelationSet
from .category import Category, CategorySet
from .passage import Passage, PassageSet
from .ref_data import RefData, RefDataSet
from .webpage import WebPage, WebPageSet
from .media import Media, MediaSet
from .guide import Guide, GuideSet
from .topic import Topic, PersonTopic, AuthorTopic, TopicLinkType, IntraTopicLink, RefTopicLink, TopicLinkType, TopicDataSource, TopicSet, PersonTopicSet, AuthorTopicSet, TopicLinkTypeSet, RefTopicLinkSet, IntraTopicLinkSet, TopicLinkSetHelper
from .portal import Portal
from .manuscript import Manuscript, ManuscriptSet, ManuscriptPage, ManuscriptPageSet
from .linker.ref_part import RawRef
from .linker.linker import Linker
from . import dependencies

library._build_index_maps()

```

### sefaria/model/text_request_adapter.py

```
import copy
from collections import defaultdict
from functools import reduce
from typing import List
import django
django.setup()
from sefaria.model import *
from sefaria.utils.hebrew import hebrew_term
from sefaria.system.exceptions import InputError
from sefaria.datatype.jagged_array import JaggedTextArray

class TextRequestAdapter:
    """
    This class is used for getting texts for client side (API or SSR)
    It takes the same params as the api/v3/text (ref, version_params that are language and versionTitle, fill_in_missing_segments, and return_format
    It returns a JSON-like object for an HTTP response.
    """
    ALL = 'all'
    PRIMARY = 'primary'
    SOURCE = 'source'
    TRANSLATION = 'translation'

    def __init__(self, oref: Ref, versions_params: List[List[str]], fill_in_missing_segments=True, return_format='default'):
        self.versions_params = versions_params
        self.oref = oref
        self.fill_in_missing_segments = fill_in_missing_segments
        self.return_format = return_format
        self.handled_version_params = []
        self.all_versions = self.oref.versionset()

        fields = Version.optional_attrs + Version.required_attrs
        fields.remove('chapter') # not metadata
        self.return_obj = {
            'versions': [],
            'missings': [],
            'available_langs': sorted({v.languageFamilyName for v in self.all_versions}),
            'available_versions': [{f: getattr(v, f, "") for f in fields} for v in self.all_versions]
        }

    def _append_version(self, version):
        #TODO part of this function duplicate the functionality of Ref.versionlist(). maybe we should mvoe it to Version
        fields = Version.optional_attrs + Version.required_attrs
        for attr in ['chapter', 'title']:
            fields.remove(attr)
        version_details = {f: getattr(version, f, "") for f in fields}

        if self.fill_in_missing_segments:
            # we need a new VersionSet of only the relevant versions for merging. copy should be better than calling for mongo
            relevant_versions = copy.copy(self.all_versions)
            relevant_versions.remove(lambda v: v.languageFamilyName != version.languageFamilyName)
        else:
            relevant_versions = [version]
        text_range = TextRange(self.oref, version.languageFamilyName, version.versionTitle,
                               self.fill_in_missing_segments, relevant_versions)
        version_details['text'] = text_range.text

        sources = getattr(text_range, 'sources', None)
        if sources is not None:
            version_details['sources'] = sources

        if self.oref.is_book_level():
            first_section_ref = version.first_section_ref() or version.get_index().nodes.first_leaf().first_section_ref()
            version_details['firstSectionRef'] = first_section_ref.normal()
        self.return_obj['versions'].append(version_details)

    def _append_required_versions(self, lang: str, vtitle: str) -> None:
        if lang == self.PRIMARY:
            lang_condition = lambda v: getattr(v, 'isPrimary', False)
        elif lang == self.SOURCE:
            lang_condition = lambda v: getattr(v, 'isSource', False)
        elif lang == self.TRANSLATION:
            lang_condition = lambda v: not getattr(v, 'isSource', False)
        elif lang:
            lang_condition = lambda v: v.languageFamilyName.lower() == lang
        else:
            lang_condition = lambda v: True
        if vtitle and vtitle != self.ALL:
            versions = [v for v in self.all_versions if lang_condition(v) and v.versionTitle == vtitle]
        else:
            versions = [v for v in self.all_versions if lang_condition(v)]
            if vtitle != self.ALL and versions:
                versions = [max(versions, key=lambda v: getattr(v, 'priority', 0))]
        for version in versions:
            if all(version.languageFamilyName != v['languageFamilyName'] or version.versionTitle != v['versionTitle'] for v in self.return_obj['versions']):
                #do not return the same version even if included in two different version params
                self._append_version(version)
        if not versions:
            self.return_obj['missings'].append((lang, vtitle))

    def _add_ref_data_to_return_obj(self) -> None:
        oref = self.oref
        self.return_obj.update({
            'ref': oref.normal(),
            'heRef': oref.he_normal(),
            'sections': oref.normal_sections(), #that means it will be string. in the previous api talmud sections were strings while integers remained integets. this is more consistent but we should check it works
            'toSections': oref.normal_toSections(),
            'sectionRef': oref.section_ref().normal(),
            'heSectionRef': oref.section_ref().he_normal(),
            'firstAvailableSectionRef': oref.first_available_section_ref().normal(),
            'isSpanning': oref.is_spanning(),
            'next': oref.next_section_ref().normal() if oref.next_section_ref() else None,
            'prev': oref.prev_section_ref().normal() if oref.prev_section_ref() else None,
            'title': oref.context_ref().normal(),
            'book': oref.book,
            'heTitle': oref.context_ref().he_normal(),
            'primary_category': oref.primary_category,
            'type': oref.primary_category, #same as primary category
        })
        if self.return_obj['isSpanning']:
            self.return_obj['spanningRefs'] = [r.normal() for r in oref.split_spanning_ref()]

    def _add_index_data_to_return_obj(self) -> None:
        index = self.oref.index
        self.return_obj.update({
            'indexTitle': index.title,
            'categories': index.categories,
            'heIndexTitle': index.get_title('he'),
            'isComplex': index.is_complex(),
            'isDependant': index.is_dependant_text(),
            'order': getattr(index, 'order', ''),
            'collectiveTitle': getattr(index, 'collective_title', ''),
            'heCollectiveTitle': hebrew_term(getattr(index, 'collective_title', '')),
            'alts': index.get_trimmed_alt_structs_for_ref(self.oref),
        })

    def _add_node_data_to_return_obj(self) -> None:
        inode = self.oref.index_node
        if getattr(inode, "lengths", None):
            self.return_obj["lengths"] = getattr(inode, "lengths")
            if len(self.return_obj["lengths"]):
                self.return_obj["length"]  = self.return_obj["lengths"][0]
        elif getattr(inode, "length", None):
            self.return_obj["length"] = getattr(inode, "length")
        self.return_obj.update({
            'textDepth': getattr(inode, "depth", None),
            'sectionNames': getattr(inode, "sectionNames", None),
            'addressTypes': getattr(inode, "addressTypes", None),
            'heTitle': inode.full_title("he"),
            'titleVariants': inode.all_tree_titles("en"),
            'heTitleVariants': inode.all_tree_titles("he"),
        })
        if not inode.is_virtual:
            self.return_obj['index_offsets_by_depth'] = inode.trim_index_offsets_by_sections(self.oref.sections, self.oref.toSections)

    def _format_text(self):
        # Early return for default format to avoid any processing
        if self.return_format == 'default':
            return

        # Pre-compute shared data outside the version loop
        shared_data = {}
        if self.return_format == 'wrap_all_entities':
            # Check for links and load segment refs only once
            shared_data['all_segment_refs'] = self.oref.all_segment_refs()

            query = self.oref.ref_regex_query()
            query.update({"inline_citation": True})
            shared_data['has_links'] = bool(Link().load(query))

        def make_named_entities_dict(version_title, language):
            # Cache named entities per version to avoid repeated DB queries
            cache_key = f"{version_title}_{language}"
            if cache_key not in shared_data:
                named_entities = RefTopicLinkSet({"expandedRefs": {"$in": [r.normal() for r in shared_data['all_segment_refs']]},
                                                  "charLevelData.versionTitle": version_title,
                                                  "charLevelData.language": language})
                # assumption is that refTopicLinks are all to unranged refs
                ne_by_secs = defaultdict(list)
                for ne in named_entities:
                    try:
                        ne_ref = Ref(ne.ref)
                    except InputError:
                        continue
                    ne_by_secs[ne_ref.sections[-1]-1,] += [ne]
                shared_data[cache_key] = ne_by_secs
            return shared_data[cache_key]

        # helper to build a segment-level link-wrapper once per version
        def build_link_wrapper(lang, version_text):
            # Compile regex once for entire version to cover all titles that appear anywhere
            reg, title_nodes = library.get_regex_and_titles_for_ref_wrapping(version_text, lang=lang, citing_only=True)

            # Return a function that wraps refs in an individual segment using the precompiled regex
            return lambda string, _: library.get_wrapped_refs_string(string, lang=lang, citing_only=True,
                                                                      reg=reg, title_nodes=title_nodes)

        # Define text modification functions based on return format
        text_modification_funcs = []



        if self.return_format == 'text_only':
            # Combine all text_only operations into a single function to minimize passes
            def combined_text_only(string, _):
                string = text.AbstractTextRecord.strip_itags(string)
                string = text.AbstractTextRecord.remove_html(string)
                return ' '.join(string.split())
            text_modification_funcs = [combined_text_only]

        elif self.return_format == 'strip_only_footnotes':
            # Combine strip operations into a single function
            def combined_strip_footnotes(string, _):
                string = text.AbstractTextRecord.strip_itags(string)
                return ' '.join(string.split())
            text_modification_funcs = [combined_strip_footnotes]

        # Process each version
        for version in self.return_obj['versions']:
            current_funcs = text_modification_funcs.copy()
            
            if self.return_format == 'wrap_all_entities':
                language = 'he' if version['direction'] == 'rtl' else 'en'
                ne_by_secs = make_named_entities_dict(version['versionTitle'], language)
                
                # Create closure-safe functions by capturing values explicitly
                def make_ne_wrapper(ne_dict):
                    return lambda string, sections: library.get_wrapped_named_entities_string(ne_dict[(sections[-1],)], string)
                
                current_funcs.append(make_ne_wrapper(ne_by_secs))
                
                # Build link-wrapper once per version
                flat_version_text = " ".join(JaggedTextArray(version['text']).flatten_to_array())
                if shared_data['has_links']:
                    current_funcs.append(build_link_wrapper(language, flat_version_text))

            # Only process if there are functions to apply
            if current_funcs:
                ja = JaggedTextArray(version['text'])
                
                # Combine all functions into one to minimize passes over the text
                if len(current_funcs) == 1:
                    composite_func = current_funcs[0]
                else:
                    composite_func = lambda string, sections: reduce(lambda s, f: f(s, sections), current_funcs, string)
                
                version['text'] = ja.modify_by_function(composite_func)

    def get_versions_for_query(self) -> dict:
        self.oref = self.oref.default_child_ref()
        for lang, vtitle in self.versions_params:
            self._append_required_versions(lang, vtitle)
        self._add_ref_data_to_return_obj()
        self._add_index_data_to_return_obj()
        self._add_node_data_to_return_obj()
        self._format_text()
        return self.return_obj

```

### sefaria/model/lexicon.py

```
# -*- coding: utf-8 -*-
"""
Writes to MongoDB Collection: word_form, lexicon_entry
"""
import re
import unicodedata
from . import abstract as abst
from sefaria.datatype.jagged_array import JaggedTextArray
from sefaria.system.exceptions import InputError
from sefaria.utils.hebrew import has_hebrew, strip_cantillation, has_cantillation


class WordForm(abst.AbstractMongoRecord):

    collection = 'word_form'
    required_attrs = [
        "form",
        "lookups",
    ]

    optional_attrs = [
        "c_form",
        "refs",
        "language_code",
        "generated_by"
    ]

    def load(self, query, proj=None):
        if 'form' in query and isinstance(query['form'], str):
            query['form'] = {"$regex": "^"+query['form']+"$", "$options": "i"}
        return super(WordForm, self).load(query, proj=None)

    def _sanitize(self):
        pass


class WordFormSet(abst.AbstractMongoSet):
    recordClass = WordForm


class Lexicon(abst.AbstractMongoRecord):
    collection = 'lexicon'
    required_attrs = [
        "name",
        'language',
        'to_language',
        'text_categories'
    ]

    optional_attrs = [
        'title',
        'pub_location',
        'pub_date',
        'editor',
        'year',
        'source',
        'source_url',
        'attribution',
        'attribution_url',
        'text_categories',
        'index_title',          # The title of the Index record that corresponds to this Lexicon
        'version_title',        # The title of the Version record that corresponds to this Lexicon
        'version_lang',         # The language of the Version record that corresponds to this Lexicon
        'should_autocomplete',   # enables search box
        'needsRefsWrapping'
    ]

    def word_count(self):
        return sum([e.word_count() for e in self.entry_set()])

    def entry_set(self):
        return LexiconEntrySet({"parent_lexicon": self.name})


class LexiconSet(abst.AbstractMongoSet):
    recordClass = Lexicon


class Dictionary(Lexicon):
    pass


class LexiconEntry(abst.AbstractMongoRecord):
    collection   = 'lexicon_entry'

    required_attrs = [
        "headword",
        "parent_lexicon",
    ]
    optional_attrs = [
        "transliteration",
        "pronunciation",
        "morphology",
        "language_code",
        "refs",
        "related_words",
        "number",
        "language_reference",
        "number",
        "content",
        "citations",
        "plural_form",
        "binyan_form",
        "alt_headwords",
        "derivatives",
        "quotes",
        "prev_hw",
        "next_hw",
        "notes",
        "alternative",
        "strong_number",
        "orig_word",
        "orig_ref",
        "catane_number",
        "rid",
        "strong_numbers",
        "GK",
        "TWOT",
        'peculiar',
        'all_cited',
        'ordinal',
        'brackets',
        'headword_suffix',
        'root',
        'occurrences'
    ]
    ALLOWED_TAGS    = ("i", "b", "br", "u", "strong", "em", "big", "small", "img", "sup", "sub", "span", "a")
    ALLOWED_ATTRS   = {
        'span':['class', 'dir'],
        'i': ['data-commentator', 'data-order', 'class', 'data-label', 'dir'],
        'img': lambda name, value: name == 'src' and value.startswith("data:image/"),
        'a': ['dir', 'class', 'href', 'data-ref'],
    }

    def _sanitize(self):
        pass

    def factory(self, lexicon_name):
        pass

    def contents(self, **kwargs):
        cts = super(LexiconEntry, self).contents()
        parent_lexicon = Lexicon().load({'name': self.parent_lexicon})
        cts['parent_lexicon_details'] = parent_lexicon.contents()
        return cts


class DictionaryEntry(LexiconEntry):

    def get_sense(self, sense):
        text = ''
        text += sense.get('number', '')
        if text:
            text = "<b>{}</b> ".format(text)
        for field in ['definition', 'alternative', 'notes']:
            text += sense.get(field, '')
        return text

    def headword_string(self):
        headwords = [self.headword] + getattr(self, 'alt_headwords', [])
        string = ', '.join(
            ['<strong dir="rtl">{}</strong>'.format(hw) for hw in headwords])
        return string

    def word_count(self):
        return JaggedTextArray(self.as_strings()).word_count()

    def as_strings(self, with_headword=True):
        new_content = ""
        next_line = ""

        if with_headword:
            next_line = self.headword_string()

        for field in ['morphology']:
            if field in self.content:
                next_line += " " + self.content[field]

        lang = ''
        if hasattr(self, 'language_code'):
            lang += " " + self.language_code
        if hasattr(self, 'language_reference'):
            if lang:
                lang += ' '
            lang += self.language_reference
        if lang:
            next_line += lang

        for sense in self.content['senses']:
            if 'grammar' in sense:
                # This is where we would start a new segment for the new form
                new_content += next_line
                next_line = '<br/>&nbsp;&nbsp;&nbsp;&nbsp;<strong>{}</strong> - '.format(sense['grammar']['verbal_stem'])
                next_line += ', '.join(
                    ['<strong dir="rtl">{}</strong>'.format(b) for b in sense['grammar']['binyan_form']])
                try:
                    for binyan_sense in sense['senses']:
                        next_line += " " + self.get_sense(binyan_sense)
                except KeyError:
                    pass
            else:
                next_line += " " + self.get_sense(sense)

        if hasattr(self, 'notes'):
            next_line += " " + self.notes
        if hasattr(self, 'derivatives'):
            next_line += " " + self.derivatives

        if next_line:
            new_content += next_line
        return [new_content]

    def get_alt_headwords(self):
        return getattr(self, "alt_headwords", [])


class StrongsDictionaryEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["content", "strong_number"]


class RashiDictionaryEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["content", "orig_word", "orig_ref", "catane_number"]


class HebrewDictionaryEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["rid"]

    def headword_string(self):
        headwords = [self.headword] + getattr(self, 'alt_headwords', [])
        string = ', '.join([f'<strong><big>{hw}</big></strong>' for hw in headwords]) + '\xa0\xa0'
        return string


class JastrowDictionaryEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["rid"]

    def get_sense(self, sense):
        text = ''
        text += sense.get('number', '')
        if text:
            text = "<b>{}</b> ".format(text)
        for field in ['definition']:
            text += sense.get(field, '')
        return text

    def headword_string(self):
        line = ""
        for hw in [self.headword] + getattr(self, 'alt_headwords', []):
            hw = re.sub(r' [\u00B2\u00B3\u2074\u2075\u2076]', '', hw)  # Drop superscripts from presentation
            for txt in re.split(r'([^ IV\u0590-\u05fe\'\-\"])', hw):
                if re.search(r'[IV\u0590-\u05fe\'\-\"]', txt):
                    line += '<strong dir="rtl">{}</strong>'.format(txt)
                else:
                    line += txt
            line += ', '
        line = line[:-2]
        return line


class KleinDictionaryEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["content", "rid"]

    def get_sense(self, sense):
        text = ''
        for field in ['plural_form', 'language_code', 'alternative']:
            text += sense.get(field, '') + ' '
        num = sense.get('number', '')
        if num:
            text += "<b>{}</b> ".format(num)
        for field in ['definition', 'notes']:
            text += sense.get(field, '') + ' '
        return text[:-1]

class BDBEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["content", "rid"]
    optional_attrs = ['strong_numbers', 'next_hw', 'prev_hw', 'peculiar', 'all_cited', 'ordinal', 'brackets', 'headword_suffix', 'alt_headwords', 'root', 'occurrences', 'quotes', 'GK', 'TWOT']

    def headword_string(self):
        hw = f'<span dir="rtl">{re.sub("[]*", "", self.headword)}</span>'
        if hasattr(self, 'occurrences') and not hasattr(self, 'headword_suffix'):
            hw += f'</big><sub>{self.occurrences}</sub><big>' #the sub shouldn't be in big
        alts = []
        if hasattr(self, 'alt_headwords'):
            for alt in self.alt_headwords:
                a = f'<span dir="rtl">{alt["word"]}</span>'
                if 'occurrences' in alt:
                    a += f'</big><sub>{alt["occurrences"]}</sub><big>' #the sub shouldn't be in big
                alts.append(a)
        if getattr(self, 'brackets', '') == 'all':
            if hasattr(self, 'headword_suffix'):
                hw = f'[{hw}{self.headword_suffix}]' #if there's a space, it'll be part of headword_suffix
                if hasattr(self, 'occurrences'):
                    hw += f'</big><sub>{self.occurrences}</sub><big>'
            else:
                hw = f'[{", ".join([hw] + alts)}]'
        else:
            if hasattr(self, 'brackets') and self.brackets == 'first_word':
                hw = f'[{hw}]'
            if hasattr(self, 'alt_headwords'):
                hw = ", ".join([hw] + alts)
        hw = f'<big>{hw}</big>'
        if hasattr(self, 'root'):
            hw = re.sub('(</?big>)', r'\1\1', hw)
        if hasattr(self, 'ordinal'):
            hw = f'{self.ordinal} {hw}'
        if hasattr(self, 'all_cited'):
            hw = f' {hw}'
        if hasattr(self, 'peculiar'):
            hw = f' {hw}'
        hw = re.sub('<big></big>', '', hw)
        return hw

    def get_alt_headwords(self):
        alts = getattr(self, "alt_headwords", [])
        return [a['word'] for a in alts]

    def get_sense(self, sense):
        string = ''
        if 'note' in sense:
            string = '<em>Note.</em>'
        if 'pre_num' in sense:
            string += f"{sense['pre_num']} "
        if 'all_cited' in sense:
            string += ''
        if 'form' in sense:
            if 'note' in sense:
                string += f' {sense["num"]}'
            else:
                string += f'<strong>{sense["form"]}</strong>'
        elif 'num' in sense:
            string += f'<strong>{sense["num"]}</strong>'
        if 'occurrences' in sense:
            string += f'<sub>{sense["occurrences"]}</sub>'
        string += ' '
        if 'definition' in sense:
            return string + sense['definition']
        else:
            senses = []
            for s in sense['senses']:
                subsenses = self.get_sense(s)
                if type(subsenses) == list:
                    senses += subsenses
                else:
                    senses.append(subsenses)
            senses[0] = string + senses[0]
            return senses

    def as_strings(self, with_headword=True):
        strings = []
        for sense in self.content['senses']:
            sense = self.get_sense(sense)
            if type(sense) == list:
                strings.append(' '.join(sense))
            else:
                strings.append(sense)
        if with_headword:
            strings[0] = self.headword_string() + ' ' + strings[0]
        return ['<br>'.join(strings)]


class KovetzYesodotEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["content", "rid"]

    def headword_string(self):
        return f'<big><b>{self.headword}</b></big>'

    def as_strings(self, with_headword=True):
        strings = []
        if with_headword:
            strings.append(self.headword_string())
        for key, value in self.content.items():
            if key != 'reference':
                strings.append(f'<br><small>{key}</small>')
            strings += value
        return ['<br>'.join(strings)]


class KrupnikEntry(DictionaryEntry):
    required_attrs = DictionaryEntry.required_attrs + ["content", "rid"]
    optional_attrs = DictionaryEntry.optional_attrs + ['biblical', 'no_binyan_kal', 'emendation', 'used_in', 'equals', 'pos_list']

    pos_schema = {'pos': {'type': 'string'}}
    hw_related_schemas = {
        'biblical': {'type': 'boolean'},
        'no_binyan_kal': {'type': 'boolean'},
        'emendation': {'type': 'string'},
        'used_in': {'type': 'string'},
        'pos_list': {'type': 'list', 'schema': {'type': 'string'}}
    }
    senses_schema = {'senses':
                         {'type': 'list',
                          'schema': {
                              'type': 'dict',
                              'schema': {
                                  'number': {'type': 'integer'},
                                  **pos_schema,
                                  'definition': {'type': 'string'},
                                  'notes': {'type': 'string'},
                              }}}}
    attr_schemas = {
        'headword': {'type': 'string'},
        'equals': {'type': 'list', 'schema': {'type': 'string'}},
        **hw_related_schemas,
        'alt_headwords': {
            'type': 'list',
            'schema': {
                'type': 'dict',
                'schema': {
                    'word': {'type': 'string', 'required': True},
                    **hw_related_schemas
                },
            }
        },
        'content': {
            'oneof': [
                {'type': 'string'},
                {'type': 'dict',
                 'schema': {'binyans': {
                     'type': 'list',
                     'schema': {
                         'type': 'list',
                         'schema': {
                             'type': 'dict',
                             'required': True,
                             'oneof_schema': [
                                 senses_schema,
                                 pos_schema,
                                 {'binyan-form': {'type': 'string'}},
                                 {'binyan-name': {'type': 'string'}}
                             ]},
                     }}}},
                {'type': 'dict',
                 'schema': senses_schema}
            ]
        }
    }

    def format_pos(self, pos):
        return f'<small>{pos}</small>'

    def format_headword(self, hw, is_primary):
        getter = lambda x, y: getattr(x, y, None) if is_primary else x.get(y)
        hw_string = ''
        attrs_to_funcs_map = {
            'headword' if is_primary else 'word': lambda x: re.sub("[]", "", x),
            'biblical': lambda _: f'{hw_string}',
            'no_binyan_kal': lambda _: f'({hw_string})',
            'emendation': lambda x: f'{hw_string} [{x}]',
            'used_in': lambda x: f'{hw_string}; {x}',
            'equals': lambda x: f'{hw_string} (={", ".join(x)})',
        }
        for attr, func in attrs_to_funcs_map.items():
            value = getter(hw, attr)
            if value:
                hw_string = func(value)
        hw_string = f'<big><big>{hw_string}</big></big>'
        pos_list = getter(hw, 'pos_list')
        if pos_list:
            pos_string = ' '.join([self.format_pos(pos) for pos in pos_list])
            hw_string = f'{hw_string} {pos_string}'
        return hw_string

    def headword_string(self):
        headwords = [self] + getattr(self, "alt_headwords", [])
        formatted_headwords = [self.format_headword(hw, i == 0) for i, hw in enumerate(headwords)]
        return ', '.join(formatted_headwords)

    def get_alt_headwords(self):
        alts = getattr(self, "alt_headwords", [])
        return [a['word'] for a in alts]

    def get_sense(self, sense):
        number = sense.get('number')
        if number:
            number = f'{number}) '
        pos = sense.get('pos')
        if pos:
            pos = self.format_pos(pos)
        definition = sense.get('definition')
        notes = sense.get('notes')
        existing_parts = [part for part in [number, pos, definition, notes] if part]
        return ' '.join(existing_parts)

    def get_binyan(self, binyan):
        text = ''
        for part in binyan: # any part is a dict of one {key: value} dict
            if 'senses' in part:
                text += ' '.join([self.get_sense(sense) for sense in part['senses']])
            elif 'pos' in part:
                text += self.format_pos(part['pos'])
            elif 'binyan-form' in part:
                text = f'<b>{next(iter(part.values()))}</b>'
            else:
                text += next(iter(part.values()))
            text += ' '
        return text.strip()

    def get_content(self):
        if isinstance(self.content, str):
            return self.content
        elif 'binyans' in self.content:
            parts = [self.get_binyan(binyan) for binyan in self.content['binyans']]
        else:
            parts = [self.get_sense(sense) for sense in self.content['senses']]
        content = '<br>'.join(parts)
        if len(parts) > 1:
            content = f'<br>{content}'
        return content


    def as_strings(self, with_headword=True):
        return [self.headword_string() + ' ' + self.get_content()]


class LexiconEntrySubClassMapping(object):
    lexicon_class_map = {
        'BDB Augmented Strong': StrongsDictionaryEntry,
        'Rashi Foreign Lexicon': RashiDictionaryEntry,
        'Jastrow Dictionary': JastrowDictionaryEntry,
        "Jastrow Unabbreviated" : JastrowDictionaryEntry,
        'Klein Dictionary': KleinDictionaryEntry,
        'Sefer HaShorashim': HebrewDictionaryEntry,
        'Animadversions by Elias Levita on Sefer HaShorashim': HebrewDictionaryEntry,
        'BDB Dictionary': BDBEntry,
        'BDB Aramaic Dictionary': BDBEntry,
        'Kovetz Yesodot VaChakirot': KovetzYesodotEntry,
        'Krupnik Dictionary': KrupnikEntry,
    }

    @classmethod
    def class_factory(cls, name):
        if name in cls.lexicon_class_map:
            return cls.lexicon_class_map[name]
        else:
            return LexiconEntry

    @classmethod
    def instance_factory(cls, name, attrs=None):
        return cls.class_factory(name)(attrs)

    @classmethod
    def instance_from_record_factory(cls, record):
        return cls.instance_factory(record['parent_lexicon'], record)


class LexiconEntrySet(abst.AbstractMongoSet):
    recordClass = LexiconEntry

    def __init__(self, query=None, page=0, limit=0, sort=[("_id", 1)], proj=None, hint=None, primary_tuples=None):
        super(LexiconEntrySet, self).__init__(query, page, limit, sort, proj, hint)
        self._primary_tuples = primary_tuples

    def _read_records(self):
        def is_primary(entry):
            return not (entry.headword, entry.parent_lexicon) in self._primary_tuples

        if self.records is None:
            self.records = []
            for rec in self.raw_records:
                self.records.append(LexiconEntrySubClassMapping.instance_from_record_factory(rec))
            self.max = len(self.records)
            if self._primary_tuples:
                self.records.sort(key=is_primary)


class LexiconLookupAggregator(object):

    @classmethod
    def _split_input(cls, input_str):
        input_str = re.sub(r"[:\u05c3\u05be\u05c0.]", " ", input_str)
        return [s.strip() for s in input_str.split()]

    @classmethod
    def _create_ngrams(cls, input_words, n):
        gram_list = []
        for k in range(1, n + 1):
            gram_list += [" ".join(input_words[i:i + k]) for i in range(len(input_words) - k + 1)]
        return gram_list

    @classmethod
    def get_word_form_objects(cls, input_word, lookup_key='form', **kwargs):
        from sefaria.model import Ref

        lookup_ref = kwargs.get("lookup_ref", None)
        wform_pkey = lookup_key
        if has_hebrew(input_word):
            # This step technically used to happen in the lookup main method `lexicon_lookup` if there were no initial results, but in case where a
            # consonantal form was supplied in the first place, this optimizes queries.
            input_word = strip_cantillation(input_word)
            if not has_cantillation(input_word, detect_vowels=True):
                wform_pkey = 'c_form'
        query_obj = {wform_pkey: input_word}
        if lookup_ref:
            nref = Ref(lookup_ref).normal()
            query_obj["refs"] = {'$regex': '^{}'.format(nref)}
        forms = WordFormSet(query_obj)
        if lookup_ref and len(forms) == 0:
            del query_obj["refs"]
            forms = WordFormSet(query_obj)
        return forms


    @classmethod
    def _single_lookup(cls, input_word, lookup_key='form', **kwargs):
        forms = cls.get_word_form_objects(input_word, lookup_key=lookup_key, **kwargs)
        if len(forms) > 0:
            headword_query = []
            for form in forms:
                for lookup in form.lookups:
                    headword_query.append(lookup)
            return headword_query
        else:
            return []

    @classmethod
    def _ngram_lookup(cls, input_str, **kwargs):
        words = cls._split_input(input_str)
        input_length = len(words)
        queries = []
        for i in reversed(list(range(input_length))):
            ngrams = cls._create_ngrams(words, i)
            for ng in ngrams:
                res = cls._single_lookup(ng, **kwargs)
                if res:
                    queries += res
        return queries

    @classmethod
    def lexicon_lookup(cls, input_str, **kwargs):
        input_str = unicodedata.normalize("NFC", input_str)
        results = cls._single_lookup(input_str, **kwargs)
        if not results or kwargs.get('always_consonants', False):
            results += cls._single_lookup(strip_cantillation(input_str, True), lookup_key='c_form', **kwargs)
        if not kwargs.get('never_split', None) and (len(results) == 0 or kwargs.get("always_split", None)):
            ngram_results = cls._ngram_lookup(input_str, **kwargs)
            results += ngram_results
        if len(results):
            primary_tuples = set()
            query = set() #TODO: optimize number of word form lookups? there can be a lot of duplicates... is it needed?
            for r in results:
                # extract the lookups with "primary" field so it can be used for sorting lookup in the LexiconEntrySet,
                # but also delete it, because its not part of the query obj
                if "primary" in r:
                    if r["primary"] is True:
                        primary_tuples.add((r["headword"], r["parent_lexicon"]))
                    del r["primary"]
            return LexiconEntrySet({"$or": results}, primary_tuples=primary_tuples)
        else:
            return None
```

### sefaria/model/following.py

```
"""
following.py - handle following relationships between users

Writes to MongoDB Collection: following
"""
from datetime import datetime

from sefaria.system.database import db
from sefaria.system.cache import django_cache

import structlog

logger = structlog.get_logger(__name__)


class FollowRelationship(object):
    def __init__(self, follower=None, followee=None):
        self.follower = follower
        self.followee = followee
        self.follow_date = datetime.now()

    def exists(self):
        return bool(db.following.find_one({"follower": self.follower, "followee": self.followee}))

    def follow(self):
        from sefaria.model.notification import Notification

        db.following.insert_one(vars(self))

        # Notification for the Followee
        notification = Notification({"uid": self.followee})
        notification.make_follow(follower_id=self.follower)
        notification.save()

        return self

    def unfollow(self):
        db.following.delete_one({"follower": self.follower, "followee": self.followee})


class FollowSet(object):
    def __init__(self):
        self.uids = []
        return self

    @property
    def count(self):
        return len(self.uids)


class FollowersSet(FollowSet):
    def __init__(self, uid):
        self.uids = db.following.find({"followee": uid}).distinct("follower")


class FolloweesSet(FollowSet):
    def __init__(self, uid):
        self.uids = db.following.find({"follower": uid}).distinct("followee")


@django_cache(timeout=60 * 60 * 24)
def aggregate_profiles(lang="english", limit=None):
    match_stage = {"status": "public"} if lang == "english" else {"status": "public", "sheetLanguage": "hebrew"}
    pipeline = [
        {"$match": match_stage},  # get all the sheets matching the criteria
        {"$sortByCount": "$owner"}  # group them by owner and count how many each owner has
    ]

    if limit is not None:
        pipeline += [
            {"$match": {"count": {"$gte": limit}}}
            # limit to owners with 3 or more sheets ("count" field is a result of the previous stage) that matched the first match
        ]

    pipeline += [
        {"$lookup": {
            # perform a "left join", use the "_id" field from the last stage, which contains the user/owner id of sheets, to look up corresponding profile obj
            "from": "profiles",
            "localField": "_id",
            "foreignField": "id",
            "as": "user"}},
        {"$unwind": {
            # not sure this does anything, if there are accidental multiple user profiles for one user id, it unwinds them
            "path": "$user",
            "preserveNullAndEmptyArrays": True
        }}
    ]
    results = db.sheets.aggregate(pipeline)
    try:
        profiles = {r["user"]["id"]: r for r in results if "user" in r}
    except KeyError:
        logger.error("Encountered sheet owner with no profile record.  No users will be recommended for following.")
        profiles = {}
    return profiles


creators = None


def general_follow_recommendations(lang="english", n=4):
    """
    Recommend people to follow without any information about the person we're recommending for.
    """
    from random import choices
    from django.contrib.auth.models import User
    from sefaria.system.database import db

    global creators
    if not creators:
        creators = []
        profiles = aggregate_profiles(lang=lang, limit=3)
        user_records = User.objects.in_bulk(profiles.keys())
        creators = []
        for id, u in user_records.items():
            fullname = u.first_name + " " + u.last_name
            user = {
                "name": fullname,
                "url": "/profile/" + profiles[id]["user"]["slug"],
                "uid": id,
                "image": profiles[id]["user"]["profile_pic_url_small"],
                "organization": profiles[id]["user"]["organization"],
                "sheetCount": profiles[id]["count"],
            }
            creators.append(user)
        creators = sorted(creators, key=lambda x: -x["sheetCount"])

    top_creators = creators[:1300]
    recommendations = choices(top_creators, k=n) if len(top_creators) else []

    return recommendations

```

### sefaria/model/blocking.py

```
"""
blocking.py - handle block relationships between users

Writes to MongoDB Collection: blocking
"""
from datetime import datetime

from sefaria.system.database import db
from sefaria.system.cache import django_cache

import structlog

logger = structlog.get_logger(__name__)


class BlockRelationship(object):
    def __init__(self, blocker=None, blockee=None):
        self.blocker = blocker
        self.blockee = blockee
        self.block_date = datetime.now()

    def exists(self):
        return bool(db.blocking.find_one({"blocker": self.blocker, "blockee": self.blockee}))

    def block(self):
        db.blocking.replace_one(
            {"blocker": self.blocker, "blockee": self.blockee},
            vars(self),
            upsert=True
        )
        return self

    def unblock(self):
        db.blocking.delete_one({"blocker": self.blocker, "blockee": self.blockee})


class BlockSet(object):
    def __init__(self):
        self.uids = []
        return self

    @property
    def count(self):
        return len(self.uids)


class BlockersSet(BlockSet):
    def __init__(self, uid):
        self.uids = db.blocking.find({"blockee": uid}).distinct("blocker")


class BlockeesSet(BlockSet):
    def __init__(self, uid):
        self.uids = db.blocking.find({"blocker": uid}).distinct("blockee")


```

### sefaria/model/text.py

```
# -*- coding: utf-8 -*-
"""
text.py
"""

import time
import structlog
from functools import reduce, partial
from typing import Optional, Union
logger = structlog.get_logger(__name__)

import sys
import regex
import copy
import bleach
import json
import itertools
from collections import defaultdict
from bs4 import BeautifulSoup, Tag
import re2 as re
from . import abstract as abst
from .schema import deserialize_tree, AltStructNode, VirtualNode, DictionaryNode, JaggedArrayNode, TitledTreeNode, DictionaryEntryNode, SheetNode, AddressTalmud, Term, TermSet, TitleGroup, AddressType
from sefaria.system.database import db

import sefaria.system.cache as scache
from sefaria.system.cache import in_memory_cache
from sefaria.system.exceptions import InputError, BookNameError, PartialRefInputError, IndexSchemaError, \
    NoVersionFoundError, DictionaryEntryNotFoundError, MissingKeyError, ComplexBookLevelRefError
from sefaria.utils.hebrew import has_hebrew, is_all_hebrew, hebrew_term
from sefaria.utils.util import list_depth, truncate_string
from sefaria.datatype.jagged_array import JaggedTextArray, JaggedArray
from sefaria.settings import DISABLE_INDEX_SAVE, USE_VARNISH, MULTISERVER_ENABLED, RAW_REF_MODEL_BY_LANG_FILEPATH, RAW_REF_PART_MODEL_BY_LANG_FILEPATH, DISABLE_AUTOCOMPLETER
from sefaria.system.multiserver.coordinator import server_coordinator
from sefaria.constants import model as constants

"""
                ----------------------------------
                         Index, IndexSet
                ----------------------------------
"""


class AbstractIndex(object):
    def contents(self, raw=False, **kwargs):
        pass

    def versionSet(self):
        return VersionSet({"title": self.title})

    def versionState(self):
        from . import version_state
        return version_state.VersionState(self.title)

    def is_new_style(self):
        return bool(getattr(self, "nodes", None))

    def get_title(self, lang="en"):
        if lang == "en":
            return self._title

        return self.nodes.primary_title(lang)

    def set_title(self, title, lang="en"):
        if getattr(self, 'nodes', None) is None:
            if lang == "en":
                self._title = title
            return

        if lang == "en":
            self._title = title  # we need to store the title attr in a physical storage, note that .title is a virtual property
            self.nodes.key = title

        old_primary = self.nodes.primary_title(lang)
        self.nodes.add_title(title, lang, True, True)
        if old_primary != title:  # then remove the old title, we don't want it.
            self.nodes.remove_title(old_primary, lang)

    title = property(get_title, set_title)

    def all_section_refs(self):
        refs = []
        vs = self.versionState()
        content_nodes = self.nodes.get_leaf_nodes()
        for c in content_nodes:
            try:
                state_ja = vs.state_node(c).ja("all")
                for indxs in state_ja.non_empty_sections():
                    sections = [a + 1 for a in indxs]
                    refs += [Ref(
                        _obj={
                            "index": vs.index,
                            "book": vs.index.nodes.full_title("en"),
                            "primary_category": vs.index.get_primary_category(),
                            "index_node": c,
                            "sections": sections,
                            "toSections": sections
                        }
                    )]
            except Exception as e:
                logger.warning("Failed to generate references for {}, section {}. {}".format(c.full_title("en"), ".".join([str(s) for s in sections]) if sections else "-", str(e)))
        return refs

    def all_segment_refs(self):
        seg_refs = []
        for sec_ref in self.all_section_refs():
            seg_refs += sec_ref.all_subrefs()
        return seg_refs

    def all_top_section_refs(self):
        """Returns a list of refs one step below root"""
        section_refs = self.all_section_refs()
        tally = {}
        refs = []
        for oref in section_refs:
            top_ref = oref.top_section_ref()
            if not top_ref.normal() in tally:
                tally[top_ref.normal()] = 1
                refs.append(top_ref)
        return refs

    def author_objects(self):
        from . import topic
        return [topic.Topic.init(slug) for slug in getattr(self, "authors", []) if topic.Topic.init(slug)]

    def composition_time_period(self):
        return None

    def composition_place(self):
        return None

    def publication_place(self):
        return None

    def publication_time_period(self):
        return None

    def contents_with_content_counts(self):
        """
        Returns the `contents` dictionary with each node annotated with section lengths info
        from version_state.
        """
        contents = self.contents()
        vstate   = self.versionState()

        def simplify_version_state(vstate_node):
            return aggregate_available_texts(vstate_node["_all"]["availableTexts"])

        def aggregate_available_texts(available):
            """Returns a jagged arrary of ints that counts the number of segments in each section,
            (by throwing out the number of versions of each segment)"""
            if len(available) == 0 or type(available[0]) is int:
                return len(available)
            else:
                return [aggregate_available_texts(x) for x in available]

        def annotate_schema(schema, vstate):
            if "nodes" in schema:
                for node in schema["nodes"]:
                    if "key" in node:
                        annotate_schema(node, vstate[node["key"]])
            else:
                schema["content_counts"] = simplify_version_state(vstate)

        annotate_schema(contents["schema"], vstate.content)
        return contents


class Index(abst.AbstractMongoRecord, AbstractIndex):
    """
    Index objects define the names and structure of texts stored in the system.
    There is an Index object for every text.

    """
    collection = 'index'
    history_noun = 'index'
    criteria_field = 'title'
    criteria_override_field = 'oldTitle'  # used when primary attribute changes. field that holds old value.
    track_pkeys = True
    pkeys = ["title", "compPlace", "pubPlace"]

    required_attrs = [
        "title",
        "categories"
    ]
    optional_attrs = [
        "schema",             # required for new style
        "alt_structs",        # optional for new style
        "default_struct",     # optional for new style
        "exclude_structs",    # optional, specifies which structs the client should ignore when displaying navigation ToCs
        "order",              # optional for old style and new
        "authors",
        "enDesc",
        "heDesc",
        "enShortDesc",
        "heShortDesc",
        "pubDate",
        "hasErrorMargin",     # (bool) whether or not compDate is exact.  used to be 'errorMargin' which was an integer amount that compDate was off by
        "compDate",
        "compPlace",
        "pubPlace",
        "era",
        "dependence",           # (str) Values: "Commentary" or "Targum" - to denote commentaries and other potential not standalone texts
        "base_text_titles",     # (list) the base book(s) this one is dependant on
        "base_text_mapping",    # (str) string that matches a key in sefaria.helper.link.AutoLinkerFactory._class_map
        "collective_title",     # (str) string value for a group of index records - the former commentator name. Requires a matching term.
        "is_cited",             # (bool) only indexes with this attribute set to True will be picked up as a citation in a text by default
        "lexiconName",          # (str) For dictionaries - the name used in the Lexicon collection
        "dedication",           # (dict) Dedication texts, keyed by language
        "hidden",               # (bool) Default false.  If not present, Index is visible in all TOCs.  True value hides the text in the main TOC, but keeps it in the search toc.
        "corpora",              # (list[str]) List of corpora that this index is included in. Currently these are just strings without validation. First element is used to group texts for determining version preference within a corpus.
    ]

    def __str__(self):
        return "Index: {}".format(self.title)

    def __repr__(self):  # Wanted to use orig_tref, but repr can not include Unicode
        return "{}().load({{'title': '{}'}})".format(self.__class__.__name__, self.title)

    def save(self, override_dependencies=False):
        if DISABLE_INDEX_SAVE:
            raise InputError("Index saving has been disabled on this system.")
        return super(Index, self).save(override_dependencies=override_dependencies)

    def _set_derived_attributes(self):
        if getattr(self, "schema", None):
            self.nodes = deserialize_tree(self.schema, index=self)
            # Our pattern has been to validate on save, not on load
            # self.nodes.validate()
        else:
            self.nodes = None
        self._set_struct_objs()

    def _set_struct_objs(self):
        self.struct_objs = {}
        if getattr(self, "alt_structs", None) and self.nodes:
            for name, struct in list(self.alt_structs.items()):
                self.struct_objs[name] = deserialize_tree(struct, index=self, struct_class=AltStructNode)
                self.struct_objs[name].title_group = self.nodes.title_group

    def is_complex(self):
        return getattr(self, "nodes", None) and self.nodes.has_children()

    def contents(self, raw=False, with_content_counts=False, with_related_topics=False, **kwargs):
        if raw:
            contents = super(Index, self).contents()
        else:
            # adds a set of legacy fields like 'titleVariants', expands alt structures with preview, etc.
            contents = self.nodes.as_index_contents()
            if with_content_counts:
                contents["schema"] = self.annotate_schema_with_content_counts(contents["schema"])
                contents["firstSectionRef"] = Ref(self.title).first_available_section_ref().normal()

            contents = self.expand_metadata_on_contents(contents)
        return contents


    def annotate_schema_with_content_counts(self, schema):
        """
        Returns the `schema` dictionary with each node annotated with section lengths info
        from version_state.
        """
        vstate = self.versionState()

        def simplify_version_state(vstate_node):
            return aggregate_available_texts(vstate_node["_all"]["availableTexts"])

        def aggregate_available_texts(available):
            """Returns a jagged arrary of ints that counts the number of segments in each section,
            (by throwing out the number of versions of each segment)"""
            if len(available) == 0 or type(available[0]) is int:
                return len(available)
            else:
                return [aggregate_available_texts(x) for x in available]

        def annotate_schema(schema, vstate):
            if "nodes" in schema:
                for node in schema["nodes"]:
                    if "key" in node:
                        annotate_schema(node, vstate[node["key"]])
            else:
                schema["content_counts"] = simplify_version_state(vstate)

        annotate_schema(schema, vstate.content)

        return schema

    def expand_metadata_on_contents(self, contents):
        """
        Decorates contents with expanded meta data such as Hebrew author names, human readable date strings etc.
        :param contents: the initial dictionary of contents
        :return: a dictionary of contents with additional fields
        """
        authors = self.author_objects()
        if len(authors):
            contents["authors"] = [{"en": author.get_primary_title("en"), "he": author.get_primary_title("he"), "slug": author.slug} for author in authors]

        if getattr(self, "collective_title", None):
            contents["collective_title"] = {"en": self.collective_title, "he": hebrew_term(self.collective_title)}

        if getattr(self, "base_text_titles", None):
            contents["base_text_titles"] = [{"en": btitle, "he": hebrew_term(btitle)} for btitle in self.base_text_titles]

        contents["heCategories"] = list(map(hebrew_term, self.categories))
        contents = self.time_period_and_place_contents(contents)
        return contents

    def time_period_and_place_contents(self, contents):
        """ Used to expand contents for date and time info """
        for k, f in [("compDateString", self.composition_time_period), ("pubDateString", self.publication_time_period)]:
            time_period = f()
            if time_period:
                contents[k] = {"en": time_period.period_string('en'), 'he': time_period.period_string('he')}

        for k, f in [("compPlaceString", self.composition_place), ("pubPlaceString", self.publication_place)]:
            place = f()
            if place:
                contents[k] = {"en": place.primary_name('en'), 'he': place.primary_name('he')}
        return contents

    def _saveable_attrs(self):
        d = {k: getattr(self, k) for k in self._saveable_attr_keys() if hasattr(self, k)}
        if getattr(self, "nodes", None):
            d["schema"] = self.nodes.serialize()
        if getattr(self, "struct_objs", None):
            d["alt_structs"] = {}
            for name, obj in list(self.struct_objs.items()):
                c = obj.serialize()
                del c["titles"]
                d["alt_structs"][name] = c
        return d

    def versions_are_sparse(self):
        """
            This function is just a convenience function!
            It's left as legacy code to estimate completion on a sparse text.
            Do not write code that depends on it.
        """
        return getattr(self, 'base_text_mapping', None) == 'many_to_one'

    def is_dependant_text(self):
        return getattr(self, 'dependence', None) is not None

    def all_titles(self, lang):
        if self.nodes:
            return self.nodes.all_tree_titles(lang)
        else:
            return None

    '''         Alternate Title Structures          '''
    def set_alt_structure(self, name, struct_obj):
        """
        :param name: String
        :param struct_obj:  :py.class:`TitledTreeNode`
        :return:
        """
        self.struct_objs[name] = struct_obj
        self.struct_objs[name].title_group = self.nodes.title_group

    def get_alt_structure(self, name):
        """
        :returns: :py.class:`TitledTreeNode`
        """
        return self.struct_objs.get(name)

    def get_alt_structures(self):
        return self.struct_objs

    def has_alt_structures(self):
        return bool(self.struct_objs)

    #These next 3 functions parallel functions on Library, but are simpler.  Refactor?
    def alt_titles_dict(self, lang):
        title_dict = {}
        for key, tree in list(self.get_alt_structures().items()):
            title_dict.update(tree.title_dict(lang))
        return title_dict

    def alt_titles_regex(self, lang):
        full_title_list = list(self.alt_titles_dict(lang).keys())
        alt_titles = list(map(re.escape, full_title_list))
        reg = '(?P<title>' + '|'.join(sorted(alt_titles, key=len, reverse=True)) + r')($|[:., ]+)'
        # increase max memory b/c re2 is RAM limited and the titles regex exceeds this limit
        options = re.Options()
        options.max_mem = 384 * 1024 * 1024
        return re.compile(reg, options=options)

    def get_alt_struct_node(self, title, lang=None):
        if not lang:
            lang = "he" if has_hebrew(title) else "en"
        return self.alt_titles_dict(lang).get(title)

    def get_alt_struct_roots(self):
        """
        Return list of the highest alt struct nodes that have real content. Currently, the highest level alt struct node
        has no useful information.
        @return:
        """
        return reduce(lambda a, b: a + b.children, self.get_alt_structures().values(), [])

    def get_alt_struct_leaves(self):

        def alt_struct_nodes_helper(node, nodes):
            if node.is_leaf():
                nodes.append(node)
            else:
                for child in node.children:
                    alt_struct_nodes_helper(child, nodes)

        nodes = []
        for node in self.get_alt_struct_roots():
            alt_struct_nodes_helper(node, nodes)
        return nodes

    def get_trimmed_alt_structs_for_ref(self, oref) -> dict:
        """
        this function takes the index's alt_structs and reduce it to the relevant ref
        """
        # Set up empty Array that mirrors text structure
        alts_ja = JaggedArray()
        for key, struct in self.get_alt_structures().items():
            # Assuming these are in order, continue if it is before ours, break if we see one after
            for n in struct.get_leaf_nodes():
                wholeRef = Ref(n.wholeRef).default_child_ref().as_ranged_segment_ref()
                if wholeRef.ending_ref().precedes(oref):
                    continue
                if wholeRef.starting_ref().follows(oref):
                    break

                # It's in our territory
                wholeRefStart = wholeRef.starting_ref()
                if oref.contains(wholeRefStart) and not wholeRefStart.contains(oref):
                    indxs = [k - 1 for k in wholeRefStart.in_terms_of(oref)]
                    val = {"en": [], "he": []}
                    try:
                        val = alts_ja.get_element(indxs) or val
                    except IndexError:
                        pass
                    val["en"] += [n.primary_title("en")]
                    val["he"] += [n.primary_title("he")]
                    val["whole"] = True
                    alts_ja.set_element(indxs, val)

                if getattr(n, "refs", None):
                    for i, r in enumerate(n.refs):
                        # hack to skip Rishon, skip empty refs
                        if i == 0 or not r:
                            continue
                        subRef = Ref(r)
                        subRefStart = subRef.starting_ref()
                        if oref.contains(subRefStart) and not subRefStart.contains(oref):
                            indxs = [k - 1 for k in subRefStart.in_terms_of(oref)]
                            val = {"en": [], "he": []}
                            try:
                                val = alts_ja.get_element(indxs) or val
                            except IndexError:
                                pass
                            val["en"] += [n.sectionString([i + 1], "en", title=False)]
                            val["he"] += [n.sectionString([i + 1], "he", title=False)]
                            alts_ja.set_element(indxs, val)
                        elif subRefStart.follows(oref):
                            break

        return alts_ja.array()

    def composition_place(self):
        from . import place
        if getattr(self, "compPlace", None) is None:
            return None
        return place.Place().load({"key": self.compPlace})

    def publication_place(self):
        from . import place
        if getattr(self, "pubPlace", None) is None:
            return None
        return place.Place().load({"key": self.pubPlace})

    # This is similar to logic on GardenStop
    def composition_time_period(self):
        return self._get_time_period("compDate", margin_field="hasErrorMargin")

    def publication_time_period(self):
        return self._get_time_period("pubDate")

    def best_time_period(self):
        """
        :return: TimePeriod: First tries to return `compDate`.
        If no compDate or compDate is an empty list, _get_time_period returns None and it then looks at author info
        """
        compDatePeriod = self._get_time_period('compDate', margin_field="hasErrorMargin")
        if compDatePeriod:
            return compDatePeriod
        else:
            author = self.author_objects()[0] if len(self.author_objects()) > 0 else None
            tp = author and author.most_accurate_time_period()
            return tp

    def _get_time_period(self, date_field, margin_field=""):
        """
        Assumes that value of `date_field` ('pubDate' or 'compDate') is a list of integers.
        """
        from . import timeperiod
        years = getattr(self, date_field, [])
        if years is None or len(years) == 0:
            return None
        try:
            error_margin = getattr(self, margin_field, False) if margin_field else False
        except ValueError:
            error_margin = False
        startIsApprox = endIsApprox = error_margin
        if len(years) > 1:
            start, end = years
        else:
            start = end = years[0]
        return timeperiod.TimePeriod({
        "start": start,
        "startIsApprox": startIsApprox,
        "end": end,
        "endIsApprox": endIsApprox
    })

    # Index changes behavior of load_from_dict, so this circumvents that changed behavior to call load_from_dict on the abstract superclass
    def update_from_dict(self, d):
        return super(Index, self).load_from_dict(d, is_init=False)

    def load_from_dict(self, d, is_init=False):
        if d:
            if not d.get("categories"):
                raise InputError("Please provide category for Index record: {}.".format(d.get("title")))

            # Data is being loaded from dict in old format, rewrite to new format,
            # Used by GUI Index tools (Index Editor and the /add/new index creation tool) that do not pass a 'schema'
            # Assumption is that d has a complete title collection
            if "schema" not in d:
                node = getattr(self, "nodes", None)
                if node:
                    node._init_title_defaults()
                else:
                    node = JaggedArrayNode()

                node.key = d.get("title")

                if node.is_flat():
                    sn = d.pop("sectionNames", None)
                    if sn:
                        node.sectionNames = sn
                        node.depth = len(node.sectionNames)
                    else:
                        raise InputError("Please specify section names for Index record.")

                    if self.is_new():
                        if d["categories"][0] == "Talmud" and d["categories"][1] == "Bavli":
                            node.addressTypes = ["Talmud", "Integer"]
                        else:
                            node.addressTypes = ["Integer" for _ in range(node.depth)]

                #Build titles
                node.add_title(d["title"], "en", True)

                tv = d.pop("titleVariants", None)
                if tv:
                    for t in tv:
                        lang = "he" if has_hebrew(t) else "en"
                        node.add_title(t, lang)

                ht = d.pop("heTitle", None)
                if ht:
                    node.add_title(ht, "he", True)

                htv = d.pop("heTitleVariants", None)
                if htv:
                    for t in htv:
                        node.add_title(t, "he")

                d["schema"] = node.serialize()

            # todo: should this functionality be on load()?
            if "oldTitle" in d and "title" in d and d["oldTitle"] != d["title"]:
                self.load({"title": d["oldTitle"]})
                # self.titleVariants.remove(d["oldTitle"])  # let this be determined by user
        return super(Index, self).load_from_dict(d, is_init)


    @staticmethod
    def get_title_quotations_variants(title):
        """
        If there is a quotation, fancy quotation, or gershayim in title, return two titles also with quotations.
        For example, if title is 'S"A', return a list of 'SA' and
        'SA'
        :param title: str
        :return: list
        """
        titles = []
        quotes = ['"', '', '']
        found_quotes = [quote for quote in quotes if quote in title]
        for found_quote_char in found_quotes:
            titles += [title.replace(found_quote_char, quote_char) for quote_char in quotes if quote_char != found_quote_char]
        return titles

    def normalize_titles_with_quotations(self):
        # for all Index and node hebrew titles, this function does the following:
        # 1. any title that has regular quotes, gershayim, or fancy quotes will now have two corresponding
        # titles where the characters are exactly the same except for the type of quote
        # 2. all primary titles will not have gershayim or fancy quotes, but only have regular quotes or none at all.
        # 3. all titles have either gershayim or fancy quotes or regular quotes or none at all,
        # so that no title can have two different types of quotes.
        primary_title = self.get_title('he').replace('', '"').replace('', '"')
        self.nodes.add_title(primary_title, 'he', True, True)
        index_titles = [primary_title]
        for title in self.schema["titles"]:
            if title["lang"] == "he" and title.get("primary", False) == False:
                index_titles.append(title["text"])

        for title in index_titles:
            title = title.replace('', '"').replace('', '"')
            new_titles = [title] + self.get_title_quotations_variants(title)
            for new_title in new_titles:
                if new_title not in index_titles:
                    self.nodes.add_title(new_title, 'he')

        for node in self.nodes.children:
            if getattr(node, "default", False) == False and getattr(node, "sharedTitle", "") == "":
                primary_title = node.get_primary_title('he')
                primary_title = primary_title.replace('', '"').replace('', '"')
                node.add_title(primary_title, 'he', True, True)
                node_titles = node.get_titles('he')
                for node_title in node_titles:
                    node_title = node_title.replace('', '"').replace('', '"')
                    new_titles = [node_title] + self.get_title_quotations_variants(node_title)
                    for new_title in new_titles:
                        if new_title not in node_titles:
                            node.add_title(new_title, 'he')

    def _normalize(self):
        self.title = self.title.strip()
        self.title = self.title[0].upper() + self.title[1:]

        if getattr(self, "is_cited", False):
            self.normalize_titles_with_quotations()

        if isinstance(getattr(self, "authors", None), str):
            self.authors = [self.authors]

        if not self.is_new():
            for t in [self.title, self.nodes.primary_title("en"), self.nodes.key]:  # This sets a precedence order
                if t != self.pkeys_orig_values["title"]:  # One title changed, update all of them.
                    self.title = t
                    self.nodes.key = t
                    self.nodes.add_title(t, "en", True, True)
                    break
            self._update_alt_structs_on_title_change()

        """
        Make sure these fields do not appear:
        "titleVariants",      # required for old style
        "sectionNames",       # required for old style simple texts, sometimes erroneously present for commnetary
        "heTitle",            # optional for old style
        "heTitleVariants",    # optional for old style
        "maps",               # deprecated
        "length",             # optional for old style
        "lengths",            # optional for old style
        "transliteratedTitle",# optional for old style
        """
        deprecated_attrs = ["titleVariants","sectionNames","heTitle","heTitleVariants","maps","length","lengths", "transliteratedTitle"]
        for attr in deprecated_attrs:
            if getattr(self, attr, None):
                delattr(self, attr)

        # Index Editor can set collective_title and dependence to empty string if admin doesn't fill in a value.
        # likewise, it can set base_text_titles to [] but we don't want empty strings/lists for these fields in the database
        if getattr(self, "base_text_titles", None) == []:  # if base_text_titles is present but is empty list
            delattr(self, "base_text_titles")
        if getattr(self, "dependence", None) == "":
            delattr(self, "dependence")
        if hasattr(self, "collective_title"):
            if self.collective_title == "":
                del self.collective_title

    def _update_alt_structs_on_title_change(self):
        old_title = self.pkeys_orig_values["title"]
        new_title = self.nodes.primary_title("en")
        def change_alt_node_refs(node):
            if 'wholeRef' in node:
                node['wholeRef'] = node['wholeRef'].replace(old_title, new_title)
            if 'refs' in node:
                node['refs'] = [r.replace(old_title, new_title) for r in node['refs']]
            if 'nodes' in node:
                for n in node['nodes']:
                    change_alt_node_refs(n)
        alts = getattr(self, 'alt_structs', None)
        if alts and old_title != new_title:
            for alt in alts.values():
                change_alt_node_refs(alt)
            self._set_struct_objs()

    def _validate(self):
        assert super(Index, self)._validate()

        # Keys that should be non empty lists
        non_empty = ["categories"]

        for key in non_empty:
            if not isinstance(getattr(self, key, None), list) or len(getattr(self, key, [])) == 0:
                raise InputError("{} field must be a non empty list of strings.".format(key))

        #allow only ASCII in text titles
        if not self.title.isascii():
            raise InputError("Text title may contain only simple English characters.")

        # Disallow special characters in text titles
        if any((c in ':.-\\/') for c in self.title):
            raise InputError("Text title may not contain periods, hyphens or slashes.")

        # Disallow special character in categories
        for cat in self.categories:
            if any((c in '.-') for c in cat):
                raise InputError("Categories may not contain periods or hyphens.")

        for btitle in getattr(self, "base_text_titles", []):
            try:
                library.get_index(btitle)
            except BookNameError:
                raise InputError("Base Text Titles must point to existing texts in the system.")

        from sefaria.model import Category
        if not Category().load({"path": self.categories}):
            raise InputError("You must create category {} before adding texts to it.".format("/".join(self.categories)))

        for date_key in ['compDate', 'pubDate']:
            if hasattr(self, date_key):
                val = getattr(self, date_key)
                if not isinstance(val, list) or not all([isinstance(x, int) for x in val]):
                    raise InputError(f"Optional attribute '{date_key}' must be list of integers.")

        '''
        for cat in self.categories:
            if not hebrew_term(cat):
                raise InputError("You must add a hebrew translation Term for any new Category title: {}.".format(cat))
        '''

        if getattr(self, "collective_title", None) and not hebrew_term(getattr(self, "collective_title", None)):
            raise InputError("You must add a hebrew translation Term for any new Collective Title: {}.".format(
                self.collective_title))

        #complex style records- all records should now conform to this
        if self.nodes:
            # Make sure that all primary titles match
            if self.title != self.nodes.primary_title("en") or self.title != self.nodes.key:
                raise InputError("Primary titles mismatched in Index Record: {}, {}, {}"
                                 .format(self.title, self.nodes.primary_title("en"), self.nodes.key))

            # Make sure all titles are unique
            for lang in ["en", "he"]:
                all_titles = self.all_titles(lang)
                """
                # Note: Because these titles come from the keys of TitledTreeNode.titleDict(), there's no possibility for name collision.
                # todo: actually test for name collision
                if len(all_titles) != len(set(all_titles)):
                    for title in all_titles:
                        if all_titles.count(title) > 1:
                            raise InputError(u'The title {} occurs twice in this Index record'.format(title))
                """
                for title in all_titles:
                    existing = library.get_schema_node(title, lang)
                    existing_index = existing.index if existing else Index().load({"title": title})
                    if existing_index and not self.same_record(existing_index) and existing_index.title != self.pkeys_orig_values.get("title"):
                        raise InputError('A text called "{}" already exists.'.format(title))

            self.nodes.validate()
            for key, tree in list(self.get_alt_structures().items()):
                tree.validate()

        else:  # old style commentator record are no longer supported
            raise InputError('All new Index records must have a valid schema.')

        if getattr(self, "authors", None):
            from .topic import Topic, AuthorTopic
            if not isinstance(self.authors, list):
                raise InputError(f'{self.title} authors must be a list.')
            for author_slug in self.authors:
                topic = Topic.init(author_slug)
                assert isinstance(topic, AuthorTopic), f"Author with slug {author_slug} does not match any valid AuthorTopic instance. Make sure the slug exists in the topics collection and has the subclass 'author'."

        return True

    def get_toc_index_order(self):
        order = getattr(self, 'order', None)
        if order:
            return order[0]
        return None

    def get_base_text_order(self):
        if getattr(self, 'base_text_titles', None):
            base_orders = [a for a in filter(None, [library.get_index(x).get_toc_index_order() for x in self.base_text_titles])]
            if len(base_orders) > 0:
                return min(base_orders) or 10000
        return 10000

    def slim_toc_contents(self):
        toc_contents_dict = {
            "title": self.get_title(),
            "heTitle": self.get_title("he"),
        }
        order = self.get_toc_index_order()
        if order:
            toc_contents_dict["order"] = order

        base_text_order = self.get_base_text_order()
        if base_text_order:
            toc_contents_dict["base_text_order"] = base_text_order

        return toc_contents_dict

    def toc_contents(self, include_first_section=False, include_flags=False, include_base_texts=False):
        """Returns to a dictionary used to represent this text in the library wide Table of Contents"""
        toc_contents_dict = {
            "title": self.get_title(),
            "heTitle": self.get_title("he"),
            "categories": self.categories[:],
            "enShortDesc": getattr(self, "enShortDesc", ""),
            "heShortDesc": getattr(self, "heShortDesc", ""),
            "primary_category" : self.get_primary_category(),
        }

        if getattr(self, "dependence", False):
            toc_contents_dict["dependence"] = self.dependence

        if len(getattr(self, "corpora", [])) > 0:
            # first elem in corpora is the main corpus
            toc_contents_dict["corpus"] = self.corpora[0]

        if include_first_section:
            firstSection = Ref(self.title).first_available_section_ref()
            toc_contents_dict["firstSection"] = firstSection.normal() if firstSection else None

        if include_flags:
            vstate = self.versionState()
            toc_contents_dict["enComplete"] = bool(vstate.get_flag("enComplete"))
            toc_contents_dict["heComplete"] = bool(vstate.get_flag("heComplete"))

        order = self.get_toc_index_order()
        if order:
            toc_contents_dict["order"] = order

        if hasattr(self, "collective_title"):
            toc_contents_dict["commentator"] = self.collective_title # todo: deprecate Only used in s1 js code
            toc_contents_dict["heCommentator"] = hebrew_term(self.collective_title) # todo: deprecate Only used in s1 js code
            toc_contents_dict["collectiveTitle"] = self.collective_title
            toc_contents_dict["heCollectiveTitle"] = hebrew_term(self.collective_title)

        if include_base_texts and hasattr(self, 'base_text_titles'):
            toc_contents_dict["base_text_titles"] = self.base_text_titles
            toc_contents_dict["base_text_order"] = self.get_base_text_order()
            if include_first_section:
                toc_contents_dict["refs_to_base_texts"] = self.get_base_texts_and_first_refs()
            if "collectiveTitle" not in toc_contents_dict:
                toc_contents_dict["collectiveTitle"] = self.title
                toc_contents_dict["heCollectiveTitle"] = self.get_title("he")
        elif hasattr(self, 'base_text_titles'):
            toc_contents_dict["base_text_order"] = self.get_base_text_order()

        if include_base_texts and hasattr(self, 'base_text_mapping'):
            toc_contents_dict["base_text_mapping"] = self.base_text_mapping

        if hasattr(self, 'hidden'):
            toc_contents_dict["hidden"] = self.hidden

        return toc_contents_dict

    #todo: the next 3 functions seem to come at an unacceptable performance cost. Need to review performance or when they are called.
    def get_base_texts_and_first_refs(self):
        return {btitle: self.get_first_ref_in_base_text(btitle) for btitle in self.base_text_titles}

    def get_first_ref_in_base_text(self, base_text_title):
        from sefaria.model.link import Link
        orig_ref = Ref(self.title)
        base_text_ref = Ref(base_text_title)
        first_link = Link().load(
            {'$and': [orig_ref.ref_regex_query(), base_text_ref.ref_regex_query()], 'is_first_comment': True}
        )
        if first_link:
            if orig_ref.contains(Ref(first_link.refs[0])):
                return Ref(first_link.refs[0]).section_ref().normal()
            else:
                return Ref(first_link.refs[1]).section_ref().normal()
        else:
            firstSection = orig_ref.first_available_section_ref()
            return firstSection.section_ref().normal() if firstSection else None

    def find_string(self, regex_str, cleaner=lambda x: x, strict=True, lang='he', vtitle=None):
        """
        See TextChunk.find_string
        :param regex_str:
        :param cleaner:
        :param strict:
        :param lang:
        :param vtitle:
        :return:
        """
        return self.nodes.find_string(regex_str, cleaner=cleaner, strict=strict, lang=lang, vtitle=vtitle)

    def text_index_map(self, tokenizer=lambda x: re.split(r'\s+', x), strict=True, lang='he', vtitle=None):
        """
        See TextChunk.text_index_map
        :param tokenizer:
        :param strict:
        :param lang:
        :return:
        """
        return self.nodes.text_index_map(tokenizer=tokenizer, strict=strict, lang=lang, vtitle=vtitle)

    def get_primary_category(self):
        if self.is_dependant_text():
            return self.dependence.capitalize()
        else:
            return self.categories[0]

    def get_primary_corpus(self):
        """
        Primary corpus used for setting version preference by
        """
        corpora = getattr(self, "corpora", [])
        if len(corpora) > 0:
            return corpora[0]

    def referenceable_children(self):
        """
        parallel to TreeNodes's `children`. Allows full traversal of an index's nodes

        @return:
        """
        default_struct_children = self.nodes.children
        if len(default_struct_children) == 0:
            # simple text. Use root as only child.
            default_struct_children = [self.nodes]
        return default_struct_children + self.get_alt_struct_roots()

    def get_referenceable_alone_nodes(self):
        """
        Return list of nodes on Index where each node has at least one match template with scope "alone"
        @return: List of TitledTreeNodes
        """
        alone_nodes = []
        for child in self.referenceable_children():
            if child.has_scope_alone_match_template():
                alone_nodes += [child]
            alone_nodes += child.get_referenceable_alone_nodes()
        return alone_nodes


class IndexSet(abst.AbstractMongoSet):
    """
    A set of :class:`Index` objects.
    """
    recordClass = Index

    # Index changes behavior of load_from_dict, so this circumvents that changed behavior to call load_from_dict on the abstract superclass
    def update(self, attrs):
        for rec in self:
            rec.update_from_dict(attrs).save()


"""
                    -------------------
                     Versions & Chunks
                    -------------------
"""

class AbstractSchemaContent(object):
    content_attr = "content"

    def get_content(self):
        return getattr(self, self.content_attr, None)

    def content_node(self, snode):
        """
        :param snode:
        :type snode SchemaContentNode:
        :return:
        """
        return self.sub_content(snode.version_address())

    def sub_content_with_ref(self, ref=None, value=None):
        assert isinstance(ref, Ref)
        assert not ref.is_range()
        return self.sub_content(ref.index_node.version_address(), [i - 1 for i in ref.sections], value)

    def sub_content(self, key_list=None, indx_list=None, value=None):
        """
        Gets or sets values deep within the content of this version.
        This returns the result by reference, NOT by value.
        http://stackoverflow.com/questions/27339165/slice-nested-list-at-variable-depth
        :param key_list: The node keys to traverse to get to the content node
        :param indx_list: The indexes of the subsection to get/set
        :param value: The value to set.  If present, the method acts as a setter.  If None, it acts as a getter.
        """
        # todo check that the shape of value matches the shape of the piece being set

        if not key_list:
            key_list = []
        if not indx_list:
            indx_list = []
        node = reduce(lambda d, k: d[k], key_list, self.get_content())
        if indx_list:  # accessing/setting index with jagged array node
            if value is not None:
                # NOTE: JaggedArrays modify their store in place, so this change will affect `self`
                JaggedArray(node).set_element(indx_list, value, '')
            return reduce(lambda a, i: a[i], indx_list, node)
        else: # accessing/setting index in schema nodes
            if value is not None:
                if isinstance(value, list):  # we assume if value is a list, you want to modify the entire contents of the jagged array node
                    node[:] = value
                else:  # this change is to a schema node that's not a leaf. need to explicitly set contents on the parent so this change affects `self` 
                    if len(key_list) == 0:
                        setattr(self, self.content_attr, value)
                    elif len(key_list) == 1:
                        self.get_content()[key_list[0]] = value
                    else:
                        node_parent = reduce(lambda d, k: d[k], key_list[:-1], self.get_content())
                        node_parent[key_list[-1]] = value
            return node


class AbstractTextRecord(object):
    """
    """
    text_attr = "chapter"
    ALLOWED_TAGS    = constants.ALLOWED_TAGS_IN_ABSTRACT_TEXT_RECORD
    ALLOWED_ATTRS   = constants.ALLOWED_ATTRS_IN_ABSTRACT_TEXT_RECORD

    def word_count(self):
        """ Returns the number of words in this text """
        try:
            wc = self.ja(remove_html=True).word_count()
        except AttributeError:
            wc = 0
        return wc

    def char_count(self):
        """ Returns the number of characters in this text """
        return self.ja(remove_html=True).char_count()

    def verse_count(self):
        """ Returns the number of verses in this text """
        return self.ja().verse_count()

    def ja(self, remove_html=False): #don't cache locally unless change is handled.  Pontential to cache on JA class level
        base_text = getattr(self, self.text_attr, None)
        if base_text and remove_html:
            base_text = AbstractTextRecord.remove_html(base_text)
        return JaggedTextArray(base_text)

    def get_top_level_jas(self) -> tuple:
        """
        Returns tuple with two items 
            1) ja_list: list of highest level JaggedArrays
            2) parent_key_list: list of tuples (parent, ja_key) where parent is the SchemaNode parent of the corresponding ja in ja_list and ja_key is the key of that ja in parent
        parent_key_list is helpful if you need to update each jagged array
        """
        return self._get_top_level_jas_helper(getattr(self, self.text_attr, None))

    def get_node_by_key_list(self, key_list: list) -> tuple:
        """
        Given return node at self.text_attr[addr1][addr2]...[addr_n] where addr_i in address_list
        There doesn't seem to be a nice way to do this in Python
        Returns tuple of three items
            1) node at key_list
            2) parent node
            3) key of node in parent node
        Returns (None, None, None) if address_list has a non-existing key
        """
        curr_node = getattr(self, self.text_attr, None)
        parent, node_key = None, None
        for key in key_list:
            parent = curr_node
            node_key = key
            curr_node = curr_node.get(key)
            if curr_node is None:
                return None, None, None
        return curr_node, parent, node_key
    
    def _get_top_level_jas_helper(self, item: Union[dict, list], parent=None, item_key=None) -> tuple:
        """
        Helper function for get_top_level_jas to help with recursion
        """
        jas = []
        parent_key_list = []
        if isinstance(item, dict):
            for key, child in item.items():
                temp_jas, temp_parent_key_list = self._get_top_level_jas_helper(child, item, key)
                jas += temp_jas
                parent_key_list += temp_parent_key_list
        elif isinstance(item, list):
            jas += [item]
            parent_key_list = [(parent, item_key)]
        return jas, parent_key_list

    def _trim_ending_whitespace(self):
        """
        Trims blank segments from end of every section
        :return:
        """
        jas, parent_key_list = self.get_top_level_jas()
        for ja, (parent_node, ja_key) in zip(jas, parent_key_list):
            new_ja = JaggedTextArray(ja).trim_ending_whitespace().array()
            if parent_node is None:
                setattr(self, self.text_attr, new_ja)
            else:
                parent_node[ja_key] = new_ja

    def as_string(self):
        content = getattr(self, self.text_attr, None)
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return self.ja().flatten_to_string()
        else:
            return ""

    def as_sized_string(self, min_char=240, max_char=360):
        """
        Return a starting substring of this text.
        If the entire text is less than min_char, return the entire text.
        If a segment boundary occurs between min_char and max_char, split there.
        Otherwise, attempt to break on a period, semicolon, or comma between min_char and max_char.
        Otherwise, break on a space between min_char and max_char.
        :param min_char:
        :param max_char:
        :return:
        """
        balance = lambda doc: str(BeautifulSoup(doc, "html.parser"))

        as_array = self.ja().flatten_to_array()

        previous_state = None
        accumulator = ''

        for segment in as_array:
            segment = self.strip_itags(segment)
            joiner = " " if previous_state is not None else ""
            previous_state = accumulator
            accumulator += joiner + segment

            cur_len = len(accumulator)
            prev_len = len(previous_state)
            # If a segment boundary occurs between min_char and max_char, return.
            # Get the longest instance where that's true.
            if cur_len > max_char >= prev_len >= min_char:
                if previous_state[-1] == ".":
                    return previous_state[:-1] + ""
                else:
                    return previous_state + ""

            # We're too big, and the previous chunk was too small.  Break on a signal character.
            if cur_len > max_char and min_char > prev_len:

                # get target lengths
                at_least = min_char - prev_len
                at_most = max_char - prev_len
                return balance(previous_state + joiner + truncate_string(segment, at_least, at_most))

        # We've reached the end, it's not longer than max_char, and it's what we've got.
        return accumulator


    @classmethod
    def sanitize_text(cls, t):
        if isinstance(t, list):
            for i, v in enumerate(t):
                t[i] = AbstractTextRecord.sanitize_text(v)
        elif isinstance(t, str):
            t = bleach.clean(t, tags=cls.ALLOWED_TAGS, attributes=cls.ALLOWED_ATTRS)
        else:
            return False
        return t

    @staticmethod
    def remove_html(t):

        def conditional_replace(match):
            tag = match.group()
            if tag in ["<br/>", "<br>"]:
                return " "
            return ""

        if isinstance(t, list):
            for i, v in enumerate(t):
                if isinstance(v, str):
                    t[i] = re.sub('<[^>]+>', conditional_replace, v)
                else:
                    t[i] = AbstractTextRecord.remove_html(v)
        elif isinstance(t, str):
            t = re.sub('<[^>]+>', conditional_replace, t)
        else:
            return False
        return t

    @staticmethod
    def remove_html_and_make_presentable(t):
        if isinstance(t, list):
            for i, v in enumerate(t):
                if isinstance(v, str):
                    t[i] = re.sub(r'<[^>]+>', " ", v)
                    t[i] = re.sub(r'[ ]{2,}', " ", t[i])
                    t[i] = re.sub(r'(\S) ([.?!,])', r"\1\2", t[i])  # Remove spaces preceding punctuation
                    t[i] = t[i].strip()
                else:
                    t[i] = AbstractTextRecord.remove_html_and_make_presentable(v)
        elif isinstance(t, str):
            t = re.sub(r'<[^>]+>', " ", t)
            t = re.sub(r'[ ]{2,}', " ", t)
            t = re.sub(r'(\S) ([.?!,])', r"\1\2", t)  # Remove spaces preceding punctuation
            t = t.strip()
        else:
            return False
        return t

    @staticmethod
    def find_all_itags(s, only_footnotes=False):
        soup = BeautifulSoup("<root>{}</root>".format(s), 'lxml')
        itag_list = soup.find_all(AbstractTextRecord._find_itags)
        if only_footnotes:
            itag_list = list(filter(lambda itag: AbstractTextRecord._itag_is_footnote(itag), itag_list))
        return soup, itag_list

    @staticmethod
    def _itag_is_footnote(tag):
        return tag.name == "sup" and isinstance(tag.next_sibling, Tag) and tag.next_sibling.name == "i" and 'footnote' in tag.next_sibling.get('class', '')

    @staticmethod
    def _find_itags(tag):
        if isinstance(tag, Tag):
            is_inline_commentator = tag.name == "i" and len(tag.get('data-commentator', '')) > 0
            is_page_marker = tag.name == "i" and len(tag.get('data-overlay','')) > 0
            is_tanakh_end_sup = tag.name == "sup" and 'endFootnote' in tag.get('class', [])  # footnotes like this occur in JPS english
            return AbstractTextRecord._itag_is_footnote(tag) or is_inline_commentator or is_page_marker or is_tanakh_end_sup
        return False

    @staticmethod
    def strip_imgs(s, sections=None):
        soup = BeautifulSoup("<root>{}</root>".format(s), 'lxml')
        imgs = soup.find_all('img')
        for img in imgs:
            img.decompose()
        return soup.root.encode_contents().decode()  # remove divs added

    @staticmethod
    def strip_itags(s, sections=None):
        soup, itag_list = AbstractTextRecord.find_all_itags(s)
        for itag in itag_list:
            try:
                if AbstractTextRecord._itag_is_footnote(itag):
                    itag.next_sibling.decompose()  # it's a footnote
            except AttributeError:
                pass  # it's an inline commentator
            itag.decompose()
        return soup.root.encode_contents().decode()  # remove divs added

    def _get_text_after_modifications(self, text_modification_funcs, start_sections=None):
        """
        :param text_modification_funcs: list(func). functions to apply in order on each segment in text chunk
        :return ja: Return jagged array after applying text_modification_funcs iteratively on each segment
        """
        if len(text_modification_funcs) == 0:
            return getattr(self, self.text_attr)

        def modifier(string, sections):
            for func in text_modification_funcs:
                string = func(string, sections)
            return string
        start_sections = None if start_sections is None else [s-1 for s in start_sections]  # zero-indexed for ja
        return self.ja().modify_by_function(modifier, start_sections)

    # Currently assumes that text is JA
    def _sanitize(self):
        setattr(self, self.text_attr,
                self.sanitize_text(getattr(self, self.text_attr, None))
        )

    def has_manually_wrapped_refs(self):
        return True


class Version(AbstractTextRecord, abst.AbstractMongoRecord, AbstractSchemaContent):
    """
    A version of a text.
    NOTE: AbstractTextRecord is inherited before AbstractMongoRecord in order to overwrite ALLOWED_TAGS
    Relates to a complete single record from the texts collection.

    A new version is created with a dict of correlating information inside. Two example fields are below:
    new_version = Version({"versionTitle": "ABCD",
                            "versionSource": "EFGHI"
                            ......})

    An existing version is queried for with a slightly different syntax:
    existing_version = Version().load({Mongo-query-for-that-specific-version})

    For basic operations such as loading, saving, and updating existing versions, see abst.AbstractMongoRecord
    in abstract.py - the parent class for the Version class.
    """
    history_noun = 'text'
    collection = 'texts'
    content_attr = "chapter"
    track_pkeys = True
    pkeys = ["title", "versionTitle"]

    required_attrs = [
        "language",
        "title",    # FK to Index.title
        "versionSource",
        "versionTitle",
        "chapter",  # required.  change to "content"?
        "actualLanguage",  # ISO language code
        'languageFamilyName', # full name of the language, but without specificity (for Judeo Arabic actualLanguage=jrb, languageFamilyName=arabic
        'isSource',  # bool, True if this version is not a translation
        'isPrimary',  # bool, True if we see it as a primary version (usually equals to isSource, but Hebrew Kuzarif or example is primary but not source)
        'direction',  # 'rtl' or 'ltr'
    ]

    """
    Regarding the strange naming of the parameters versionTitleInHebrew and versionNotesInHebrew: These names were
    chosen to avoid naming conflicts and ambiguity on the TextAPI. See TextFamily for more details.
    """
    optional_attrs = [
        "status",
        "priority",
        "license",
        "versionNotes",
        "formatAsPoetry",
        "digitizedBySefaria",
        "method",
        "heversionSource",  # bad data?
        "versionUrl",  # bad data?
        "versionTitleInHebrew",  # stores the Hebrew translation of the versionTitle
        "versionNotesInHebrew",  # stores VersionNotes in Hebrew
        "shortVersionTitle",
        "shortVersionTitleInHebrew",
        "extendedNotes",
        "extendedNotesHebrew",
        "purchaseInformationImage",
        "purchaseInformationURL",
        "hasManuallyWrappedRefs",  # true for texts where refs were manually wrapped in a-tags. no need to run linker at run-time.
    ]

    def __str__(self):
        return "Version: {} <{}>".format(self.title, self.versionTitle)

    def __repr__(self):  # Wanted to use orig_tref, but repr can not include Unicode
        return "{}().load({{'title': '{}', 'versionTitle': '{}'}})".format(self.__class__.__name__, self.title, self.versionTitle)

    def _validate(self):
        assert super(Version, self)._validate()
        """
        Old style database text record have a field called 'chapter'
        Version records in the wild have a field called 'text', and not always a field called 'chapter'
        """
        if getattr(self,"language", None) not in ["en", "he"]:
            raise InputError("Version language must be either 'en' or 'he'")
        index = self.get_index()
        if index is None:
            raise InputError("Versions cannot be created for non existing Index records")
        assert self._check_node_offsets(self.chapter, index.nodes), 'there are more sections than index_offsets_by_depth'
        if getattr(self, "direction", None) not in ["rtl", "ltr"]:
            raise InputError("Version direction must be either 'rtl' or 'ltr'")
        assert isinstance(getattr(self, "isSource", False), bool), "'isSource' must be bool"
        assert isinstance(getattr(self, "isPrimary", False), bool), "'isPrimary' must be bool"
        isAnyOtherVersionPrimary = any([v.isPrimary for v in VersionSet({"title": self.title}) if v.versionTitle != self.versionTitle])
        if not any([self.isPrimary, isAnyOtherVersionPrimary]):  # if all are False, return true
            raise InputError("There must be at least one version that is primary.")
        return True

    def _check_arrays_lengths(self, array1, array2):
        if len(array1) < len(array2):
            return False
        if isinstance(array1[0], list):
            for subarray1, subarray2 in zip(array1, array2):
                if not self._check_arrays_lengths(subarray1, subarray2):
                    return False
        return True

    def _check_node_offsets(self, content, node):
        if isinstance(content, list) and hasattr(node, 'index_offsets_by_depth'):
            for depth, nums in node.index_offsets_by_depth.items():
                if int(depth) > 1 and not self._check_arrays_lengths(nums, content):
                    return False
                elif depth == '1':
                    if not isinstance(nums, int):
                        return False
            return True
        elif isinstance(content, dict):
            for k, v in content.items():
                if not self._check_node_offsets(v, node.get_child_by_key(k)):
                    return False
        return True

    def _normalize(self):
        # add actualLanguage -- TODO: migration to get rid of bracket notation completely
        actualLanguage = getattr(self, "actualLanguage", None)
        versionTitle = getattr(self, "versionTitle", None)
        if not actualLanguage and versionTitle:
            languageCode = re.search(r"\[([a-z]{2})\]$", versionTitle)
            if languageCode and languageCode.group(1):
                actualLanguage = languageCode.group(1)
        self.actualLanguage = actualLanguage or self.language

        if not hasattr(self, 'languageFamilyName'):
            self.languageFamilyName = constants.LANGUAGE_CODES.get(self.actualLanguage) or constants.LANGUAGE_CODES[self.language]
        self.isSource = getattr(self, "isSource", self.actualLanguage == 'he')
        if not hasattr(self, "isPrimary"):
            self.isPrimary = self.isSource or not VersionSet({'title': self.title}) #first version is primary
        if not hasattr(self, 'direction'):
            self.direction = 'rtl' if self.language == 'he' else 'ltr'

        if getattr(self, "priority", None):
            try:
                self.priority = float(self.priority)
            except ValueError as e:
                self.priority = None
        self._trim_ending_whitespace()

    def _sanitize(self):
        # sanitization happens on TextChunk saving
        pass

    def get_index(self):
        return Index().load({'title': self.title})

    def first_section_ref(self):
        """
        Returns a :class:`Ref` to the first non-empty location in this version.
        """
        index = self.get_index()
        leafnodes = index.nodes.get_leaf_nodes()
        for leaf in leafnodes:
            try:
                ja = JaggedTextArray(self.content_node(leaf))

            except AttributeError:
                assert leaf.is_virtual
                return leaf.first_child().ref()

            indx_array = ja.next_index()
            if indx_array:
                oref = Ref(_obj={
                    "index": index,
                    "book": leaf.full_title("en"),
                    "primary_category": index.get_primary_category(),
                    "index_node": leaf,
                    "sections": [i + 1 for i in indx_array],
                    "toSections": [i + 1 for i in indx_array]
                })
                if index.is_complex() or index.nodes.depth != 1:
                    # For depth 1 texts, consider the first segment as the first section
                    oref = oref.section_ref()
                return oref
        return None

    def ja(self, remove_html=False):
        # the quickest way to check if this is a complex text
        if isinstance(getattr(self, self.text_attr, None), dict):
            nodes = self.get_index().nodes.get_leaf_nodes()
            if remove_html:
                return JaggedTextArray([AbstractTextRecord.remove_html(self.content_node(node)) for node in nodes if not node.is_virtual])
            else:
                return JaggedTextArray([self.content_node(node) for node in nodes])
        else:
            return super(Version, self).ja(remove_html=remove_html)

    def is_copyrighted(self):
        return "Copyright" in getattr(self, "license", "")

    def walk_thru_contents(self, action, heTref=None, schema=None, terms_dict=None):
        """
        Walk through the contents of a version and run `action` for each segment. Only required parameter to call is `action`
        :param func action: (segment_str, tref, he_tref, version) => None

        action() is a callback function that can have any behavior you would like. It should return None.
        A common use case is to define action() to append segments to a nonlocal array, to get an entire text of a
        Version in a list. The 'magic' of walk_thru_contents is that this function will iterate through the segments
        of the given Version, and apply the action() callback to each segment.

        Here's an example:

        .. highlight:: python
        .. code-block:: python

            all_text = []

            def action(segment_str, tref, he_tref, version):
                global all_text
                all_text.append(segment_str)

            talmud_berakhot = Version().load(
                {"title": 'Berakhot', "versionTitle": 'William Davidson Edition - English'})
            if talmud_berakhot:
                talmud_berakhot.walk_thru_contents(action)

        ...

        The result will be all_text populated with all segments from Masekhet Berakhot.

        """
        args = self.__initialize_walk_thru_contents_params(schema, heTref)
        return self.__walk_thru_contents_recursive(action, *args, terms_dict=terms_dict)

    def __initialize_walk_thru_contents_params(self, schema, heTref):
        item = self.chapter
        tref = self.title
        index = None
        if schema is None:
            index = self.get_index()
            schema = index.schema
        if heTref is None:
            heTref = index.get_title('he') if index else ""  # NOTE: heTref initialization is dependent on schema initialization
        addressTypes = None
        index_offsets_by_depth = None
        section_indexes = []

        return item, tref, schema, heTref, addressTypes, index_offsets_by_depth, section_indexes

    def __walk_thru_contents_recursive(self, action, *recursive_args, terms_dict=None):
        item = recursive_args[0]

        if isinstance(item, dict):
            self.__walk_thru_node_tree(action, *recursive_args, terms_dict=terms_dict)
        elif isinstance(item, list):
            self.__walk_thru_jagged_array(action, *recursive_args)
        elif isinstance(item, str):
            self.__apply_action_to_segment(action, *recursive_args)

    def __walk_thru_node_tree(self, action, item, tref, schema, heTref, *walk_thru_contents_args, terms_dict=None):
        def get_primary_title(lang, titles):
            return [t for t in titles if t.get("primary") and t.get("lang", "") == lang][0]["text"]

        for node in schema["nodes"]:
            try:
                is_virtual_node = VirtualNode in globals()[node.get("nodeType", "")].__bases__
            except KeyError:
                is_virtual_node = False
            if node.get("default", False) or is_virtual_node:
                node_title_en = node_title_he = ""
            elif node.get("sharedTitle", False):
                titles = terms_dict[node["sharedTitle"]]["titles"] if terms_dict is not None else Term().load({"name": node["sharedTitle"]}).titles
                node_title_en = ", " + get_primary_title("en", titles)
                node_title_he = ", " + get_primary_title("he", titles)
            else:
                node_title_en = ", " + get_primary_title("en", node["titles"])
                node_title_he = ", " + get_primary_title("he", node["titles"])

            if is_virtual_node:
                curr_ref = Ref(tref)
                vnode = next(x for x in curr_ref.index_node.children if hasattr(x, 'nodeType') and x.nodeType == node.get("nodeType", "") and x.firstWord == node["firstWord"])
                for vchild in vnode.all_children():
                    vstring = " ".join(vchild.get_text())
                    vref = vchild.ref()
                    self.__walk_thru_contents_recursive(action, vstring, vref.normal(), node, vref.he_normal(), *walk_thru_contents_args)
            else:
                self.__walk_thru_contents_recursive(action, item[node["key"]], tref + node_title_en, node, heTref + node_title_he, *walk_thru_contents_args)

    def __walk_thru_jagged_array(self, action, item, tref, schema, heTref, addressTypes, index_offsets_by_depth, section_indexes):
        if schema is not None:
            if addressTypes is None:
                addressTypes = schema.get("addressTypes", None)
            if index_offsets_by_depth is None:
                index_offsets_by_depth = schema.get("index_offsets_by_depth", None)

        for section_index, ja in enumerate(item):
            try:
                offset = JaggedArrayNode.get_index_offset(section_indexes, index_offsets_by_depth)
                next_section_indexes = section_indexes + [section_index+offset]
                self.__walk_thru_contents_recursive(action, ja, tref, {}, heTref, addressTypes, index_offsets_by_depth, next_section_indexes)
            except IndexError as e:
                print(str(e))
                print("index error for addressTypes {} ref {} - vtitle {}".format(addressTypes, tref, self.versionTitle))

    def __apply_action_to_segment(self, action, segment_str, tref, schema, heTref, addressTypes, index_offsets_by_depth, section_indexes):
        segment_tref = self.__add_sections_to_tref(tref, "en", addressTypes, section_indexes)
        segment_heTref = self.__add_sections_to_tref(heTref, "he", addressTypes, section_indexes)
        action(segment_str, segment_tref, segment_heTref, self)

    @staticmethod
    def __add_sections_to_tref(tref, lang, addressTypes, section_indexes):
        for depth, section_index in enumerate(section_indexes):
            section_str = AddressType.to_str_by_address_type(addressTypes[depth], lang, section_index+1)
            tref += f"{' ' if depth == 0 else ':'}{section_str}"
        return tref


class VersionSet(abst.AbstractMongoSet):
    """
    A collection of :class:`Version` objects

    You can call a VersionSet by running something like the following:
    my_version_set = VersionSet(mongo-query-here)

    Even if it yields only a single result, the results will always be a list of the matching versions
    that came up for the given query. 
    """
    recordClass = Version

    def __init__(self, query={}, page=0, limit=0, sort=[["priority", -1], ["_id", 1]], proj=None):
        super(VersionSet, self).__init__(query, page, limit, sort, proj)

    def word_count(self):
        return sum([v.word_count() for v in self])

    def char_count(self):
        return sum([v.char_count() for v in self])

    def verse_count(self):
        return sum([v.verse_count() for v in self])

    def merge(self, node=None, prioritized_vtitle=None):
        """
        Returns merged result, but does not change underlying data
        :param prioritized_vtitle: optional vtitle which should have top priority, even if it generally has lower priority
        """
        for v in self:
            if not getattr(v, "versionTitle", None):
                logger.error("No version title for Version: {}".format(vars(v)))
        if node is None:
            return merge_texts([getattr(v, "chapter", []) for v in self], [getattr(v, "versionTitle", None) for v in self])
        versions = self.array()
        if prioritized_vtitle:
            vindex = next((i for (i, v) in enumerate(versions) if v.versionTitle == prioritized_vtitle), None)
            if vindex is not None:
                # move versions[vindex] to front of list
                versions.insert(0, versions.pop(vindex))
        return merge_texts([v.content_node(node) for v in versions], [getattr(v, "versionTitle", None) for v in versions])


# used in VersionSet.merge(), merge_text_versions(), and export.export_merged()
# todo: move this to JaggedTextArray class?
# Doesn't work for complex texts
def merge_texts(text, sources):
    """
    This is a recursive function that merges the text in multiple
    translations to fill any gaps and deliver as much text as
    possible.
    e.g. [["a", ""], ["", "b", "c"]] becomes ["a", "b", "c"]
    """
    if not (len(text) and len(sources)):
        return ["", []]

    depth = list_depth(text)
    if depth > 2:
        results = []
        result_sources = []
        for x in range(max(list(map(len, text)))):    # Let longest text determine how many times to iterate
            translations = [_ for _ in itertools.zip_longest(*text)][x]  # transpose, and take section x
            remove_nones = lambda x: x or []
            result, source = merge_texts(list(map(remove_nones, translations)), sources)
            results.append(result)
            # NOTE - the below flattens the sources list, so downstream code can always expect
            # a one dimensional list, but in so doing the mapping of source names to segments
            # is lost for merged texts of depth > 2 (this mapping is not currenly used in general)
            result_sources += source
        return [results, result_sources]

    if depth == 1:
        text = [[x] for x in text]

    merged = itertools.zip_longest(*text)  # transpose
    text = []
    text_sources = []
    for verses in merged:
        # Look for the first non empty version (which will be the oldest, or one with highest priority)
        index, value = 0, ""
        for i, version in enumerate(verses):
            if version:
                index = i
                value = version
                break
        text.append(value)
        text_sources.append(sources[index])

    if depth == 1:
        # strings were earlier wrapped in lists, now unwrap
        text = text[0]
    return [text, text_sources]


class TextFamilyDelegator(type):
    """
    Metaclass to delegate virtual text records
    """

    def __call__(cls, *args, **kwargs):
        if len(args) >= 1:
            oref = args[0]
        else:
            oref = kwargs.get("oref")

        if oref and oref.index_node.is_virtual:
            return VirtualTextChunk(*args, **kwargs)
        else:
            return super(TextFamilyDelegator, cls).__call__(*args, **kwargs)


class TextRange:
    """
    This class is planned to replace TextChunk, using real language rather than he/en
    For now it's used by v3 texts api
    It can be used for getting text, but not yet for saving
    The versions param is for better performance when the version(s) were already loaded from mongo
    """

    def __init__(self, oref, lang, vtitle, merge_versions=False, versions=None):
        if isinstance(oref.index_node, JaggedArrayNode) or isinstance(oref.index_node, DictionaryEntryNode): #text cannot be SchemaNode
            self.oref = oref
        elif oref.has_default_child(): #use default child:
            self.oref = oref.default_child_ref()
        else:
            raise ComplexBookLevelRefError(book_ref=oref.normal())
        self.lang = lang
        self.vtitle = vtitle
        self.merge_versions = merge_versions
        self._text = None
        self.sources = None
        self._set_versions(versions)

    def _set_versions(self, versions):
        if versions:
            self._validate_versions(versions)
            self._versions = versions
        else:
            condition_query = self.oref.condition_query(self.lang) if self.merge_versions else \
                {'title': self.oref.index.title, 'languageFamilyName': self.lang, 'versionTitle': self.vtitle}
            self._versions = VersionSet(condition_query, proj=self.oref.part_projection())

    def _validate_versions(self, versions):
        if not self.merge_versions and len(versions) > 1:
            raise InputError("Got many versions instead of one")
        for version in versions:
            condition = version.title == self.oref.index.title and version.languageFamilyName == self.lang
            if not self.merge_versions:
                condition = condition and version.versionTitle == self.vtitle
            if not condition:
                raise InputError(f"Given version, {version}, is not matching to title, language or versionTitle")

    def _trim_text(self, text):
        """
        part_projection trims only the upper level of the jagged array. this function trims its lower levels and get rid of 1 element arrays wrappings
        """
        #TODO can we get the specific text directly from mongo?
        text = copy.deepcopy(text)
        for s, section in enumerate(self.oref.toSections[1:], 1): #start cut from end, for cutting from the start will change the indexes
            subtext = reduce(lambda x, _: x[-1], range(s), text)
            del subtext[section:]
        for s, section in enumerate(self.oref.sections[1:], 1):
            subtext = reduce(lambda x, _: x[0], range(s), text)
            del subtext[:section-1]
        matching_sections = itertools.takewhile(lambda pair: pair[0] == pair[1], zip(self.oref.sections, self.oref.toSections))
        redundant_depth = len(list(matching_sections))
        return reduce(lambda x, _: x[0], range(redundant_depth), text)

    @property
    def text(self):
        if self._text is None:
            if self.merge_versions and len(self._versions) > 1:
                merged_text, sources = self._versions.merge(self.oref.index_node, prioritized_vtitle=self.vtitle)
                self._text = self._trim_text(merged_text)
                if len(sources) > 1:
                    self.sources = sources
            elif self.oref.index_node.is_virtual:
                self._text = self.oref.index_node.get_text()
            else:
                self._text = self._trim_text(self._versions[0].content_node(self.oref.index_node)) #todo if there is no version it will fail
        return self._text


class TextChunk(AbstractTextRecord, metaclass=TextFamilyDelegator):
    """
    A chunk of text corresponding to the provided :class:`Ref`, language, and optional version name.
    If it is possible to get a more complete text by merging multiple versions, a merged result will be returned.

    :param oref: :class:`Ref`
    :param lang: "he" or "en". "he" means all rtl languages and "en" means all ltr languages
    :param vtitle: optional. Title of the version desired.
    :param actual_lang: optional. if vtitle isn't specified, prefer to find a version with ISO language `actual_lang`. As opposed to `lang` which can only be "he" or "en", `actual_lang` can be any valid 2 letter ISO language code.
    """

    text_attr = "text"

    def __init__(self, oref, lang="en", vtitle=None, exclude_copyrighted=False, actual_lang=None, fallback_on_default_version=False):
        """
        :param oref:
        :type oref: Ref
        :param lang: "he" or "en"
        :param vtitle:
        :return:
        """
        if isinstance(oref.index_node, JaggedArrayNode):
            self._oref = oref
        else:
            child_ref = oref.default_child_ref()
            if child_ref == oref:
                raise InputError("Can not get TextChunk at this level, please provide a more precise reference")
            self._oref = child_ref
        self._ref_depth = len(self._oref.sections)
        self._versions = []
        self._version_ids = None
        self._saveable = False  # Can this TextChunk be saved?

        self.lang = lang
        self.is_merged = False
        self.sources = []
        self.text = self._original_text = self.empty_text()
        self.vtitle = vtitle

        self.full_version = None
        self.versionSource = None  # handling of source is hacky

        if lang and vtitle and not fallback_on_default_version:
            self._saveable = True
            v = Version().load({"title": self._oref.index.title, "language": lang, "versionTitle": vtitle}, self._oref.part_projection())
            if exclude_copyrighted and v.is_copyrighted():
                raise InputError("Can not provision copyrighted text. {} ({}/{})".format(oref.normal(), vtitle, lang))
            if v:
                self._versions += [v]
                try:
                    self.text = self._original_text = self.trim_text(v.content_node(self._oref.index_node))
                except TypeError:
                    raise MissingKeyError(f'The version {vtitle} exists but has no key for the node {self._oref.index_node}')
        elif lang:
            if actual_lang is not None:
                self._choose_version_by_lang(oref, lang, exclude_copyrighted, actual_lang, prioritized_vtitle=vtitle)
            else:
                self._choose_version_by_lang(oref, lang, exclude_copyrighted, prioritized_vtitle=vtitle)
        else:
            raise Exception("TextChunk requires a language.")

    def _choose_version_by_lang(self, oref, lang: str, exclude_copyrighted: bool, actual_lang: str = None, prioritized_vtitle: str = None) -> None:
        if prioritized_vtitle:
            actual_lang = None
        vset = VersionSet(self._oref.condition_query(lang, actual_lang), proj=self._oref.part_projection())
        if len(vset) == 0:
            if VersionSet({"title": self._oref.index.title}).count() == 0:
                raise NoVersionFoundError("No text record found for '{}'".format(self._oref.index.title))
            return
        if len(vset) == 1:
            v = vset[0]
            if exclude_copyrighted and v.is_copyrighted():
                raise InputError("Can not provision copyrighted text. {} ({}/{})".format(oref.normal(), v.versionTitle, v.language))
            self._versions += [v]
            self.text = self.trim_text(v.content_node(self._oref.index_node))
            #todo: Should this instance, and the non-merge below, be made saveable?
        else:  # multiple versions available, merge
            if exclude_copyrighted:
                vset.remove(Version.is_copyrighted)
            merged_text, sources = vset.merge(self._oref.index_node, prioritized_vtitle=prioritized_vtitle)  #todo: For commentaries, this merges the whole chapter.  It may show up as merged, even if our part is not merged.
            self.text = self.trim_text(merged_text)
            if len(set(sources)) == 1:
                for v in vset:
                    if v.versionTitle == sources[0]:
                        self._versions += [v]
                        break
            else:
                self.sources = sources
                self.is_merged = True
                self._versions = vset.array()

    def __str__(self):
        args = "{}, {}".format(self._oref, self.lang)
        if self.vtitle:
            args += ", {}".format(self.vtitle)
        return args

    def __repr__(self):  # Wanted to use orig_tref, but repr can not include Unicode
        args = "{}, {}".format(self._oref, self.lang)
        if self.vtitle:
            args += ", {}".format(self.vtitle)
        return "{}({})".format(self.__class__.__name__, args)

    def version_ids(self):
        if self._version_ids is None:
            if self._versions:
                vtitle_query = [{'versionTitle': v.versionTitle} for v in self._versions]
                query = {"title": self._oref.index.title, "$or": vtitle_query}
                self._version_ids = VersionSet(query).distinct("_id")
            else:
                self._version_ids = []
        return self._version_ids

    def is_empty(self):
        return self.ja().is_empty()

    def ja(self, remove_html=False):
        if remove_html:
            return JaggedTextArray(AbstractTextRecord.remove_html(self.text))
        else:
            return JaggedTextArray(self.text)

    def save(self, force_save=False):
        """
        For editing in place (i.e. self.text[3] = "Some text"), it is necessary to set force_save to True. This is
        because by editing in place, both the self.text and the self._original_text fields will get changed,
        causing the save to abort.
        :param force_save: If set to True, will force a save even if no change was detected in the text.
        :return:
        """
        assert self._saveable, "Tried to save a read-only text: {}".format(self._oref.normal())
        assert not self._oref.is_range(), "Only non-range references can be saved: {}".format(self._oref.normal())
        #may support simple ranges in the future.
        #self._oref.is_range() and self._oref.range_index() == len(self._oref.sections) - 1
        if not force_save:
            if self.text == self._original_text:
                logger.warning("Aborted save of {}. No change in text.".format(self._oref.normal()))
                return False

        self._validate()
        self._sanitize()
        self._trim_ending_whitespace()

        if not self.version():
            self.full_version = Version(
                {
                    "chapter": self._oref.index.nodes.create_skeleton(),
                    "versionTitle": self.vtitle,
                    "versionSource": self.versionSource,
                    "language": self.lang,
                    "title": self._oref.index.title
                }
            )
        else:
            self.full_version = Version().load({"title": self._oref.index.title, "language": self.lang, "versionTitle": self.vtitle})
            assert self.full_version, "Failed to load Version record for {}, {}".format(self._oref.normal(), self.vtitle)
            if self.versionSource:
                self.full_version.versionSource = self.versionSource  # hack

        content = self.full_version.sub_content(self._oref.index_node.version_address())
        self._pad(content)
        self.full_version.sub_content(self._oref.index_node.version_address(), [i - 1 for i in self._oref.sections], self.text)

        self._check_available_text_pre_save()

        self.full_version.save()
        self._oref.recalibrate_next_prev_refs(len(self.text))
        self._update_link_language_availability()

        return self

    def _pad(self, content):
        """
        Pads the passed content to the dimension of self._oref.
        Acts on the input variable 'content' in place
        Does not yet handle ranges
        :param content:
        :return:
        """

        for pos, val in enumerate(self._oref.sections):
            # at pos == 0, parent_content == content
            # at pos == 1, parent_content == chapter
            # at pos == 2, parent_content == verse
            # etc
            parent_content = reduce(lambda a, i: a[i - 1], self._oref.sections[:pos], content)

            # Pad out existing content to size of ref
            if len(parent_content) < val:
                for _ in range(len(parent_content), val):
                    parent_content.append("" if pos == self._oref.index_node.depth - 1 else [])

            # check for strings where arrays expected, except for last pass
            if pos < self._ref_depth - 2 and isinstance(parent_content[val - 1], str):
                parent_content[val - 1] = [parent_content[val - 1]]

    def _check_available_text_pre_save(self):
        """
        Stores the availability of this text in before a save is made,
        so that we can know if segments have been added or deleted overall.
        """
        self._available_text_pre_save = {}
        langs_checked = [self.lang] # swtich to ["en", "he"] when global availability checks are needed
        for lang in langs_checked:
            try:
                self._available_text_pre_save[lang] = self._oref.text(lang=lang).text
            except NoVersionFoundError:
                self._available_text_pre_save[lang] = []

    def _check_available_segments_changed_post_save(self, lang=None):
        """
        Returns a list of tuples containing a Ref and a boolean availability
        for each Ref that was either made available or unavailble for `lang`.
        If `lang` is None, returns changed availability across all langauges.
        """
        if lang:
            old_refs_available = self._text_to_ref_available(self._available_text_pre_save[self.lang])
        else:
            # Looking for availability of in all langauges, merge results of Hebrew and English
            old_en_refs_available = self._text_to_ref_available(self._available_text_pre_save["en"])
            old_he_refs_available = self._text_to_ref_available(self._available_text_pre_save["he"])
            zipped = list(itertools.zip_longest(old_en_refs_available, old_he_refs_available))
            old_refs_available = []
            for item in zipped:
                en, he = item[0], item[1]
                ref = en[0] if en else he[0]
                old_refs_available.append((ref, (en and en[1] or he and he[1])))

        new_refs_available = self._text_to_ref_available(self.text)

        changed = []
        zipped = list(itertools.zip_longest(old_refs_available, new_refs_available))
        for item in zipped:
            old_text, new_text = item[0], item[1]
            had_previously = old_text and old_text[1]
            have_now = new_text and new_text[1]

            if not had_previously and have_now:
                changed.append(new_text)
            elif had_previously and not have_now:
                # Current save is deleting a line of text, but it could still be
                # available in a different version for this language. Check again.
                if lang:
                    text_still_available = bool(old_text[0].text(lang=lang).text)
                else:
                    text_still_available = bool(old_text[0].text("en").text) or bool(old_text[0].text("he").text)
                if not text_still_available:
                    changed.append([old_text[0], False])

        return changed

    def _text_to_ref_available(self, text):
        """Converts a JaggedArray of text to flat list of (Ref, bool) if text is availble"""
        flat = JaggedArray(text).flatten_to_array_with_indices()
        refs_available = []
        for item in flat:
            d = self._oref._core_dict()
            d["sections"] = d["sections"] + item[0]
            d["toSections"] = d["sections"]
            ref = Ref(_obj=d)
            available = bool(item[1])
            refs_available += [[ref, available]]
        return refs_available

    def _update_link_language_availability(self):
        """
        Check if current save has changed the overall availabilty of text for refs
        in this language, pass refs to update revelant links if so.
        """
        changed = self._check_available_segments_changed_post_save(lang=self.lang)

        if len(changed):
            from . import link
            for change in changed:
                link.update_link_language_availabiliy(change[0], self.lang, change[1])

    def _validate(self):
        """
        validate that depth/breadth of the TextChunk.text matches depth/breadth of the Ref
        :return:
        """
        posted_depth = 0 if isinstance(self.text, str) else list_depth(self.text)
        ref_depth = self._oref.range_index() if self._oref.is_range() else self._ref_depth
        implied_depth = ref_depth + posted_depth
        if implied_depth != self._oref.index_node.depth:
            raise InputError(
                "Text Structure Mismatch. The stored depth of {} is {}, but the text posted to {} implies a depth of {}."
                .format(self._oref.index_node.full_title(), self._oref.index_node.depth, self._oref.normal(), implied_depth)
            )

        #validate that length of the array matches length of the ref
        #todo: double check for depth >= 3
        if self._oref.is_spanning():
            span_size = self._oref.span_size()
            if posted_depth == 0: #possible?
                raise InputError(
                        "Text Structure Mismatch. {} implies a length of {} sections, but the text posted is a string."
                        .format(self._oref.normal(), span_size)
                )
            elif posted_depth == 1: #possible?
                raise InputError(
                        "Text Structure Mismatch. {} implies a length of {} sections, but the text posted is a simple list."
                        .format(self._oref.normal(), span_size)
                )
            else:
                posted_length = len(self.text)
                if posted_length != span_size:
                    raise InputError(
                        "Text Structure Mismatch. {} implies a length of {} sections, but the text posted has {} elements."
                        .format(self._oref.normal(), span_size, posted_length)
                    )
                #todo: validate last section size if provided

        elif self._oref.is_range():
            range_length = self._oref.range_size()
            if posted_depth == 0:
                raise InputError(
                        "Text Structure Mismatch. {} implies a length of {}, but the text posted is a string."
                        .format(self._oref.normal(), range_length)
                )
            elif posted_depth == 1:
                posted_length = len(self.text)
                if posted_length != range_length:
                    raise InputError(
                        "Text Structure Mismatch. {} implies a length of {}, but the text posted has {} elements."
                        .format(self._oref.normal(), range_length, posted_length)
                    )
            else:  # this should never happen.  The depth check should catch it.
                raise InputError(
                    "Text Structure Mismatch. {} implies an simple array of length {}, but the text posted has depth {}."
                    .format(self._oref.normal(), range_length, posted_depth)
                )

    #maybe use JaggedArray.subarray()?
    def trim_text(self, txt):
        """
        Trims a text loaded from Version record with self._oref.part_projection() to the specifications of self._oref
        This works on simple Refs and range refs of unlimited depth and complexity.
        (in place?)
        :param txt:
        :return: List|String depending on depth of Ref
        """
        range_index = self._oref.range_index()
        sections = self._oref.sections
        toSections = self._oref.toSections

        if not sections:
            pass
        else:
            for i in range(0, self._ref_depth):
                if i == 0 == range_index:  # First level slice handled at DB level
                    pass
                elif range_index > i:  # Either not range, or range begins later.  Return simple value.
                    if i == 0 and len(txt):   # We already sliced the first level w/ Ref.part_projection()
                        txt = txt[0]
                    elif len(txt) >= sections[i]:
                        txt = txt[sections[i] - 1]
                    else:
                        return self.empty_text()
                elif range_index == i:  # Range begins here
                    start = sections[i] - 1
                    end = toSections[i]
                    txt = txt[start:end]
                else:  # range_index < i, range continues here
                    begin = end = txt
                    for _ in range(range_index, i - 1):
                        begin = begin[0]
                        end = end[-1]
                    begin[0] = begin[0][sections[i] - 1:]
                    end[-1] = end[-1][:toSections[i]]

        return txt

    def empty_text(self):
        """
        :return: Either empty array or empty string, depending on depth of Ref
        """
        if not self._oref.is_range() and self._ref_depth == self._oref.index_node.depth:
            return ""
        else:
            return []

    def version(self):
        """
        Returns the Version record for this chunk
        :return Version:
        :raises Exception: if the TextChunk is merged
        """
        if not self._versions:
            return None
        if len(self._versions) == 1:
            return self._versions[0]
        else:
            raise Exception("Called TextChunk.version() on merged TextChunk.")

    def has_manually_wrapped_refs(self):
        try:
            return getattr(self.version(), 'hasManuallyWrappedRefs', False)
        except:
            # merged version
            return False

    def nonempty_segment_refs(self):
        """

        :return: list of segment refs with content in this TextChunk
        """
        r = self._oref
        ref_list = []


        if r.is_range():
            input_refs = r.range_list()
        else:
            input_refs = [r]
        for temp_ref in input_refs:
            temp_tc = temp_ref.text(lang=self.lang, vtitle=self.vtitle)
            ja = temp_tc.ja()
            jarray = ja.mask().array()

            #TODO do I need to check if this ref exists for this version?
            if temp_ref.is_segment_level():
                if jarray: #it's an int if ref is segment_level
                    ref_list.append(temp_ref)
            elif temp_ref.is_section_level():
                ref_list += [temp_ref.subref(i + 1) for i, v in enumerate(jarray) if v]
            else: # higher than section level
                ref_list += [temp_ref.subref([j + 1 for j in ne] + [i + 1])
                             for ne in ja.non_empty_sections()
                             for i, v in enumerate(ja.subarray(ne).mask().array()) if v]

        return ref_list

    def find_string(self, regex_str, cleaner=lambda x: x, strict=True):
        """
        Regex search in TextChunk
        :param regex_str: regex string to search for
        :param cleaner: f(str)->str. function to clean a semgent before searching
        :param strict: if True, throws error if len(ind_list) != len(ref_list). o/w truncates longer array to length of shorter
        :return: list[(Ref, Match, str)] - list of tuples. each tuple has a segment ref, match object for the match, and text for the segment
        """
        ref_list = self.nonempty_segment_refs()
        text_list = [x for x in self.ja().flatten_to_array() if len(x) > 0]
        if len(text_list) != len(ref_list):
            if strict:
                raise ValueError("The number of refs doesn't match the number of starting words. len(refs)={} len(inds)={}".format(len(ref_list),len(ind_list)))
            else:
                print("Warning: The number of refs doesn't match the number of starting words. len(refs)={} len(inds)={} {}".format(len(ref_list),len(ind_list),str(self._oref)))

        matches = []
        for r, t in zip(ref_list, text_list):
            cleaned = cleaner(t)
            for m in re.finditer(regex_str,cleaned):
                matches += [(r, m, cleaned)]

        return matches

    def text_index_map(self, tokenizer=lambda x: re.split(r'\s+', x), strict=True, ret_ja=False):
        """
        Primarily used for depth-2 texts in order to get index/ref pairs relative to the full text string
         indexes are the word index in word_list

        tokenizer: f(str)->list(str) - function to split up text
        strict: if True, throws error if len(ind_list) != len(ref_list). o/w truncates longer array to length of shorter
        :param ret_ja: True if you want to return the flattened ja
        :return: (list,list) - index_list (0 based index of start word of each segment ref as compared with the text chunk ref), ref_list
        """
        #TODO there is a known error that this will fail if the text version you're using has fewer segments than the VersionState.
        ind_list = []
        ref_list = self.nonempty_segment_refs()

        total_len = 0
        text_list = self.ja().flatten_to_array()
        for i,segment in enumerate(text_list):
            if len(segment) > 0:
                ind_list.append(total_len)
                total_len += len(tokenizer(segment))

        if len(ind_list) != len(ref_list):
            if strict:
                raise ValueError("The number of refs doesn't match the number of starting words. len(refs)={} len(inds)={}".format(len(ref_list),len(ind_list)))
            else:
                print("Warning: The number of refs doesn't match the number of starting words. len(refs)={} len(inds)={} {}".format(len(ref_list),len(ind_list),str(self._oref)))
                if len(ind_list) > len(ref_list):
                    ind_list = ind_list[:len(ref_list)]
                else:
                    ref_list = ref_list[:len(ind_list)]

        if ret_ja:
            return ind_list, ref_list, total_len, text_list
        else:
            return ind_list, ref_list, total_len


class VirtualTextChunk(AbstractTextRecord):
    """
    Delegated from TextChunk
    Should only arrive here if oref.index_node is virtual.
    """

    text_attr = "text"

    def __init__(self, oref, lang="en", vtitle=None, exclude_copyrighted=False, actual_lang=None, fallback_on_default_version=False):

        self._oref = oref
        self._ref_depth = len(self._oref.sections)
        self._saveable = False

        self.lang = lang
        self.is_merged = False
        self.sources = []

        if self._oref.index_node.parent and not self._oref.index_node.parent.supports_language(self.lang):
            self.text = []
            self._versions = []
            return

        try:
            self.text = self._oref.index_node.get_text()  # <- This is where the magic happens
        except:
            self.text = []
            self._versions = []
            return

        v = Version().load({
            "title": self._oref.index_node.get_index_title(),
            "versionTitle": self._oref.index_node.get_version_title(self.lang),
            "language": self.lang
        }, {"chapter": 0})    # Currently vtitle is thrown out.  There's only one version of each lexicon.
        self._versions = [v] if v else []

    def version(self):
        return self._versions[0] if self._versions else None

    def version_ids(self):
        return [self._versions[0]._id] if self._versions else []

    def has_manually_wrapped_refs(self):
        return not getattr(self._oref.index_node.parent.lexicon, 'needsRefsWrapping', False)


# This was built as a bridge between the object model and existing front end code, so has some hallmarks of that legacy.
class TextFamily(object):
    """
    A text with its translations and optionally the commentary on it.

    Can be instantiated with just the first argument.

    :param oref: :class:`Ref`.  This is the only required argument.
    :param int context: Default: 1. How many context levels up to go when getting commentary.  See :func:`Ref.context_ref`
    :param bool commentary: Default: True. Include commentary?
    :param version: optional. Name of version to use when getting text.
    :param lang: None, "en" or "he".  Default: None.  If None, include both languages.
    :param version2: optional. Additional name of version to use.
    :param bool pad: Default: True.  Pads the provided ref before processing.  See :func:`Ref.padded_ref`
    :param bool alts: Default: False.  Adds notes of where alternate structure elements begin
    """

    ## Attribute maps used for generating dict format ##
    """
    A bit of a naming conflict has arisen here. The TextFamily bundles two versions - one with English text and one
    with Hebrew text. versionTitle refers to the English title of the English version, while heVersionTitle refers to
    the English title of the Hebrew version.

    Later on we decided to translate all of our versionTitles into Hebrew. To avoid direct conflict with the text api,
    these got the names versionTitleInHebrew and versionNotesInHebrew.
    """
    text_attr_map = {
        "en": "text",
        "he": "he"
    }

    attr_map = {
        "versionTitle": {
            "en": "versionTitle",
            "he": "heVersionTitle"
        },
        "versionTitleInHebrew": {
            "en": "versionTitleInHebrew",
            "he": "heVersionTitleInHebrew",
        },
        "shortVersionTitle": {
            "en": "shortVersionTitle",
            "he": "heShortVersionTitle",
        },
        "shortVersionTitleInHebrew": {
            "en": "shortVersionTitleInHebrew",
            "he": "heShortVersionTitleInHebrew",
        },
        "versionSource": {
            "en": "versionSource",
            "he": "heVersionSource"
        },
        "status": {
            "en": "versionStatus",
            "he": "heVersionStatus"
        },
        "versionNotes": {
            "en": "versionNotes",
            "he": "heVersionNotes"
        },
        "extendedNotes": {
            "en": "extendedNotes",
            "he": "heExtendedNotes"
        },
        "extendedNotesHebrew": {
            "en": "extendedNotesHebrew",
            "he": "heExtendedNotesHebrew"
        },
        "versionNotesInHebrew": {
            "en": "versionNotesInHebrew",
            "he": "heVersionNotesInHebrew",
        },
        "digitizedBySefaria": {
            "en": "digitizedBySefaria",
            "he": "heDigitizedBySefaria",
            "default": False,
        },
        "license": {
            "en": "license",
            "he": "heLicense",
            "default": "unknown"
        },
        "formatAsPoetry": { # Setup for Fox translation. Perhaps we want in other places as well?
            "he": "formatHeAsPoetry",
            "en": "formatEnAsPoetry",
            "default": False,
        }
    }
    sourceMap = {
        "en": "sources",
        "he": "heSources"
    }

    def __init__(self, oref, context=1, commentary=True, version=None, lang=None,
                 version2=None, lang2=None, pad=True, alts=False, wrapLinks=False, stripItags=False,
                 wrapNamedEntities=False, translationLanguagePreference=None, fallbackOnDefaultVersion=False):
        """
        :param oref:
        :param context:
        :param commentary:
        :param version:
        :param lang:
        :param version2:
        :param lang2:
        :param pad:
        :param alts: Adds notes of where alt elements begin
        :param wrapLinks: whether to return the text requested with all internal citations marked up as html links <a>
        :param stripItags: whether to strip inline commentator tags and inline footnotes from text
        :param wrapNamedEntities: whether to return the text requested with all known named entities marked up as html links <a>.
        :return:
        """
        if pad:
            oref = oref.padded_ref()
        elif oref.has_default_child():
            oref = oref.default_child_ref()

        if version:
            version = version.replace("_", " ")
        if version2:
            version2 = version2.replace("_", " ")

        self.ref            = oref.normal()
        self.heRef          = oref.he_normal()
        self.isComplex      = oref.index.is_complex()
        self.text           = None
        self.he             = None
        self._nonExistantVersions = {}
        self._lang          = lang
        self._original_oref = oref
        self._context_oref  = None
        self._chunks        = {}
        self._inode         = oref.index_node
        self._alts          = []

        if not isinstance(oref.index_node, JaggedArrayNode) and not oref.index_node.is_virtual:
            raise InputError("Unable to find text for that ref")

        for i in range(0, context):
            oref = oref.context_ref()
        self._context_oref = oref

        # processes "en" and "he" TextChunks, and puts the text in self.text and self.he, respectively.
        for language, attr in list(self.text_attr_map.items()):
            tc_kwargs = dict(oref=oref, lang=language, fallback_on_default_version=fallbackOnDefaultVersion)
            if language == 'en': tc_kwargs['actual_lang'] = translationLanguagePreference
            if language in {lang, lang2}:
                curr_version = version if language == lang else version2
                c = TextChunk(vtitle=curr_version, **tc_kwargs)
                if len(c._versions) == 0:  # indicates `version` doesn't exist
                    if tc_kwargs.get('actual_lang', False) and not curr_version:
                        # actual_lang is only used if curr_version is not passed
                        tc_kwargs.pop('actual_lang', None)
                        c = TextChunk(vtitle=curr_version, **tc_kwargs)
                    elif curr_version:
                        self._nonExistantVersions[language] = curr_version
            else:
                c = TextChunk(**tc_kwargs)
            self._chunks[language] = c
            text_modification_funcs = []
            if wrapNamedEntities and len(c._versions) > 0:
                from . import RefTopicLinkSet
                named_entities = RefTopicLinkSet({"expandedRefs": {"$in": [r.normal() for r in oref.all_segment_refs()]}, "charLevelData.versionTitle": c._versions[0].versionTitle, "charLevelData.language": language})
                if len(named_entities) > 0:
                    # assumption is that refTopicLinks are all to unranged refs
                    ne_by_secs = defaultdict(list)
                    for ne in named_entities:
                        try:
                            temp_ref = Ref(ne.ref)
                        except InputError:
                            continue
                        temp_secs = tuple(s-1 for s in temp_ref.sections)
                        ne_by_secs[temp_secs] += [ne]
                    text_modification_funcs += [lambda s, secs: library.get_wrapped_named_entities_string(ne_by_secs[tuple(secs)], s)]
            if stripItags:
                text_modification_funcs += [lambda s, secs: c.strip_itags(s), lambda s, secs: ' '.join(s.split()).strip()]
            if wrapLinks and c.version_ids() and not c.has_manually_wrapped_refs():
                #only wrap links if we know there ARE links- get the version, since that's the only reliable way to get it's ObjectId
                #then count how many links came from that version. If any- do the wrapping.
                from . import Link
                query = oref.ref_regex_query()
                query.update({"inline_citation": True})  # , "source_text_oid": {"$in": c.version_ids()}
                if Link().load(query) is not None:
                    link_wrapping_reg, title_nodes = library.get_regex_and_titles_for_ref_wrapping(c.ja().flatten_to_string(), lang=language, citing_only=True)
                    text_modification_funcs += [lambda s, secs: library.get_wrapped_refs_string(s, lang=language, citing_only=True, reg=link_wrapping_reg, title_nodes=title_nodes)]
            padded_sections, _ = oref.get_padded_sections()
            setattr(self, self.text_attr_map[language], c._get_text_after_modifications(text_modification_funcs, start_sections=padded_sections))

        if oref.is_spanning():
            self.spanning = True
        #// todo: should this parameter be renamed? it gets all links, not strictly commentary...
        if commentary:
            from sefaria.client.wrapper import get_links
            if not oref.is_spanning():
                links = get_links(oref.normal())  #todo - have this function accept an object
            else:
                links = [get_links(r.normal()) for r in oref.split_spanning_ref()]
            self.commentary = links if "error" not in links else []

        # get list of available versions of this text
        self.versions = oref.version_list()

        # Adds decoration for the start of each alt structure reference
        if alts:
            self._alts = oref.index.get_trimmed_alt_structs_for_ref(oref)
        if self._inode.is_virtual:
            self._index_offsets_by_depth = None
        else:
            self._index_offsets_by_depth = self._inode.trim_index_offsets_by_sections(oref.sections, oref.toSections)

    def contents(self):
        """
        :return dict: Returns the contents of the text family.
        """
        d = {k: getattr(self, k) for k in list(vars(self).keys()) if k[0] != "_"}

        d["textDepth"]       = getattr(self._inode, "depth", None)
        d["sectionNames"]    = getattr(self._inode, "sectionNames", None)
        d["addressTypes"]    = getattr(self._inode, "addressTypes", None)
        if getattr(self._inode, "lengths", None):
            d["lengths"]     = getattr(self._inode, "lengths")
            if len(d["lengths"]):
                d["length"]  = d["lengths"][0]
        elif getattr(self._inode, "length", None):
            d["length"]      = getattr(self._inode, "length")
        d["textDepth"]       = self._inode.depth
        d["heTitle"]         = self._inode.full_title("he")
        d["titleVariants"]   = self._inode.all_tree_titles("en")
        d["heTitleVariants"] = self._inode.all_tree_titles("he")
        d["type"]            = getattr(self._original_oref, "primary_category")
        d["primary_category"] = getattr(self._original_oref, "primary_category")
        d["book"]            = getattr(self._original_oref, "book")

        for attr in ["categories", "order"]:
            d[attr] = getattr(self._inode.index, attr, "")
        for attr in ["sections", "toSections"]:
            d[attr] = getattr(self._original_oref, attr)[:]

        if getattr(self._inode.index, 'collective_title', None):
            d["commentator"] = getattr(self._inode.index, 'collective_title', "") # todo: deprecate Only used in s1 js code
            d["heCommentator"] = hebrew_term(getattr(self._inode.index, 'collective_title', "")) # todo: deprecate Only used in s1 js code
            d["collectiveTitle"] = getattr(self._inode.index, 'collective_title', "")
            d["heCollectiveTitle"] = hebrew_term(getattr(self._inode.index, 'collective_title', ""))

        if len(self._nonExistantVersions) > 0:
            d['nonExistantVersions'] = self._nonExistantVersions

        if self._inode.index.is_dependant_text():
            #d["commentaryBook"] = getattr(self._inode.index, 'base_text_titles', "")
            #d["commentaryCategories"] = getattr(self._inode.index, 'related_categories', [])
            d["baseTexTitles"] = getattr(self._inode.index, 'base_text_titles', [])

        d["isComplex"]    = self.isComplex
        d["isDependant"] = self._inode.index.is_dependant_text()
        d["indexTitle"]   = self._inode.index.title
        d["heIndexTitle"] = self._inode.index.get_title("he")
        d["sectionRef"]   = self._original_oref.section_ref().normal()
        try:
            d["firstAvailableSectionRef"] = self._original_oref.first_available_section_ref().normal()
        except AttributeError:
            pass
        d["heSectionRef"] = self._original_oref.section_ref().he_normal()
        d["isSpanning"]   = self._original_oref.is_spanning()
        if d["isSpanning"]:
            d["spanningRefs"] = [r.normal() for r in self._original_oref.split_spanning_ref()]

        for language, attr in list(self.text_attr_map.items()):
            chunk = self._chunks.get(language)
            if chunk.is_merged:
                d[self.sourceMap[language]] = chunk.sources
            else:
                ver = chunk.version()
                if ver:
                    for key, val in list(self.attr_map.items()):
                        d[val[language]] = getattr(ver, key, val.get("default", ""))

        # replace ints with daf strings (3->"2a") for Talmud addresses
        # this could be simpler if was done for every value - but would be slower.
        if "Talmud" in self._inode.addressTypes:
            for i in range(len(d["sections"])):
                if self._inode.addressTypes[i] == "Talmud":
                    d["sections"][i] = AddressTalmud.toStr("en", d["sections"][i])
                    if "toSections" in d:
                        d["toSections"][i] = AddressTalmud.toStr("en", d["toSections"][i])

            d["title"] = self._context_oref.normal()
            if "heTitle" in d:
                d["heBook"] = d["heTitle"]
                d["heTitle"] = self._context_oref.he_normal()
            """if d["type"] == "Commentary" and self._context_oref.is_talmud() and len(d["sections"]) > 1:
                d["title"] = "%s Line %d" % (d["title"], d["sections"][1])"""

        """elif self._context_oref.is_commentary():
            dep = len(d["sections"]) if len(d["sections"]) < 2 else 2
            d["title"] = d["book"] + " " + ":".join(["%s" % s for s in d["sections"][:dep]])"""

        d["alts"] = self._alts
        d['index_offsets_by_depth'] = self._index_offsets_by_depth

        return d

"""
                    -------------------
                           Refs
                    -------------------

"""


class RefCacheType(type):
    """
    Metaclass for Ref class.
    Caches all Ref instances according to the string they were instantiated with and their normal form.
    Returns cached instance on instantiation if either instantiation string or normal form are matched.
    """

    def __init__(cls, name, parents, dct):
        super(RefCacheType, cls).__init__(name, parents, dct)
        cls.__tref_oref_map = {}
        cls.__index_tref_map = {}

    def cache_size(cls):
        return len(cls.__tref_oref_map)

    def cache_size_bytes(cls):
        from sefaria.utils.util import get_size
        return get_size(cls.__tref_oref_map)

    def cache_dump(cls):
        return [(a, repr(b)) for (a, b) in cls.__tref_oref_map.items()]

    def _raw_cache(cls):
        return cls.__tref_oref_map

    def clear_cache(cls):
        cls.__tref_oref_map = {}
        cls.__index_tref_map = {}

    def remove_index_from_cache(cls, index_title):
        """
        Removes all refs to Index with title `index_title` from the Ref cache
        :param cls:
        :param index_title:
        :return:
        """
        try:
            for tref in cls.__index_tref_map[index_title]:
                try:
                    del cls.__tref_oref_map[tref]
                except KeyError:
                    continue
        except KeyError:
            pass

    def __call__(cls, *args, **kwargs):
        if len(args) == 1:
            tref = args[0]
        else:
            tref = kwargs.get("tref")

        obj_arg = kwargs.get("_obj")

        if tref:
            if tref in cls.__tref_oref_map:
                return cls.__tref_oref_map[tref]
            else:
                result = super(RefCacheType, cls).__call__(*args, **kwargs)
                uid = result.uid()
                title = result.index.title
                if uid in cls.__tref_oref_map:
                    #del result  #  Do we need this to keep memory clean?
                    cls.__tref_oref_map[tref] = cls.__tref_oref_map[uid]
                    try:
                        cls.__index_tref_map[title] += [tref]
                    except KeyError:
                        cls.__index_tref_map[title] = [tref]
                    return cls.__tref_oref_map[uid]
                cls.__tref_oref_map[uid] = result
                cls.__tref_oref_map[tref] = result
                try:
                    cls.__index_tref_map[title] += [tref]
                except KeyError:
                    cls.__index_tref_map[title] = [tref]
                cls.__index_tref_map[title] += [uid]

                return result
        elif obj_arg:
            result = super(RefCacheType, cls).__call__(*args, **kwargs)
            uid = result.uid()
            title = result.index.title
            if uid in cls.__tref_oref_map:
                #del result  #  Do we need this to keep memory clean?
                return cls.__tref_oref_map[uid]
            cls.__tref_oref_map[uid] = result
            try:
                cls.__index_tref_map[title] += [uid]
            except KeyError:
                cls.__index_tref_map[title] = [uid]
            return result
        else:  # Default.  Shouldn't be used.
            return super(RefCacheType, cls).__call__(*args, **kwargs)


class Ref(object, metaclass=RefCacheType):
    """
        A Ref is a reference to a location. A location could be to a *book*, to a specific *segment* (e.g. verse or mishnah), to a *section* (e.g chapter), or to a *range*.

        Instantiated with a string representation of the reference, e.g.:

        ::

            >>> Ref("Genesis 1:3")
            >>> Ref("Rashi on Genesis 1:3")
            >>> Ref("Genesis 1:3-2:4")
            >>> Ref("Shabbat 4b")
            >>> Ref("Rashi on Shabbat 4b-5a")
    """

    __slots__ = (
        'index', 'book', 'primary_category', 'sections', 'toSections', 'index_node',
        '_lang', 'tref', 'orig_tref', '_normal', '_he_normal', '_url', '_next', '_prev',
        '_padded', '_context', '_first_spanned_ref', '_spanned_refs', '_ranged_refs',
        '_range_depth', '_range_index', 'legacy_tref',
    )

    def __init__(self, tref=None, _obj=None):
        """
        Object is generally initialized with a textual reference - ``tref``

        Internally, the _obj argument can be used to instantiate a ref with a complete dict composing the Ref data
        """
        self.index = None
        self.book = None
        self.primary_category = None  # used to be named 'type' but that was very confusing
        self.sections = []
        self.toSections = []
        self.index_node = None
        self.__init_ref_pointer_vars()

        if tref:
            self.orig_tref = tref
            self._lang = "he" if is_all_hebrew(tref) else "en"
            self.tref = self.__clean_tref(tref, self._lang)
            self.__init_tref()
            self._validate()
        elif _obj:
            for key, value in list(_obj.items()):
                setattr(self, key, value)
            self.tref = self.normal()
            self._validate()

    def __init_ref_pointer_vars(self):
        self._normal = None
        self._he_normal = None
        self._url = None
        self._next = None
        self._prev = None
        self._padded = None
        self._context = None
        self._first_spanned_ref = None
        self._spanned_refs = None
        self._ranged_refs = None
        self._range_depth = None
        self._range_index = None

    def _validate(self):
        self.__validate_sections_in_range()
        self.__validate_toSections()

    def __validate_sections_in_range(self):
        checks = [self.sections, self.toSections]
        for check in checks:
            for c, che in enumerate(check):
                if che < 1:
                    raise InputError(f'{self.book} {"".join([str(x) for x in check[:c]])} starts at {1+self._get_offset([x-1 for x in check[:c]])}')
            if getattr(self.index_node, "lengths", None) and len(check):
                if check[0] > self.index_node.lengths[0]:
                    display_size = self.index_node.address_class(0).toStr("en", self.index_node.lengths[0])
                    raise InputError("{} ends at {} {}.".format(self.book, self.index_node.sectionNames[0], display_size))

    def __validate_toSections(self):
        if len(self.sections) != len(self.toSections):
            raise InputError("{} is an invalid range. depth of beginning of range must equal depth of end of range")

        for i in range(len(self.sections)):
            if self.toSections[i] > self.sections[i]:
                break
            if self.toSections[i] < self.sections[i]:
                raise InputError("{} is an invalid range.  Ranges must end later than they begin.".format(self.normal()))

    @staticmethod
    def __clean_tref(tref, lang):
        tref = tref.strip().replace("", "-").replace("\u2011", "-").replace("_", " ")

        # don't replace : in Hebrew, where it can indicate amud
        if lang == "he":
            return tref

        tref = tref.replace(":", ".")

        try:
            # capitalize first letter (don't title case all to avoid e.g., "Song Of Songs")
            tref = tref[0].upper() + tref[1:]
        except IndexError:
            pass

        return tref

    def __reinit_tref(self, new_tref):
        logger.debug("__reinit_tref from {} to {}".format(self.tref, new_tref))
        self.tref = self.__clean_tref(new_tref, self._lang)
        self._lang = "en"
        self.__init_tref()

    def __init_tref(self):
        """
        Parse self.tref
        Populate self.index, self.index_node, self.type, self.book, self.sections, self.toSections, ...
        :return:
        """
        # Split ranges based on all '-' symbol, store in `parts` variable
        parts = [s.strip() for s in re.split("[-\u2010-\u2015]", self.tref)]
        if len(parts) > 2:
            raise InputError("Couldn't understand ref '{}' (too many -'s).".format(self.tref))
        if any([not p for p in parts]):
            raise InputError("Couldn't understand ref '{}' (beginning or ending -)".format(self.tref))

        base = parts[0]
        title = None

        tndict = library.get_title_node_dict(self._lang)
        termdict = library.get_term_dict(self._lang)

        self.index_node = tndict.get(base)  # Match index node before term
        if not self.index_node:
            new_tref = termdict.get(base)   # Check if there's a term match, reinit w/ term
            if new_tref:
                self.__reinit_tref(new_tref)
                return

        # Remove letter from end of base reference until TitleNode matched, set `title` variable with matched title
        for l in range(len(base), 0, -1):
            if l != len(base) and base[l] not in ' ,.:_':
                continue #do not stop in the middle of a word

            self.index_node = tndict.get(base[0:l])

            if self.index_node:
                title = base[0:l]
                if base[l - 1] == "." and l < len(base):   # Take care of Refs like "Exo.14.15", where the period shouldn't get swallowed in the name.
                    title = base[0:l - 1]
                break

        # At this point, `title` is something like "Exodus" or "Rashi on Exodus" or "Pesach Haggadah, Magid, Four Sons"
        if title:
            assert isinstance(self.index_node, TitledTreeNode)
            self.index = self.index_node.index
            self.book = self.index_node.full_title("en")

            # checkFirst is used on Bavli records to check for a Mishnah pattern match first
            if getattr(self.index_node, "checkFirst", None) and self.index_node.checkFirst.get(self._lang):
                try:
                    check_node = library.get_schema_node(self.index_node.checkFirst[self._lang], self._lang)
                    assert isinstance(check_node, JaggedArrayNode)  # Initially used with Mishnah records.  Assumes JaggedArray.
                    reg = check_node.full_regex(title, self._lang, strict=True)
                    self.sections = self.__get_sections(reg, base, use_node=check_node)
                except InputError:  # Regex doesn't work
                    pass
                except AttributeError:  # Can't find node for check_node
                    pass
                else:
                    old_index_node = self.index_node

                    self.index_node = check_node
                    self.index = self.index_node.index
                    self.book = self.index_node.full_title("en")
                    self.toSections = self.sections[:]

                    try:
                        self._validate()
                    except InputError:  # created Ref doesn't validate, back it out
                        self.index_node = old_index_node
                        self.sections = []

            # Don't accept references like "Rashi" (deleted in commentary refactor)

        else:  # This may be a new version, try to build a schema node.
            raise InputError("Could not find title in reference: {}".format(self.tref))

        self.primary_category = self.index.get_primary_category()
        if title == base:  # Bare book, like "Genesis" or "Rashi on Genesis".
            if self.index_node.is_default():  # Without any further specification, match the parent of the fall-through node
                self.index_node = self.index_node.parent
                self.book = self.index_node.full_title("en")
            return

        reg = None
        try:
            reg = self.index_node.full_regex(title, self._lang, terminated=True)  # Try to treat this as a JaggedArray
        except AttributeError:
            if self.index_node.is_virtual:
                # The line below will raise InputError (or DictionaryEntryNotFoundError) if no match
                self.index_node = self.index_node.create_dynamic_node(title, base)
                self.book = self.index_node.full_title("en")
                self.sections = self.index_node.get_sections()
                self.toSections = self.sections[:]
                return

            elif self.index.has_alt_structures():
                # Give an opportunity for alt structure parsing, below
                pass

            else:
                # We matched a schema node followed by an illegal number. (Are there other cases here?)
                matched = self.index_node.full_title(self._lang)
                msg = "Partial reference match for '{}' - failed to find continuation for '{}'.\nValid continuations are:\n".format(self.tref, matched)
                continuations = []
                for child in self.index_node.children:
                    continuations += child.all_node_titles(self._lang)
                msg += ",\n".join(continuations)
                raise PartialRefInputError(msg, matched, continuations)

        # Numbered Structure node - try numbered structure parsing
        if reg and self.index_node.children and getattr(self.index_node, "_addressTypes", None):
            try:
                loose_reg = self.index_node.full_regex(title, self._lang)
                struct_indexes = self.__get_sections(loose_reg, base)
                self.index_node = reduce(lambda a, i: a.children[i], [s - 1 for s in struct_indexes], self.index_node)
                title = self.book = self.index_node.full_title("en")
                base = regex.sub(loose_reg, title, base)
                reg = self.index_node.full_regex(title, self._lang, terminated=True)
            except InputError:
                pass
            #todo: ranges that cross structures

        # Numbered Structure node parsed - return. (Verify this comment.  Should this be indented?)
        if title == base:
            return

        # Content node -  Match primary structure address (may be stage two of numbered structure parsing)
        if reg and not self.index_node.children and getattr(self.index_node, "_addressTypes", None):
            try:
                self.sections = self.__get_sections(reg, base)
            except InputError:
                pass

        # Look for alternate structure
        # todo: handle commentator on alt structure
        if not self.sections:
            alt_struct_regex = self.index.alt_titles_regex(self._lang)
            if alt_struct_regex:
                match = alt_struct_regex.match(base)
                if match:
                    title = match.group('title')
                    alt_struct_node = self.index.get_alt_struct_node(title, self._lang)

                    # Exact match alt structure node
                    if title == base:
                        new_tref = alt_struct_node.get_ref_from_sections([])
                        if new_tref:
                            self.__reinit_tref(new_tref)
                            return

                    try:  # Some structure nodes don't have .regex() methods.
                        reg = alt_struct_node.full_regex(title, self._lang)  # Not strict, since the array map portion will go beyond
                    except AttributeError:
                        pass
                    else:
                        # Alternate numbered structure
                        if alt_struct_node.children and getattr(alt_struct_node, "_addressTypes", None):
                            try:
                                struct_indexes = self.__get_sections(reg, base)
                                alt_struct_node = reduce(lambda a, i: a.children[i], [s - 1 for s in struct_indexes], alt_struct_node)
                                title = alt_struct_node.full_title("en")
                                base = regex.sub(reg, title, base)
                                reg = alt_struct_node.full_regex(title, self._lang, terminated=True)
                            except InputError:
                                pass

                        # Alt struct map node -  (may be stage two of numbered structure parsing)
                        if title == base:  # not a repetition of similar test above - title may have changed in numbered structure parsing
                            alt_struct_indexes = []
                        else:
                            alt_struct_indexes = self.__get_sections(reg, base, use_node=alt_struct_node)
                        try:
                            new_tref = alt_struct_node.get_ref_from_sections(alt_struct_indexes)
                        except IndexError:
                            raise InputError("Sections {} not found in {}".format(alt_struct_indexes, alt_struct_node.full_title()))
                        if new_tref:
                            self.__reinit_tref(new_tref)
                            return

        if not self.sections:
            msg = f"Failed to parse sections for ref {self.orig_tref}"
            raise PartialRefInputError(msg, title, None)

        self.toSections = self.sections[:]

        # retrieve the address class of the last section in the ref
        address_class = AddressType.to_class_by_address_type(self.index_node.addressTypes[len(self.sections)-1])

        if hasattr(address_class, "parse_range_end"):
            base_wout_title = base.replace(title + " ", "")
            address_class.parse_range_end(self, parts, base_wout_title)
        elif len(parts) == 2: # Parse range end portion, if it exists
            try:
                second_part = Ref(parts[1])
                assert second_part.book == self.book, "the two sides of the range have different books"
                self.toSections = Ref(parts[1]).sections
            except InputError:
                self._parse_range_end(re.split("[.:, ]+", parts[1]))
            except AssertionError:
                raise InputError("the two sides of the range have different books: '{}'.".format(self.tref))


    def _parse_range_end(self, range_parts):
        self.__init_ref_pointer_vars()  # clear out any mistaken partial representations
        delta = len(self.sections) - len(range_parts)
        for i in range(delta, len(self.sections)):
            offset = self._get_offset([x-1 for x in self.toSections[:i]])
            try:
                self.toSections[i] = self.index_node._addressTypes[i].toNumber(self._lang,
                                                                                range_parts[i - delta], sections=self.sections[i]) - offset
            except (ValueError, IndexError):
                raise InputError("Couldn't understand text sections: '{}'.".format(self.tref))

    def _get_offset(self, section_indexes, use_node=None):
        use_node = use_node if use_node else self.index_node
        index_offsets_by_depth = getattr(use_node, 'index_offsets_by_depth', None)
        return JaggedArrayNode.get_index_offset(section_indexes, index_offsets_by_depth)

    def __get_sections(self, reg, tref, use_node=None):
        use_node = use_node or self.index_node
        sections = []
        ref_match = reg.match(tref)
        if not ref_match:
            raise InputError("Can not parse sections from ref: {}".format(tref))

        gs = ref_match.groupdict()
        indexes = []
        for i in range(0, use_node.depth):
            gname = "a{}".format(i)
            if gs.get(gname) is not None:
                try:
                    offset = self._get_offset(indexes, use_node)
                except IndexError:
                    raise InputError(f"Can not parse sections from ref: {tref}")
                sections.append(use_node._addressTypes[i].toNumber(self._lang, gs.get(gname)) - offset)
            indexes.append(sections[-1]-1)
        return sections


    def __eq__(self, other):
        return isinstance(other, Ref) and self.uid() == other.uid()

    def __hash__(self):
        return hash(self.uid())

    def __ne__(self, other):
        return not self.__eq__(other)

    @staticmethod
    def is_ref(tref):
        """
        Static method for testing if a string is valid for instantiating a Ref object.

        :param string tref: the string to test
        :return bool:
        """
        try:
            Ref(tref)
            return True
        except InputError:
            return False

    def is_talmud(self):
        """
        Is this a Talmud reference?

        :return bool:
        """
        return getattr(self.index_node, "addressTypes", None) and len(self.index_node.addressTypes) and self.index_node.addressTypes[0] == "Talmud"

    def is_bavli(self):
        """
        Is this a Talmud Bavli or related text reference?
        :return bool:
        """
        return "Bavli" in self.index.categories

    def is_commentary(self):
        """
        Is this a commentary reference?

        :return bool:
        """
        # TODO: -deprecate
        return getattr(self.index, 'dependence', "").capitalize() == "Commentary"

    def is_dependant(self):
        return self.index.is_dependant_text()

    def is_range(self):
        """
        Is this reference a range?

        A Ref is range if it's starting point and ending point are different, i.e. it has a dash in its text form.
        References can cover large areas of text without being a range - in the case where they are references to chapters.

        ::

            >>> Ref("Genesis 3").is_range()
            False
            >>> Ref("Genesis 3-5").is_range()
            True

        :return bool:
        """
        return self.sections != self.toSections

    def range_size(self):
        """
        How large is the range?

        :return int:
        """
        #todo: rewrite with range_index to handle ranges across higher level sections
        return self.toSections[-1] - self.sections[-1] + 1

    def range_index(self):
        """
        At what section index does the range begin?

        ::

            >>> Ref("Leviticus 15:3 - 17:12").range_index()
            0
            >>> Ref("Leviticus 15-17").range_index()
            0
            >>> Ref("Leviticus 15:17-21").range_index()
            1
            >>> Ref("Leviticus 15:17").range_index()
            2

        :return int:
        """
        if not self._range_index:
            self._set_range_data()
        return self._range_index

    def range_depth(self):
        """
        How deep is the range?

        ::

            >>> Ref("Leviticus 15:3 - 17:12").range_depth()
            2
            >>> Ref("Leviticus 15-17").range_depth()
            2
            >>> Ref("Leviticus 15:17-21").range_depth()
            1
            >>> Ref("Leviticus 15:17").range_depth()
            0

        :return int:
        """
        if not self._range_depth:
            self._set_range_data()
        return self._range_depth

    def _set_range_data(self):
        if not self.is_range():
            self._range_depth = 0
            self._range_index = self.index_node.depth

        else:
            for i in range(0, self.index_node.depth):
                if self.sections[i] != self.toSections[i]:
                    self._range_depth = self.index_node.depth - i
                    self._range_index = i
                    break

    def all_segment_refs(self):
        """
        A function that returns all lowest level refs under this ref. 
        TODO: This function was never adapted to serve for complex refs and only works for Refs that are themselves "section level". More specifically it only works for 
        `supported_classes` and fails otherwise 
        
        Note: There is a similar function present on class sefaria.model.text.AbstractIndex
        :return: list of all segment level refs under this Ref.  
        """
        supported_classes = (JaggedArrayNode, DictionaryEntryNode, SheetNode)
        assert self.index_node is not None
        if not isinstance(self.index_node, supported_classes):
            # search for default node child
            for child in self.index_node.children:
                if child.is_default():
                    return child.ref().all_segment_refs()
            assert isinstance(self.index_node, supported_classes)

        if self.is_range():
            input_refs = self.range_list()
        else:
            input_refs = [self]

        ref_list = []
        for temp_ref in input_refs:
            if temp_ref.is_segment_level():
                ref_list.append(temp_ref)
            elif temp_ref.is_section_level():
                ref_list += temp_ref.all_subrefs()
            else: # you're higher than section level
                if self.index_node.is_virtual:
                    sub_refs = temp_ref.all_subrefs()
                    ref_list_list = [sub_ref.all_segment_refs() for sub_ref in sub_refs]
                    ref_list += [refs for refs in ref_list_list]
                else:
                    state_ja = self.get_state_ja("all")
                    sub_ja = state_ja.subarray_with_ref(temp_ref)
                    ref_list_sections = [temp_ref.subref([i + 1 for i in k ]) for k in sub_ja.non_empty_sections() ]
                    ref_list += [ref_seg for ref_sec in ref_list_sections for ref_seg in ref_sec.all_subrefs(state_ja=state_ja)]

        return ref_list

    def is_spanning(self):
        """
        :return bool: True if the Ref spans across text sections.

        ::

            >>> Ref("Shabbat 13a-b").is_spanning()
            True
            >>> Ref("Shabbat 13a:3-14").is_spanning()
            False
            >>> Ref("Job 4:3-5:3").is_spanning()
            True
            >>> Ref("Job 4:5-18").is_spanning()
            False

        """
        return self.span_size() > 1

    def span_size(self):
        """
        How many sections does the span cover?

        ::

            >>> Ref("Leviticus 15:3 - 17:12").span_size()
            3
            >>> Ref("Leviticus 15-17").span_size()
            3
            >>> Ref("Leviticus 15:17-21").span_size()
            1
            >>> Ref("Leviticus 15:17").span_size()
            1

        :return int:
        """
        if not getattr(self.index_node, "depth", None) or self.index_node.depth == 1:
            # text with no depth or depth 1 can't be spanning
            return 0

        if len(self.sections) == 0:
            # can't be spanning if no sections set
            return 0

        if len(self.sections) <= self.index_node.depth - 2:
            point = len(self.sections) - 1
        else:
            point = self.index_node.depth - 2

        for i in range(0, point + 1):
            size = self.toSections[i] - self.sections[i] + 1
            if size > 1:
                return size

        return 1

    def is_book_level(self):
        """
        Is this a Ref to the whole book level?

        ::

            >>> Ref("Leviticus").is_book_level()
            True
            >>> Ref("Leviticus 15").is_book_level()
            False
            >>> Ref("Rashi on Leviticus").is_book_level()
            True
            >>> Ref("Rashi on Leviticus 15").is_book_level()
            False

        :return bool:
        """
        return len(self.sections) == 0 and not self.index_node.parent

    def is_section_level(self):
        """
        Is this Ref section (e.g. Chapter) level?

        ::

            >>> Ref("Leviticus 15:3").is_section_level()
            False
            >>> Ref("Leviticus 15").is_section_level()
            True
            >>> Ref("Rashi on Leviticus 15:3").is_section_level()
            True
            >>> Ref("Rashi on Leviticus 15:3:1").is_section_level()
            False
            >>> Ref("Leviticus 15-17").is_section_level()
            True


        :return bool:
        """
        if getattr(self.index_node, "depth", None) is None:
            return False
        return len(self.sections) == self.index_node.depth - 1

    def is_segment_level(self):
        """
        Is this Ref segment (e.g. Verse) level?

        ::

            >>> Ref("Leviticus 15:3").is_segment_level()
            True
            >>> Ref("Leviticus 15").is_segment_level()
            False
            >>> Ref("Rashi on Leviticus 15:3").is_segment_level()
            False
            >>> Ref("Rashi on Leviticus 15:3:1").is_segment_level()
            True

        :return bool:
        """
        if getattr(self.index_node, "depth", None) is None:
            return False
        return len(self.sections) == self.index_node.depth

    def is_sheet(self):
        """
        Is this Ref a Sheet Ref?
        ::
            >>> Ref("Leviticus 15:3").is_sheet()
            False
            >>> Ref("Sheet 15").is_sheet()
            True
        :return bool:
        """
        return self.index.title == 'Sheet'

    """ Methods to generate new Refs based on this Ref """
    def _core_dict(self):
        return {
            "index": self.index,
            "book": self.book,
            "primary_category": self.primary_category,
            "index_node": self.index_node,
            "sections": self.sections[:],
            "toSections": self.toSections[:]
        }

    def has_default_child(self):
        return self.index_node.has_default_child()

    def default_child_ref(self):
        """
        Return ref to the default node underneath this node
        If there is no default node, return self
        :return:
        """
        if not self.has_default_child():
            return self
        d = self._core_dict()
        d["index_node"] = self.index_node.get_default_child()
        return Ref(_obj=d)

    def surrounding_ref(self, size=1):
        """
        Return a reference with 'size' additional segments added to each side.

        Currently does not extend to sections beyond the original ref's span.

        :param int size:
        :return: :class:`Ref`
        """

        if self.starting_ref().sections[-1] > size:
            start = self.starting_ref().sections[-1] - size
        else:
            start = 1

        ending_sections = self.ending_ref().sections
        ending_section_length = self.get_state_ja().sub_array_length([s - 1 for s in ending_sections[:-1]])

        if ending_sections[-1] + size < ending_section_length:
            end = ending_sections[-1] + size
        else:
            end = ending_section_length

        d = self._core_dict()
        d["sections"] = d["sections"][:-1] + [start]
        d["toSections"] = d["toSections"][:-1] + [end]
        return Ref(_obj=d)

    def as_ranged_segment_ref(self):
        """
        Expresses a section level (or higher) Ref as a ranged ref at segment level.

        :param depth: Desired depth of the range. If not specified will drop to segment level
        :return: Ref
        """
        # Only for section level or higher.
        # If segment level, return self
        # Only works for text that span a single jaggedArray

        if self.is_segment_level():
            return self

        d = self._core_dict()

        # create a temporary helper ref for finding the end of the range
        if self.is_range():
            current_ending_ref = self.ending_ref()
        else:
            current_ending_ref = self

        max_depth = self.index_node.depth - len(self.sections)  # calculate the number of "paddings" required to get down to segment level

        if len(d['sections']) == 0:
            segment_refs = self.all_segment_refs()
            if segment_refs == []:
                return self
            d["sections"] = segment_refs[0].sections
        else:
            d['sections'] += [1] * max_depth

        state_ja = current_ending_ref.get_state_ja()

        for _ in range(max_depth):
            size = state_ja.sub_array_length([i - 1 for i in current_ending_ref.sections])
            if size and size > 0:
                d['toSections'].append(size)
            else:
                d['toSections'].append(1)

            # get the next level ending ref
            temp_d = current_ending_ref._core_dict()
            temp_d['sections'] = temp_d['toSections'][:] = d['toSections'][:]
            current_ending_ref = Ref(_obj=temp_d)

        return Ref(_obj=d)

    def starting_ref(self):
        """
        For ranged Refs, return the starting Ref

        :return: :class:`Ref`
        """
        if not self.is_range():
            return self
        d = self._core_dict()
        d["toSections"] = self.sections[:]
        return Ref(_obj=d)

    def ending_ref(self):
        """
        For ranged Refs, return the ending Ref

        :return: :class:`Ref`
        """
        if not self.is_range():
            return self
        d = self._core_dict()
        d["sections"] = self.toSections[:]
        return Ref(_obj=d)

    def section_ref(self):
        """
        Return the section level Ref

        For texts of depth 2, this has the same behavior as :meth:`top_section_ref`

        ::

            >>> Ref("Rashi on Genesis 2:3:1").section_ref()
            Ref("Rashi on Genesis 2:3")
            >>> Ref("Genesis 2:3").section_ref()
            Ref("Genesis 2")

        :return: :class:`Ref`
        """
        if not self.is_segment_level():
            return self
        return self.padded_ref().context_ref()

    def top_section_ref(self):
        """
        Return the highest level section Ref.

        For texts of depth 2, this has the same behavior as :meth:`section_ref`

        ::

            >>> Ref("Rashi on Genesis 2:3:1").top_section_ref()
            Ref("Rashi on Genesis 2")
            >>> Ref("Genesis 2:3").top_section_ref()
            Ref("Genesis 2")

        :return: :class:`Ref`
        """
        return self.padded_ref().context_ref(self.index_node.depth - 1)

    def next_section_ref(self, vstate=None):
        """
        Returns a Ref to the next section (e.g. Chapter).

        If this is the last section, returns ``None``

        :return: :class:`Ref`
        """
        if not self._next:
            if self.index_node.is_virtual:
                nl = self.index_node.next_leaf()
                self._next = nl.ref() if nl else None
                return self._next
            self._next = self._iter_text_section(vstate=vstate)
            if self._next is None and not self.index_node.children:
                current_leaf = self.index_node
                #we now need to iterate over the next leaves, finding the first available section
                while True:
                    next_leaf = current_leaf.next_leaf() #next schema/JANode
                    if next_leaf and next_leaf.is_virtual:
                        if next_leaf.first_child():
                            return next_leaf.first_child().ref()
                        else:
                            return None
                    if next_leaf:
                        next_node_ref = next_leaf.ref() #get a ref so we can do the next lines
                        potential_next = next_node_ref._iter_text_section(depth_up=0 if next_leaf.depth == 1 else 1, vstate=vstate)
                        if potential_next:
                            self._next = potential_next
                            break
                        current_leaf = next_leaf
                    else:
                        self._next = None
                        break
        return self._next

    def prev_section_ref(self, vstate=None):
        """
        Returns a Ref to the previous section (e.g. Chapter).

        If this is the first section, returns ``None``

        :return: :class:`Ref`
        """
        if not self._prev:
            if self.index_node.is_virtual:
                pl = self.index_node.prev_leaf()
                self._prev = pl.ref() if pl else None
                return self._prev
            self._prev = self._iter_text_section(False, vstate=vstate)
            if self._prev is None and not self.index_node.children:
                current_leaf = self.index_node
                # we now need to iterate over the prev leaves, finding the first available section
                while True:
                    prev_leaf = current_leaf.prev_leaf()  # prev schema/JANode
                    if prev_leaf and prev_leaf.is_virtual:
                        if prev_leaf.last_child():
                            return prev_leaf.last_child().ref()
                        else:
                            return None
                    if prev_leaf:
                        prev_node_ref = prev_leaf.ref()  # get a ref so we can do the next lines
                        potential_prev = prev_node_ref._iter_text_section(forward=False, depth_up=0 if prev_leaf.depth == 1 else 1, vstate=vstate)
                        if potential_prev:
                            self._prev = potential_prev
                            break
                        current_leaf = prev_leaf
                    else:
                        self._prev = None
                        break
        return self._prev

    def recalibrate_next_prev_refs(self, add_self=True):
        """
        Internal. Called when a section is inserted or deleted.

        :param add_self:
        :return: None
        """
        next_ref = self.next_section_ref()
        prev_ref = self.prev_section_ref()
        if next_ref:
            next_ref._prev = self if add_self else prev_ref
        if prev_ref:
            prev_ref._next = self if add_self else next_ref

    def prev_segment_ref(self):
        """
        Returns a :class:`Ref` to the next previous populated segment.

        If this ref is not segment level, will return ``self```

        :return: :class:`Ref`
        """
        r = self.starting_ref()
        if not r.is_segment_level():
            return r
        if r.sections[-1] > 1:
            d = r._core_dict()
            d["sections"] = d["toSections"] = r.sections[:-1] + [r.sections[-1] - 1]
            return Ref(_obj=d)
        else:
            r = r.prev_section_ref()
            if not r:
                return None
            d = r._core_dict()
            newSections = r.sections + [self.get_state_ja().sub_array_length([i - 1 for i in r.sections])]
            d["sections"] = d["toSections"] = newSections
            return Ref(_obj=d)

    def next_segment_ref(self):
        """
        Returns a :class:`Ref` to the next populated segment.

        If this ref is not segment level, will return ``self```

        :return: :class:`Ref`
        """
        r = self.ending_ref()
        if not r.is_segment_level():
            return r
        sectionRef = r.section_ref()
        sectionLength = self.get_state_ja().sub_array_length([i - 1 for i in sectionRef.sections])
        if r.sections[-1] < sectionLength:
            d = r._core_dict()
            d["sections"] = d["toSections"] = r.sections[:-1] + [r.sections[-1] + 1]
            return Ref(_obj=d)
        else:
            try:
                return r.next_section_ref().subref(1)
            except AttributeError:
                # No next section
                return None

    def last_segment_ref(self):
        """
        Returns :class:`Ref` to the last segment in the current book (or complex book part).

        Not to be confused with :meth:`ending_ref`

        :return:
        """
        o = self._core_dict()
        o["sections"] = o["toSections"] = [i + 1 for i in self.get_state_ja().last_index(self.index_node.depth)]
        return Ref(_obj=o)

    def first_available_section_ref(self):
        """
        Returns a :class:`Ref` to the first section inside of or following this :class:`Ref` that has some content.
        Return first available segment ref is `self` is depth 1

        Returns ``None`` if self is empty and no following :class:`Ref` has content.

        :return: :class:`Ref`
        """
        # todo: This is now stored on the VersionState. Look for performance gains.
        if isinstance(self.index_node, JaggedArrayNode):
            r = self.padded_ref()
        elif isinstance(self.index_node, TitledTreeNode):
            if self.is_segment_level():  # dont need to use first_leaf if we're already at segment level
                r = self
            else:
                first_leaf = self.index_node.first_leaf()
                if not first_leaf:
                    return None
                try:
                    r = first_leaf.ref().padded_ref()
                except Exception as e: #VirtualNodes dont have a .ref() function so fall back to VersionState
                    if self.is_book_level():
                        return self.index.versionSet().array()[0].first_section_ref()
        else:
            return None

        if r.is_book_level():
            # r is depth 1. return first segment
            r = r.subref([1])
            return r.next_segment_ref() if r.is_empty() else r
        else:
            return r.next_section_ref() if r.is_empty() else r

    #Don't store results on Ref cache - state objects change, and don't yet propogate to this Cache
    def get_state_node(self, meta=None, hint=None):
        """
        :return: :class:`sefaria.model.version_state.StateNode`
        """
        from . import version_state
        return version_state.StateNode(snode=self.index_node, meta=meta, hint=hint)

    def get_state_ja(self, lang="all"):
        """
        :param lang: "all", "he", or "en"
        :return: :class:`sefaria.datatype.jagged_array`
        """
        #TODO: also does not work with complex texts...
        return self.get_state_node(hint=[(lang, "availableTexts")]).ja(lang)

    def is_text_fully_available(self, lang):
        """
        :param lang: "he" or "en"
        :return: True if at least one complete version of ref is available in lang.
        """
        if self.is_section_level() or self.is_segment_level():
            # Using mongo queries to slice and merge versions
            # is much faster than actually using the Version State doc
            try:
                text = self.text(lang=lang).text
                return bool(len(text) and all(text))
            except NoVersionFoundError:
                return False
        else:
            sja = self.get_state_ja(lang)
            subarray = sja.subarray_with_ref(self)
            return subarray.is_full()

    def is_text_translated(self):
        """
        :return: True if at least one complete version of this :class:`Ref` is available in English.
        """
        return self.is_text_fully_available("en")

    def is_empty(self, lang=None):
        """
        Checks if :class:`Ref` has any corresponding data in :class:`Version` records.

        :return: Bool True is there is not text at this ref in any language
        """

        # The commented code is easier to understand, but the code we're using puts a lot less on the wire.
        # return not len(self.versionset())
        # depricated
        # return db.texts.find(self.condition_query(), {"_id": 1}).count() == 0

        return db.texts.count_documents(self.condition_query(lang)) == 0

    def word_count(self, lang="he"):
        try:
            return TextChunk(self, lang).word_count()
        except InputError:
            lns = self.index_node.get_leaf_nodes()
            return sum([TextChunk(n.ref(), lang).word_count() for n in lns])

    def _iter_text_section(self, forward=True, depth_up=1, vstate=None):
        """
        Iterate forwards or backwards to the next available :class:`Ref` in a text

        :param forward: Boolean indicating direction to iterate
        :depth_up: if we want to traverse the text at a higher level than most granular. Defaults to one level above
        :param vstate: VersionState for this index. Pass this down to avoid calling expensive database lookups
        :return: :class:`Ref`
        """
        if self.index_node.depth <= depth_up:  # if there is only one level of text, don't even waste time iterating.
            return None

        # arrays are 0 based. text sections are 1 based. so shift the numbers back.
        if not forward:
            # Going backward, start from begginning of Ref
            starting_points = [s - 1 for s in self.sections[:self.index_node.depth - depth_up]]
        else:
            # Going forward start form end of Ref
            starting_points = [s - 1 for s in self.toSections[:self.index_node.depth - depth_up]]


        # start from the next one
        if len(starting_points) > 0:
            starting_points[-1] += 1 if forward else -1

        # let the counts obj calculate the correct place to go.
        if vstate:
            c = vstate.state_node(self.index_node).ja("all", "availableTexts")
        else:
            c = self.get_state_node(hint=[("all","availableTexts")]).ja("all", "availableTexts")
        new_section = c.next_index(starting_points) if forward else c.prev_index(starting_points)

        # we are also scaling back the sections to the level ABOVE the lowest section type (eg, for bible we want chapter, not verse)
        if new_section:
            d = self._core_dict()
            d["toSections"] = d["sections"] = [(s + 1) for s in new_section[:-depth_up]]
            return Ref(_obj=d)
        else:
            return None

    def pad_to_last_segment_ref(self):
        """
        From current position in jagged array, pad :class:`Ref` so that it reaches the last segment ref
        ``self`` remains unchanged.
        E.g. for input:
            - segment ref -> unchanged
            - section ref -> last segment ref in section
            - book ref -> last segment ref in book (equivalent to :meth:`last_segment_ref`)
        :return:
        """

        ja = self.get_state_ja()

        r = self
        while not r.is_segment_level():
            sublen = ja.sub_array_length([s-1 for s in r.toSections],until_last_nonempty=True)
            r = r.subref([sublen])

        return r

    def to(self, toref):
        """
        Return a reference that begins at this :class:`Ref`, and ends at toref

        :param toref: :class:`Ref` that denotes the end of the new ranged :class:`Ref`
        :return: :class:`Ref`
        """
        assert self.book == toref.book
        d = self._core_dict()
        d["toSections"] = toref.toSections[:]


        #pad sections and toSections so they're the same length. easier to just make them both segment level
        if len(d['sections']) != len(d['toSections']):
            if not self.is_segment_level():
                d['sections'] = self.first_available_section_ref().sections + [1]
            d['toSections'] = toref.pad_to_last_segment_ref().toSections


        return Ref(_obj=d)

    def subref(self, subsections):
        """
        Returns a more specific reference than the current Ref

        :param subsections: int or list - the subsections of the current Ref.
        If a section in subsections is negative, will calculate the last section for that depth. NOTE: this requires access to state_ja so this is a bit slower.
        :return: :class:`Ref`
        """
        if isinstance(subsections, int):
            subsections = [subsections]
        new_depth = len(self.sections) + len(subsections)
        assert self.index_node.depth >= new_depth, "Tried to get subref of bottom level ref: {}".format(self.normal())
        assert not self.is_range(), "Tried to get subref of ranged ref".format(self.normal())

        d = self._core_dict()

        if any([sec < 0 for sec in subsections]):
            # only load state_ja when a negative index exists
            ja = self.get_state_ja()
            ja_inds = [sec - 1 for sec in self.sections + subsections]
            for i, sec in enumerate(subsections):
                if sec >= 0: continue
                subsections[i] = ja.sub_array_length(ja_inds[:len(self.sections) + i]) + sec + 1
        d["sections"] += subsections
        d["toSections"] += subsections
        return Ref(_obj=d)

    def subrefs(self, length: int):
        """
        Return a list of :class:`Ref` objects one level deeper than this :class:`Ref`, from 1 to `length`.

        :param length: Number of subrefs to return

        ::

            >>> Ref("Genesis").subrefs(4)
            [Ref('Genesis 1'),
             Ref('Genesis 2'),
             Ref('Genesis 3'),
             Ref('Genesis 4')]

        :return: List of :class:`Ref`
        """
        l = []
        for i in range(length):
            l.append(self.subref(i + 1))
        return l

    def all_subrefs(self, lang='all', state_ja=None):
        """
        Return a list of all the valid :class:`Ref` objects one level deeper than this :class:`Ref`.

        ::

            >>> Ref("Genesis").all_subrefs()
            [Ref('Genesis 1'),
             Ref('Genesis 2'),
             Ref('Genesis 3'),
             Ref('Genesis 4'),
             ...]

        :return: List of :class:`Ref`
        """
        # TODO this function should take Version as optional parameter to limit the refs it returns to ones existing in that Version
        assert not self.is_range(), "Ref.all_subrefs() is not intended for use on Ranges"

        if self.index_node.is_virtual:
            size = len(self.text().text)
            return self.subrefs(size)
        state_ja = state_ja or self.get_state_ja(lang)
        size = state_ja.sub_array_length([i - 1 for i in self.sections])
        if size is None:
            size = 0
        return self.subrefs(size)

    def all_context_refs(self, include_self = True, include_book = False):
        """
        :return: a list of more general refs that contain this one - out too, and including, the book level
        """

        refs = [self] if include_self else []
        try:
            current_level = self.index_node.depth - len(self.sections)
            refs += [self.context_ref(n) for n in  range(current_level + 1, self.index_node.depth + 1)]
        except AttributeError:  # If self is a Schema Node
            pass

        n = self.index_node.parent

        while n is not None:
            try:
                refs += [n.ref()]
            except AttributeError:  # Jump over VirtualNodes
                pass
            n = n.parent
        if not include_book:
            refs = refs[:-1]
        return refs

    def context_ref(self, level=1):
        """
        :param level: how many levels to 'zoom out' from the most specific possible :class:`Ref`
        :return: :class:`Ref` that is more general than this :class:`Ref`.

        ::

            >>> Ref("Genesis 4:5").context_ref(level = 1)
            Ref("Genesis 4")
            >>> Ref("Genesis 4:5").context_ref(level = 2)
            Ref("Genesis")

        If this :class:`Ref` is less specific than or equally specific to the level given, it is returned as-is.
        """
        if level == 0:
            return self

        if not self.sections and self.index_node.has_children():
            if self.index_node.has_default_child():
                return self.default_child_ref()
            return self

        if self._context is None:
            self._context = {}

        if not self._context.get(level) or not self._context[level]:
            if len(self.sections) <= self.index_node.depth - level:
                return self

            if level > self.index_node.depth:
                raise InputError("Call to Ref.context_ref of {} exceeds Ref depth of {}.".format(level, self.index_node.depth))
            d = self._core_dict()
            d["sections"] = d["sections"][:self.index_node.depth - level]
            d["toSections"] = d["toSections"][:self.index_node.depth - level]
            self._context[level] = Ref(_obj=d)
        return self._context[level]

    def padded_ref(self):
        """
        :return: :class:`Ref` with 1s inserted to make the :class:`Ref` specific to the section level

        ::

            >>> Ref("Genesis").padded_ref()
            Ref("Genesis 1")

        If this :class:`Ref` is already specific to the section or segment level, it is returned unchanged.

        ::

            >>> Ref("Genesis 1").padded_ref()
            Ref("Genesis 1")

        """
        if not self._padded:
            if not getattr(self, "index_node", None):
                raise Exception("No index_node found {}".format(vars(self)))
            try:
                if len(self.sections) >= self.index_node.depth - 1:
                    return self
            except AttributeError:  # This is a schema node, try to get a default child
                if self.has_default_child():
                    return self.default_child_ref().padded_ref()
                elif self.is_book_level():
                    raise ComplexBookLevelRefError(book_ref=self.normal())
                else:
                    raise InputError("Cannot pad a schema node ref.")

            d = self._core_dict()
            if self.is_talmud():
                if len(self.sections) == 0: #No daf specified
                    section = 3 if "Bavli" in self.index.categories and "Rif" not in self.index.categories else 1
                    d["sections"].append(section)
                    d["toSections"].append(section)
            for i in range(self.index_node.depth - len(d["sections"]) - 1):
                d["sections"].append(1)
                d["toSections"].append(1)  # todo: is this valid in all cases?
            self._padded = Ref(_obj=d)
        return self._padded

    def first_spanned_ref(self):
        """
        Returns the first section portion of a spanning :class:`Ref`.
        Designed to cut the wasted cost of running :meth:`split_spanning_ref`

        >>> Ref("Shabbat 6b-9a").first_spanned_ref()
        Ref('Shabbat 6b')
        >>> Ref("Shabbat 6b.12-9a.7").first_spanned_ref()
        Ref('Shabbat 6b:12-47')

        :return: :py:class:`Ref`
        """
        if not self._first_spanned_ref:

            if self._spanned_refs:
                self._first_spanned_ref = self._spanned_refs[0]

            elif self.index_node.depth == 1 or not self.is_spanning():
                self._first_spanned_ref = self

            else:
                ref_depth = len(self.sections)

                d = self._core_dict()
                d["toSections"] = self.sections[0:self.range_index() + 1]
                for i in range(self.range_index() + 1, ref_depth):
                    d["toSections"] += [self.get_state_ja().sub_array_length([s - 1 for s in d["toSections"][0:i]])]

                r = Ref(_obj=d)
                if self.range_depth() > 2:
                    self._first_spanned_ref = r.first_spanned_ref()
                else:
                    self._first_spanned_ref = r

        return self._first_spanned_ref

    def starting_refs_of_span(self, deep_range=False):
        """
            >>> Ref("Zohar 1:3b:12-3:12b:1").starting_refs_of_span()
            [Ref("Zohar 1:3b:12"),Ref("Zohar 2"),Ref("Zohar 3")]
            >>> Ref("Zohar 1:3b:12-1:4b:12").starting_refs_of_span(True)
            [Ref("Zohar 1:3b:12"),Ref("Zohar 1:4a"),Ref("Zohar 1:4b")]
            >>> Ref("Zohar 1:3b:12-1:4b:12").starting_refs_of_span(False)
            [Ref("Zohar 1:3b:12")]
            >>> Ref("Genesis 12:1-14:3").starting_refs_of_span()
            [Ref("Genesis 12:1"), Ref("Genesis 13"), Ref("Genesis 14")]

        :param deep_range: Default: False.  If True, returns list of refs at whatever level the range is.  If False, only returns refs for the 0th index, whether ranged or not.
        :return:
        """
        if not self.is_spanning():
            return self
        level = 0 if not deep_range else self.range_index()

        results = []

        start = self.sections[level]
        end = self.toSections[level] + 1
        for i, n in enumerate(range(start, end)):
            d = self._core_dict()
            if i != 0:
                d["sections"] = self.sections[0:level] + [self.sections[level] + i]
            d["toSections"] = d["sections"][:]
            results += [Ref(_obj=d)]

        return results

    def split_spanning_ref(self):
        """
        Return list of non-spanning :class:`Ref` objects which completely cover the area of this Ref

            >>> Ref("Shabbat 13b-14b").split_spanning_ref()
            [Ref("Shabbat 13b"), Ref("Shabbat 14a"), Ref("Shabbat 14b")]
            >>> Ref("Shabbat 13b:3 - 14b:3").split_spanning_ref()
            [Ref('Shabbat 13b:3-50'), Ref('Shabbat 14a'), Ref('Shabbat 14b:1-3')]

        """
        if self.index_node.depth == 1 or not self.is_spanning():
            self._spanned_refs = [self]

        else:
            start, end = self.sections[self.range_index()], self.toSections[self.range_index()]
            ref_depth = len(self.sections)
            to_ref_depth = len(self.toSections)

            refs = []
            for n in range(start, end + 1):
                d = self._core_dict()
                if n == start:
                    d["toSections"] = self.sections[0:self.range_index() + 1]

                    for i in range(self.range_index() + 1, ref_depth):
                        d["toSections"] += [self.get_state_ja().sub_array_length([s - 1 for s in d["toSections"][0:i]],until_last_nonempty=True)]
                elif n == end:
                    d["sections"] = self.toSections[0:self.range_index() + 1]
                    for _ in range(self.range_index() + 1, to_ref_depth):
                        d["sections"] += [1]
                else:
                    d["sections"] = self.sections[0:self.range_index()] + [n]
                    d["toSections"] = self.sections[0:self.range_index()] + [n]

                    '''  If we find that we need to expand inner refs, add this arg.
                    # It will require handling on cached ref and passing on the recursive call below.
                    if expand_middle:
                        for i in range(self.range_index() + 1, ref_depth):
                            d["sections"] += [1]
                            d["toSections"] += [self.get_state_ja().sub_array_length([s - 1 for s in d["toSections"][0:i]])]
                    '''

                if d["toSections"][-1]:  # to filter out, e.g. non-existant Rashi's, where the last index is 0
                    try:
                        refs.append(Ref(_obj=d))
                    except InputError:
                        pass

            if self.range_depth() == 2:
                self._spanned_refs = refs
            if self.range_depth() > 2: #recurse
                expanded_refs = []
                for ref in refs:
                    expanded_refs.extend(ref.split_spanning_ref())
                self._spanned_refs = expanded_refs

        return self._spanned_refs

    def range_list(self):
        """
        :return: list of :class:`Ref` objects corresponding to each point in the range of this :class:`Ref`
        """
        if self._ranged_refs is None:
            if not self.is_range():
                return [self]
            results = []
            if self.is_spanning():
                for oref in self.split_spanning_ref():
                    results += oref.range_list() if oref.is_range() else [oref] if oref.is_segment_level() else oref.all_subrefs()
            else:
                for s in range(self.sections[-1], self.toSections[-1] + 1):
                    d = self._core_dict()
                    d["sections"][-1] = s
                    d["toSections"][-1] = s
                    results.append(Ref(_obj=d))

            self._ranged_refs = results
        return self._ranged_refs

    def regex(self, as_list=False, anchored=True):
        """
        :return string: for a Regular Expression which will find any refs that match this Ref exactly, or more specifically.

        E.g., "Genesis 1" yields an RE that match "Genesis 1" and "Genesis 1:3"
        """
        # todo: move over to the regex methods of the index nodes
        patterns = []

        if self.is_range():
            if self.is_spanning():
                s_refs = self.split_spanning_ref()
                normals = []
                for s_ref in s_refs:
                    normals += [r.normal() for r in s_ref.range_list()]
            else:
                normals = [r.normal() for r in self.range_list()]

            for r in normals:
                sections = re.sub(r"^%s" % re.escape(self.book), '', r)
                patterns.append(r"%s$" % sections)   # exact match
                patterns.append(r"%s:" % sections)   # more granualar, exact match followed by :
                patterns.append(r"%s \d" % sections) # extra granularity following space
        else:
            sections = re.sub(r"^%s" % re.escape(self.book), '', self.normal())
            patterns.append(r"%s$" % sections)   # exact match
            if self.index_node.has_titled_continuation():
                patterns.append(r"{}({}).".format(sections, "|".join(self.index_node.title_separators)))
            if self.index_node.has_numeric_continuation():
                patterns.append(r"%s:" % sections)   # more granualar, exact match followed by :
                patterns.append(r"%s \d" % sections) # extra granularity following space

        escaped_book = re.escape(self.book)
        if anchored:
            if as_list:
                return [r"^{}{}".format(escaped_book, p) for p in patterns]
            else:
                return r"^%s(%s)" % (escaped_book, "|".join(patterns))
        else:
            if as_list:
                return [r"{}{}".format(escaped_book, p) for p in patterns]
            else:
                return r"%s(%s)" % (escaped_book, "|".join(patterns))

    def ref_regex_query(self):
        """
        Convenience method to wrap the lines of logic used to generate a broken out list of ref queries from one regex.
        The regex in the list will naturally all be anchored.
        :return: dict of the form {"$or" [{"refs": {"$regex": r1}},{"refs": {"$regex": r2}}...]}
        """
        reg_list = self.regex(as_list=True)
        ref_clauses = [{"refs": {"$regex": r}} for r in reg_list]
        return {"$or": ref_clauses}

    def get_padded_sections(self, section_end=None):
        """
        pad sections and toSections to index_node.depth.
        In the case of toSections, pad with section_end, a placeholder for the end of the section
        """
        sections, toSections = self.sections[:], self.toSections[:]
        for _ in range(self.index_node.depth - len(sections)):
            sections += [1]
        for _ in range(self.index_node.depth - len(toSections)):
            toSections += [section_end]
        return sections, toSections

    """ Comparisons """
    def contains(self, other):
        """
        Does this Ref completely contain ``other`` Ref?
        In the case where other is less specific than self, a database lookup is required

        :param other:
        :return bool:
        """
        assert isinstance(other, Ref)
        if not self.index_node == other.index_node:
            return self.index_node.is_ancestor_of(other.index_node)

        if len(self.sections) > len(other.sections): # other is less specific than self
            if len(other.sections) == 0:  # other is a whole book
                if any([x != 1 for x in self.sections]):  # self is not a whole book
                    return False  # performance optimization to avoid call to as_ranged_segment_ref
            # we need to get the true extent of other
            other = other.as_ranged_segment_ref()

        smallest_section_len = min([len(self.sections), len(other.sections)])

        # at each level of shared specificity
        for i in range(smallest_section_len):
            # If other's end is after my end, I don't contain it
            if other.toSections[i] > self.toSections[i]:
                return False

            # if other's end is before my end, I don't need to keep checking
            if other.toSections[i] < self.toSections[i]:
                break

        # at each level of shared specificity
        for i in range(smallest_section_len):
            # If other's start is before my start, I don't contain it
            if other.sections[i] < self.sections[i]:
                return False

            # If other's start is after my start, I don't need to keep checking
            if other.sections[i] > self.sections[i]:
                break

        return True

    def overlaps(self, other):
        """
        Does this Ref overlap ``other`` Ref?

        :param other: Ref
        :return bool:
        """
        assert isinstance(other, Ref)
        if not self.index_node == other.index_node:
            return self.index_node.is_ancestor_of(other.index_node)

        smallest_section_len = min([len(self.sections), len(other.sections)])

        # at each level of shared specificity
        for i in range(smallest_section_len):
            # If I start after your end, we don't overlap
            if self.sections[i] > other.toSections[i]:
                return False
            # If I start before your end, we don't need to keep checking
            if self.sections[i] < other.toSections[i]:
                break

        # at each level of shared specificity
        for i in range(smallest_section_len):
            # If I end before your start, we don't overlap
            if self.toSections[i] < other.sections[i]:
                return False

            # If I end after your start, we don't need to keep checking
            if self.toSections[i] > other.sections[i]:
                break

        return True

    def precedes(self, other) -> bool:
        """
        Does this Ref completely precede ``other`` Ref?

        :param other:
        :return bool:
        """
        assert isinstance(other, Ref)
        if not self.index_node == other.index_node:
            return False

        my_end = self.ending_ref()
        other_start = other.starting_ref()

        smallest_section_len = min([len(my_end.sections), len(other_start.sections)])

        # Bare book references never precede or follow
        if smallest_section_len == 0:
            return False

        # Compare all but last section
        for i in range(smallest_section_len - 1):
            if my_end.sections[i] < other_start.sections[i]:
                return True
            if my_end.sections[i] > other_start.sections[i]:
                return False

        # Compare last significant section
        if my_end.sections[smallest_section_len - 1] < other_start.sections[smallest_section_len - 1]:
            return True

        return False

    def follows(self, other) -> bool:
        """
        Does this Ref completely follow ``other`` Ref?

        :param other:
        :return bool:
        """
        assert isinstance(other, Ref)
        if not self.index_node == other.index_node:
            return False

        my_start = self.starting_ref()
        other_end = other.ending_ref()

        smallest_section_len = min([len(my_start.sections), len(other_end.sections)])

        # Bare book references never precede or follow
        if smallest_section_len == 0:
            return False

        # Compare all but last section
        for i in range(smallest_section_len - 1):
            if my_start.sections[i] > other_end.sections[i]:
                return True
            if my_start.sections[i] < other_end.sections[i]:
                return False

        # Compare last significant section
        if my_start.sections[smallest_section_len - 1] > other_end.sections[smallest_section_len - 1]:
            return True

        return False

    def in_terms_of(self, other):
        """
        Returns the current reference sections in terms of another, containing reference.

        Returns an array of ordinal references, not array indexes.  (Meaning first is 1)

        Must be called on a point Reference, not a range

        ""

            >>> Ref("Genesis 6:3").in_terms_of("Genesis 6")
            [3]
            >>> Ref("Genesis 6:3").in_terms_of("Genesis")
            [6,3]
            >>> Ref("Genesis 6:3").in_terms_of("Genesis 6-7")
            [1,3]
            >>> Ref("Genesis 6:8").in_terms_of("Genesis 6:3-7:3")
            [1, 6]

        :param other: :class:`Ref`
        :return: array of indexes

        """

        #What's best behavior for these cases?
        assert isinstance(other, Ref)
        if not self.index_node == other.index_node:
            return None

        if self.is_range():
            raise Exception("Ref.in_terms_of() called on ranged Ref: {}".format(self))

        if not other.contains(self):
            return None

        ret = []

        if not other.is_range():
            ret = self.sections[len(other.sections):]
        else:
            for i in range(other.range_index(), self.index_node.depth):
                ret.append(self.sections[i] + 1 - other.sections[i])
                if other.sections[i] != self.sections[i] or len(other.sections) <= i + 1:
                    ret += self.sections[i + 1:]
                    break
        return ret

    def order_id(self):
        """
        Returns a unique id for this reference that establishes an ordering of references across the whole catalog.
        This id will change as the ordering of the categories changes, and may begin to overlap with other numbers because of those changes.
        However, at any point in time these ids will be unique across the catalog.
        Used to sort results from ElasticSearch queries

        :return string:
        """

        cats = self.index.categories[:]

        key = "/".join(cats + [self.index.title])
        try:
            base = library.category_id_dict()[key]
            if self.index.is_complex() and self.index_node.parent:
                child_order = self.index.nodes.get_child_order(self.index_node)
                base += str(format(child_order, '03')) if isinstance(child_order, int) else child_order

            res = reduce(lambda x, y: x + str(format(y, '04')), self.sections, base)
            if self.is_range():
                res = reduce(lambda x, y: x + str(format(y, '04')), self.toSections, res + "-")
            return res
        except Exception as e:
            logger.warning("Failed to execute order_id for {} : {}".format(self, e))
            return "Z"

    """ Methods for working with Versions and VersionSets """
    def storage_address(self, format="string"):
        """
        Return the storage location within a Version for this Ref.

        :return string or list: if format == 'string' return string where each address is separated by period else return list of addresses
        """
        address_list = ["chapter"] + self.index_node.address()[1:]
        if format == "list": return address_list
        return ".".join(address_list)

    def part_projection(self):
        """
        Returns the slice and storage address to return top-level sections for Versions of this ref

        Used as:

        ::

            Version().load({...},oref.part_projection())

        **Regarding projecting complex texts:**
        By specifying a projection that includes a non-existing element of our dictionary at the level of our selection,
        we cause all other elements of the dictionary to be unselected.
        A bit non-intuitive, but a huge savings of document size and time on the data transfer.
        http://stackoverflow.com/a/15798087/213042
        """
        # todo: reimplement w/ aggregation pipeline (see above)
        # todo: special case string 0?

        if self.index_node.is_virtual:
            return

        projection = {k: 1 for k in Version.required_attrs + Version.optional_attrs}
        del projection[Version.content_attr]  # Version.content_attr == "chapter"
        projection["_id"] = 0

        if not self.sections:
            # For simple texts, self.store_address() == "chapter".
            # For complex texts, it can be a deeper branch of the dictionary: "chapter.Bereshit.Torah" or similar
            projection[self.storage_address()] = 1
        else:
            offset = self.sections[0] - 1
            limit = 1 if self.range_index() > 0 else self.toSections[0] - self.sections[0] + 1
            slce = {"$slice": [offset, limit]}
            projection[self.storage_address()] = slce
            if len(self.index_node.address()) > 1:
                # create dummy key at level of our selection - see above.
                dummy_limiter = ".".join(["chapter"] + self.index_node.address()[1:-1] + ["hacky_dummy_key"])
                projection[dummy_limiter] = 1

        return projection

    def condition_query(self, lang=None, actual_lang=None):
        """
        Return condition to select only versions with content at the location of this Ref.
        `actual_lang` is a 2 letter ISO lang code that represents the actual language of the version
        this is as opposed to `lang` which can currently only be "he" or "en"
        Usage:

        ::

            VersionSet(oref.condition_query(lang))

        Can be combined with :meth:`part_projection` to only return the content indicated by this ref:

        ::

            VersionSet(oref.condition_query(lang), proj=oref.part_projection())

        :return: dict containing a query in the format expected by VersionSet
        """
        d = {
            "title": self.index.title,
        }
        if actual_lang:
            import re as pyre  # pymongo can only encode re.compile objects, not regex or re2.
            pattern = r"^(?!.*\[[a-z]{2}\]$).*" if actual_lang in {'en', 'he'} else fr"\[{actual_lang}\]$"
            d.update({"versionTitle": pyre.compile(pattern)})
        if lang:
            d.update({"language": lang})

        if self.index_node.is_virtual:
            try:
                d.update({"versionTitle": self.index_node.parent.lexicon.version_title})
            except:
                pass
            return d

        condition_addr = self.storage_address()
        if not isinstance(self.index_node, JaggedArrayNode):
            # This will also return versions with no content in this Ref location - since on the version, there is a dictionary present.
            # We could enter the dictionary and check each array, but it's not clear that it's necessary.
            d.update({
                condition_addr: {"$exists": True}
            })
        elif not self.sections:
            d.update({
                condition_addr: {"$exists": True, "$elemMatch": {"$nin": ["", [], 0]}}  # any non-empty element will do
            })
        elif not self.is_spanning():
            for s in range(0, len(self.sections) if not self.is_range() else len(self.sections) - 1):
                condition_addr += ".{}".format(self.sections[s] - 1)
            if len(self.sections) == self.index_node.depth and not self.is_range():
                d.update({
                    condition_addr: {"$exists": True, "$nin": ["", [], 0]}
                })
            else:
                d.update({
                    condition_addr: {"$exists": True, "$elemMatch": {"$nin": ["", [], 0]}}
                })
        else:
            # todo: If this method gets cached, then copies need to be made before the del below.
            parts = []
            refs = self.split_spanning_ref()
            for r in refs:
                q = r.condition_query()
                del q["title"]
                parts.append(q)
                d.update({
                    "$or": parts
                })

        return d

    def versionset(self, lang=None):
        """
        :class:`VersionsSet` of :class:`Version` objects that have content for this Ref in lang, projected

        :param lang: "he", "en", or None
        :return: :class:`VersionSet`
        """
        return VersionSet(self.condition_query(lang), proj=self.part_projection())

    def version_list(self):
        """
        A list of available text versions metadata matching this ref.
        If this ref is book level, decorate with the first available section of content per version.

        :return list: each list element is an object with keys 'versionTitle' and 'language'
        """
        fields = Version.optional_attrs + Version.required_attrs
        fields.remove('chapter') # not metadata
        versions = VersionSet(self.condition_query())
        version_list = []
        if self.is_book_level():
            for v in versions:
                version = {f: getattr(v, f, "") for f in fields}
                oref = v.first_section_ref() or v.get_index().nodes.first_leaf().first_section_ref()
                version["firstSectionRef"] = oref.normal()
                version_list.append(version)
            return version_list
        else:
            return [
                {f: getattr(v, f, "") for f in fields}
                for v in VersionSet(self.condition_query(), proj={f: 1 for f in fields})
            ]

    """ String Representations """
    def __str__(self):
        return self.uid()

    def __repr__(self):  # Wanted to use orig_tref, but repr can not include Unicode
        return self.__class__.__name__ + "('" + str(self.uid()) + "')"

    def he_book(self):
        return self.index_node.full_title("he")

    def _get_normal(self, lang):
        normal = self.index_node.full_title(lang)
        if not normal:
            if lang != "en":
                return self.normal()
            else:
                raise InputError("Failed to get English normal form for ref")

        if len(self.sections) == 0:
            return normal

        normal += " "

        normal += ":".join(
            [self.normal_section(i, lang) for i in range(len(self.sections))]
        )

        for i in range(len(self.sections)):
            if not self.sections[i] == self.toSections[i]:
                normal += "-{}".format(
                    ":".join(
                        [self.normal_section(i + j, lang, 'toSections') for j in range(len(self.toSections[i:]))]
                    )
                )
                break

        return normal

    def normal_sections(self, lang="en"):
        return [self.normal_section(i, lang) for i in range(len(self.sections))]

    def normal_toSections(self, lang="en"):
        return [self.normal_section(i, lang, 'toSections') for i in range(len(self.toSections))]

    def normal_section(self, section_index, lang='en', attr='sections', **kwargs):
        sections = getattr(self, attr)
        assert len(sections) > section_index
        offset = self._get_offset([x-1 for x in sections[:section_index]])
        return self.index_node.address_class(section_index).toStr(lang, sections[section_index]+offset, **kwargs)

    def normal_last_section(self, lang="en", **kwargs):
        """
        Return the display form of the last section
        Does not support ranges
        :param lang:
        :param kwargs:
            dotted=<bool> - Use dotted form for Hebrew talmud?,
            punctuation=<bool> - Use geresh for Hebrew numbers?
        :return:
        """
        assert not self.is_range()
        length = len(self.sections)
        if length == 0:
            return ""
        return self.normal_section(length - 1, lang, **kwargs)

    def he_normal(self):
        """
        :return string: Normal Hebrew string form
        """
        '''
            18 June 2015: Removed the special casing for Hebrew Talmud sub daf numerals
            Previously, Talmud lines had been normalised as Arabic numerals
        '''
        return self.normal('he')

    def uid(self):
        """
        To handle the fact that default nodes have the same name as their parents
        :return:
        """
        return self.normal() + ("<d>" if self.index_node.is_default() else "")

    def normal(self, lang='en') -> str:
        """
        :return string: Normal English or Hebrew string form
        """
        normal_attr = "_normal" if lang == 'en' else "_he_normal"
        if not getattr(self, normal_attr, None):
            #check if the second last section has function normal_range and the ref is a range. if true, parse
            #using address_class's normal_range function.  this is necessary to return Shabbat 7a-8b as Shabbat 7-8
            if len(self.sections) > 0 and hasattr(AddressType.to_class_by_address_type(self.index_node.addressTypes[len(self.sections) - 1]), "normal_range") and self.is_range():
                address_class = AddressType.to_class_by_address_type(self.index_node.addressTypes[len(self.sections) - 1])
                normal_form = address_class.normal_range(self, lang)
            else:
                normal_form = self._get_normal(lang)
            setattr(self, normal_attr, normal_form)
        return getattr(self, normal_attr)

    def text(self, lang="en", vtitle=None, exclude_copyrighted=False):
        """
        :param lang: "he" or "en"
        :param vtitle: optional. text title of the Version to get the text from
        :return: :class:`TextChunk` corresponding to this Ref
        """
        return TextChunk(self, lang, vtitle, exclude_copyrighted=exclude_copyrighted)

    def url(self, encode_html=True):
        """
        :param encode_html: boolean - True for encoding also HTML chars, or only our own things (like space to underscore)
        :return string: normal url form
        """
        if not self._url or not encode_html:
            url = self.normal()

            html_encoding_map = {'?': '%3F'}
            pretty_url_map = {' ': '_', ':': '.'}
            replace_map = pretty_url_map if not encode_html else pretty_url_map | html_encoding_map
            for key, value in replace_map.items():
                url = url.replace(key, value)

            # Change "Mishna_Brachot_2:3" to "Mishna_Brachot.2.3", but don't run on "Mishna_Brachot"
            if len(self.sections) > 0:
                url = '.'.join(url.rsplit('_', 1))

            if not encode_html:
                return url
            self._url = url
        return self._url

    def noteset(self, public=True, uid=None):
        """
        :return: :class:`NoteSet` for this Ref
        """
        from . import NoteSet
        if public and uid:
            query = {"ref": {"$regex": self.regex()}, "$or": [{"public": True}, {"owner": uid}]}
        elif public:
            query = {"ref": {"$regex": self.regex()}, "public": True}
        elif uid:
            query = {"ref": {"$regex": self.regex()}, "owner": uid}
        else:
            raise InputError("Can not get anonymous private notes")

        return NoteSet(query, sort=[("_id", -1)])

    def linkset(self):
        """
        :return: :class:`LinkSet` for this Ref
        """
        from . import LinkSet
        return LinkSet(self)

    def topiclinkset(self, with_char_level_links=False):
        from . import RefTopicLinkSet
        regex_list = self.regex(as_list=True)
        query = {"$or": [{"expandedRefs": {"$regex": r}} for r in regex_list]}
        if not with_char_level_links:
            query["charLevelData"] = {"$exists": False}
        return RefTopicLinkSet(query)

    def autolinker(self, **kwargs):
        """
        Returns the class best suited to perform auto linking,
        according to the "base_text_mapping" attr on the Index record.
        :return:
        """
        from sefaria.helper.link import AutoLinkerFactory
        if self.is_dependant() and getattr(self.index, 'base_text_mapping', None):
            return AutoLinkerFactory.instance_factory(self.index.base_text_mapping, self, **kwargs)
        else:
            return None

    def distance(self, ref, max_dist=None):
        """

        :param ref: ref which you want to compare distance with
        :param max_dist: maximum distance beyond which the function will return -1. it's suggested you set this param b/c alternative is very slow
        :return: int: num refs between self and ref. -1 if self and ref aren't in the same index
        """
        if self.index_node != ref.index_node:
            return -1

        # convert to base 0
        sec1 = self.sections[:]
        sec2 = ref.sections[:]
        for i in range(len(sec1)):
            sec1[i] -= 1
        for i in range(len(sec2)):
            sec2[i] -= 1

        distance = self.get_state_ja().distance(sec1,sec2)
        if max_dist and distance > max_dist:
            return -1
        else:
            return distance

    def get_all_anchor_refs(self, expanded_self, document_tref_list, document_tref_expanded):
        """
        Return all refs in document_ref_list that overlap with self. These are your anchor_refs. Useful for related API.
        :param list(str): expanded_self. precalculated list of segment trefs for self
        :param list(str): document_tref_list. list of trefs to from document in which you want to find archor refs
        :param list(Ref): document_tref_expanded. unique list of trefs that results from running Ref.expand_refs(document_tref_list)
        Returns tuple(list(Ref), list(list(Ref))). returns two lists. First are the anchor_refs for self. The second is a 2D list, where the inner list represents the expanded anchor refs for the corresponding position in anchor_ref_list
        """

        # narrow down search space to avoid excissive Ref instantiation
        unique_anchor_ref_expanded_set = set(expanded_self) & set(document_tref_expanded)
        document_tref_list = [tref for tref in document_tref_list if tref.startswith(self.index.title)]

        unique_anchor_ref_expanded_list = []
        for tref in unique_anchor_ref_expanded_set:
            try:
                oref = Ref(tref)
                unique_anchor_ref_expanded_list += [oref]
            except InputError:
                continue
        document_oref_list = []
        for tref in document_tref_list:
            try:
                oref = Ref(tref)
                document_oref_list += [oref]
            except InputError:
                continue
        anchor_ref_list = list(filter(lambda document_ref: self.overlaps(document_ref), document_oref_list))
        anchor_ref_expanded_list = [list(filter(lambda document_segment_ref: anchor_ref.overlaps(document_segment_ref), unique_anchor_ref_expanded_list)) for anchor_ref in anchor_ref_list]
        return anchor_ref_list, anchor_ref_expanded_list

    @staticmethod
    def expand_refs(refs):
        """
        Expands `refs` into list of unique segment refs. Usually used to preprocess database objects that reference refs
        :param refs: list of trefs to expand
        :return: list(trefs). unique segment refs derived from `refs`
        """

        expanded_set = set()
        for tref in refs:
            try:
                oref = Ref(tref)
            except (InputError, IndexError):
                continue
            try:
                expanded_set |= {r.normal() for r in oref.all_segment_refs()}
            except AssertionError:
                continue
        return list(expanded_set)

    @staticmethod
    def instantiate_ref_with_legacy_parse_fallback(tref: str) -> 'Ref':
        """
        Tries the following in order and returns the first that works
        - Instantiate `tref` as is
        - Use appropriate `LegacyRefParser` to parse `tref`
        - If ref has partial match, return partially matched ref
        Can raise an `InputError`
        @param tref: textual ref to parse
        @return: best `Ref` according to rules above
        """
        from sefaria.helper.legacy_ref import legacy_ref_parser_handler, LegacyRefParserError

        try:
            oref = Ref(tref)
            try:
                # this field can be set if a legacy parsed ref is pulled from cache
                delattr(oref, 'legacy_tref')
            except AttributeError:
                pass
            return oref
        except PartialRefInputError as e:
            matched_ref = Ref(e.matched_part)
            try:
                tref = Ref.__clean_tref(tref, matched_ref._lang)
                # replace input title with normalized title in case input was an alt title
                tref = tref.replace(e.matched_part, matched_ref.normal())
                legacy_ref_parser = legacy_ref_parser_handler[matched_ref.index.title]
                return legacy_ref_parser.parse(tref)
            except LegacyRefParserError:
                return matched_ref


class Library(object):
    """
    Operates as a singleton, through the instance called ``library``.

    Stewards the in-memory and in-cache objects that cover the entire collection of texts.

    Exposes methods to add, remove, or register change of an index record.
    These are primarily called by the dependencies mechanism on Index Create/Update/Destroy.


    Dependencies

    Initialization of the library happens in stages
    1. On load of this file, library instance is created.
        - Terms maps created
    2. On load of full model with `from sefaria.model import *`
        - Indexes are built
    3. On load of reader/views
        - toc tree is built (categories loaded)
        - autocompleters are created


    """

    def __init__(self):
        # Timestamp when library last stored shared cache items (toc, terms, etc)
        self.last_cached = None

        self.langs = ["en", "he"]

        # Maps, keyed by language, from index key to array of titles
        self._index_title_maps = {lang:{} for lang in self.langs}

        # Maps, keyed by language, from titles to schema nodes
        self._title_node_maps = {lang:{} for lang in self.langs}

        # Lists of full titles, keys are string generated from a combination of language code and "terms".  See method `full_title_list()`
        # Contains a list of only those titles from which citations are recognized in the auto-linker. Keyed by "citing-<lang>"
        self._full_title_lists = {}

        # Lists of full titles, including simple and commentary texts, keyed by language
        self._full_title_list_jsons = {}

        # Title regex strings & objects, keys are strings generated from a combination of arguments to `all_titles_regex` and `all_titles_regex_string`
        self._title_regex_strings = {}
        self._title_regexes = {}

        # Maps, keyed by language, from term names to text refs
        self._term_ref_maps = {lang: {} for lang in self.langs}

        # Map from index title to index object
        self._index_map = {}

        # Table of Contents
        self._toc = None
        self._toc_json = None
        self._toc_tree = None
        self._topic_toc = None
        self._topic_toc_json = None
        self._topic_toc_category_mapping = None
        self._topic_link_types = None
        self._topic_data_sources = None
        self._category_id_dict = None
        self._toc_size = 16

        # Spell Checking and Autocompleting
        self._full_auto_completer = {}
        self._lexicon_auto_completer = {}
        self._cross_lexicon_auto_completer = None

        # Term Mapping
        self._simple_term_mapping = {}
        self._full_term_mapping = {}
        self._simple_term_mapping_json = None
        self._linker_by_lang = {}

        # Topics
        self._topic_mapping = {}

        # Virtual books
        self._virtual_books = []

        # Initialization Checks
        # These values are set to True once their initialization is complete
        self._toc_tree_is_ready = False
        self._full_auto_completer_is_ready = False
        self._lexicon_auto_completer_is_ready = False
        self._cross_lexicon_auto_completer_is_ready = False
        self._topic_auto_completer_is_ready = False

        if not hasattr(sys, '_doc_build'):  # Can't build cache without DB
            self.get_simple_term_mapping() # this will implicitly call self.build_term_mappings() but also make sure its cached.

    def _build_index_maps(self):
        """
        Build index and title node dicts in an efficient way
        """

        # self._index_title_commentary_maps if index_object.is_commentary() else self._index_title_maps
        # simple texts
        self._index_map = {i.title: i for i in IndexSet() if i.nodes}
        forest = [i.nodes for i in list(self._index_map.values())]
        self._title_node_maps = {lang: {} for lang in self.langs}
        self._index_title_maps = {lang:{} for lang in self.langs}

        for tree in forest:
            try:
                for lang in self.langs:
                    tree_titles = tree.title_dict(lang)
                    self._index_title_maps[lang][tree.key] = list(tree_titles.keys())
                    self._title_node_maps[lang].update(tree_titles)
            except IndexSchemaError as e:
                logger.error("Error in generating title node dictionary: {}".format(e))

    def _reset_index_derivative_objects(self, include_auto_complete=False):
        """
        Resets the objects which are derivatives of the index
        """
        self._full_title_lists = {}
        self._full_title_list_jsons = {}
        self._title_regex_strings = {}
        self._title_regexes = {}
        # TOC is handled separately since it can be edited in place

    def rebuild(self, include_toc = False, include_auto_complete=False):
        self.get_simple_term_mapping_json(rebuild=True)
        self._build_topic_mapping()
        self._build_index_maps()
        self._full_title_lists = {}
        self._full_title_list_jsons = {}
        self.reset_text_titles_cache()
        self._title_regex_strings = {}
        self._title_regexes = {}
        Ref.clear_cache()
        in_memory_cache.reset_all()
        if include_toc:
            self.rebuild_toc()

    def rebuild_toc(self, skip_toc_tree=False):
        """
        Rebuilds the TocTree representation at startup time upon load of the Library class singleton.
        The ToC is a tree of nodes that represents the ToC as seen on the Sefaria homepage.
        This function also builds other critical data structures, such as the topics ToC.
        While building these ToC data structures, this function also builds the equivalent JSON structures
        as an API optimization.

        :param skip_toc_tree: Boolean
        """
        if not skip_toc_tree:
            self._toc_tree = self.get_toc_tree(rebuild=True)
        self._toc = self.get_toc(rebuild=True)
        self._toc_json = self.get_toc_json(rebuild=True)
        self._topic_toc = self.get_topic_toc(rebuild=True)
        self._topic_toc_json = self.get_topic_toc_json(rebuild=True)
        self._topic_toc_category_mapping = self.get_topic_toc_category_mapping(rebuild=True)
        self._category_id_dict = None
        scache.delete_template_cache("texts_list")
        scache.delete_template_cache("texts_dashboard")
        self._full_title_list_jsons = {}

    def init_shared_cache(self, rebuild=False):
        self.get_toc(rebuild=rebuild)
        self.get_toc_json(rebuild=rebuild)
        self.get_topic_mapping(rebuild=rebuild)
        self.get_topic_toc(rebuild=rebuild)
        self.get_topic_toc_json(rebuild=rebuild)
        self.get_topic_toc_category_mapping(rebuild=rebuild)
        self.get_text_titles_json(rebuild=rebuild)
        self.get_simple_term_mapping(rebuild=rebuild)
        self.get_simple_term_mapping_json(rebuild=rebuild)
        self.get_virtual_books(rebuild=rebuild)
        if rebuild:
            scache.delete_shared_cache_elem("regenerating")

    def get_last_cached_time(self):
        if not self.last_cached:
            self.last_cached = scache.get_shared_cache_elem("last_cached")
        if not self.last_cached:
            self.set_last_cached_time()
        return self.last_cached

    def set_last_cached_time(self):
        self.last_cached = time.time() # just use the unix timestamp, we dont need any fancy timezone faffing, just objective point in time.
        scache.set_shared_cache_elem("last_cached", self.last_cached)

    def get_toc(self, rebuild=False):
        """
        Returns the ToC Tree from the cache, DB or by generating it, as needed.
        """
        if rebuild or not self._toc:
            if not rebuild:
                self._toc = scache.get_shared_cache_elem('toc')
            if rebuild or not self._toc:
                self._toc = self.get_toc_tree().get_serialized_toc()  # update_table_of_contents()
                scache.set_shared_cache_elem('toc', self._toc)
                self.set_last_cached_time()
        return self._toc

    def get_toc_json(self, rebuild=False):
        """
        Returns as JSON representation of the ToC. This is generated on Library start up as an
        optimization for the API, to allow retrieval of the data with a single call.
        """
        if rebuild or not self._toc_json:
            if not rebuild:
                self._toc_json = scache.get_shared_cache_elem('toc_json')
            if rebuild or not self._toc_json:
                self._toc_json = json.dumps(self.get_toc(), ensure_ascii=False)
                scache.set_shared_cache_elem('toc_json', self._toc_json)
                self.set_last_cached_time()
        return self._toc_json

    def get_toc_tree(self, rebuild=False, mobile=False):
        """
        :param mobile: (Aug 30, 2021) Added as a patch after navigation redesign launch. Currently only adds
        'firstSection' to toc for mobile export. This field is no longer required on prod but is still required
        on mobile until the navigation redesign happens there.
        """
        if rebuild or not self._toc_tree:
            from sefaria.model.category import TocTree
            self._toc_tree = TocTree(self, mobile=mobile)
        self._toc_tree_is_ready = True
        return self._toc_tree

    def get_topic_toc(self, rebuild=False):
        """
        Returns dictionary representation of Topics ToC.
         """
        if rebuild or not self._topic_toc:
            if not rebuild:
                self._topic_toc = scache.get_shared_cache_elem('topic_toc')
            if rebuild or not self._topic_toc:
                self._topic_toc = self.get_topic_toc_json_recursive()
                scache.set_shared_cache_elem('topic_toc', self._topic_toc)
                self.set_last_cached_time()
        return self._topic_toc

    def get_topic_toc_json(self, rebuild=False):
        """
        Returns JSON representation of Topics ToC.
        :param rebuild: Boolean
        """
        if rebuild or not self._topic_toc_json:
            if not rebuild:
                self._topic_toc_json = scache.get_shared_cache_elem('topic_toc_json')
            if rebuild or not self._topic_toc_json:
                self._topic_toc_json = json.dumps(self.get_topic_toc(), ensure_ascii=False)
                scache.set_shared_cache_elem('topic_toc_json', self._topic_toc_json)
                self.set_last_cached_time()
        return self._topic_toc_json

    def get_topic_toc_json_recursive(self, topic=None, explored=None, with_descriptions=False):
        """
        Returns JSON representation of Topics ToC
        :param topic: Topic
        :param explored: Set
        :param with_descriptions: Boolean
        """
        from .topic import Topic, TopicSet, IntraTopicLinkSet
        explored = explored or set()
        unexplored_top_level = False    # example would be the first case of 'Holidays' encountered as it is top level,
                                        # this variable will allow us to force all top level categories to have children
        if topic is None:
            ts = TopicSet({"isTopLevelDisplay": True})
            children = [t.slug for t in ts]
            topic_json = {}
        else:
            children = [] if topic.slug in explored else [l.fromTopic for l in IntraTopicLinkSet({"linkType": "displays-under", "toTopic": topic.slug})]
            topic_json = {
                "slug": topic.slug,
                "shouldDisplay": True if len(children) > 0 else topic.should_display(),
                "en": topic.get_primary_title("en"),
                "he": topic.get_primary_title("he"),
                "displayOrder": getattr(topic, "displayOrder", 10000)
            }

            with_descriptions = True # TODO revisit for data size / performance
            if with_descriptions:
                if getattr(topic, "categoryDescription", False):
                    topic_json['categoryDescription'] = topic.categoryDescription
                description = getattr(topic, "description", None)
                if description is not None and getattr(topic, "description_published", False):
                    topic_json['description'] = description

            unexplored_top_level = getattr(topic, "isTopLevelDisplay", False) and getattr(topic, "slug",
                                                                                          None) not in explored
            explored.add(topic.slug)
        if len(children) > 0 or topic is None or unexplored_top_level:
            # make sure root gets children no matter what and make sure that unexplored top-level topics get children no matter what
            topic_json['children'] = []
        for child in children:
            child_topic = Topic().load({'slug': child})
            if child_topic is None:
                logger.warning("While building topic TOC, encountered non-existant topic slug: {}".format(child))
                continue
            topic_json['children'] += [self.get_topic_toc_json_recursive(child_topic, explored, with_descriptions)]
        if len(children) > 0:
            topic_json['children'].sort(key=lambda x: x['displayOrder'])
        if topic is None:
            return topic_json['children']
        return topic_json

    def build_topic_toc_category_mapping(self) -> dict:
        """
        Maps every slug in topic toc to its parent slug. This is usually the top level category, but in
        the case of laws it is the second-level category
        """
        topic_toc_category_mapping = {}
        topic_toc = self.get_topic_toc()
        discovered_slugs = set()
        topic_stack = [t for t in topic_toc]
        while len(topic_stack) > 0:
            curr_topic = topic_stack.pop()
            if curr_topic['slug'] in discovered_slugs: continue
            discovered_slugs.add(curr_topic['slug'])
            for child_topic in curr_topic.get('children', []):
                topic_stack += [child_topic]
                topic_toc_category_mapping[child_topic['slug']] = curr_topic['slug']
        return topic_toc_category_mapping

    def get_topic_toc_category_mapping(self, rebuild=False) -> dict:
        """
        Returns the category mapping as a dictionary for the topics ToC. Loads on Library startup.
        :param rebuild: Boolean
        """
        if rebuild or not self._topic_toc_category_mapping:
            if not rebuild:
                self._topic_toc_category_mapping = scache.get_shared_cache_elem('topic_toc_category_mapping')
            if rebuild or not self._topic_toc_category_mapping:
                self._topic_toc_category_mapping = self.build_topic_toc_category_mapping()
                scache.set_shared_cache_elem('topic_toc_category_mapping', self._topic_toc_category_mapping)
                self.set_last_cached_time()
        return self._topic_toc_category_mapping

    def get_search_filter_toc(self):
        """
        Returns TOC, modified  according to `Category.searchRoot` flags to correspond to the filters
        """
        from sefaria.model.category import TocTree, CategorySet, TocCategory
        toctree = TocTree(self)     # Don't use the cached one.  We're going to rejigger it.
        root = toctree.get_root()
        toc_roots = [x.lastPath for x in sorted(library.get_top_categories(full_records=True), key=lambda x: x.order)]
        reroots = CategorySet({"searchRoot": {"$exists": True}})

        # Get all the unique new roots, create nodes for them, and attach them to the tree
        new_root_titles = list({c.searchRoot for c in reroots})

        def root_title_sorter(t):
            # .split() to remove " Commentary"
            sort_key = t.split()[0]
            try:
                return toc_roots.index(sort_key)
            except ValueError:
                return 10000

        new_root_titles.sort(key=root_title_sorter)
        new_roots = {}
        for t in new_root_titles:
            tc = TocCategory()
            tc.add_title(t, "en", primary=True)
            tc.add_title(Term.normalize(t, "he"), "he", primary=True)
            tc.append_to(root)
            new_roots[t] = tc

        # Re-parent all of the nodes with "searchRoot"
        for cat in reroots:
            tocnode = toctree.lookup(cat.path)
            tocnode.detach()
            tocnode.append_to(new_roots[cat.searchRoot])

        # todo: return 'thin' param when search toc is retired.
        return [c.serialize(thin=True) for c in root.children]

    def get_topic_link_type(self, link_type):
        """
        Returns a TopicLinkType with a slug of link_type (parameter) if not already present
        :param link_type: String
        """
        from .topic import TopicLinkTypeSet
        if not self._topic_link_types:
            # pre-populate topic link types
            self._topic_link_types = {
                link_type.slug: link_type for link_type in TopicLinkTypeSet()
            }
        return self._topic_link_types.get(link_type, None)

    def get_topic_data_source(self, data_source):
        """
        Returns a TopicDataSource with the data_source (parameter) slug if not already present
        :param data_source: String
        """
        from .topic import TopicDataSourceSet
        if not self._topic_data_sources:
            # pre-populate topic data sources
            self._topic_data_sources = {
                data_source.slug: data_source for data_source in TopicDataSourceSet()
            }
        return self._topic_data_sources.get(data_source, None)

    def get_collections_in_library(self):
        """
        Calls itself on the _toc_tree attribute to get all the collections in the Library upon
        loading.
        """
        return self._toc_tree.get_collections_in_library()

    def build_full_auto_completer(self):
        """
        Builds full auto completer across people, topics, categories, parasha, users, and collections
        for each of the languages in the library.
        Sets internal boolean to True upon successful completion to indicate auto completer is ready.
        """
        from .autospell import AutoCompleter
        self._full_auto_completer = {
            lang: AutoCompleter(lang, library, include_people=True, include_topics=True, include_categories=True, include_parasha=False, include_users=True, include_collections=True) for lang in self.langs
        }

        for lang in self.langs:
            self._full_auto_completer[lang].set_other_lang_ac(self._full_auto_completer["he" if lang == "en" else "en"])
        self._full_auto_completer_is_ready = True

    def build_lexicon_auto_completers(self):
        """
        Sets lexicon autocompleter for each lexicon in LexiconSet using a LexiconTrie
        Sets internal boolean to True upon successful completion to indicate auto completer is ready.

        """
        from .autospell import LexiconTrie
        from .lexicon import LexiconSet
        self._lexicon_auto_completer = {
            lexicon.name: LexiconTrie(lexicon.name) for lexicon in LexiconSet({'should_autocomplete': True})
        }
        self._lexicon_auto_completer_is_ready = True

    def build_cross_lexicon_auto_completer(self):
        """
        Builds the cross lexicon auto completer excluding titles
        Sets internal boolean to True upon successful completion to indicate auto completer is ready.
        """
        from .autospell import AutoCompleter
        self._cross_lexicon_auto_completer = AutoCompleter("he", library, include_titles=False, include_lexicons=True)
        self._cross_lexicon_auto_completer_is_ready = True


    def cross_lexicon_auto_completer(self):
        """
        Returns the cross lexicon auto completer. If the auto completer was not initially loaded,
        it rebuilds before returning, emitting warnings to the logger.
        """
        if self._cross_lexicon_auto_completer is None:
            logger.warning("Failed to load cross lexicon auto completer, rebuilding.")
            self.build_cross_lexicon_auto_completer()  # I worry that these could pile up.
            logger.warning("Built cross lexicon auto completer.")
        return self._cross_lexicon_auto_completer

    def lexicon_auto_completer(self, lexicon):
        """
        Returns the value of the lexicon auto completer map given a lexicon key. If the key
        is not present, it assumes the need to rebuild the lexicon_auto_completer and calls the build
        function with appropriate logger warnings before returning the desired result

        :param lexicon: String
        """
        try:
            return self._lexicon_auto_completer[lexicon]
        except KeyError:
            logger.warning("Failed to load {} auto completer, rebuilding.".format(lexicon))
            self.build_lexicon_auto_completers()  # I worry that these could pile up.
            logger.warning("Built {} auto completer.".format(lexicon))
            return self._lexicon_auto_completer[lexicon]

    def full_auto_completer(self, lang):
        try:
            return self._full_auto_completer[lang]
        except KeyError:
            logger.warning("Failed to load full {} auto completer, rebuilding.".format(lang))
            self.build_full_auto_completer()  # I worry that these could pile up.
            logger.warning("Built full {} auto completer.".format(lang))
            return self._full_auto_completer[lang]

    def recount_index_in_toc(self, indx):
        # This is used in the case of a remotely triggered multiserver update
        if isinstance(indx, str):
            indx = Index().load({"title": indx})

        self.get_toc_tree().update_title(indx, recount=True)

        self.rebuild_toc(skip_toc_tree=True)

    def delete_category_from_toc(self, category):
        # This is used in the case of a remotely triggered multiserver update
        toc_node = self.get_toc_tree().lookup(category.path)
        if toc_node:
            self.get_toc_tree().remove_category(toc_node)

    def delete_index_from_toc(self, indx, categories = None):
        """
        :param indx: The Index object.  When called remotely, in multiserver mode, the string title of the index
        :param categories: Only explicitly passed when called remotely, in multiserver mode
        :return:
        """
        cats = categories or indx.categories
        title = indx.title if isinstance(indx, Index) else indx

        toc_node = self.get_toc_tree().lookup(cats, title)
        if toc_node:
            self.get_toc_tree().remove_index(toc_node)

        self.rebuild_toc(skip_toc_tree=True)

    def update_index_in_toc(self, indx, old_ref=None):
        """
        :param indx: The Index object.  When called remotely, in multiserver mode, the string title of the index
        :param old_ref:
        :return:
        """

        # This is used in the case of a remotely triggered multiserver update
        if isinstance(indx, str):
            indx = Index().load({"title": indx})

        self.get_toc_tree().update_title(indx, old_ref=old_ref, recount=False)

        self.rebuild_toc(skip_toc_tree=True)

    def get_index(self, bookname):
        """
        Factory - returns a :class:`Index` object that has the given bookname

        :param string bookname: Name of the book.
        :return:
        """
        # look for result in indices cache
        if not bookname:
            raise BookNameError("No book provided.")

        indx = self._index_map.get(bookname)
        if not indx:
            bookname = (bookname[0].upper() + bookname[1:]).replace("_", " ")  # todo: factor out method

            # todo: cache
            lang = "he" if has_hebrew(bookname) else "en"
            node = self._title_node_maps[lang].get(bookname)
            if node:
                indx = node.index

            if not indx:
                raise BookNameError("No book named '{}'.".format(bookname))

            self._index_map[bookname] = indx

        return indx

    def add_index_record_to_cache(self, index_object = None, rebuild = True):
        """
        Update library title dictionaries and caches with information from provided index.
        Index can be passed with primary title in `index_title` or as an object in `index_object`
        :param index_object: Index record
        :param rebuild: Perform a rebuild of derivative objects afterwards?  False only in cases of batch update.
        :return:
        """
        assert index_object, "Library.add_index_record_to_cache called without index"

        # This is used in the case of a remotely triggered multiserver update
        if isinstance(index_object, str):
            index_object = Index().load({"title": index_object})

        self._index_map[index_object.title] = index_object
        try:
            for lang in self.langs:
                title_dict = index_object.nodes.title_dict(lang)
                self._index_title_maps[lang][index_object.title] = list(title_dict.keys())
                self._title_node_maps[lang].update(title_dict)
        except IndexSchemaError as e:
            logger.error("Error in generating title node dictionary: {}".format(e))

        if rebuild:
            self._reset_index_derivative_objects()

    def remove_index_record_from_cache(self, index_object=None, old_title=None, rebuild = True):
        """
        Update provided index from library title dictionaries and caches
        :param index_object: In the local case - the index object to remove.  In the remote case, the name of the index object to remove.
        :param old_title: In the case of a title change - the old title of the Index record
        :param rebuild: Perform a rebuild of derivative objects afterwards?
        :return:
        """

        index_object_title = old_title if old_title else (index_object.title if isinstance(index_object, Index) else index_object)
        Ref.remove_index_from_cache(index_object_title)

        for lang in self.langs:
            simple_titles = self._index_title_maps[lang].get(index_object_title)
            if simple_titles:
                for key in simple_titles:
                    try:
                        del self._title_node_maps[lang][key]
                    except KeyError:
                        logger.warning("Tried to delete non-existent title '{}' of index record '{}' from title-node map".format(key, index_object_title))
                    try:
                        del self._index_map[key]
                    except KeyError:
                        pass
                del self._index_title_maps[lang][index_object_title]
            else:
                logger.warning("Failed to remove '{}' from {} index-title and title-node cache: nothing to remove".format(index_object_title, lang))
                return

        if rebuild:
            self._reset_index_derivative_objects()

    def refresh_index_record_in_cache(self, index_object, old_title = None):
        """
        Update library title dictionaries and caches for provided index
        :param index_object: In the local case - the index object to remove.  In the remote case, the name of the index object to remove.
        :param old_title: In the case of a title change - the old title of the Index record
        :return:
        """
        index_object_title = index_object.title if isinstance(index_object, Index) else index_object
        self.remove_index_record_from_cache(index_object, old_title=old_title, rebuild=False)
        new_index = Index().load({"title": index_object_title})
        assert new_index, "No Index record found for {}: {}".format(index_object.__class__.__name__, index_object_title)
        self.add_index_record_to_cache(new_index, rebuild=True)

    # todo: the for_js path here does not appear to be in use.
    # todo: Rename, as method not gauraunteed to return all titles
    def all_titles_regex_string(self, lang="en", with_terms=False, citing_only=False): #, for_js=False):
        """
        :param lang: "en" or "he"
        :param with_terms: Include terms in regex.  (Will have no effect if citing_only is True)
        :param citing_only: Match only those texts which have is_cited set to True
        :param for_js:
        :return:
        """
        key = lang
        if citing_only:
            key += "_citations"
        elif with_terms:
            key += "_terms"
        re_string = self._title_regex_strings.get(key)
        if not re_string:
            re_string = ""
            if citing_only:
                simple_books = list(map(re.escape, self.citing_title_list(lang)))
            else:
                simple_books = list(map(re.escape, self.full_title_list(lang, with_terms=with_terms)))
            simple_book_part = r'|'.join(sorted(simple_books, key=len, reverse=True))  # Match longer titles first

            # re_string += ur'(?:^|[ ([{>,-]+)' if for_js else u''  # Why don't we check for word boundaries internally as well?
            # re_string += ur'(?:\u05d5?(?:\u05d1|\u05de|\u05dc|\u05e9|\u05d8|\u05d8\u05e9)?)' if for_js and lang == "he" else u'' # likewise leading characters in Hebrew?
            # re_string += ur'(' if for_js else
            re_string = r'(?P<title>'
            re_string += simple_book_part
            re_string += r')'
            re_string += r'($|[:., <]+)'
            self._title_regex_strings[key] = re_string

        return re_string

    #WARNING: Do NOT put the compiled re2 object into redis.  It gets corrupted.
    def all_titles_regex(self, lang="en", with_terms=False, citing_only=False):
        """
        :return: A regular expression object that will match any known title in the library in the provided language
        :param lang: "en" or "he"
        :param bool with_terms: Default False.  If True, include shared titles ('terms'). (Will have no effect if citing_only is True)
        :param citing_only: Match only those texts which have is_cited set to True
        :raise: InputError: if lang == "he" and commentary == True

        Uses re2 if available.  See https://github.com/Sefaria/Sefaria-Project/wiki/Regular-Expression-Engines
        """
        if citing_only:
            key = "citing_titles_regex_" + lang
        else:
            key = "all_titles_regex_" + lang
            key += "_terms" if with_terms else ""
        reg = self._title_regexes.get(key)
        if not reg:
            re_string = self.all_titles_regex_string(lang, with_terms, citing_only)
            # increase max memory b/c re2 is RAM limited and the titles regex exceeds this limit
            options = re.Options()
            options.max_mem = 512 * 1024 * 1024
            reg = re.compile(re_string, options=options)
            self._title_regexes[key] = reg
        return reg

    def ref_list(self):
        """
        :return: list of all section-level Refs in the library
        """
        section_refs = []
        for indx in self.all_index_records():
            try:
                section_refs += indx.all_section_refs()
            except Exception as e:
                logger.warning("Failed to get section refs for {}: {}".format(getattr(indx, "title", "unknown index"), e))
        return section_refs

    def get_term_dict(self, lang="en"):
        """
        :return: dict of shared titles that have an explicit ref
        :param lang: "he" or "en"
        """
        # key = "term_dict_" + lang
        # term_dict = self.local_cache.get(key)
        term_dict = self._term_ref_maps.get(lang)
        if not term_dict:
            self.build_term_mappings()
            term_dict = self._term_ref_maps.get(lang)

        return term_dict

    def build_term_mappings(self):
        """
           Build simple and full term mappings
           A full term mapping has the term name as the key, and the term as the value.
           A simple term mapping has the term name as the key, and a dictionary containing the English and Hebrew
           primary titles for the terms as the value.
        """
        self._simple_term_mapping = {}
        self._full_term_mapping = {}
        for term in TermSet():
            self._full_term_mapping[term.name] = term
            self._simple_term_mapping[term.name] = {"en": term.get_primary_title("en"),
                                                    "he": term.get_primary_title("he")}
            if hasattr(term, "ref"):
                for lang in self.langs:
                    for title in term.get_titles(lang):
                        self._term_ref_maps[lang][title] = term.ref

    def get_simple_term_mapping(self, rebuild=False):
        if rebuild or not self._simple_term_mapping:
            if not rebuild:
                self._simple_term_mapping = scache.get_shared_cache_elem('term_mapping')
            if rebuild or not self._simple_term_mapping:
                self.build_term_mappings()
                scache.set_shared_cache_elem('term_mapping', self._simple_term_mapping)
                self.set_last_cached_time()
        return self._simple_term_mapping

    def get_simple_term_mapping_json(self, rebuild=False):
        """
        Returns JSON representation of terms.
        """
        if rebuild or not self._simple_term_mapping_json:
            if not rebuild:
                self._simple_term_mapping_json = scache.get_shared_cache_elem('term_mapping_json')
            if rebuild or not self._simple_term_mapping_json:
                self._simple_term_mapping_json = json.dumps(self.get_simple_term_mapping(rebuild=rebuild), ensure_ascii=False)
                scache.set_shared_cache_elem('term_mapping_json', self._simple_term_mapping_json)
                self.set_last_cached_time()
        return self._simple_term_mapping_json

    def get_term(self, term_name):
        """
        Returns the full term, if mapping not present, builds the full term mapping.
        :param term_name: String
        :returns: full Term (Mongo Record)
        """
        if not self._full_term_mapping:
            self.build_term_mappings()
        return self._full_term_mapping.get(term_name) if term_name in self._full_term_mapping else Term().load({"name": term_name})



    def get_topic(self, slug):
        """
        Returns a dictionary containing the keys "en" and "he".
        The "en" field has a value of the topic's English primary title, and the "he" field has a
        value of the topic's Hebrew primary title.
        :param slug: String
        :returns: topic map for the given slug Dictionary
        """
        return self._topic_mapping[slug]

    def get_topic_mapping(self, rebuild=False):
        """
        Returns the topic mapping if it exists, if not rebuilds it and returns
        :param rebuild: Boolean (optional, default set to False)
        """
        tm = self._topic_mapping
        if not tm or rebuild:
            tm = self._build_topic_mapping()
        return tm

    def _build_topic_mapping(self):
        """
        Builds the topic mapping. The topic mapping is a dictionary with keys, where each key
        is a slug of a topic.
        That key contains the value of another dictionary, with the keys "en" and "he".
        The "en" field has a value of the topic's English primary title, and the "he" field has a
        value of the topic's Hebrew primary title.
        :returns: topic map for the given slug Dictionary
        """
        from .topic import Topic, TopicSet
        self._topic_mapping = {t.slug: {"en": t.get_primary_title("en"), "he": t.get_primary_title("he")} for t in TopicSet()}
        return self._topic_mapping

    def get_linker(self, lang: str, rebuild=False):
        linker = self._linker_by_lang.get(lang)
        if not linker or rebuild:
            linker = self.build_linker(lang)
        return linker

    def build_linker(self, lang: str):
        from sefaria.model.linker.linker import Linker

        logger.info("Loading Spacy Model")

        named_entity_resolver = self._build_named_entity_resolver(lang)
        ref_resolver = self._build_ref_resolver(lang)
        named_entity_recognizer = self._build_named_entity_recognizer(lang)
        cat_resolver = self._build_category_resolver(lang)
        self._linker_by_lang[lang] = Linker(named_entity_recognizer, ref_resolver, named_entity_resolver, cat_resolver)
        return self._linker_by_lang[lang]

    @staticmethod
    def _build_named_entity_resolver(lang: str):
        from .linker.named_entity_resolver import TopicMatcher, NamedEntityResolver

        named_entity_types_to_topics = {
            "PERSON": {"ontology_roots": ['people'], "single_slugs": ['god', 'the-tetragrammaton']},
            "GROUP": {'ontology_roots': ["group-of-people"]},
        }
        return NamedEntityResolver(TopicMatcher(lang, named_entity_types_to_topics))

    @staticmethod
    def _build_named_entity_recognizer(lang: str):
        from .linker.linker_entity_recognizer import LinkerEntityRecognizer
        from .linker.named_entity_recognizer import NERFactory

        return LinkerEntityRecognizer(
            lang,
            NERFactory.create('spacy', RAW_REF_MODEL_BY_LANG_FILEPATH[lang]),
            NERFactory.create('spacy', RAW_REF_PART_MODEL_BY_LANG_FILEPATH[lang])
        )

    def _build_category_resolver(self, lang: str):
        from sefaria.model.category import CategorySet, Category
        from .linker.category_resolver import CategoryResolver, CategoryMatcher
        categories: list[Category] = CategorySet({"match_templates": {"$exists": True}}).array()
        return CategoryResolver(CategoryMatcher(lang, categories))

    def _build_ref_resolver(self, lang: str):
        from .linker.match_template import MatchTemplateTrie
        from .linker.ref_resolver import RefResolver, TermMatcher
        from sefaria.model.schema import NonUniqueTermSet

        root_nodes = list(filter(lambda n: getattr(n, 'match_templates', None) is not None, self.get_index_forest()))
        alone_nodes = reduce(lambda a, b: a + b.index.get_referenceable_alone_nodes(), root_nodes, [])
        non_unique_terms = NonUniqueTermSet()

        return RefResolver(
           lang, MatchTemplateTrie(lang, nodes=(root_nodes + alone_nodes), scope='alone'),
           TermMatcher(lang, non_unique_terms),
        )

    def get_index_forest(self):
        """
        :return: list of root Index nodes.
        """
        root_nodes = [i.nodes for i in self.all_index_records()]
        return root_nodes

    def all_index_records(self):
        """
        Returns an array of all index records
        """
        return [self._index_map[k] for k in list(self._index_title_maps["en"].keys())]

    def get_title_node_dict(self, lang="en"):
        """
        :param lang: "he" or "en"
        :return:  dictionary of string titles and the nodes that they point to.

        Does not include bare commentator names, like *Rashi*.
        """
        return self._title_node_maps[lang]

    # todo: handle terms
    def get_schema_node(self, title, lang=None):
        """
        :param string title:
        :param lang: "en" or "he"
        :return: a particular SchemaNode that matches the provided title and language
        :rtype: :class:`sefaria.model.schema.SchemaNode`
        """
        if not lang:
            lang = "he" if has_hebrew(title) else "en"
        title = title.replace("_", " ")
        return self.get_title_node_dict(lang).get(title)

    def citing_title_list(self, lang="en"):
        """
        :param lang: "he" or "en"
        :return: list of all titles that can be recognized as an inline citation
        """
        key = "citing-{}".format(lang)
        titles = self._full_title_lists.get(key)
        if not titles:
            titles = []
            for i in IndexSet({"is_cited": True}):
                titles.extend(self._index_title_maps[lang][i.title])
            self._full_title_lists[key] = titles
        return titles

    def full_title_list(self, lang="en", with_terms=False):
        """
        :param lang: "he" or "en"
        :param with_terms: if True, includes shared titles ('terms')
        :return: list of strings of all possible titles
        """
        key = lang
        key += "_terms" if with_terms else ""
        titles = self._full_title_lists.get(key)
        if not titles:
            titles = list(self.get_title_node_dict(lang).keys())
            if with_terms:
                titles += list(self.get_term_dict(lang).keys())
            self._full_title_lists[key] = titles
        return titles

    def build_text_titles_json(self, lang="en"):
        """
        :return: JSON of full texts list, (cached)
        """
        title_list = self.full_title_list(lang=lang)
        if lang == "en":
            toc_titles = self.get_toc_tree().flatten()
            secondary_list = list(set(title_list) - set(toc_titles))
            title_list = toc_titles + secondary_list
        return title_list

    def get_text_titles_json(self, lang="en", rebuild=False):
        """
        Returns the json text title list
        :param lang: String (optional, default set to 'en')
        :param rebuild: Boolean (optional, default set to False)
        """
        if rebuild or not self._full_title_list_jsons.get(lang):
            if not rebuild:
                self._full_title_list_jsons[lang] = scache.get_shared_cache_elem('books_'+lang+'_json')
            if rebuild or not self._full_title_list_jsons.get(lang):
                title_list = self.build_text_titles_json(lang=lang)
                title_list_json = json.dumps(title_list, ensure_ascii=False)
                self._full_title_list_jsons[lang] = title_list_json
                scache.set_shared_cache_elem('books_' + lang, title_list)
                scache.set_shared_cache_elem('books_'+lang+'_json', title_list_json)
                self.set_last_cached_time()
        return self._full_title_list_jsons[lang]

    def reset_text_titles_cache(self):
        """
        Resets the text titles for all languages by clearing the existing titles from the cache.
        """
        for lang in self.langs:
            scache.delete_shared_cache_elem('books_' + lang)
            scache.delete_shared_cache_elem('books_' + lang + '_json')

    def get_text_categories(self):
        """
        :return: List of all known text categories.
        """
        return IndexSet().distinct("categories")

    def get_indexes_in_category(self, category, include_dependant=False, full_records=False):
        """
        :param string category: Name of category
        :param bool include_dependant: If true includes records of Commentary and Targum
        :param bool full_records: If True will return the actual :class: 'IndexSet' otherwise just the titles
        :return: :class:`IndexSet` of :class:`Index` records in the specified category
        """

        if not include_dependant:
            q = {"categories": category, 'dependence': {'$in': [False, None]}}
        else:
            q = {"categories": category}

        return IndexSet(q) if full_records else IndexSet(q).distinct("title")

    def get_indexes_in_category_path(self, path: list, include_dependant=False, full_records=False) -> Union[IndexSet, list]:
        """
        :param list path: list of category names, starting from root.
        :param bool include_dependant: If true includes records of Commentary and Targum
        :param bool full_records: If True will return the actual :class: 'IndexSet' otherwise just the titles
        :return: :class:`IndexSet` of :class:`Index` records in the specified category path
        """
        q = {} if include_dependant else {'dependence': {'$in': [False, None]}}
        for icat, cat in enumerate(path):
            q[f'categories.{icat}'] = cat

        return IndexSet(q) if full_records else IndexSet(q).distinct("title")

    def get_indexes_in_corpus(self, corpus: str, include_dependant=False, full_records=False) -> Union[IndexSet, list]:
        q = {'corpora': corpus}
        if not include_dependant:
            q['dependence'] = {'$in': [False, None]}
        return IndexSet(q, sort="order.0") if full_records else IndexSet(q, sort="order.0").distinct("title")

    def get_indices_by_collective_title(self, collective_title, full_records=False):
        q = {'collective_title': collective_title}
        return IndexSet(q) if full_records else IndexSet(q).distinct("title")

    # TODO: add category filtering here or in another method?
    def get_dependant_indices(self, book_title=None, dependence_type=None, structure_match=False, full_records=False):
        """
        Replacement for all get commentary title methods
        :param book_title: Title of the base text. If book_title is None, returns all matching dependent texts
        :param dependence_type: none, "Commentary" or "Targum" - generally used to get Commentary and leave out Targum.  If none, returns all indexes.
        :param structure_match: If True, returns records that follow the base text structure
        :param full_records: If True, returns an IndexSet, if False returns list of titles.
        :return: IndexSet or List of titles.
        """
        if dependence_type:
            q = {'dependence': dependence_type}
        else:
            q = {'dependence': {'$exists': True}}
        if book_title:
            q['base_text_titles'] = book_title
        if structure_match:  # get only indices who's "base_text_mapping" is one that indicates it has the similar underlying schema as the base
            from sefaria.helper.link import AbstractStructureAutoLinker
            from sefaria.utils.util import get_all_subclass_attribute
            q['base_text_mapping'] = {'$in': get_all_subclass_attribute(AbstractStructureAutoLinker, "class_key")}
        return IndexSet(q) if full_records else IndexSet(q).distinct("title")

    def get_virtual_books(self, rebuild=False):
        if rebuild or not self._virtual_books:
            if not rebuild:
                self._virtual_books = scache.get_shared_cache_elem('virtualBooks')
            if rebuild or not self._virtual_books:
                self.build_virtual_books()
                scache.set_shared_cache_elem('virtualBooks', self._virtual_books)
                self.set_last_cached_time()
        return self._virtual_books

    def build_virtual_books(self):
        self._virtual_books = [index.title for index in IndexSet({'lexiconName': {'$exists': True}})]
        return self._virtual_books

    def get_titles_in_string(self, s, lang=None, citing_only=False):
        """
        Returns the titles found in the string.

        :param s: The string to search
        :param lang: "en" or "he"
        :return list: titles found in the string
        """
        if not lang:
            lang = "he" if has_hebrew(s) else "en"
        return [m.group('title') for m in self.all_titles_regex(lang, citing_only=citing_only).finditer(s)]

    def get_refs_in_string(self, st, lang=None, citing_only=False):
        """
        Returns a list of Ref objects derived from string

        :param string st: the input string
        :param lang: "he" or "en"
        :param citing_only: boolean whether to use only records explicitly marked as being referenced in text.
        :return: list of :class:`Ref` objects
            Order is not guaranteed
        """
        # todo: only match titles of content nodes

        refs = []
        if lang is None:
            lang = "he" if has_hebrew(st) else "en"
        if lang == "he":
            from sefaria.utils.hebrew import strip_nikkud
            st = strip_nikkud(st)
            unique_titles = set(self.get_titles_in_string(st, lang, citing_only))
            for title in unique_titles:
                try:
                    res = self._build_all_refs_from_string(title, st)
                    refs += res
                except AssertionError as e:
                    logger.info("Skipping Schema Node: {}".format(title))
                except TypeError as e:
                    logger.info("Error finding ref for {} in: {}".format(title, st))

        else:  # lang == "en"
            for match in self.all_titles_regex(lang, citing_only=citing_only).finditer(st):
                title = match.group('title')
                if not title:
                    continue
                try:
                    res = self._build_ref_from_string(title, st[match.start():])  # Slice string from title start
                    refs += res
                except AssertionError as e:
                    logger.info("Skipping Schema Node: {}".format(title))
                except InputError as e:
                    logger.info("Input Error searching for refs in string: {}".format(e))
                except TypeError as e:
                    logger.info("Error finding ref for {} in: {}".format(title, st))

        return refs

    def get_regex_and_titles_for_ref_wrapping(self, st, lang, citing_only=False):
        """
        Returns a compiled regex and dictionary of title:node correspondences to match the references in this string

        :param string st: the input string
        :param lang: "he" or "en"
        :param citing_only: boolean whether to use only records explicitly marked as being referenced in text
        :return: Compiled regex, dict of title:node correspondences

        """
        unique_titles = set(self.get_titles_in_string(st, lang, citing_only))
        title_nodes = {title: self.get_schema_node(title,lang) for title in unique_titles}
        all_reg = self.get_multi_title_regex_string(unique_titles, lang)
        reg = regex.compile(all_reg, regex.VERBOSE) if all_reg else None
        return reg, title_nodes

    def get_wrapped_refs_string(self, st, lang=None, citing_only=False, reg=None, title_nodes=None):
        """
        Returns a string with the list of Ref objects derived from string wrapped in <a> tags,
        excluding refs that are already wrapped in the data

        :param string st: the input string
        :param lang: "he" or "en"
        :param citing_only: boolean whether to use only records explicitly marked as being referenced in text
        :return: string:
        """
        if '<a ' not in st:  # This is 30 times faster than re.split, and applies for most cases
            substrings = [st]
        else:
            html_a_tag_reg = '(<a [^<>]*>.*?</a>)'  # Assuming no nested <a> within <a>
            substrings = re.split(html_a_tag_reg, st)
        new_string = ''
        for i, substring in enumerate(substrings):
            if i % 2 == 1:  # An <a> tag
                new_string += substring
            elif i % 2 == 0 and substring:
                new_string += self.apply_action_for_all_refs_in_string(substring, self._wrap_ref_match, lang,
                                                                       citing_only, reg, title_nodes)
        return new_string

    def apply_action_for_all_refs_in_string(self, st, action, lang=None, citing_only=None, reg=None, title_nodes=None):
        """

        @param st:
        @param action: function of the form `(ref, regex_match) -> Optional[str]`. return value will be used to replace regex_match in `st` if returned.
        @param lang:
        @param citing_only:
        @param reg:
        @param title_nodes:
        @return:
        """
        # todo: only match titles of content nodes
        if lang is None:
            lang = "he" if has_hebrew(st) else "en"

        if reg is None or title_nodes is None:
            reg, title_nodes = self.get_regex_and_titles_for_ref_wrapping(st, lang, citing_only)

        if reg is not None:
            sub_action = partial(self._apply_action_for_ref_match, title_nodes, lang, action)
            if lang == "en":
                return reg.sub(sub_action, st)
            else:
                outer_regex_str = r"[({\[].+?[)}\]]"
                outer_regex = regex.compile(outer_regex_str, regex.VERBOSE)
                return outer_regex.sub(lambda match: reg.sub(sub_action, match.group(0)), st)
        return st

    def get_multi_title_regex_string(self, titles, lang, for_js=False, anchored=False):
        """
        Capture title has to be true.
        :param titles:
        :param lang:
        :param for_js:
        :param anchored:
        :return:
        """
        nodes_by_address_type = defaultdict(list)
        regex_components = []

        for title in titles:
            try:
                node = self.get_schema_node(title, lang)
                nodes_by_address_type[tuple(node.addressTypes)] += [(title, node)]
            except AttributeError as e:
                # This chatter fills up the logs:
                # logger.warning(u"Library._wrap_all_refs_in_string() failed to create regex for: {}.  {}".format(title, e))
                continue

        if lang == "en" or for_js:  # Javascript doesn't support look behinds.
            for address_tuple, title_node_tuples in list(nodes_by_address_type.items()):
                node = title_node_tuples[0][1]
                titles = "|".join([regex.escape(tup[0]) for tup in title_node_tuples])
                regex_components += [node.full_regex(titles, lang, for_js=for_js, match_range=True, compiled=False, anchored=anchored, capture_title=True, escape_titles=False)]
            return "|".join(regex_components)

        if lang == "he":
            full_regex = ""
            for address_tuple, title_node_tuples in list(nodes_by_address_type.items()):
                node = title_node_tuples[0][1]
                titles = "|".join([regex.escape(tup[0]) for tup in title_node_tuples])

                regex_components += [r"(?:{}".format(r"(?P<title>{})".format(titles))  \
                           + node.after_title_delimiter_re \
                           + node.address_regex(lang, for_js=for_js, match_range=True) + ")"]

            all_interal = "|".join(regex_components)
            if all_interal:
                full_regex = r"""(?:
                    """ + all_interal + r"""
                    )
                    (?=\W|$)                                        # look ahead for non-word char
                    """
            return full_regex

    # do we want to move this to the schema node? We'd still have to pass the title...
    def get_regex_string(self, title, lang, for_js=False, anchored=False, capture_title=False, parentheses=False):
        """
        Given a book title, this function returns a regex for a Ref.
        This works for references not in Sefaria format (i.e. "See Genesis 2 3" as opposed to "Genesis 2:3",
        as well as for references in Sefaria format.
        If the language is 'en', it calls the full_regex() function which returns the regex, whereas for 'he' we
        limit the regex creation to content inside parenthesis to limit false positives (i.e. the phrase   
        could be caught by mistake as Shabbat 31)
        :param title: String
        :param lang: 'en' or 'he'
        :param for_js: Boolean (default set to False, optional)
        :param anchored: Boolean (default set to False, optional)
        :param capture_title: Boolean (default set to False, optional)
        :param parentheses: Boolean (default set to False, optional)
        """
        node = self.get_schema_node(title, lang)
        assert isinstance(node, JaggedArrayNode)  # Assumes that node is a JaggedArrayNode

        if lang == "en" or for_js:
            return node.full_regex(title, lang, for_js=for_js, match_range=True, compiled=False, anchored=anchored, capture_title=capture_title, parentheses=parentheses)
        elif lang == "he":
            return r"""(?<=							# look behind for opening brace
                    [({]										# literal '(', brace,
                    [^})]*										# anything but a closing ) or brace
                )
                """ + r"{}".format(r"(?P<title>{})".format(regex.escape(title)) if capture_title else regex.escape(title)) \
                   + node.after_title_delimiter_re \
                   + node.address_regex(lang, for_js=for_js, match_range=True) \
                   + r"""
                (?=\W|$)                                        # look ahead for non-word char
                (?=												# look ahead for closing brace
                    [^({]*										# match of anything but an opening '(' or brace
                    [)}]										# zero-width: literal ')' or brace
                )"""

    def _get_ref_from_match(self, ref_match, node, lang):
        sections = []
        toSections = []
        gs = ref_match.groupdict()
        for i in range(0, node.depth):
            gname = "a{}".format(i)
            if gs.get(gname) is not None:
                sections.append(node._addressTypes[i].toNumber(lang, gs.get(gname)))

        curr_address_index = len(sections) - 1  # start from the lowest depth matched in `sections` and go backwards
        for i in range(node.depth-1, -1, -1):
            toGname = "ar{}".format(i)
            if gs.get(toGname):
                toSections.append(node._addressTypes[curr_address_index].toNumber(lang, gs.get(toGname), sections=sections[curr_address_index]))
                curr_address_index -= 1

        if len(toSections) == 0:
            toSections = sections
        elif len(toSections) > len(sections):
            raise InputError("Match {} invalid. Length of toSections is greater than length of sections: {} > {}".format(ref_match.group(0), len(toSections), len(sections)))
        else:
            # pad toSections until it reaches the length of sections.
            while len(toSections) < len(sections):
                toSections.append(sections[len(sections) - len(toSections) - 1])
            toSections.reverse()
        # return seems to ignore all previous logic...
        # leaving this function in case errors that were thrown in above logic act as validation?
        return Ref(ref_match.group())

    def _build_ref_from_string(self, title=None, st=None, lang="en"):
        """
        Build a Ref object given a title and a string.  The title is assumed to be at position 0 in the string.
        This is used primarily for English matching.  Hebrew matching is done with _build_all_refs_from_string()
        :param title: The title used in the text to refer to this Index node
        :param st: The source text for this reference
        :return: Ref
        """
        return self._internal_ref_from_string(title, st, lang, stIsAnchored=True)

    def _build_all_refs_from_string(self, title=None, st=None, lang="he"):
        """
        Build all Ref objects for title found in string.  By default, only match what is found between braces (as in Hebrew).
        This is used primarily for Hebrew matching.  English matching uses _build_ref_from_string()
        :param title: The title used in the text to refer to this Index node
        :param st: The source text for this reference
        :return: list of Refs
        """
        return self._internal_ref_from_string(title, st, lang)

    def _internal_ref_from_string(self, title=None, st=None, lang=None, stIsAnchored=False, return_locations = False):
        node = self.get_schema_node(title, lang)
        if not isinstance(node, JaggedArrayNode):
            # TODO fix when not JaggedArrayNode
            # Assumes that node is a JaggedArrayNode
            return None

        refs = []
        try:
            re_string = self.get_regex_string(title, lang, anchored=stIsAnchored)
        except AttributeError as e:
            logger.warning(
                "Library._internal_ref_from_string() failed to create regex for: {}.  {}".format(title, e))
            return refs

        reg = regex.compile(re_string, regex.VERBOSE)
        if stIsAnchored:
            m = reg.match(st)
            matches = [m] if m else []
        else:
            matches = reg.finditer(st)
        for ref_match in matches:
            try:
                res = (self._get_ref_from_match(ref_match, node, lang), ref_match.span()) if return_locations else self._get_ref_from_match(ref_match, node, lang)
                refs.append(res)
            except (InputError, ValueError) as e:
                continue
        return refs

    @staticmethod
    def _wrap_ref_match(ref, match):
        return '<a class ="refLink" href="/{}" data-ref="{}">{}</a>'.format(ref.url(), ref.normal(), match.group(0))

    def _apply_action_for_ref_match(self, title_node_dict, lang, action, match):
        try:
            gs = match.groupdict()
            assert gs.get("title") is not None
            node = title_node_dict[gs.get("title")]
            ref = self._get_ref_from_match(match, node, lang)
            replacement = action(ref, match)
            if replacement is None:
                return match.group(0)
            return replacement
        except (InputError, KeyError) as e:
            logger.warning("Wrap Ref Warning: Ref:({}) {}".format(match.group(0), str(e)))
            return match.group(0)

    @staticmethod
    def get_wrapped_named_entities_string(links, s):
        """
        Parallel to library.get_wrapped_refs_string
        Returns `s` with every link in `links` wrapped in an a-tag
        """
        if len(links) == 0:
            return s
        links.sort(key=lambda x: x.charLevelData['startChar'])

        # replace all mentions with `dummy_char` so they can later be easily replaced using re.sub()
        # this ensures char locations are preserved
        dummy_char = ""
        char_list = list(s)
        start_char_to_slug = {}
        for link in links:
            start = link.charLevelData['startChar']
            end = link.charLevelData['endChar']
            mention = s[start:end]
            if mention != link.charLevelData['text']:
                # dont link if current text at startChar:endChar doesn't match text on link
                continue
            start_char_to_slug[start] = (mention, link.toTopic, getattr(link, 'unambiguousToTopic', None))
            char_list[start:end] = list(dummy_char*(end-start))
        dummy_text = "".join(char_list)

        def repl(match):
            try:
                mention, slug, unambiguous_slug = start_char_to_slug[match.start()]
            except KeyError:
                return match.group()
            link_slug = unambiguous_slug or slug
            return f"""<a href="/topics/{link_slug}" class="namedEntityLink" data-slug="{slug}">{mention}</a>"""
        return re.sub(fr"{dummy_char}+", repl, dummy_text)

    def category_id_dict(self, toc=None, cat_head="", code_head=""):
        """Returns a dict of unique category ids based on the ToC, with the
           values being the category IDs.
            :param toc: ToC object (optional, default is None)
            :param cat_head: String, (optional, default is "" - an empty string)
            :param code_head: String, (optional, default is "" - an empty string)
        """
        if toc is None:
            if not self._category_id_dict:
                self._category_id_dict = self.category_id_dict(self.get_toc())
            return self._category_id_dict

        d = {}

        for i, c in enumerate(toc):
            name = c["category"] if "category" in c else c["title"]
            if cat_head:
                key = "/".join([cat_head, name])
                val = code_head + format(i, '03')
            else:
                key = name
                val = "A" + format(i, '03')

            d[key] = val
            if "contents" in c:
                d.update(self.category_id_dict(c["contents"], key, val))

        return d

    def simplify_toc(self, lang=None, toc_node=None, path=None):
        """
        Simplifies the table of contents (ToC)
        :param lang: 'en' or 'he', default is None (optional)
        :param toc_node: ToC Node, default is None (optional)
        :param path: Node Path, default is None (optional)
        """
        is_root = toc_node is None and path is None
        toc_node = toc_node if toc_node else self.get_toc()
        path = path if path else []
        simple_nodes = []
        for x in toc_node:
            node_name = x.get("category", None) or x.get("title", None)
            node_path = path + [node_name]
            simple_node = {
                "name": node_name,
                "path": node_path
            }
            if "category" in x:
                if "contents" not in x:
                    continue
                simple_node["type"] = "category"
                simple_node["children"] = self.simplify_toc(lang, x["contents"], node_path)
            elif "title" in x:
                query = {"title": x["title"]}
                if lang:
                    query["language"] = lang
                simple_node["type"] = "index"
                simple_node["children"] = [{
                    "name": "{} ({})".format(v.versionTitle, v.language),
                    "path": node_path + ["{} ({})".format(v.versionTitle, v.language)],
                    "size": v.word_count(),
                    "type": "version"
                } for v in VersionSet(query)]
            simple_nodes.append(simple_node)

        if is_root:
            return {
                "name": "Whole Library" + " ({})".format(lang if lang else ""),
                "path": [],
                "children": simple_nodes
            }
        else:
            return simple_nodes

    def word_count(self, ref_or_cat, lang="he", dependents_regex=None):
        """
        :param ref_or_cat:
        :param lang:
        :param dependents_regex: string - filter dependents by those that have this string (treat this as a category)
        :return:
        """
        if isinstance(ref_or_cat, Ref):
            return ref_or_cat.word_count(lang)
        try:
            return Ref(ref_or_cat).word_count(lang)
        except InputError:
            if dependents_regex:
                raw_ins = library.get_indexes_in_category(ref_or_cat, True)
                ins = filter(lambda s: re.search(dependents_regex,s), raw_ins)
            else:
                ins = library.get_indexes_in_category(ref_or_cat)
            return sum([Ref(r).word_count(lang) for r in ins])

    def is_initialized(self):
        """
        Returns True if the following fields are initialized
            * self._toc_tree
            * self._full_auto_completer
            * self._lexicon_auto_completer
            * self._cross_lexicon_auto_completer
        """

        # Given how the object is initialized and will always be non-null,
        # I will likely have to add fields to the object to be changed once

        # Avoid allocation here since it will be called very frequently
        are_autocompleters_ready = self._full_auto_completer_is_ready and self._lexicon_auto_completer_is_ready and self._cross_lexicon_auto_completer_is_ready
        is_initialized = self._toc_tree_is_ready and (DISABLE_AUTOCOMPLETER or are_autocompleters_ready)
        if not is_initialized:
            logger.warning({"message": "Application not fully initialized", "Current State": {
                "toc_tree_is_ready": self._toc_tree_is_ready,
                "full_auto_completer_is_ready": self._full_auto_completer_is_ready,
                "lexicon_auto_completer_is_ready": self._lexicon_auto_completer_is_ready,
                "cross_lexicon_auto_completer_is_ready": self._cross_lexicon_auto_completer_is_ready,
            }})
        return is_initialized

    @staticmethod
    def get_top_categories(full_records=False):
        from sefaria.model.category import CategorySet
        return CategorySet({'depth': 1}) if full_records else CategorySet({'depth': 1}).distinct('path')


library = Library()


def prepare_index_regex_for_dependency_process(index_object, as_list=False):
    """
    :return string: Regular Expression which will find any titles that match this index title exactly, or more specifically.

    Simplified version of Ref.regex()
    """
    patterns = []
    patterns.append(r"$")   # exact match
    if index_object.nodes.has_titled_continuation():
        patterns.append(r"({}).".format(r"|".join(index_object.nodes.title_separators)))
    if index_object.nodes.has_numeric_continuation():
        patterns.append(r":")   # more granualar, exact match followed by :
        patterns.append(r" \d") # extra granularity following space

    escaped_book = re.escape(index_object.title)
    if as_list:
        return [r"^{}{}".format(escaped_book, p) for p in patterns]
    else:
        return r"^%s(%s)" % (escaped_book, "|".join(patterns))


def process_index_title_change_in_versions(indx, **kwargs):
    VersionSet({"title": kwargs["old"]}).update({"title": kwargs["new"]})


def process_index_title_change_in_dependant_records(indx, **kwargs):
    dependent_indices = library.get_dependant_indices(kwargs["old"], full_records=True)
    for didx in dependent_indices:
        pos = didx.base_text_titles.index(kwargs["old"])
        didx.base_text_titles.pop(pos)
        didx.base_text_titles.insert(pos, kwargs["new"])
        didx.save()

def process_index_title_change_in_sheets(indx, **kwargs):
    print("Cascading refs in sheets {} to {}".format(kwargs['old'], kwargs['new']))

    regex_list = [pattern.replace(re.escape(kwargs["new"]), re.escape(kwargs["old"]))
                for pattern in Ref(kwargs["new"]).regex(as_list=True)]
    ref_clauses = [{"includedRefs": {"$regex": r}} for r in regex_list]
    query = {"$or": ref_clauses }
    sheets = db.sheets.find(query)
    for sheet in sheets:
        sheet["includedRefs"] = [r.replace(kwargs["old"], kwargs["new"], 1) if re.search('|'.join(regex_list), r) else r for r in sheet.get("includedRefs", [])]
        sheet["expandedRefs"] = Ref.expand_refs(sheet["includedRefs"])
        for source in sheet.get("sources", []):
            if "ref" in source:
                source["ref"] = source["ref"].replace(kwargs["old"], kwargs["new"], 1) if re.search('|'.join(regex_list), source["ref"]) else source["ref"]
        db.sheets.replace_one({"_id":sheet["_id"]}, sheet, upsert=True)


def process_index_delete_in_versions(indx, **kwargs):
    VersionSet({"title": indx.title}).delete()


def process_index_title_change_in_core_cache(indx, **kwargs):
    old_title = kwargs["old"]

    library.refresh_index_record_in_cache(indx, old_title=old_title)
    library.reset_text_titles_cache()

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "refresh_index_record_in_cache", [indx.title, old_title])
    elif USE_VARNISH:
        from sefaria.system.varnish.wrapper import invalidate_title
        invalidate_title(old_title)


def process_index_change_in_core_cache(indx, **kwargs):
    if kwargs.get("is_new"):
        library.add_index_record_to_cache(indx)
        library.reset_text_titles_cache()

        if MULTISERVER_ENABLED:
            server_coordinator.publish_event("library", "add_index_record_to_cache", [indx.title])

    else:
        library.refresh_index_record_in_cache(indx)
        library.reset_text_titles_cache()

        if MULTISERVER_ENABLED:
            server_coordinator.publish_event("library", "refresh_index_record_in_cache", [indx.title])
        elif USE_VARNISH:
            from sefaria.system.varnish.wrapper import invalidate_title
            invalidate_title(indx.title)


def process_index_change_in_toc(indx, **kwargs):
    old_ref = kwargs.get('orig_vals').get('title') if kwargs.get('orig_vals') else None
    library.update_index_in_toc(indx, old_ref=old_ref)

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "update_index_in_toc", [indx.title, old_ref])


def process_index_delete_in_toc(indx, **kwargs):
    library.delete_index_from_toc(indx)

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "delete_index_from_toc", [indx.title, indx.categories])


def process_index_delete_in_core_cache(indx, **kwargs):
    library.remove_index_record_from_cache(indx)
    library.reset_text_titles_cache()

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "remove_index_record_from_cache", [indx.title])
    elif USE_VARNISH:
        from sefaria.system.varnish.wrapper import invalidate_title
        invalidate_title(indx.title)


def reset_simple_term_mapping(o, **kwargs):
    library.get_simple_term_mapping(rebuild=True)

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "build_term_mappings")


def rebuild_library_after_category_change(*args, **kwargs):
    library.rebuild(include_toc=True)

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "rebuild", [True])

```

### sefaria/model/lock.py

```
"""
lock.py - Edit Locks for Sefaria texts.

Writes to MongoDB Collection: locks

NOTE: Locks currently assume references at the segment level only.
E.g., locking "Genesis 4" will probably break something.
"""

#Most lock work happens in workflow.py.
#Referenced in reader.views.translation_flow.
#Aim is to handle it not just on campaign pages, but in the reader as well.

import datetime

from . import abstract as abst

LOCK_TIMEOUT = 300  # seconds after which locks expire


class Lock(abst.AbstractMongoRecord):
    collection = 'locks'
    required_attrs = [
        "ref",
        "lang",
        "version",
        "user",
        "time"
    ]
    optional_attrs = []


class LockSet(abst.AbstractMongoSet):
    recordClass = Lock


def set_lock(ref, lang, version, user):
    """
    Creats a lock for ref/lang/version/user.
    user 0 indicates anonymous lock.
    """

    return Lock({
        "ref": ref,
        "lang": lang,
        "version": version,
        "user": user,
        "time": datetime.datetime.now(),
    }).save()


def release_lock(ref, lang, version):
    """
    Deletes locks matching ref/lang/version.
    """
    Lock().delete_by_query({
        "ref": ref,
        "lang": lang,
        "version": version,
    })


def check_lock(ref, lang, version):
    """
    Returns True if a current lock for ref/lang/version exists.
    """
    return bool(Lock().load({
        "ref": ref,
        "lang": lang,
        "version": version
    }))


def expire_locks():
    """
    Remove all locks older than expiry time.
    """
    cutoff = datetime.datetime.now() - datetime.timedelta(seconds=LOCK_TIMEOUT)
    LockSet({"time": {"$lt": cutoff}}).delete()

```

### sefaria/model/topic.py

```
from typing import Union, Optional
from . import abstract as abst
from .schema import AbstractTitledObject, TitleGroup
from .text import Ref, IndexSet, AbstractTextRecord, Index, Term
from .category import Category
from django_topics.models import Topic as DjangoTopic
from django_topics.models import TopicPool, PoolType
from sefaria.system.exceptions import InputError, DuplicateRecordError
from sefaria.model.timeperiod import TimePeriod, LifePeriod
from sefaria.system.validators import validate_url
from sefaria.model.portal import Portal
from sefaria.system.database import db
import structlog, bleach
from sefaria.model.place import Place
import regex as re
from typing import Type
logger = structlog.get_logger(__name__)
from dataclasses import dataclass, field
from typing import Tuple, List, Dict
from abc import ABC, abstractmethod


@dataclass
class SubCatBookSet:
    depth: int
    sub_cat_path: Tuple[str, ...]
    books: List[Index] = field(default_factory=list)

    def __eq__(self, other):
        if not isinstance(other, SubCatBookSet):
            return False
        return self.sub_cat_path == other.sub_cat_path

    def __repr__(self):
        return f"Sub Path: {self.sub_cat_path}"

    def __hash__(self):
        return hash(self.sub_cat_path)


class AuthorWorksAggregation(ABC):
    @abstractmethod
    def get_description(self, lang):
        pass

    @abstractmethod
    def get_title(self, lang):
        pass

    @abstractmethod
    def get_url(self):
        pass


class AuthorIndexAggregation(AuthorWorksAggregation):
    def __init__(self, index):
        self._index: Index = index

    def get_description(self, lang):
        desc = getattr(self._index, f'{lang}ShortDesc', None)
        return desc

    def get_title(self, lang):
        return self._index.get_title(lang)

    def get_url(self):
        return f'/{self._index.title.replace(" ", "_").replace("?", "%3F")}'


class AuthorCategoryAggregation(AuthorWorksAggregation):
    def __init__(self, index_category, collective_term, base_category):
        self._index_category: Category = index_category
        self._collective_title_term: Term = collective_term
        self._base_category: Category = base_category

    def get_description(self, lang):
        desc = getattr(self._index_category, f'{lang}ShortDesc', None)
        return desc

    def get_title(self, lang):
        if self._collective_title_term is None:
            cat_term = Term().load({"name": self._index_category.sharedTitle})
            return cat_term.get_primary_title(lang)
        else:
            preposition = 'on' if lang != 'he' else ''
            return f'{self._collective_title_term.get_primary_title(lang)} {preposition} {self._base_category.get_primary_title(lang)}'

    def get_url(self):
        return f'/texts/{"/".join(self._index_category.path)}'


class AuthorAggregationFactory:
    @staticmethod
    def create(index=None, index_category=None, collective_term=None, base_category=None):
        if index:
            return AuthorIndexAggregation(index)
        elif index_category:
            return AuthorCategoryAggregation(
                index_category,
                collective_term,
                base_category
            )
        else:
            raise ValueError("Invalid parameters for aggregation creation")


@dataclass
class DisjointBookSet:
    base_cat_path: Tuple[str, ...]
    collective_title: str
    sub_cat_book_sets: Dict[Tuple[str, ...], SubCatBookSet] = field(default_factory=dict)

    def __eq__(self, other):
        if not isinstance(other, DisjointBookSet):
            return False
        return self.base_cat_path == other.base_cat_path and self.collective_title == other.collective_title

    def __repr__(self):
        return f"Collective Title: {self.collective_title}, Path: {self.base_cat_path}"

    def __hash__(self):
        return hash((self.collective_title, self.base_cat_path))


class Topic(abst.SluggedAbstractMongoRecord, AbstractTitledObject):
    collection = 'topics'
    history_noun = 'topic'
    slug_fields = ['slug']
    title_group = None
    subclass_map = {
        'person': 'PersonTopic',
        'author': 'AuthorTopic',
    }
    pkeys = ["description"]
    track_pkeys = True
    reverse_subclass_map = {v: k for k, v in subclass_map.items()}
    required_attrs = [
        'slug',
        'titles',
    ]
    optional_attrs = [
        'subclass',  # str which indicates which subclass of `Topic` this instance is
        'alt_ids',
        'properties',
        'description',  # dictionary, keys are 2-letter language codes
        'categoryDescription',  # dictionary, keys are 2-letter language codes
        'isTopLevelDisplay',
        'displayOrder',
        'numSources',  # total number of refLinks, to texts and sheets.
        'shouldDisplay',
        'parasha',  # name of parsha as it appears in `parshiot` collection
        'ref',  # dictionary for topics with refs associated with them (e.g. parashah) containing strings `en`, `he`, and `url`.
        'good_to_promote',
        'description_published',  # bool to keep track of which descriptions we've vetted
        'isAmbiguous',  # True if topic primary title can refer to multiple other topics
        "data_source",  #any topic edited manually should display automatically in the TOC and this flag ensures this
        'image',
        'secondary_image_uri',
        "portal_slug",  # slug to relevant Portal object
    ]

    attr_schemas = {
        "image": {
            'type': 'dict',
            'schema': {'image_uri': {'type': 'string',
                                     'required': True,
                                     'regex': '^https://storage\\.googleapis\\.com/img\\.sefaria\\.org/topics/.*?'},
                       'image_caption': {'type': 'dict',
                                         'required': True,
                                         'schema': {'en': {'type': 'string', 'required': True},
                                                    'he': {'type': 'string', 'required': True}}}}
            },
    }

    ROOT = "Main Menu"  # the root of topic TOC is not a topic, so this is a fake slug.  we know it's fake because it's not in normal form
                        # this constant is helpful in the topic editor tool functions in this file

    def load(self, query, proj=None):
        if self.__class__ != Topic:
            subclass_names = [self.__class__.__name__] + [klass.__name__ for klass in self.all_subclasses()]
            query['subclass'] = {"$in": [self.reverse_subclass_map[name] for name in subclass_names]}
        topic = super().load(query, proj)
        if getattr(topic, 'subclass', False):
            Subclass = globals()[self.subclass_map[topic.subclass]]
            topic = Subclass(topic._saveable_attrs())
        return topic

    def _set_derived_attributes(self):
        self.set_titles(getattr(self, "titles", None))
        if self.__class__ != Topic and not getattr(self, "subclass", False):
            # in a subclass. set appropriate "subclass" attribute
            setattr(self, "subclass", self.reverse_subclass_map[self.__class__.__name__])

    def _pre_save(self):
        super()._pre_save()
        django_topic, created = DjangoTopic.objects.get_or_create(slug=self.slug)
        django_topic.en_title = self.get_primary_title('en')
        django_topic.he_title = self.get_primary_title('he')
        django_topic.save()

    def _validate(self):
        super(Topic, self)._validate()
        if getattr(self, 'subclass', False):
            assert self.subclass in self.subclass_map, f"Field `subclass` set to {self.subclass} which is not one of the valid subclass keys in `Topic.subclass_map`. Valid keys are {', '.join(self.subclass_map.keys())}"
        if getattr(self, 'portal_slug', None):
            Portal.validate_slug_exists(self.portal_slug)
        if getattr(self, "image", False):
            img_url = self.image.get("image_uri")
            if img_url: validate_url(img_url)

    def _normalize(self):
        super()._normalize()
        for title in self.title_group.titles:
            title['text'] = title['text'].strip()
        self.titles = self.title_group.titles
        slug_field = self.slug_fields[0]
        slug = getattr(self, slug_field)
        displays_under_link = IntraTopicLink().load({"fromTopic": slug, "linkType": "displays-under"})
        if getattr(displays_under_link, "toTopic", "") == "authors":
            self.subclass = "author"

    def _sanitize(self):
        super()._sanitize()
        for attr in ['description', 'categoryDescription']:
            p = getattr(self, attr, {})
            for k, v in p.items():
                p[k] = bleach.clean(v, tags=[], strip=True)
            setattr(self, attr, p)

    def get_pools(self) -> list[str]:
        slug = getattr(self, "slug", None)
        return list(DjangoTopic.objects.get_pools_by_topic_slug(str(slug))) if slug is not None else []

    def has_pool(self, pool: str) -> bool:
        return pool in self.get_pools()

    def add_pool(self, pool_name: str) -> None:
        pool = TopicPool.objects.get(name=pool_name)
        DjangoTopic.objects.get(slug=self.slug).pools.add(pool)
        if not self.has_pool(pool_name):
            self.get_pools().append(pool_name)
        DjangoTopic.objects.build_slug_to_pools_cache(rebuild=True)

    def remove_pool(self, pool_name) -> None:
        pool = TopicPool.objects.get(name=pool_name)
        DjangoTopic.objects.get(slug=self.slug).pools.remove(pool)
        if self.has_pool(pool_name):
            self.get_pools().remove(pool_name)
        DjangoTopic.objects.build_slug_to_pools_cache(rebuild=True)

    def set_titles(self, titles):
        self.title_group = TitleGroup(titles)

    def title_is_transliteration(self, title, lang):
        return self.title_group.get_title_attr(title, lang, 'transliteration') is not None

    def get_types(self, types=None, curr_path=None, search_slug_set=None):
        """
        WARNING: Expensive, lots of database calls
        Gets all `is-a` ancestors of self. Returns early if `search_slug_set` is passed and it reaches any element in `search_slug_set`
        :param types: set(str), current known types, for recursive calls
        :param curr_path: current path of this recursive call
        :param search_slug_set: if passed, will return early once/if any element of `search_slug_set` is found
        :return: set(str)
        """
        types = types or {self.slug}
        curr_path = curr_path or [self.slug]
        isa_set = {l.toTopic for l in IntraTopicLinkSet({"fromTopic": self.slug, "linkType": TopicLinkType.isa_type})}
        types |= isa_set
        if search_slug_set is not None and len(search_slug_set.intersection(types)) > 0:
            return types
        for isa_slug in isa_set:
            new_path = [p for p in curr_path]
            if isa_slug in new_path:
                logger.warning("Circular path starting from {} and ending at {} detected".format(new_path[0], isa_slug))
                continue
            new_path += [isa_slug]
            new_topic = Topic.init(isa_slug)
            if new_topic is None:
                logger.warning("{} is None. Current path is {}".format(isa_slug, ', '.join(new_path)))
                continue
            new_topic.get_types(types, new_path, search_slug_set)
        return types


    def change_description(self, desc, cat_desc=None):
        """
        Sets description in all cases and sets categoryDescription if this is a top level topic

        :param desc: Dictionary of descriptions, with keys being two letter language codes
        :param cat_desc: Optional. Dictionary of category descriptions, with keys being two letter language codes
        :return:
        """
        if cat_desc is None:
            cat_desc = {"en": "", "he": ""}
        if desc is None:
            desc = {"en": "", "he": ""}
        self.description = desc
        if getattr(self, "isTopLevelDisplay", False):
            self.categoryDescription = cat_desc
        elif getattr(self, "categoryDescription", False):
            delattr(self, "categoryDescription")

    def topics_by_link_type_recursively(self, **kwargs):
        topics, _ = self.topics_and_links_by_link_type_recursively(**kwargs)
        return topics
    
    def topics_and_links_by_link_type_recursively(self, linkType='is-a', only_leaves=False, reverse=False, max_depth=None, min_sources=None):
        """
        Gets all topics linked to `self` by `linkType`. The query is recursive so it's most useful for 'is-a' and 'displays-under' linkTypes
        :param linkType: str, the linkType to recursively traverse.
        :param only_leaves: bool, if True only return last level traversed
        :param reverse: bool, if True traverse the inverse direction of `linkType`. E.g. if linkType == 'is-a' and reverse == True, you will traverse 'is-category-of' links
        :param max_depth: How many levels below this one to traverse. 1 returns only this node's children, 0 returns only this node and None means unlimited.
        :return: list(Topic)
        """
        topics, links, below_min_sources = self._topics_and_links_by_link_type_recursively_helper(linkType, only_leaves, reverse, max_depth, min_sources)
        links = list(filter(lambda x: x.fromTopic not in below_min_sources and x.toTopic not in below_min_sources, links))
        return topics, links

    def _topics_and_links_by_link_type_recursively_helper(self, linkType, only_leaves, reverse, max_depth, min_sources, explored_set=None, below_min_sources_set=None):
        """
        Helper function for `topics_and_links_by_link_type_recursively()` to carry out recursive calls
        :param explored_set: set(str), set of slugs already explored. To be used in recursive calls.
        """
        explored_set = explored_set or set()
        below_min_sources_set = below_min_sources_set or set()
        topics = []
        dir1 = "to" if reverse else "from"
        dir2 = "from" if reverse else "to"
        links = IntraTopicLinkSet({f"{dir2}Topic": self.slug, "linkType": linkType}).array()
        children = [getattr(l, f"{dir1}Topic") for l in links]
        if len(children) == 0:
            if min_sources is not None and self.numSources < min_sources:
                return [], [], {self.slug}
            return [self], [], set()
        else:
            if not only_leaves:
                topics += [self]
            for slug in children:
                if slug in explored_set:
                    continue
                child_topic = Topic.init(slug)
                explored_set.add(slug)
                if child_topic is None:
                    logger.warning(f"{slug} is None")
                    continue
                if max_depth is None or max_depth > 0:
                    next_depth = max_depth if max_depth is None else max_depth - 1
                    temp_topics, temp_links, temp_below_min_sources = child_topic._topics_and_links_by_link_type_recursively_helper(linkType, only_leaves, reverse, next_depth, min_sources, explored_set, below_min_sources_set)
                    topics += temp_topics
                    links += temp_links
                    below_min_sources_set |= temp_below_min_sources
        return topics, links, below_min_sources_set

    def has_types(self, search_slug_set) -> bool:
        """
        WARNING: Expensive, lots of database calls
        Checks if `self` has any slug in `search_slug_set` as an ancestor when traversing `is-a` links
        :param search_slug_set: set(str), slugs to search for. returns True if any slug is found
        :return: bool
        """
        types = self.get_types(search_slug_set=search_slug_set)
        return len(search_slug_set.intersection(types)) > 0

    def should_display(self) -> bool:
        return getattr(self, 'shouldDisplay', True) and (getattr(self, 'numSources', 0) > 0 or self.has_description() or getattr(self, "data_source", "") == "sefaria")

    def has_description(self) -> bool:
        """
        returns True if self has description in at least on language
        """
        has_desc = False
        for temp_desc in getattr(self, 'description', {}).values():
            has_desc = has_desc or (isinstance(temp_desc, str) and len(temp_desc) > 0)
        for temp_desc in getattr(self, 'categoryDescription', {}).values():
            has_desc = has_desc or (isinstance(temp_desc, str) and len(temp_desc) > 0)
        return has_desc

    def set_slug_to_primary_title(self) -> None:
        new_slug = self.get_primary_title('en')
        if len(new_slug) == 0:
            new_slug = self.get_primary_title('he')
        new_slug = self.normalize_slug(new_slug)
        if new_slug != self.slug:
            self.set_slug(new_slug)

    def set_slug(self, new_slug) -> None:
        slug_field = self.slug_fields[0]
        old_slug = getattr(self, slug_field)
        setattr(self, slug_field, new_slug)
        setattr(self, slug_field, self.normalize_slug_field(slug_field))
        DjangoTopic.objects.filter(slug=old_slug).update(slug=new_slug)
        self.save()  # so that topic with this slug exists when saving links to it
        self.merge(old_slug)

    def merge(self, other: Union['Topic', str]) -> None:
        """
        Merge `other` into `self`. This means that all data from `other` will be merged into self.
        Data from self takes precedence in the event of conflict.
        Links to `other` will be changed to point to `self` and `other` will be deleted.
        :param other: Topic or old slug to migrate from
        :return: None
        """
        from sefaria.system.database import db
        if other is None:
            return
        other_slug = other if isinstance(other, str) else other.slug
        if other_slug == self.slug:
            logger.warning(f'Cant merge slug into itself. Slug == {self.slug}')
            return

        # links
        for link in TopicLinkSetHelper.find({"$or": [{"toTopic": other_slug}, {"fromTopic": other_slug}]}):
            if link.toTopic == getattr(link, 'fromTopic', None):  # self-link where fromTopic and toTopic were equal before slug was changed
                link.fromTopic = self.slug
                link.toTopic = self.slug
            else:
                attr = 'toTopic' if link.toTopic == other_slug else 'fromTopic'
                setattr(link, attr, self.slug)
                if getattr(link, 'fromTopic', None) == link.toTopic:  # self-link where fromTopic and toTopic are equal AFTER slug was changed
                    link.delete()
                    continue
            try:
                link.save()
            except (InputError, DuplicateRecordError) as e:
                link.delete()
            except AssertionError as e:
                link.delete()
                logger.warning('While merging {} into {}, link assertion failed with message "{}"'.format(other_slug, self.slug, str(e)))

        # source sheets
        db.sheets.update_many({'topics.slug': other_slug}, {"$set": {'topics.$[element].slug': self.slug}}, array_filters=[{"element.slug": other_slug}])

        # indexes
        for index in IndexSet({"authors": other_slug}):
            index.authors = [self.slug if author_slug == other_slug else author_slug for author_slug in index.authors]
            props = index._saveable_attrs()
            db.index.replace_one({"title":index.title}, props)

        if isinstance(other, Topic):
            # titles
            for title in other.titles:
                if title.get('primary', False) and self.get_primary_title(title['lang']):
                    # delete primary flag if self already has primary in this language
                    del title['primary']
            self.titles += other.titles

            # dictionary attributes
            for dict_attr in ['alt_ids', 'properties']:
                temp_dict = getattr(self, dict_attr, {})
                for k, v in getattr(other, dict_attr, {}).items():
                    if k in temp_dict:
                        logger.warning('Key {} with value {} already exists in {} for topic {}. Current value is {}'.format(k, v, dict_attr, self.slug, temp_dict[k]))
                        continue
                    temp_dict[k] = v
                if len(temp_dict) > 0:
                    setattr(self, dict_attr, temp_dict)
            setattr(self, 'numSources', getattr(self, 'numSources', 0) + getattr(other, 'numSources', 0))

            # everything else
            already_merged = ['slug', 'titles', 'alt_ids', 'properties', 'numSources']
            for attr in filter(lambda x: x not in already_merged, self.required_attrs + self.optional_attrs):
                if not getattr(self, attr, False) and getattr(other, attr, False):
                    setattr(self, attr, getattr(other, attr))
            self.save()
            other.delete()

    def link_set(self, _class='intraTopic', query_kwargs: dict = None, **kwargs):
        """
        :param str _class: could be 'intraTopic' or 'refTopic' or `None` (see `TopicLinkHelper`)
        :param query_kwargs: dict of extra query keyword arguments
        :return: link set of topic links to `self`
        """
        intra_link_query = {"$or": [{"fromTopic": self.slug}, {"toTopic": self.slug}]}
        if query_kwargs is not None:
            intra_link_query.update(query_kwargs)
        if _class == 'intraTopic':
            kwargs['record_kwargs'] = {'context_slug': self.slug}
            return IntraTopicLinkSet(intra_link_query, **kwargs)
        elif _class == 'refTopic':
            ref_link_query = {'toTopic': self.slug}
            if query_kwargs is not None:
                ref_link_query.update(query_kwargs)
            return RefTopicLinkSet(ref_link_query, **kwargs)
        elif _class is None:
            kwargs['record_kwargs'] = {'context_slug': self.slug}
            return TopicLinkSetHelper.find(intra_link_query, **kwargs)

    def get_ref_links(self, is_sheet, query_kwargs=None, **kwargs):
        query_kwargs = query_kwargs or {}
        query_kwargs['is_sheet'] = is_sheet
        return self.link_set('refTopic', query_kwargs, **kwargs)

    def contents(self, **kwargs):
        mini = kwargs.get('minify', False)
        d = {'slug': self.slug} if mini else super(Topic, self).contents(**kwargs)
        d['primaryTitle'] = {}
        for lang in ('en', 'he'):
            d['primaryTitle'][lang] = self.get_primary_title(lang=lang, with_disambiguation=kwargs.get('with_disambiguation', True))
        if not kwargs.get("with_html"):
            for k, v in d.get("description", {}).items():
                d["description"][k] = re.sub("<[^>]+>", "", v or "")
        return d

    def get_primary_title(self, lang='en', with_disambiguation=True):
        title = super(Topic, self).get_primary_title(lang=lang)
        if with_disambiguation:
            disambig_text = self.title_group.get_title_attr(title, lang, 'disambiguation')
            if disambig_text:
                title += f' ({disambig_text})'
            elif getattr(self, 'isAmbiguous', False) and len(title) > 0:
                title += ' [Ambiguous]'
        return title

    def get_titles(self, lang=None, with_disambiguation=True):
        if with_disambiguation:
            titles = []
            for title in self.get_titles_object():
                if not (lang is None or lang == title['lang']):
                    continue
                text = title['text']
                disambig_text = title.get('disambiguation', None)
                if disambig_text:
                    text += f' ({disambig_text})'
                titles += [text]
            return titles
        return super(Topic, self).get_titles(lang)

    def get_property(self, property, default=None, value_only=True):
        properties = getattr(self, 'properties', {})
        value = properties.get(property, {}).get('value', default)
        data_source = properties.get(property, {}).get('dataSource', default)
        if value_only:
            return value
        return value, data_source

    def set_property(self, property, value, data_source):
        if getattr(self, 'properties', None) is None:
            self.properties = {}
        self.properties[property] = {
            'value': value,
            'dataSource': data_source
        }

    @staticmethod
    def get_uncategorized_slug_set() -> set:
        categorized_topics = IntraTopicLinkSet({"linkType": TopicLinkType.isa_type}).distinct("fromTopic")
        all_topics = TopicSet().distinct("slug")
        return set(all_topics) - set(categorized_topics)

    def __str__(self):
        return self.get_primary_title("en")

    def __repr__(self):
        return "{}.init('{}')".format(self.__class__.__name__, self.slug)

    def update_after_link_change(self, pool):
        """
        updating the pools 'sheets' or 'textual' according to the existence of links and the numSources
        :param pool: 'sheets' or 'textual'
        """
        links = self.get_ref_links(pool == PoolType.SHEETS.value)
        if self.has_pool(pool) and not links:
            self.remove_pool(pool)
        elif not self.has_pool(pool) and links:
            self.add_pool(pool)
        self.numSources = self.link_set('refTopic').count()
        self.save()


class PersonTopic(Topic):
    """
    Represents a topic which is a person. Not necessarily an author of a book.
    """
    @staticmethod
    def get_person_by_key(key: str):
        """
        Find topic corresponding to deprecated Person key
        """
        return PersonTopic().load({"alt_ids.old-person-key": key})

    def annotate_place(self, d):
        properties = d.get('properties', {})
        for k in ['birthPlace', 'deathPlace']:
            place = properties.get(k)
            heKey = 'he' + k[0].upper() + k[1:]  # birthPlace => heBirthPlace
            if place and heKey not in properties:
                value, dataSource = place['value'], place['dataSource']
                place_obj = Place().load({"key": value})
                if place_obj:   
                    name = place_obj.primary_name('he')
                    d['properties'][heKey] = {'value': name, 'dataSource': dataSource}
        return d

    def contents(self, **kwargs):
        annotate_time_period = kwargs.get('annotate_time_period', False)
        d = super(PersonTopic, self).contents(**kwargs)
        if annotate_time_period:
            d = self.annotate_place(d)
            tp = self.most_accurate_life_period()
            if tp is not None:
                d['timePeriod'] = {
                    "name": {
                        "en": tp.primary_name("en"),
                        "he": tp.primary_name("he")
                    },
                    "yearRange": {
                        "en": re.sub(r'[()]', '', tp.period_string("en")),
                        "he": re.sub(r'[()]', '', tp.period_string("he")),
                    }
                }
        return d
    
    # A person may have an era, a generation, or a specific birth and death years, which each may be approximate.
    # They may also have none of these...
    def _most_accurate_period(self, time_period_class: Type[TimePeriod]) -> Optional[LifePeriod]:
        if self.get_property("birthYear") and self.get_property("deathYear"):
            return time_period_class({
                "start": self.get_property("birthYear"),
                "startIsApprox": self.get_property("birthYearIsApprox", False),
                "end": self.get_property("deathYear"),
                "endIsApprox": self.get_property("deathYearIsApprox", False)
            })
        elif self.get_property("birthYear") and self.get_property("era", "CO"):
            return time_period_class({
                "start": self.get_property("birthYear"),
                "startIsApprox": self.get_property("birthYearIsApprox", False),
            })
        elif self.get_property("deathYear"):
            return time_period_class({
                "end": self.get_property("deathYear"),
                "endIsApprox": self.get_property("deathYearIsApprox", False)
            })
        elif self.get_property("generation"):
            return time_period_class().load({"symbol": self.get_property("generation")})
        elif self.get_property("era"):
            return time_period_class().load({"symbol": self.get_property("era")})
        else:
            return None

    def most_accurate_time_period(self):
        '''
        :return: most accurate period as TimePeriod (used when a person's LifePeriod should be formatted like a general TimePeriod)
        '''
        return self._most_accurate_period(TimePeriod)

    def most_accurate_life_period(self):
        '''
        :return: most accurate period as LifePeriod. currently the only difference from TimePeriod is the way the time period is formatted as a string.
        '''
        return self._most_accurate_period(LifePeriod)

class AuthorTopic(PersonTopic):
    """
    Represents a topic which is an author of a book. Can be used on the `authors` field of `Index`
    """

    def get_authored_indexes(self):
        ins = IndexSet({"authors": self.slug})
        return sorted(ins, key=lambda i: Ref(i.title).order_id())

    def _authors_indexes_fill_category(self, indexes, path, include_dependant):
        from .text import library

        temp_index_title_set = {i.title for i in indexes}
        indexes_in_path = library.get_indexes_in_category_path(path, include_dependant, full_records=True)
        if indexes_in_path.count() == 0:
            # could be these are dependent texts without a collective title for some reason
            indexes_in_path = library.get_indexes_in_category_path(path, True, full_records=True)
            if indexes_in_path.count() == 0:
                return False
        path_end_set = {tuple(i.categories[len(path):]) for i in indexes}
        for index_in_path in indexes_in_path:
            if tuple(index_in_path.categories[len(path):]) in path_end_set:
                if index_in_path.title not in temp_index_title_set and self.slug not in set(getattr(index_in_path, 'authors', [])):
                    return False
        return True

    def _category_matches_author(self, category: Category) -> bool:
        from .schema import Term

        cat_term = Term().load({"name": category.sharedTitle})
        return len(set(cat_term.get_titles()) & set(self.get_titles())) > 0

    def aggregate_authors_indexes_by_category(self):
        from .text import library
        from .schema import Term

        def sort_sub_cat_book_sets(book_sets: List[SubCatBookSet]):
            # a subcategory which covers more books is preferable,
            # if parent and child cover the same amount of  books (i.e the parent contains the child only), prefer child
            return sorted(book_sets, key=lambda book_set: (len(book_set.books), book_set.depth), reverse=True)

        def index_is_commentary(index):
            return getattr(index, 'base_text_titles', None) is not None and len(index.base_text_titles) > 0 and getattr(
                index, 'collective_title', None) is not None

        indexes = self.get_authored_indexes()

        final_aggregations = []

        blocks = {}

        MAX_ICAT_FROM_END_TO_CONSIDER = 2
        for index in indexes:
            is_comm = index_is_commentary(index)
            base = library.get_index(index.base_text_titles[0]) if is_comm else index
            collective_title = index.collective_title if is_comm else None
            base_cat_path = tuple(base.categories[:-MAX_ICAT_FROM_END_TO_CONSIDER + 1])

            new_empty_block = DisjointBookSet(base_cat_path=base_cat_path, collective_title=collective_title)
            if new_empty_block in blocks:
                block = blocks[new_empty_block]
            else:
                blocks[new_empty_block] = new_empty_block
                block = new_empty_block

            for icat in range(len(base.categories) - MAX_ICAT_FROM_END_TO_CONSIDER, len(base.categories)):
                sub_cat_book_set = SubCatBookSet(depth=icat+1, sub_cat_path=tuple(base.categories[:icat + 1]))
                if sub_cat_book_set not in block.sub_cat_book_sets:
                    block.sub_cat_book_sets[sub_cat_book_set] = sub_cat_book_set

                block.sub_cat_book_sets[sub_cat_book_set].books.append(index)

        for block in blocks:
            cat_choices_sorted = sort_sub_cat_book_sets(book_sets=block.sub_cat_book_sets.values())
            collective_title = block.collective_title
            best_base_cat_path = cat_choices_sorted[0].sub_cat_path
            temp_indexes = cat_choices_sorted[0].books
            if len(temp_indexes) == 1:
                final_aggregations += [AuthorAggregationFactory.create(index=temp_indexes[0])]
                continue
            if best_base_cat_path == ('Talmud', 'Bavli'):
                best_base_cat_path = ('Talmud',)  # hard-coded to get 'Rashi on Talmud' instead of 'Rashi on Bavli'

            base_category = Category().load({"path": list(best_base_cat_path)})
            if collective_title is None:
                index_category = base_category
                collective_title_term = None
            else:
                index_category = Category.get_shared_category(temp_indexes)
                collective_title_term = Term().load({"name": collective_title})
            if index_category is None or not self._authors_indexes_fill_category(temp_indexes, index_category.path,
                                                                                 collective_title is not None) or (
                    collective_title is None and self._category_matches_author(index_category)):
                for temp_index in temp_indexes:
                    final_aggregations += [AuthorAggregationFactory.create(index=temp_index)]
                continue
            final_aggregations += [AuthorAggregationFactory.create(index_category=index_category, collective_term=collective_title_term, base_category=base_category)]
        return final_aggregations


    def get_aggregated_urls_for_authors_indexes(self) -> list:
        """
        Aggregates author's works by category when possible and
        returns a dictionary. Each dictionary is of shape {"url": str, "title": {"en": str, "he": str}, "description": {"en": str, "he": str}}
        corresponding to an index or category of indexes of this author's works.
        """
        aggregations = self.aggregate_authors_indexes_by_category()
        unique_urls = []
        for agg in aggregations:
            unique_urls.append({"url": agg.get_url(),
                                "title": {"en": agg.get_title('en'), "he": agg.get_title('he')},
                                "description": {"en": agg.get_description('en'), "he": agg.get_description('he')}})
        return unique_urls

    @staticmethod
    def is_author(slug):
        t = Topic.init(slug)
        return t and isinstance(t, AuthorTopic)


class TopicSet(abst.AbstractMongoSet):
    recordClass = Topic

    def __init__(self, query=None, *args, **kwargs):
        if self.recordClass != Topic:
            # include class name of recordClass + any class names of subclasses
            query = query or {}
            subclass_names = [self.recordClass.__name__] + [klass.__name__ for klass in self.recordClass.all_subclasses()]
            query['subclass'] = {"$in": [self.recordClass.reverse_subclass_map[name] for name in subclass_names]}
        
        super().__init__(query=query, *args, **kwargs)

    @staticmethod
    def load_by_title(title):
        query = {'titles.text': title}
        return TopicSet(query=query)

    def _read_records(self):
        super()._read_records()
        for rec in self.records:
            if getattr(rec, 'subclass', False):
                Subclass = globals()[self.recordClass.subclass_map[rec.subclass]]
                rec.__class__ = Subclass  # cast to relevant subclass


class PersonTopicSet(TopicSet):
    recordClass = PersonTopic


class AuthorTopicSet(PersonTopicSet):
    recordClass = AuthorTopic


class TopicLinkHelper(object):
    """
    Used to collect attributes and functions that are useful for both IntraTopicLink and RefTopicLink
    Decided against superclass arch b/c instantiated objects will be of type super class.
    This is inconvenient when validating the attributes of object before saving (since subclasses have different required attributes)
    """
    collection = 'topic_links'
    required_attrs = [
        'toTopic',
        'linkType',
        'class',  # can be 'intraTopic' or 'refTopic'
        'dataSource',

    ]
    optional_attrs = [
        'generatedBy',
        'order',  # dict with some data on how to sort this link. can have key 'custom_order' which should trump other data
        'isJudgementCall',
    ]
    generated_by_sheets = "sheet-topic-aggregator"

    @staticmethod
    def init_by_class(topic_link, context_slug=None):
        """
        :param topic_link: dict from `topic_links` collection
        :return: either instance of IntraTopicLink or RefTopicLink based on 'class' field of `topic_link`
        """
        if topic_link['class'] == 'intraTopic':
            return IntraTopicLink(topic_link, context_slug=context_slug)
        if topic_link['class'] == 'refTopic':
            return RefTopicLink(topic_link)


class IntraTopicLink(abst.AbstractMongoRecord):
    collection = TopicLinkHelper.collection
    required_attrs = TopicLinkHelper.required_attrs + ['fromTopic']
    optional_attrs = TopicLinkHelper.optional_attrs
    valid_links = []

    def __init__(self, attrs=None, context_slug=None):
        """

        :param attrs:
        :param str context_slug: if this link is being used in a specific context, give the topic slug which represents the context. used to set if the link should be considered inverted
        """
        super(IntraTopicLink, self).__init__(attrs=attrs)
        self.context_slug = context_slug

    def load(self, query, proj=None):
        query = TopicLinkSetHelper.init_query(query, 'intraTopic')
        return super().load(query, proj)

    def _normalize(self):
        setattr(self, "class", "intraTopic")

    def _pre_save(self):
        pass

    def _validate(self):
        super(IntraTopicLink, self)._validate()

        # check everything exists
        TopicLinkType.validate_slug_exists(self.linkType, 0)
        Topic.validate_slug_exists(self.fromTopic)
        Topic.validate_slug_exists(self.toTopic)
        TopicDataSource.validate_slug_exists(self.dataSource)

        # check for duplicates
        duplicate = IntraTopicLink().load({"linkType": self.linkType, "fromTopic": self.fromTopic, "toTopic": self.toTopic,
                 "class": getattr(self, 'class'), "_id": {"$ne": getattr(self, "_id", None)}})
        if duplicate is not None:
            raise DuplicateRecordError(
                "Duplicate intra topic link for linkType '{}', fromTopic '{}', toTopic '{}'".format(
                    self.linkType, self.fromTopic, self.toTopic))

        link_type = TopicLinkType.init(self.linkType, 0)
        if link_type.slug == link_type.inverseSlug:
            duplicate_inverse = IntraTopicLink().load({"linkType": self.linkType, "toTopic": self.fromTopic, "fromTopic": self.toTopic,
             "class": getattr(self, 'class'), "_id": {"$ne": getattr(self, "_id", None)}})
            if duplicate_inverse is not None:
                raise DuplicateRecordError(
                    "Duplicate intra topic link in the inverse direction of the symmetric linkType '{}', fromTopic '{}', toTopic '{}' exists".format(
                        duplicate_inverse.linkType, duplicate_inverse.fromTopic, duplicate_inverse.toTopic))

        # check types of topics are valid according to validFrom/To
        from_topic = Topic.init(self.fromTopic)
        to_topic = Topic.init(self.toTopic)
        if getattr(link_type, 'validFrom', False):
            assert from_topic.has_types(set(link_type.validFrom)), "from topic '{}' does not have valid types '{}' for link type '{}'. Instead, types are '{}'".format(self.fromTopic, ', '.join(link_type.validFrom), self.linkType, ', '.join(from_topic.get_types()))
        if getattr(link_type, 'validTo', False):
            assert to_topic.has_types(set(link_type.validTo)), "to topic '{}' does not have valid types '{}' for link type '{}'. Instead, types are '{}'".format(self.toTopic, ', '.join(link_type.validTo), self.linkType, ', '.join(to_topic.get_types()))

        # assert this link doesn't create circular paths (in is_a link type)
        # should consider this test also for other non-symmetric link types such as child-of
        if self.linkType == TopicLinkType.isa_type:
            to_topic = Topic.init(self.toTopic)
            ancestors = to_topic.get_types()
            assert self.fromTopic not in ancestors, "{} is an is-a ancestor of {} creating an illogical circle in the topics graph, here are {} ancestors: {}".format(self.fromTopic, self.toTopic, self.toTopic, ancestors)

    def contents(self, **kwargs):
        d = super(IntraTopicLink, self).contents(**kwargs)
        if not (self.context_slug is None or kwargs.get('for_db', False)):
            d['isInverse'] = self.is_inverse
            d['topic'] = self.topic
            del d['toTopic']
            del d['fromTopic']
            if d.get('order', None) is not None:
                d['order']['tfidf'] = self.tfidf
                d['order'].pop('toTfidf', None)
                d['order'].pop('fromTfidf', None)
        return d

    # PROPERTIES

    def get_is_inverse(self):
        return self.context_slug == self.toTopic

    def get_topic(self):
        return self.fromTopic if self.is_inverse else self.toTopic

    def get_tfidf(self):
        order = getattr(self, 'order', {})
        return order.get('fromTfidf' if self.is_inverse else 'toTfidf', 0)

    topic = property(get_topic)
    tfidf = property(get_tfidf)
    is_inverse = property(get_is_inverse)


class RefTopicLink(abst.AbstractMongoRecord):
    collection = TopicLinkHelper.collection

    # is_sheet and expandedRef: defaulted automatically in normalize
    required_attrs = TopicLinkHelper.required_attrs + ['ref', 'expandedRefs', 'is_sheet']

    # unambiguousToTopic: used when linking to an ambiguous topic. There are some instance when you need to decide on one of the options (e.g. linking to an ambiguous rabbi in frontend). this can be used as a proxy for toTopic in those cases.
    # descriptions: Titles and learning prompts for this Ref in this Topic context.  Structured as follows:
    # descriptions: {
    #     en: {
    #         title: Str,
    #         prompt: Str,
    #         primacy: Int
    #     },
    #     he: {
    #         title: Str,
    #         prompt: Str,
    #         primacy: Int
    #     }
    # }
    optional_attrs = TopicLinkHelper.optional_attrs + ['charLevelData', 'unambiguousToTopic', 'descriptions']

    def set_description(self, lang, title, prompt):
        d = getattr(self, "descriptions", {})
        d[lang] = {
            "title": title,
            "prompt": prompt,
        }
        self.descriptions = d
        return self

    def get_related_pool(self):
        return PoolType.SHEETS.value if self.is_sheet else PoolType.LIBRARY.value

    def get_topic(self):
        return Topic().load({'slug': self.toTopic})

    def save(self, override_dependencies=False):
        super(RefTopicLink, self).save(override_dependencies)
        topic = self.get_topic()
        topic.update_after_link_change(self.get_related_pool())

    def delete(self, force=False, override_dependencies=False):
        topic = self.get_topic()
        pool = self.get_related_pool()
        super(RefTopicLink, self).delete(force, override_dependencies)
        if topic:
            topic.update_after_link_change(pool)

    def _sanitize(self):
        super()._sanitize()
        for lang, d in getattr(self, "descriptions", {}).items():
            for k, v in d.items():
                if isinstance(v, str):
                    self.descriptions[lang][k] = bleach.clean(v, tags=self.ALLOWED_TAGS, attributes=self.ALLOWED_ATTRS)

    def load(self, query, proj=None):
        query = TopicLinkSetHelper.init_query(query, 'refTopic')
        return super().load(query, proj)

    def _normalize(self):
        super(RefTopicLink, self)._normalize()
        self.is_sheet = bool(re.search(r"Sheet \d+$", self.ref))
        setattr(self, "class", "refTopic")
        if self.is_sheet:
            self.expandedRefs = [self.ref]
        else:  # Ref is a regular Sefaria Ref
            self.ref = Ref(self.ref).normal()
            self.expandedRefs = [r.normal() for r in Ref(self.ref).all_segment_refs()]

    def _sanitize(self):
        """
        Sanitize the "title" and "prompt" for all descriptions.
        Since they're human editable they are candidates for XSS.
        @return:
        """
        for lang in ("en", "he"):
            description = getattr(self, "descriptions", {}).get(lang)
            if description:
                for field in ("title", "prompt"):
                    value = description.get(field)
                    if value:
                        description[field] = bleach.clean(value, tags=self.ALLOWED_TAGS, attributes=self.ALLOWED_ATTRS)
                self.descriptions[lang] = description

    def _validate(self):
        Topic.validate_slug_exists(self.toTopic)
        TopicLinkType.validate_slug_exists(self.linkType, 0)
        to_topic = Topic.init(self.toTopic)
        link_type = TopicLinkType.init(self.linkType, 0)
        if getattr(link_type, 'validTo', False):
            assert to_topic.has_types(set(link_type.validTo)), "to topic '{}' does not have valid types '{}' for link type '{}'. Instead, types are '{}'".format(self.toTopic, ', '.join(link_type.validTo), self.linkType, ', '.join(to_topic.get_types()))
    
    def _pre_save(self):
        if getattr(self, "_id", None) is None:
            # check for duplicates
            query = {"linkType": self.linkType, "ref": self.ref, "toTopic": self.toTopic, "dataSource": getattr(self, 'dataSource', {"$exists": False}), "class": getattr(self, 'class')}
            if getattr(self, "charLevelData", None):
                query["charLevelData.startChar"] = self.charLevelData['startChar']
                query["charLevelData.endChar"] = self.charLevelData['endChar']
                query["charLevelData.versionTitle"] = self.charLevelData['versionTitle']
                query["charLevelData.language"] = self.charLevelData['language']

            duplicate = RefTopicLink().load(query)
            if duplicate is not None:
                raise DuplicateRecordError("Duplicate ref topic link for linkType '{}', ref '{}', toTopic '{}', dataSource '{}'".format(
                self.linkType, self.ref, self.toTopic, getattr(self, 'dataSource', 'N/A')))

    def contents(self, **kwargs):
        d = super(RefTopicLink, self).contents(**kwargs)
        if not kwargs.get('for_db', False):
            d['topic'] = d['toTopic']
            d.pop('toTopic')
        return d

class TopicLinkSetHelper(object):
    @staticmethod
    def init_query(query, link_class):
        query = query or {}
        query['class'] = link_class
        return query

    @staticmethod
    def find(query=None, page=0, limit=0, sort=[("_id", 1)], proj=None, record_kwargs=None):
        from sefaria.system.database import db
        record_kwargs = record_kwargs or {}
        raw_records = getattr(db, TopicLinkHelper.collection).find(query, proj).sort(sort).skip(page * limit).limit(limit)
        return [TopicLinkHelper.init_by_class(r, **record_kwargs) for r in raw_records]


class IntraTopicLinkSet(abst.AbstractMongoSet):
    recordClass = IntraTopicLink

    def __init__(self, query=None, *args, **kwargs):
        query = TopicLinkSetHelper.init_query(query, 'intraTopic')
        super().__init__(query=query, *args, **kwargs)


class RefTopicLinkSet(abst.AbstractMongoSet):
    recordClass = RefTopicLink

    def __init__(self, query=None, *args, **kwargs):
        query = TopicLinkSetHelper.init_query(query, 'refTopic')
        super().__init__(query=query, *args, **kwargs)


class TopicLinkType(abst.SluggedAbstractMongoRecord):
    collection = 'topic_link_types'
    slug_fields = ['slug', 'inverseSlug']
    required_attrs = [
        'slug',
        'inverseSlug',
        'displayName',
        'inverseDisplayName'
    ]
    optional_attrs = [
        'pluralDisplayName',
        'inversePluralDisplayName',
        'alt_ids',
        'inverse_alt_ids',
        'shouldDisplay',
        'inverseShouldDisplay',
        'groupRelated',
        'inverseGroupRelated',
        'devDescription',
        'validFrom',
        'validTo'
    ]
    related_type = 'related-to'
    isa_type = 'is-a'
    possibility_type = 'possibility-for'

    def _validate(self):
        super(TopicLinkType, self)._validate()
        # Check that validFrom and validTo contain valid topic slugs if exist

        for validToTopic in getattr(self, 'validTo', []):
            Topic.validate_slug_exists(validToTopic)

        for validFromTopic in getattr(self, 'validFrom', []):
            Topic.validate_slug_exists(validFromTopic)

    def get(self, attr, is_inverse, default=None):
        attr = 'inverse{}{}'.format(attr[0].upper(), attr[1:]) if is_inverse else attr
        return getattr(self, attr, default)


class TopicLinkTypeSet(abst.AbstractMongoSet):
    recordClass = TopicLinkType


class TopicDataSource(abst.SluggedAbstractMongoRecord):
    collection = 'topic_data_sources'
    slug_fields = ['slug']
    required_attrs = [
        'slug',
        'displayName',
    ]
    optional_attrs = [
        'url',
        'description',
    ]


class TopicDataSourceSet(abst.AbstractMongoSet):
    recordClass = TopicDataSource


def process_index_title_change_in_topic_links(indx, **kwargs):
    from sefaria.system.exceptions import InputError

    print("Cascading Topic Links from {} to {}".format(kwargs['old'], kwargs['new']))

    # ensure that the regex library we're using here is the same regex library being used in `Ref.regex`
    from .text import re as reg_reg
    patterns = [pattern.replace(reg_reg.escape(indx.title), reg_reg.escape(kwargs["old"]))
                for pattern in Ref(indx.title).regex(as_list=True)]
    queries = [{'ref': {'$regex': pattern}} for pattern in patterns]
    objs = RefTopicLinkSet({"$or": queries})
    for o in objs:
        o.ref = o.ref.replace(kwargs["old"], kwargs["new"], 1)
        try:
            o.save()
        except InputError:
            logger.warning("Failed to convert ref data from: {} to {}".format(kwargs['old'], kwargs['new']))

def process_index_delete_in_topic_links(indx, **kwargs):
    from sefaria.model.text import prepare_index_regex_for_dependency_process
    pattern = prepare_index_regex_for_dependency_process(indx)
    RefTopicLinkSet({"ref": {"$regex": pattern}}).delete()

def process_topic_delete(topic):
    RefTopicLinkSet({"toTopic": topic.slug}).delete()
    IntraTopicLinkSet({"$or": [{"toTopic": topic.slug}, {"fromTopic": topic.slug}]}).delete()
    for sheet in db.sheets.find({"topics.slug": topic.slug}):
        sheet["topics"] = [t for t in sheet["topics"] if t["slug"] != topic.slug]
        db.sheets.replace_one({"_id":sheet["_id"]}, sheet, upsert=True)
    try:
        DjangoTopic.objects.get(slug=topic.slug).delete()
    except DjangoTopic.DoesNotExist:
        print('Topic {} does not exist in django'.format(topic.slug))

def process_topic_description_change(topic, **kwargs):
    """
    Upon topic description change, get rid of old markdown links and save any new ones
    """
    # need to delete currently existing links but dont want to delete link if its still in the description
    # so load up a dictionary of relevant data -> link
    IntraTopicLinkSet({"toTopic": topic.slug, "linkType": "related-to", "dataSource": "learning-team-editing-tool"}).delete()
    refLinkType = 'popular-writing-of' if getattr(topic, 'subclass', '') == 'author' else 'about'
    RefTopicLinkSet({"toTopic": topic.slug, "linkType": refLinkType, "dataSource": "learning-team-editing-tool"}).delete()

    markdown_links = set()
    for lang, val in kwargs['new'].items():   # put each link in a set so we dont try to create duplicate of same link
        for m in re.findall('\[.*?\]\((.*?)\)', val):
            markdown_links.add(m)

    for markdown_link in markdown_links:
        if markdown_link.startswith("/topics"):
            other_slug = markdown_link.split("/")[-1]
            intra_topic_dict = {"toTopic": topic.slug, "linkType": "related-to",
                                "dataSource": "learning-team-editing-tool", 'fromTopic': other_slug}
            try:
                IntraTopicLink(intra_topic_dict).save()
            except DuplicateRecordError as e:  # there may be identical IntraTopicLinks with a different dataSource or inverted fromTopic and toTopic
                pass
        else:
            if markdown_link.startswith("/"):
                markdown_link = markdown_link[1:]  # assume link starts with a '/'
            try:
                ref = Ref(markdown_link).normal()
            except InputError as e:
                continue
            ref_topic_dict = {"toTopic": topic.slug, "dataSource": "learning-team-editing-tool", "linkType": refLinkType,
                              'ref': ref}
            RefTopicLink(ref_topic_dict).save()







```

### sefaria/model/collection.py

```
"""
collections.py
Writes to MongoDB Collection: groups
"""
import bleach
import re
import secrets
from datetime import datetime

from django.utils import translation
from django.utils.translation import ugettext as _

from . import abstract as abst
from sefaria.model.user_profile import public_user_data
from sefaria.system.exceptions import InputError
from sefaria.utils import hebrew

class Collection(abst.AbstractMongoRecord):
    """
    A collection of source sheets
    """
    collection = 'groups'
    history_noun = 'group'

    track_pkeys = True
    pkeys = ["name", "slug", "listed", "headerUrl", "imageUrl", "coverUrl"]

    required_attrs = [
        "name",          # string name of collection
        "sheets",        # list of sheet ids included in this collection
        "slug",          # string of url slug
        "lastModified",  # Datetime of the last time this collection changed
        "admins",        # list of uids
        "members",       # list of uids
    ]
    optional_attrs = [
        "privateSlug",   # string of url slug that was previously used when collection was private
        "publishers",       # list of uids TODO remove post collections launch
        "invitations",      # list of dictionaries representing outstanding invitations
        "description",      # string text of short description
        "websiteUrl",       # url of a website displayed on this collection
        "headerUrl",        # url of an image to use in header
        "imageUrl",         # url of an image to use as icon
        "coverUrl",         # url of an image to use as cover
        "pinned_sheets",    # list of sheet ids, pinned to top
        "listed",           # Bool, whether to list collection publicly
        "moderationStatus", # string status code for moderator-set statuses
        "pinnedTags",       # list of strings, display order for sheet tags
        "showTagsByDefault",# Bool, whether to default to opening tags list
        "toc",              # object signaling inclusion in TOC with fields
                                # `categories` - list
                                # `title` - string
                                # `heTitle` - string
                                # `collectiveTitle` - optional dictionary with `en`, `he`, overiding title display in TOC/Sidebar.
                                # `desscription` - string
                                # `heDescription` - string
                                # `enShortDesc` - string
                                # `heShortDesc` - string
                                # `dependence` - string - "Commentary" or "Targum"
                                # These fields will override `name` and `description` for display
    ]

    def _normalize(self):
        if not getattr(self, "slug", None):
            self.assign_slug()

        defaults = (("members", []), ("sheets", []))
        for default in defaults:
            if not hasattr(self, default[0]):
                setattr(self, default[0], default[1])

        self.lastModified = datetime.now()

        website = getattr(self, "websiteUrl", False)
        if website and not website.startswith("https://"):
            if website.startswith("http://"):
                # Only allow HTTPS. If you site doesn't support it, deal with it!
                self.websiteUrl = website.replace("http://", "https://")
            else:
                self.websiteUrl = "https://" + website

        toc = getattr(self, "toc", None)
        if toc:
            tags = ["b", "i", "br", "span"]
            attrs = {"span": ["class"]}
            toc["description"] = bleach.clean(toc["description"], tags=tags, attributes=attrs)
            toc["heDescription"] = bleach.clean(toc["heDescription"], tags=tags, attributes=attrs)

    def _validate(self):
        assert super(Collection, self)._validate()

        if len(self.name) == 0:
            raise InputError(_("Please set a name for your collection."))

        return True

    def _pre_save(self):
        old_status, new_status = self.pkeys_orig_values.get("listed", None), getattr(self, "listed", None)
        if new_status and not old_status:
            # Collection is being published, assign a new slug, but save the old one for link stability
            self.privateSlug = self.slug
            self.assign_slug()

        if old_status and not new_status:
            # Public collection is going back to private, restore old slug
            if getattr(self, "privateSlug", None):
                self.slug = self.privateSlug
            else:
                self.assign_slug()

        if new_status and not old_status:
            # At moment of publishing, make checks for special requirements on public collections
            # Don't make these checks on every save so a collection can't get stuck in a state where 
            # it can't be change even to add a new public sheet. 
            if self.name_taken():
                # Require public collections to have a unique name
                raise InputError(_("A public collection with this name already exists. Please choose a different name before publishing."))
            if not getattr(self, "imageUrl", False):
                raise InputError(_("Public Collections are required to include a collection image (a square image will work best)."))
            if self.public_sheet_count() < 3:
                raise InputError(_("Public Collections are required to have at least 3 public sheets."))


        image_fields = ("imageUrl", "headerUrl", "coverUrl")
        for field in image_fields:
            old, new = self.pkeys_orig_values.get(field, None), getattr(self, field, None)
            if old != new:
                self._handle_image_change(old)

    def assign_slug(self):
        """
        Assign a slug for the collection. For public collections based on the collection 
        name, for private collections a random string.
        """
        if getattr(self, "listed", False):
            slug = self.name
            slug = slug.lower()
            slug = slug.strip()
            slug = slug.replace(" ", "-")
            slug = re.sub(r"[^a-z\u05D0-\u05ea0-9\-]", "", slug)
            self.slug = slug
            dupe_count = 0
            while self.slug_taken():
                dupe_count += 1
                self.slug = "%s%d" % (slug, dupe_count)
        else:
            while True:
                self.slug = secrets.token_urlsafe(6)
                if not self.slug_taken():
                    break

    def slug_taken(self):
        existing = Collection().load({"slug": self.slug})
        return bool(existing) and existing._id != getattr(self, "_id", None)

    def name_taken(self):
        existing = Collection().load({"name": self.name, "listed": True})
        return bool(existing) and existing._id != getattr(self, "_id", None)

    def all_names(self, lang):
        primary_name = self.primary_name(lang)
        names = [primary_name] if primary_name else []

        if hasattr(self, "toc"):
            names += [self.toc["title"]] if lang == "en" else [self.toc["heTitle"]]
            names += [self.toc["collectiveTitle"][lang]] if "collectiveTitle" in self.toc else []

        return list(set(names))

    def primary_name(self, lang):
        return self.name if (hebrew.has_hebrew(self.name) == (lang == "he")) else None

    def contents(self, with_content=False, authenticated=False):
        from sefaria.sheets import sheet_topics_counts
        contents = super(Collection, self).contents()
        if with_content:
            contents["sheets"]       = self.sheet_contents(authenticated=authenticated)
            contents["admins"]       = [public_user_data(uid) for uid in contents["admins"]]
            contents["members"]      = [public_user_data(uid) for uid in contents["members"]]
            contents["lastModified"] = str(self.lastModified)
            contents["invitations"]  = getattr(self, "invitations", []) if authenticated else []
            contents["pinnedSheets"] = getattr(self, "pinned_sheets", [])
            contents["pinnedTags"]   = getattr(self, "pinnedTags", [])
        return contents

    def listing_contents(self, uid=None):
        contents = {
            "name": self.name,
            "slug": self.slug,
            "description": getattr(self, "description", None),
            "imageUrl": getattr(self, "imageUrl", None),
            "headerUrl": getattr(self, "headerUrl", None),
            "memberCount": self.member_count(),
            "sheetCount": self.sheet_count(),
            "lastModified": str(self.lastModified),
            "listed": getattr(self, "listed", False),
        }
        if uid is not None:
            contents["membership"] = self.membership_role(uid)
        return contents

    def sheet_contents(self, authenticated=False):
        from sefaria.sheets import sheet_list
        if authenticated is False and getattr(self, "listed", False):
            query = {"status": "public", "id": {"$in": self.sheets}}
        else:
            query = {"status": {"$in": ["unlisted", "public"]}, "id": {"$in": self.sheets}}

        return sheet_list(query=query)

    def membership_role(self, uid):
        """
        Get membership level in collection
        :param uid:
        :return: either "member", "admin"
        """
        if uid in self.members:
            return "member"
        if uid in self.admins:
            return "admin"
        return None

    def add_member(self, uid, role="member"):
        """
        Adds `uid` as member of the collection in `role`.
        If `uid` is already a member, changes their role to `role`.
        """
        self.remove_member(uid)
        if role == "admin":
            self.admins.append(uid)
        else:
            self.members.append(uid)
        self.save()

    def remove_member(self, uid):
        """
        Remove `uid` from this collection.
        """
        self.admins  = [user_id for user_id in self.admins if user_id != uid]
        self.members = [user_id for user_id in self.members if user_id != uid]
        self.save()

    def invite_member(self, email, inviter, role="member"):
        """
        Invites a person by email to sign up for a Sefaria and join a collection.
        Creates on outstanding inviations record for `email` / `role`
        and sends an invitation to `email`.
        """
        self.remove_invitation(email)
        self.invitations = [{"email": email, "role": role}] + self.invitations
        self.send_invitation(email, inviter)
        self.save()

    def remove_invitation(self, email):
        """
        Removes any outstanding invitations for `email`.
        """
        if not getattr(self, "invitations", None):
            self.invitations = []
        else:
            self.invitations = [invite for invite in self.invitations if invite["email"] != email]
        self.save()

    def send_invitation(self, email, inviter_id):
        """
        Sends an email inviation to `email` from `invited_id`.
        """
        from django.core.mail import EmailMultiAlternatives
        from django.template.loader import render_to_string
        from sefaria.model import UserProfile

        inviter       = UserProfile(id=inviter_id)
        curr_lang     = translation.get_language()
        try:
            translation.activate(inviter.settings["interface_language"][0:2])
            message_html  = render_to_string("email/collection_signup_invitation_email.html",
                                            {
                                                "inviter": inviter.full_name,
                                                "collection_slug": self.slug,
                                                "registerUrl": "/register?next=%s" % self.url
                                            })
        finally:
            translation.activate(curr_lang)
        subject       = _("%(name)s invited you to a collection on Sefaria") % {'name': inviter.full_name}
        from_email    = "Sefaria <hello@sefaria.org>"
        to            = email

        msg = EmailMultiAlternatives(subject, message_html, from_email, [to])
        msg.content_subtype = "html"  # Main content is now text/html
        msg.send()

    def all_members(self):
        """
        Returns a list of all collection members, regardless of sole
        """
        return (self.admins + self.members)

    def is_member(self, uid):
        """
        Returns True if `uid` is a member of this collection, in any role
        """
        return uid in self.all_members()

    def member_count(self):
        """Returns the number of members in this collection"""
        return len(self.all_members())

    def sheet_count(self):
        """Returns the number of sheets in this collection"""
        return len(self.sheets)

    def public_sheet_count(self):
        """Returns the number of public sheets in this collection"""
        from sefaria.sheets import SheetSet
        return SheetSet({"id": {"$in": self.sheets}, "status": "public"}).count()      

    @property
    def url(self):
        """Returns the URL path for this collection"""
        return "/collections/{}".format(self.slug)

    def pin_sheet(self, sheet_id):
        """
        Adds or removes `sheet_id` from the list of pinned sheets
        """
        self.pinned_sheets = getattr(self, "pinned_sheets", [])
        if sheet_id in self.pinned_sheets:
            self.pinned_sheets = [id for id in self.pinned_sheets if id != sheet_id]
        else:
            self.pinned_sheets = [sheet_id] + self.pinned_sheets
        self.save()

    def _handle_image_change(self, old_url):
        """
        When image fields change:
        delete images that are no longer referenced
        """
        from sefaria.google_storage_manager import GoogleStorageManager
        bucket_name = GoogleStorageManager.COLLECTIONS_BUCKET
        if isinstance(old_url, str) and re.search("^https?://storage\.googleapis\.com/", old_url):  # only try to delete images in google cloud storage
            GoogleStorageManager.delete_filename(old_url, bucket_name)



class CollectionSet(abst.AbstractMongoSet):
    recordClass = Collection

    def for_user(self, uid, private=True):
        query = {"$or": [{"admins": uid}, {"members": uid}]}
        if not private:
            query["listed"] = True
        self.__init__(query, sort=[("lastModified", -1)])
        return self

    @classmethod
    def get_collection_listing(cls, userid):
        return {
            "private": [g.listing_contents() for g in cls().for_user(userid)],
            "public": [g.listing_contents() for g in
                       CollectionSet({"listed": True, "moderationStatus": {"$ne": "nolist"}}, sort=[("name", 1)])]
        }


def process_collection_slug_change_in_sheets(collection, **kwargs):
    """
    When a collections's slug changes, update all the sheets that have this collection as `displayedCollection`
    """
    from sefaria.system.database import db

    if not kwargs["old"]:
        return
    db.sheets.update_many({"displayedCollection": kwargs["old"]}, {"$set": {"displayedCollection": kwargs["new"]}})


def process_collection_delete_in_sheets(collection, **kwargs):
    """
    When a collection deleted, move any sheets out of the collection.
    """
    from sefaria.system.database import db
    db.sheets.update_many({"displayedCollection": collection.slug}, {"$set": {"displayedCollection": ""}})


def process_sheet_deletion_in_collections(sheet_id):
    """
    When a sheet is deleted remove it from any collections.
    Note: this function is not tied through dependencies.py (since Sheet mongo model isn't generlly used),
    but is called directly from sheet deletion view in sourcesheets/views.py. 
    """
    cs = CollectionSet({"sheets": sheet_id})
    for c in cs:
        c.sheets = [s for s in c.sheets if s != sheet_id]
        c.save()

```

### sefaria/model/layer.py

```
"""
layer.py
Writes to MongoDB Collection: layers
"""
from bson.objectid import ObjectId

from . import abstract as abst
from sefaria.model.text import Ref
from sefaria.model.note import Note, NoteSet


class Layer(abst.AbstractMongoRecord):
    """
    A collection of notes and sources.
    """
    collection   = 'layers'
    history_noun = 'layer'

    required_attrs = [
        "owner",
        "urlkey",
        "note_ids",
        "sources_list",
    ]
    optional_attrs = [
        "name",
        "first_ref",
        "all_refs", # list of all include refs and the refs of any note points to
    ]

    def _init_defaults(self):
        self.note_ids     = []
        self.sources_list = []

    def save(self):
        if not getattr(self, "first_ref", None):
            self.set_first_ref()
        super(Layer, self).save()

    def all(self, tref=None):
        """
        Returns all contents for this layer,
        optionally filtered for content pertaining to ref.
        """
        return self.notes(tref=tref) + self.sources(tref=tref)

    def sources(self, tref=None):
        """
        Returns sources for this layer,
        optionally filtered by sources pertaining to ref.
        """
        return []

    def notes(self, tref=None):
        """
        Returns notes for this layer,
        optionally filtered by notes on ref.
        """
        query   = {"_id": {"$in": self.note_ids}}
        if tref:
            query["ref"] = {"$regex": Ref(tref).section_ref().regex()}
        notes  = NoteSet(query=query)
        return [note for note in notes]

    def add_note(self, note_id):
        """
        Add 'note_id' to this Layer.
        """
        if isinstance(note_id, str):
            note_id = ObjectId(note_id)
        if note_id not in self.note_ids:
            self.note_ids.append(note_id)

    def set_first_ref(self):
        """
        Returns the ref of the first note in this layer
        """
        if len(self.note_ids):
            note = Note().load_by_id(self.note_ids[0])
            self.first_ref = note.ref if note else None

    def listeners(self):
        """
        Returns a list of uid for users who should be notified of changes to this layer.
        """
        listeners = NoteSet({"_id": {"$in": self.note_ids}}).distinct("owner")
        return listeners


class LayerSet(abst.AbstractMongoSet):
    recordClass = Layer


def process_note_deletion_in_layer(note, **kwargs):
    layers = LayerSet({"note_ids": note._id})
    for layer in layers:
        layer.note_ids = [nid for nid in layer.note_ids if nid != note._id]
    layers.save()
```

### sefaria/model/audio.py

```
# coding=utf-8
from urllib.parse import urlparse
import regex as re
from datetime import datetime
from collections import defaultdict

from . import abstract as abst
from . import text
from sefaria.system.database import db
from sefaria.model.text import Ref

import structlog
logger = structlog.get_logger(__name__)


class Audio(abst.AbstractMongoRecord):
    """
    Audio for sidebar connection pannel.
    """
    collection = 'audio'    
    required_attrs = [
        "audio_url",
        "source",
        "audio_type",
        "ref",
        "media",
        "license",
        "source_site",
        "description",
    ]

    def _normalize(self): # what does this do?
        self.ref = Ref(self.ref).normal()

    def client_contents(self, ref):
        d = self.contents()
        print(d)
        t = {}
        t["audio_url"]     = d["audio_url"] 
        t["source"]   = d["source"]
        t['start_time'] = ref['start_time']
        t['end_time'] = ref['end_time']
        t['anchorRef'] = ref['sefaria_ref']
        t['media'] = d['media']
        t['license'] = d['license']
        t['source_site'] = d['source_site']
        t['description'] = d['description']
        return t

class AudioSet(abst.AbstractMongoSet):
    recordClass = Audio

def get_audio_for_ref(tref):
    oref = text.Ref(tref)
    regex_list = oref.regex(as_list=True)
    ref_clauses = [{"ref.sefaria_ref": {"$regex": r}} for r in regex_list]
    query = {"$or": ref_clauses }
    results = AudioSet(query=query)
    client_results = []
    ref_re = "("+'|'.join(regex_list)+")"
    matched_ref = []
    for audio in results:
        for r in audio.ref:
            if re.match(ref_re, r['sefaria_ref']):
                matched_ref.append(r)
    for ref in matched_ref:
        audio_contents = audio.client_contents(ref) 
        
        client_results.append(audio_contents)

    return client_results        



```

### sefaria/model/note.py

```
"""
note.py
Writes to MongoDB Collection: notes
"""

import regex as re

from . import abstract as abst
from sefaria.model.text import Ref

import structlog
logger = structlog.get_logger(__name__)


class Note(abst.AbstractMongoRecord):
    """
    A note on a specific place in a text.  May be public or private.
    """
    collection    = 'notes'
    history_noun  = 'note'
    ALLOWED_TAGS  = ("i", "b", "br", "u", "strong", "em", "big", "small", "span", "div", "img", "a")
    ALLOWED_ATTRS = {
                        '*': ['class'],
                        'a': ['href', 'rel'],
                        'img': ['src', 'alt'],
                    }

    required_attrs = [
        "owner",
        "public",
        "text",
        "type",
        "ref"
    ]
    optional_attrs = [
        "title",
        "anchorText"
    ]

    def _normalize(self):
        self.ref = Ref(self.ref).normal()


class NoteSet(abst.AbstractMongoSet):
    recordClass = Note


def process_index_title_change_in_notes(indx, **kwargs):
    print("Cascading Notes {} to {}".format(kwargs['old'], kwargs['new']))
    pattern = Ref(indx.title).regex()
    pattern = pattern.replace(re.escape(indx.title), re.escape(kwargs["old"]))
    notes = NoteSet({"ref": {"$regex": pattern}})
    for n in notes:
        try:
            n.ref = n.ref.replace(kwargs["old"], kwargs["new"], 1)
            n.save()
        except Exception:
            logger.warning("Deleting note that failed to save: {}".format(n.ref))
            n.delete()

def process_index_delete_in_notes(indx, **kwargs):
    from sefaria.model.text import prepare_index_regex_for_dependency_process
    pattern = prepare_index_regex_for_dependency_process(indx)
    NoteSet({"ref": {"$regex": pattern}}).delete()

```

### sefaria/model/ref_data.py

```
import math

from . import abstract as abst
from . import text
from sefaria.system.exceptions import InputError

import structlog
logger = structlog.get_logger(__name__)


class RefData(abst.AbstractMongoRecord):
    """
    A segment ref with stats
    """
    collection = 'ref_data'
    DEFAULT_PAGERANK = 1.0
    DEFAULT_SHEETRANK = (1.0 / 5) ** 2
    DEFAULT_PAGESHEETRANK = DEFAULT_PAGERANK * DEFAULT_SHEETRANK
    required_attrs = [
        "ref",           # segment ref
        "pagesheetrank", # pagesheetrank value for segment ref
    ]

    def inverse_pagesheetrank(self):
        # returns float which is inversely proportional to pr, on a log-scale
        PR_MAX_CUTOFF = 70000
        MIN_PR = 0.1
        return 1.0 / (math.log(self.pagesheetrank) - math.log(MIN_PR)) if self.pagesheetrank < PR_MAX_CUTOFF else 0.0


class RefDataSet(abst.AbstractMongoSet):
    recordClass = RefData

    @classmethod
    def from_ref(cls, ref):
        all_refs = [r.normal() for r in ref.all_segment_refs()]
        return cls({"ref": {"$in": all_refs}})

    def top(self, n):
        return sorted(self, key=lambda rd: rd.pagesheetrank, reverse=True)[0:n]

    def nth_ref(self, n):
        return text.Ref(self.top(n)[-1].ref)

    def top_ref(self):
        return self.nth_ref(1)


def process_index_title_change_in_ref_data(indx, **kwargs):
    print("Cascading Ref Data from {} to {}".format(kwargs['old'], kwargs['new']))

    # ensure that the regex library we're using here is the same regex library being used in `Ref.regex`
    from .text import re as reg_reg
    patterns = [pattern.replace(reg_reg.escape(indx.title), reg_reg.escape(kwargs["old"]))
                for pattern in text.Ref(indx.title).regex(as_list=True)]
    queries = [{'ref': {'$regex': pattern}} for pattern in patterns]
    objs = RefDataSet({"$or": queries})
    for o in objs:
        o.ref = o.ref.replace(kwargs["old"], kwargs["new"], 1)
        try:
            o.save()
        except InputError:
            logger.warning("Failed to convert ref data from: {} to {}".format(kwargs['old'], kwargs['new']))


def process_index_delete_in_ref_data(indx, **kwargs):
    from sefaria.model.text import prepare_index_regex_for_dependency_process
    pattern = prepare_index_regex_for_dependency_process(indx)
    RefDataSet({"ref": {"$regex": pattern}}).delete()

```

### sefaria/model/category.py

```
# -*- coding: utf-8 -*-

import structlog
logger = structlog.get_logger(__name__)

from sefaria.system.database import db
from sefaria.system.exceptions import BookNameError, InputError, DuplicateRecordError
from . import abstract as abstract
from . import schema as schema
from . import text as text
from . import collection as collection
from .linker.has_match_template import HasMatchTemplates


class Category(abstract.AbstractMongoRecord, schema.AbstractTitledOrTermedObject, HasMatchTemplates):
    collection = 'category'
    history_noun = "category"

    track_pkeys = True
    criteria_field = 'path'
    criteria_override_field = 'origPath'  # used when primary attribute changes. field that holds old value.
    pkeys = ["path"]  # Needed for dependency tracking
    required_attrs = ["lastPath", "path", "depth"]
    optional_attrs = [
        "enDesc",
        "heDesc",
        "enShortDesc",
        "heShortDesc",
        "titles",
        "sharedTitle",
        "isPrimary",
        "searchRoot",
        "order",
        "match_templates",
    ]

    def __str__(self):
        return "Category: {}".format(", ".join(self.path))

    def __repr__(self):  # Wanted to use orig_tref, but repr can not include Unicode
        return "{}().load({{'path': [{}]}})".format(self.__class__.__name__, ", ".join(['"{}"'.format(x) for x in self.path]))

    def _init_defaults(self):
        self._init_title_defaults()
        self.sharedTitle = None

    def _set_derived_attributes(self):
        if hasattr(self, "origPath") and self.lastPath != self.path[-1]:
            # `origPath` is used by the Category Editor to update the path,
            # which should then propagate to the `lastPath` and `sharedTitle`
            self.change_key_name(self.path[-1])
        self._load_title_group()

    def change_key_name(self, name):
        # Doesn't yet support going from shared term to local or vise-versa.
        if self.sharedTitle and schema.Term().load({"name": name}):
            self.sharedTitle = name
            self._process_terms()
        elif not self.sharedTitle:
            self.add_title(name, "en", True, True)
        else:
            raise IndexError("Can not find Term for {}".format(name))
        self.lastPath = name
        self.path[-1] = name

    def _validate(self):
        super(Category, self)._validate()
        assert self.lastPath == self.path[-1] == self.get_primary_title("en"), "Category name not matching" + " - " + self.lastPath + " / " + self.path[-1] + " / " + self.get_primary_title("en")
        assert not hasattr(self, 'order') or isinstance(self.order, int), 'Order should be an integer'

        if self.is_new():
            duplicate = Category().load({'path': self.path})
            if duplicate:
                raise DuplicateRecordError(f'Category with path {self.path} already exists')

        if not self.sharedTitle and not self.get_titles_object():
            raise InputError("Category {} must have titles or a shared title".format(self))

        try:
            self.title_group.validate()
        except InputError as e:
            raise InputError("Category {} has invalid titles: {}".format(self, e))

        if self.sharedTitle and schema.Term().load({"name": self.sharedTitle}).titles != self.get_titles_object():
            raise InputError("Category {} with sharedTitle can not have explicit titles".format(self))

    def _normalize(self):
        super(Category, self)._normalize()

        self.depth = len(self.path)

        if not getattr(self, "lastPath", None):
            self.lastPath = self.path[-1]

        if self.sharedTitle:
            if getattr(self, "titles", None):
                del self.__dict__["titles"]
        else:
            self.titles = self.get_titles_object()

    def contents(self, **kwargs):
        d = super(Category, self).contents()
        if "lastPath" not in d:
            d["lastPath"] = self.path[-1]

        if d.get("sharedTitle", None) is not None:
            if "titles" in d:
                del d["titles"]
        else:
            d["titles"] = self.get_titles_object()

        return d

    def get_toc_object(self):
        from sefaria.model import library
        toc_tree = library.get_toc_tree()
        return toc_tree.lookup(self.path)

    def can_delete(self):
        obj = self.get_toc_object()
        if not obj:
            logger.error("Could not get TOC object for Category {}.".format("/".join(self.path)))
            return False
        if len(obj.children):
            logger.error("Can not delete category {} that has contents.".format("/".join(self.path)))
            return False
        return True

    @staticmethod
    def get_shared_category(indexes: list):
        """
        Get lowest category which includes all indexes in `indexes`
        :param list indexes: list of Index objects
        :return: Category
        """

        from collections import defaultdict

        cat_choice_dict = defaultdict(list)
        for index in indexes:
            for icat, cat in enumerate(index.categories):
                cat_path = tuple(index.categories[:icat+1])
                cat_choice_dict[(icat, cat_path)] += [index]
        sorted_cat_options = sorted(cat_choice_dict.items(), key=lambda x: (len(x[1]), x[0][0]), reverse=True)
        (_, cat_path), top_indexes = sorted_cat_options[0]
        return Category().load({"path": list(cat_path)})

class CategorySet(abstract.AbstractMongoSet):
    recordClass = Category


def process_category_path_change(changed_cat, **kwargs):
    def modify(old_val, new_val, pos):
        old_val[:pos] = new_val
        return old_val

    from sefaria.model.text import library
    from sefaria.model import Index
    tree = library.get_toc_tree()
    new_categories = kwargs["new"]
    old_toc_node = tree.lookup(kwargs["old"])
    assert isinstance(old_toc_node, TocCategory)

    collections = collection.CollectionSet({"toc": {"$exists": True}})
    pos = len(old_toc_node.ancestors())
    for c in collections:
        collection_in_old_category_tree = str(c.toc["categories"]).startswith(str(kwargs["old"]))
        if collection_in_old_category_tree:
            c.toc["categories"] = modify(c.toc["categories"], new_categories, pos)
            c.save(override_dependencies=True)

    children = old_toc_node.all_children()
    for child in children:
        if isinstance(child, TocCategory):   # change categories first since Index changes depend on the new category existing
            c = Category().load({'path': child.get_category_object().path}) # load directly from the DB to avoid a situation where the category was deleted but was still in TocTree cache
            if c is not None:
                c.path = modify(c.path, new_categories, pos)
                c.save(override_dependencies=True)

    for child in children:
        if isinstance(child, TocTextIndex):
            i = Index().load({"title": child.get_primary_title('en')})  # load directly from the DB to avoid a situation where the book was deleted but was still in TocTree cache
            if i is not None:
                i.categories = modify(i.categories, new_categories, pos)
                i.save(override_dependencies=True)



""" Object Oriented TOC """


def toc_serial_to_objects(toc):
    """
    Build TOC object tree from serial representation
    Was used to derive 1st class objects from TOC.  Not used in production.
    :param toc: Serialized TOC
    :return:
    """
    root = TocCategory()
    root.add_primary_titles("TOC", "")
    for e in toc:
        root.append(schema.deserialize_tree(e, struct_class=TocCategory, struct_title_attr="category", leaf_class=TocTextIndex, leaf_title_attr="title", children_attr="contents", additional_classes=[TocCollectionNode]))
    return root


class TocTree(object):
    def __init__(self, lib=None, mobile=False):
        """
        :param lib: Library object, in the process of being created
        """
        self._root = TocCategory()
        self._root.add_primary_titles("TOC", "")
        self._path_hash = {}
        self._library = lib
        self._collections_in_library = []

        # Store first section ref.
        vss = db.vstate.find({}, {"title": 1, "first_section_ref": 1, "flags": 1})
        self._vs_lookup = {vs["title"]: {
            "first_section_ref": vs.get("first_section_ref"),
            "heComplete": bool(vs.get("flags", {}).get("heComplete", False)),
            "enComplete": bool(vs.get("flags", {}).get("enComplete", False)),
        } for vs in vss}

        # Build Category object tree from stored Category objects
        for c in CategorySet(sort=[("depth", 1)]):
            self._add_category(c)

        # Get all of the first comment links
        ls = db.links.find({"is_first_comment": True}, {"first_comment_indexes":1, "first_comment_section_ref":1})
        self._first_comment_lookup = {frozenset(l["first_comment_indexes"]): l["first_comment_section_ref"] for l in ls}

        # Place Indexes
        indx_set = self._library.all_index_records() if self._library else text.IndexSet()
        for i in indx_set:
            if i.categories and i.categories[0] == "_unlisted":  # For the dummy sheet Index record
                continue
            node = self._make_index_node(i, mobile=mobile)
            cat = self.lookup(i.categories)
            if not cat:
                logger.warning("Failed to find category for {}".format(i.categories))
                continue
            cat.append(node)
            vs = self._vs_lookup.get(i.title, None)
            if not vs:
                continue
            # If any text in this category is incomplete, the category itself and its parents are incomplete
            for field in ("enComplete", "heComplete"):
                for acat in [cat] + list(reversed(cat.ancestors())):
                    # Start each category completeness as True, set to False whenever we hit an incomplete text below it
                    flag = False if not vs[field] else getattr(acat, field, True)
                    setattr(acat, field, flag)
                    if acat.get_primary_title() == "Commentary":
                        break # Don't consider a category incomplete for containing incomplete commentaries

            self._path_hash[tuple(i.categories + [i.title])] = node

        # Include Collections in TOC that has a `toc` field set
        collections = collection.CollectionSet({"toc": {"$exists": True}, "listed": True, "slug": {"$exists": True}})
        for c in collections:
            self._collections_in_library.append(c.slug)
            node = TocCollectionNode(collection_object=c)
            categories = node.categories
            cat  = self.lookup(node.categories)
            if not cat:
                logger.warning("Failed to find category for {}".format(categories))
                continue
            cat.append(node)
           
            self._path_hash[tuple(node.categories + [c.slug])] = node

        self._sort()

    def all_category_nodes(self, include_root = True):
        return ([self._root] if include_root else []) + [v for v in list(self._path_hash.values()) if isinstance(v, TocCategory)]

    def _sort(self):
        def _explicit_order_and_title(node):
            """
            Return sort key as tuple:  (value, value)
            :param node:
            :return:
            """

            # First sort by order attr
            try:
                return (node.order < 0, node.order) #negative order should be least

            # Sort objects w/o order attr by title
            except AttributeError:
                return (0.5, node.get_primary_title())

        for cat in self.all_category_nodes():  # iterate all categories
            if all([hasattr(ca, "base_text_order") for ca in cat.children]):
                cat.children.sort(key=lambda c: c.base_text_order)
            else:
                cat.children.sort(key=_explicit_order_and_title)

    def _make_index_node(self, index, old_title=None, mobile=False, include_first_section=False):
        d = index.toc_contents(include_first_section=include_first_section, include_flags=False, include_base_texts=True)

        title = old_title or d["title"]

        if mobile:
            vs = self._vs_lookup.get(title, {})
            d["firstSection"] = vs.get("first_section_ref", None)
        
        if "base_text_titles" in d and len(d["base_text_titles"]) > 0 and include_first_section:
            # `d["firstSection"]` assumes `include_first_section` is True
            #  this code seems to never actually get run
            d["refs_to_base_texts"] = {btitle:
                self._first_comment_lookup.get(frozenset([btitle, title]), d["firstSection"])
                for btitle in d["base_text_titles"]
                }
        
        return TocTextIndex(d, index_object=index)

    def _add_category(self, cat):
        try:
            tc = TocCategory(category_object=cat)
            parent = self._path_hash[tuple(cat.path[:-1])] if len(cat.path[:-1]) else self._root
            parent.append(tc)
            self._path_hash[tuple(cat.path)] = tc
        except KeyError:
            logger.warning(f"Failed to find parent category for {'/'.join(cat.path)}")

    def get_root(self):
        return self._root

    def get_serialized_toc(self):
        return self._root.serialize().get("contents", [])

    def get_collections_in_library(self):
        return self._collections_in_library

    def flatten(self):
        """
        Returns an array of strings which corresponds to each text in the
        Table of Contents in order.
        """
        return [n.primary_title() for n in self._root.get_leaf_nodes() if isinstance(n, TocTextIndex)]

    #todo: Get rid of the special case for "other", by placing it in the Index's category lists
    def lookup(self, cat_path, title=None):
        """
        :param cat_path: A list or tuple of the path to this category
        :param title: optional - name of text.  If present tries to return a text
        :return: TocNode
        """
        path = tuple(cat_path)
        if title is not None:
            path += tuple([title])
        try:
            return self._path_hash[path]
        except KeyError:
            # todo: remove this try, after getting rid of the "Other" cat.
            try:
                return self._path_hash[tuple(["Other"]) + path]
            except KeyError:
                return None

    def remove_category(self, toc_node):
        assert isinstance(toc_node, TocCategory)
        del self._path_hash[tuple(toc_node.get_category_object().path)]
        toc_node.detach()

    def remove_index(self, toc_node):
        assert isinstance(toc_node, TocTextIndex)
        del self._path_hash[tuple(toc_node.categories + [toc_node.primary_title()])]
        toc_node.detach()

    def update_title(self, index, old_ref=None, recount=True):
        title = old_ref or index.title
        node = self.lookup(index.categories, title)

        if recount or not node:
            from .version_state import VersionState
            try:
                vs = VersionState(title)
            except BookNameError:
                logger.warning("Failed to find VersionState for {} in TocTree.update_title()".format(title))
                return
            vs.refresh()
            # sn = vs.state_node(index.nodes)
            self._vs_lookup[title] = {
                "first_section_ref": vs.first_section_ref,
                "heComplete": vs.get_flag("heComplete"),
                "enComplete": vs.get_flag("enComplete"),
            }
        new_node = self._make_index_node(index, title)
        if node:
            node.replace(new_node)
        else:
            logger.info("Did not find TOC node to update: {} - adding.".format("/".join(index.categories + [title])))
            cat = self.lookup(index.categories)
            if not cat:
                logger.warning("Failed to find category for {}".format(index.categories))
            cat.append(new_node)

        self._path_hash[tuple(index.categories + [index.title])] = new_node


class TocNode(schema.TitledTreeNode):
    """
    Abstract superclass for all TOC nodes.
    """
    langs = ["he", "en"]
    title_attrs = {
        "en": "",
        "he": ""
    }
    thin_keys = []

    def __init__(self, serial=None, **kwargs):
        super(TocNode, self).__init__(serial, **kwargs)

        # remove title attributes after deserialization, so as not to mess with serial dicts.
        if serial is not None:
            for lang in self.langs:
                self.add_title(serial.get(self.title_attrs[lang]), lang, primary=True)
                delattr(self, self.title_attrs[lang])

    @property
    def full_path(self):
        return [n.primary_title("en") for n in self.ancestors()[1:]] + [self.primary_title("en")]

    # This varies a bit from the superclass. Seems not worthwhile to abstract into the superclass.
    def serialize(self, **kwargs):
        d = {}
        if self.children:
            d["contents"] = [n.serialize(**kwargs) for n in self.children]

        # thin param is used for generating search toc, and can be removed when search toc is retired.
        if kwargs.get("thin") is True:
            params = {k: getattr(self, k) for k in self.thin_keys if getattr(self, k, "BLANKVALUE") != "BLANKVALUE"}
        else:
            params = {k: getattr(self, k) for k in self.required_param_keys + self.optional_param_keys if
                  getattr(self, k, "BLANKVALUE") != "BLANKVALUE"}
        if any(params):
            d.update(params)

        for lang in self.langs:
            d[self.title_attrs[lang]] = self.title_group.primary_title(lang)

        return d


class TocCategory(TocNode):
    """
    "category": "",
    "heCategory": hebrew_term(""),
    "contents": []
    """
    def __init__(self, serial=None, **kwargs):
        self._category_object = kwargs.pop("category_object", None)
        super(TocCategory, self).__init__(serial, **kwargs)
        if self._category_object:
            self.add_primary_titles(self._category_object.get_primary_title("en"), self._category_object.get_primary_title("he"))
            if getattr(self._category_object, "isPrimary", False):
                self.isPrimary = True
            if getattr(self._category_object, "searchRoot", False):
                self.searchRoot = self._category_object.searchRoot
            for field in ("enDesc", "heDesc", "enShortDesc", "heShortDesc"):
                setattr(self, field, getattr(self._category_object, field, ""))
        if hasattr(self._category_object, 'order'):
            self.order = self._category_object.order

    optional_param_keys = [
        "order",
        "enComplete",
        "heComplete",
        "enDesc",
        "heDesc",
        "enShortDesc",
        "heShortDesc",
        "isPrimary",
        "searchRoot"
    ]

    title_attrs = {
        "he": "heCategory",
        "en": "category"
    }

    def get_category_object(self):
        return self._category_object


class TocTextIndex(TocNode):
    """
    categories: Array(2)
    dependence: false
    firstSection: "Mishnah Eruvin 1"
    heTitle: " "
    order: 13
    primary_category: "Mishnah"
    title: "Mishnah Eruvin"
    enComplete: true
    heComplete: true
    """

    thin_keys = ["order"]

    def __init__(self, serial=None, **kwargs):
        self._index_object = kwargs.pop("index_object", None)
        super(TocTextIndex, self).__init__(serial, **kwargs)
        if hasattr(self._index_object, 'order'):
            self.order = self._index_object.order[0]

    def get_index_object(self):
        return self._index_object

    optional_param_keys = [
        "categories",
        "dependence",
        "firstSection",
        "order",
        "primary_category",
        "heComplete",
        "enComplete",
        "enShortDesc",
        "heShortDesc",
        "collectiveTitle",
        "base_text_titles",
        "base_text_mapping",
        "heCollectiveTitle",
        "commentator",
        "heCommentator",
        "refs_to_base_texts",
        "base_text_order",
        "hidden",
        "corpus",
    ]
    title_attrs = {
        "en": "title",
        "he": "heTitle"
    }


class TocCollectionNode(TocNode):
    """
    categories: Array(2)
    name: "Some Collection"
    slug: "collection-slug"
    isCollection: true
    enComplete: true
    heComplete: true
    """
    def __init__(self, serial=None, collection_object=None, **kwargs):
        if collection_object:
            self._collection_object = collection_object
            c_contents = collection_object.contents()
            serial = {
                "categories": c_contents["toc"]["categories"],
                "name": c_contents["name"],
                "slug": c_contents["slug"],
                "title": c_contents["toc"]["collectiveTitle"]["en"] if "collectiveTitle" in c_contents["toc"] else c_contents["toc"]["title"],
                "heTitle": c_contents["toc"]["collectiveTitle"]["he"] if "collectiveTitle" in c_contents["toc"] else c_contents["toc"]["heTitle"], 
                "enShortDesc": c_contents["toc"].get("enShortDesc", ""),
                "heShortDesc": c_contents["toc"].get("heShortDesc", ""),
                "isCollection": True,
                "enComplete": True,
                "heComplete": True,
            }
        elif serial:
            self._collection_object = collection.Collection().load({"slug": serial["slug"]})

        super(TocCollectionNode, self).__init__(serial)

    def get_collection_object(self):
        return self._collection_object

    def serialize(self, **kwargs):
        d = super(TocCollectionNode, self).serialize()
        d["nodeType"] = "TocCollectionNode"
        return d

    required_param_keys = [
        "categories",
        "name",
        "slug",
        "title",
        "heTitle",
        "isCollection",
    ]

    optional_param_keys = [
        "order",
        "heComplete",
        "enComplete",
        "enShortDesc",
        "heShortDesc",
    ]

    title_attrs = {
        "en": "title",
        "he": "heTitle",
    }

```

### sefaria/model/dependencies.py

```
"""
dependencies.py -- list cross model dependencies and subscribe listeners to changes.
"""

from . import abstract, link, note, history, schema, text, layer, version_state, timeperiod, garden, notification, collection, library, category, ref_data, user_profile, manuscript, topic, place

from .abstract import subscribe, cascade, cascade_to_list, cascade_delete, cascade_delete_to_list
import sefaria.system.cache as scache

# Index Save / Create
subscribe(text.process_index_change_in_core_cache,                      text.Index, "save")
subscribe(version_state.create_version_state_on_index_creation,         text.Index, "save")
subscribe(text.process_index_change_in_toc,                             text.Index, "save")
subscribe(place.process_index_place_change, text.Index, 'attributeChange', 'compPlace')
subscribe(place.process_index_place_change, text.Index, 'attributeChange', 'pubPlace')

# Index Name Change
subscribe(text.process_index_title_change_in_core_cache,                text.Index, "attributeChange", "title")
subscribe(text.process_index_title_change_in_versions,                  text.Index, "attributeChange", "title")
subscribe(version_state.process_index_title_change_in_version_state,    text.Index, "attributeChange", "title")
subscribe(link.process_index_title_change_in_links,                     text.Index, "attributeChange", "title")
subscribe(note.process_index_title_change_in_notes,                     text.Index, "attributeChange", "title")
subscribe(history.process_index_title_change_in_history,                text.Index, "attributeChange", "title")
subscribe(text.process_index_title_change_in_dependant_records,         text.Index, "attributeChange", "title")
subscribe(text.process_index_title_change_in_sheets,                    text.Index, "attributeChange", "title")
subscribe(cascade(notification.GlobalNotificationSet, "content.index"), text.Index, "attributeChange", "title")
subscribe(ref_data.process_index_title_change_in_ref_data,              text.Index, "attributeChange", "title")
subscribe(user_profile.process_index_title_change_in_user_history,      text.Index, "attributeChange", "title")
subscribe(topic.process_index_title_change_in_topic_links,              text.Index, "attributeChange", "title")
subscribe(manuscript.process_index_title_change_in_manuscript_links,    text.Index, "attributeChange", "title")

# Taken care of on save
# subscribe(text.process_index_change_in_toc,                             text.Index, "attributeChange", "title")


# Index Delete (start with cache clearing)
subscribe(text.process_index_delete_in_core_cache,                      text.Index, "delete")
subscribe(version_state.process_index_delete_in_version_state,          text.Index, "delete")
subscribe(link.process_index_delete_in_links,                           text.Index, "delete")
subscribe(topic.process_index_delete_in_topic_links,                    text.Index, "delete")
subscribe(note.process_index_delete_in_notes,                           text.Index, "delete")
subscribe(text.process_index_delete_in_versions,                        text.Index, "delete")
subscribe(text.process_index_delete_in_toc,                             text.Index, "delete")
subscribe(cascade_delete(notification.GlobalNotificationSet, "content.index", "title"),   text.Index, "delete")
subscribe(ref_data.process_index_delete_in_ref_data,                    text.Index, "delete")


# Process in ES
# todo: handle index name change in ES
def process_version_title_change_in_search(ver, **kwargs):
    from sefaria.settings import SEARCH_INDEX_ON_SAVE
    if SEARCH_INDEX_ON_SAVE:
        from sefaria.search import delete_version, TextIndexer, get_new_and_current_index_names
        search_index_name = get_new_and_current_index_names("text")['current']
        # no reason to deal with merged index since versions don't exist. still leaving this here in case it is necessary
        # search_index_name_merged = get_new_and_current_index_names("merged")['current']
        text_index = library.get_index(ver.title)
        delete_version(text_index, kwargs.get("old"), ver.language)
        for ref in text_index.all_segment_refs():
            TextIndexer.index_ref(search_index_name, ref, kwargs.get("new"), ver.language, ver.languageFamilyName, ver.isPrimary)


# Version Title Change
subscribe(history.process_version_title_change_in_history,              text.Version, "attributeChange", "versionTitle")
subscribe(process_version_title_change_in_search,                       text.Version, "attributeChange", "versionTitle")
subscribe(cascade(notification.GlobalNotificationSet, "content.version"), text.Version, "attributeChange", "versionTitle")

subscribe(cascade_delete(notification.GlobalNotificationSet, "content.version", "versionTitle"),   text.Version, "delete")


# Note Delete
subscribe(layer.process_note_deletion_in_layer,                         note.Note, "delete")

# Topic
subscribe(topic.process_topic_delete,                                 topic.Topic, "delete")
subscribe(topic.process_topic_description_change,                       topic.Topic, "attributeChange", "description")
subscribe(topic.process_topic_delete,                                 topic.AuthorTopic, "delete")


# Terms
# TODO cascade change to Term.name.
# TODO Current locations where we know terms are used [Index, Categories]
# TODO Use Sefaria-Project/scripts/search_for_indexes_that_use_terms.py for now
subscribe(cascade(schema.TermSet, "scheme"),                                schema.TermScheme, "attributeChange", "name")
subscribe(text.reset_simple_term_mapping,                                   schema.Term, "delete")
subscribe(text.reset_simple_term_mapping,                                   schema.Term, "save")
"""
Notes on where Terms are used
Index (alt structs and schema)
Category
"""

# Time
subscribe(cascade(topic.PersonTopicSet, "properties.era.value"),          timeperiod.TimePeriod, "attributeChange", "symbol")
subscribe(cascade(topic.PersonTopicSet, "properties.generation.value"),   timeperiod.TimePeriod, "attributeChange", "symbol")

# Gardens
subscribe(cascade(garden.GardenStopSet, "garden"),                         garden.Garden, "attributeChange", "key")
subscribe(cascade_delete(garden.GardenStopSet, "garden", "key"),           garden.Garden, "delete")
subscribe(cascade(garden.GardenStopRelationSet, "garden"),                 garden.Garden, "attributeChange", "key")
subscribe(cascade_delete(garden.GardenStopRelationSet, "garden", "key"),   garden.Garden, "delete")
# from stop to stop rel

# Notifications, Stories
subscribe(cascade_delete(notification.NotificationSet, "global_id", "_id"),  notification.GlobalNotification, "delete")

# Collections
subscribe(collection.process_collection_slug_change_in_sheets,             collection.Collection, "attributeChange", "slug")
subscribe(collection.process_collection_delete_in_sheets,                  collection.Collection, "delete")
subscribe(cascade_delete(notification.NotificationSet, "content.collection_slug", "slug"), collection.Collection, "delete")


# Categories
subscribe(category.process_category_path_change,  category.Category, "attributeChange", "path")
subscribe(text.rebuild_library_after_category_change,                   category.Category, "save")

# Manuscripts
subscribe(manuscript.process_slug_change_in_manuscript,  manuscript.Manuscript, "attributeChange", "slug")
subscribe(manuscript.process_manucript_deletion,         manuscript.Manuscript, "delete")

'''
# These are contained in the library rebuild, above.
subscribe(text.reset_simple_term_mapping,                                   category.Category, "delete")
subscribe(text.reset_simple_term_mapping,                                   category.Category, "save")
'''

# todo: notes? reviews?
# todo: Scheme name change in Index
# todo: term change in nodes

```

### sefaria/model/schema.py

```
# -*- coding: utf-8 -*-
import copy
import dataclasses
from typing import Optional, List

import structlog
from functools import reduce
import re2 as re
from sefaria.system.decorators import conditional_graceful_exception

logger = structlog.get_logger(__name__)

import regex
from . import abstract as abst
from sefaria.system.database import db
from sefaria.model.lexicon import LexiconEntrySet
from sefaria.model.linker.has_match_template import HasMatchTemplates
from sefaria.system.exceptions import InputError, IndexSchemaError, DictionaryEntryNotFoundError, SheetNotFoundError
from sefaria.utils.hebrew import decode_hebrew_numeral, encode_small_hebrew_numeral, encode_hebrew_numeral, encode_hebrew_daf, hebrew_term, sanitize
from sefaria.utils.talmud import daf_to_section

"""
                -----------------------------------------
                 Titles, Terms, and Term Schemes
                -----------------------------------------
"""


class TitleGroup(object):
    """
    A collection of titles.  Used for titles of SchemaNodes and for Terms
    """
    langs = ["en", "he"]

    # Attributes required in each title
    required_attrs = [
        "lang",
        "text"
    ]

    # Attributes optional in each title
    optional_attrs = [
        "primary",
        "presentation",
        "transliteration",  # bool flag to indicate if title is transliteration
        "disambiguation",   # str to help disambiguate this title from other similar titles (often on other objects)
        "fromTerm"          # bool flag to indicate if title originated from term (used in topics)
    ]

    def __init__(self, serial=None):
        self.titles = []
        self._primary_title = {}
        if serial:
            self.load(serial)

    def validate(self):
        for lang in self.langs:
            if not self.primary_title(lang):
                raise InputError("Title Group must have a {} primary title".format(lang))
        if len(self.all_titles()) > len(list(set(self.all_titles()))):
            raise InputError("There are duplicate titles in this object's title group")
        for title in self.titles:
            if not set(title.keys()) == set(self.required_attrs) and not set(title.keys()) <= set(self.required_attrs+self.optional_attrs):
                raise InputError("Title Group titles must only contain the following keys: {}".format(self.required_attrs+self.optional_attrs))
        if '-' in self.primary_title("en"):
            raise InputError("Primary English title may not contain hyphens.")
        if not all(ord(c) < 128 for c in self.primary_title("en")):
            raise InputError("Primary English title may not contain non-ascii characters")

    def load(self, serial=None):
        if serial:
            self.titles = serial

    def copy(self):
        return self.__class__(copy.deepcopy(self.titles))

    def primary_title(self, lang="en"):
        """
        Return the primary title for this node in the language specified
        :param lang: "en" or "he"
        :return: The primary title string or None
        """
        if not self._primary_title.get(lang):
            for t in self.titles:
                if t.get("lang") == lang and t.get("primary"):
                    self._primary_title[lang] = t.get("text")
                    break
        if not self._primary_title.get(lang):
            self._primary_title[lang] = ""

        return self._primary_title.get(lang)

    def get_title_attr(self, title, lang, attr):
        """
        Get attribute `attr` for title `title`.
        For example, get attribute 'transliteration' for a certain title
        :param title: str
        :param lang: en or he
        :param attr: str
        :return: value of attribute `attr`
        """
        for t in self.titles:
            if t.get('lang') == lang and t.get('text') == title:
                return t.get(attr, None)

    def all_titles(self, lang=None):
        """
        :param lang: "en" or "he"
        :return: list of strings - the titles of this node
        """
        if lang is None:
            return [t["text"] for t in self.titles]
        return [t["text"] for t in self.titles if t["lang"] == lang]

    def secondary_titles(self, lang=None):
        if lang is None:
            raise Exception("TitleGroup.secondary_titles() needs a lang")
        return [t for t in self.all_titles(lang) if t != self.primary_title(lang)]

    def remove_title(self, text, lang):
        is_primary = len([t for t in self.titles if (t["lang"] == lang and t["text"] == text and t.get('primary'))])
        if is_primary:
            self._primary_title[lang] = None
        self.titles = [t for t in self.titles if not (t["lang"] == lang and t["text"] == text)]
        return self

    def add_title(self, text, lang, primary=False, replace_primary=False, presentation="combined"):
        """
        :param text: Text of the title
        :param language:  Language code of the title (e.g. "en" or "he")
        :param primary: Is this a primary title?
        :param replace_primary: must be true to replace an existing primary title
        :param presentation: The "presentation" field of a title indicates how it combines with earlier titles. Possible values:
            "combined" - in referencing this node, earlier titles nodes are prepended to this one (default)
            "alone" - this node is reference by this title alone
            "both" - this node is addressable both in a combined and a alone form.
        :return: the object
        """
        if any([t for t in self.titles if t["text"] == text and t["lang"] == lang]):  #already there
            if not replace_primary:
                return
            else:  #update this title as primary: remove it, then re-add below
                self.remove_title(text, lang)
        d = {
                "text": text,
                "lang": lang
        }

        if primary:
            d["primary"] = True

        if presentation == "alone" or presentation == "both":
            d["presentation"] = presentation

        has_primary = any([x for x in self.titles if x["lang"] == lang and x.get("primary")])
        if has_primary and primary:
            if not replace_primary:
                raise IndexSchemaError("Node {} already has a primary title.".format(self.primary_title()))

            old_primary = self.primary_title(lang)
            self.titles = [t for t in self.titles if t["lang"] != lang or not t.get("primary")]
            self.titles.append({"text": old_primary, "lang": lang})
            self._primary_title[lang] = None

        self.titles.append(d)
        return self


class AbstractTitledObject(object):

    def add_primary_titles(self, en_title, he_title):
        self.add_title(en_title, 'en', primary=True)
        self.add_title(he_title, 'he', primary=True)

    def add_title(self, text, lang, primary=False, replace_primary=False):
        """
        :param text: Text of the title
        :param language:  Language code of the title (e.g. "en" or "he")
        :param primary: Is this a primary title?
        :param replace_primary: must be true to replace an existing primary title
        :return: the object
        """
        return self.title_group.add_title(text, lang, primary, replace_primary)

    def remove_title(self, text, lang):
        return self.title_group.remove_title(text, lang)

    def get_titles_object(self):
        return getattr(self.title_group, "titles", None)

    def get_titles(self, lang=None):
        return self.title_group.all_titles(lang)

    def get_primary_title(self, lang='en'):
        return self.title_group.primary_title(lang)

    def has_title(self, title, lang="en"):
        return title in self.get_titles(lang)


class AbstractTitledOrTermedObject(AbstractTitledObject):
    def _init_title_defaults(self):
        # To be called at initialization time
        self.title_group = TitleGroup()

    def _load_title_group(self):
        if getattr(self, "titles", None):
            self.title_group.load(serial=self.titles)
            del self.__dict__["titles"]

        self._process_terms()

    @conditional_graceful_exception()
    def _process_terms(self):
        # To be called after raw data load
        from sefaria.model import library

        if self.sharedTitle:
            term = library.get_term(self.sharedTitle)
            try:
                self.title_group = term.title_group
            except AttributeError:
                raise IndexError("Failed to load term named {}.".format(self.sharedTitle))

    def add_shared_term(self, term):
        self.sharedTitle = term
        self._process_terms()

    def remove_shared_term(self, term):
        if self.sharedTitle == term:
            self.sharedTitle = None
            self.title_group = self.title_group.copy()
            return 1


class Term(abst.AbstractMongoRecord, AbstractTitledObject):
    """
    A Term is a shared title node.  It can be referenced and used by many different Index nodes.
    Examples:  Noah, HaChovel
    Terms that use the same TermScheme can be ordered.
    """
    collection = 'term'
    track_pkeys = True
    pkeys = ["name"]
    title_group = None
    history_noun = "term"

    required_attrs = [
        "name",
        "titles"
    ]
    optional_attrs = [
        "scheme",
        "order",
        "ref",
        "good_to_promote",
        "category",
        "description"
    ]

    def load_by_title(self, title):
        query = {'titles.text': title}
        return self.load(query=query)

    def _set_derived_attributes(self):
        self.set_titles(getattr(self, "titles", None))

    def set_titles(self, titles):
        self.title_group = TitleGroup(titles)

    def _normalize(self):
        self.titles = self.title_group.titles

    def _validate(self):
        super(Term, self)._validate()
        # do not allow duplicates:
        for title in self.get_titles():
            other_term = Term().load_by_title(title)
            if other_term and not self.same_record(other_term):
                raise InputError("A Term with the title {} in it already exists".format(title))
        self.title_group.validate()
        if self.name != self.get_primary_title():
            raise InputError("Term name {} does not match primary title {}".format(self.name, self.get_primary_title()))

    @staticmethod
    def normalize(term, lang="en"):
        """ Returns the primary title for of 'term' if it exists in the terms collection
        otherwise return 'term' unchanged """
        t = Term().load_by_title(term)
        return t.get_primary_title(lang=lang) if t else term


class TermSet(abst.AbstractMongoSet):
    recordClass = Term


class TermScheme(abst.AbstractMongoRecord):
    """
    A TermScheme is a category of terms.
    Example: Parasha, Perek
    """
    collection = 'term_scheme'
    track_pkeys = True
    pkeys = ["name"]

    required_attrs = [
        "name"
    ]
    optional_attrs = [

    ]

    def get_terms(self):
        return TermSet({"scheme": self.name})


class TermSchemeSet(abst.AbstractMongoSet):
    recordClass = TermScheme


class NonUniqueTerm(abst.SluggedAbstractMongoRecord, AbstractTitledObject):
    """
    The successor of the old `Term` class
    Doesn't require titles to be globally unique
    """
    cacheable = True
    collection = "non_unique_terms"
    required_attrs = [
        "slug",
        "titles"
    ]
    slug_fields = ['slug']
    title_group = None

    def _normalize(self):
        super()._normalize()
        self.titles = self.title_group.titles

    def set_titles(self, titles):
        self.title_group = TitleGroup(titles)

    def _set_derived_attributes(self):
        self.set_titles(getattr(self, "titles", None))

    def __repr__(self):
        return f'{self.__class__.__name__}.init("{self.slug}")'

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self.__hash__() == other.__hash__()

    def __hash__(self):
        return hash(self.slug)

    def __ne__(self, other):
        return not self.__eq__(other)


class NonUniqueTermSet(abst.AbstractMongoSet):
    recordClass = NonUniqueTerm


"""
                ---------------------------------
                 Index Schema Trees - Core Nodes
                ---------------------------------
"""


def deserialize_tree(serial=None, **kwargs):
    """
    Build a :class:`TreeNode` tree from serialized form.  Called recursively.
    :param serial: The serialized form of the subtree
    :param kwargs: keyword argument 'struct_class' specifies the class to use as the default structure node class.
    Keyword argument 'leaf_class' specifies the class to use as the default leaf node class.
    keyword argument 'children_attr' specifies the attribute where children are contained. Defaults to "nodes"
        Note the attribute of TreeNode class with the same name and function.
    Other keyword arguments are passed through to the node constructors.
    :return: :class:`TreeNode`
    """
    if kwargs.get("additional_classes"):
        for klass in kwargs.get("additional_classes"):
            globals()[klass.__name__] = klass

    klass = None
    if serial.get("nodeType"):
        try:
            klass = globals()[serial.get("nodeType")]
        except KeyError:
            raise IndexSchemaError("No matching class for nodeType {}".format(serial.get("nodeType")))

    if serial.get(kwargs.get("children_attr", "nodes")) or (kwargs.get("struct_title_attr") and serial.get(kwargs.get("struct_title_attr"))):
        # Structure class - use explicitly defined 'nodeType', code overide 'struct_class', or default SchemaNode
        struct_class = klass or kwargs.get("struct_class", SchemaNode)
        return struct_class(serial, **kwargs)
    elif klass:
        return klass(serial, **kwargs)
    elif kwargs.get("leaf_class"):
        return kwargs.get("leaf_class")(serial, **kwargs)
    else:
        raise IndexSchemaError("Schema node has neither 'nodes' nor 'nodeType' and 'leaf_class' not provided: {}".format(serial))


class TreeNode(object):
    """
    A single node in a tree.
    These trees are hierarchies - each node can have 1 or 0 parents.
    In this class, node relationships, node navigation, and general serialization are handled.
    """
    required_param_keys = []
    optional_param_keys = []
    default_children_attr = "nodes"

    def __init__(self, serial=None, **kwargs):
        self.children_attr = kwargs.get("children_attr", self.default_children_attr)
        self._init_defaults()
        if not serial:
            return
        self.__dict__.update(serial)
        if getattr(self, self.children_attr, None) is not None:
            for node in getattr(self, self.children_attr):
                self.append(deserialize_tree(node, **kwargs))
            delattr(self, self.children_attr)

    def _init_defaults(self):
        self.children = []  # Is this enough?  Do we need a dict for addressing?
        self.parent = None
        self._leaf_nodes = []

    def validate(self):
        for k in self.required_param_keys:
            if getattr(self, k, None) is None:
                raise IndexSchemaError("Missing Parameter '{}' in {}".format(k, self.__class__.__name__))
        for c in self.children:
            c.validate()

    def append(self, node):
        """
        Append node to this node
        :param node: the node to be appended to this node
        :return:
        """
        self.children.append(node)
        node.parent = self
        return self

    def replace(self, node):
        """
        Replace self with `node`
        :param node:
        :return:
        """
        parent = self.parent
        assert parent

        parent.children = [c if c != self else node for c in parent.children]

        node.parent = parent
        self.parent = None

    def detach(self):
        parent = self.parent
        assert parent
        parent.children = [c for c in parent.children if c != self]
        self.parent = None

    def append_to(self, node):
        """
        Append this node to another node
        :param node: the node to append this node to
        :return:
        """
        node.append(self)
        return self

    # todo: replace with a direct call to self.children for speed
    def has_children(self):
        """
        :return bool: True if this node has children
        """
        return bool(self.children)

    # todo: replace with a direct call to `not self.children for speed`
    def is_leaf(self):
        return not self.children

    def siblings(self):
        """
        :return list: The sibling nodes of this node
        """
        if self.parent:
            return [x for x in self.parent.children if x is not self]
        else:
            return None

    def root(self):
        if not self.parent:
            return self
        return self.parent.root()

    def first_child(self):
        if not self.children:
            return None
        return self.children[0]

    def last_child(self):
        if not self.children:
            return None
        return self.children[-1]

    def first_leaf(self):
        if not self.children: # is leaf
            return self
        return self.first_child().first_leaf()

    def last_leaf(self):
        if not self.children: # is leaf
            return self
        return self.last_child().last_leaf()

    def _prev_in_list(self, l):
        if not self.parent:
            return None
        prev = None
        for x in l:
            if x is self:
                return prev
            prev = x

    def _next_in_list(self, l):
        match = False
        for x in l:
            if match:
                return x
            if x is self:
                match = True
                continue
        return None

    def prev_sibling(self):
        if not self.parent:
            return None
        return self._prev_in_list(self.parent.children)

    def next_sibling(self):
        if not self.parent:
            return None
        return self._next_in_list(self.parent.children)

    # Currently assumes being called from leaf node - could integrate a call to first_leaf/last_leaf
    def next_leaf(self):
        return self._next_in_list(self.root().get_leaf_nodes())

    # Currently assumes being called from leaf node - could integrate a call to first_leaf/last_leaf
    def prev_leaf(self):
        return self._prev_in_list(self.root().get_leaf_nodes())

    def ancestors(self):
        if not self.parent:
            return []
        return self.parent.ancestors() + [self.parent]

    def is_ancestor_of(self, other_node):
        return any(self == anc for anc in other_node.ancestors())

    def is_root(self):
        return not self.parent

    def is_flat(self):
        """
        Is this node a flat tree, with no parents or children?
        :return bool:
        """
        return not self.parent and not self.children

    def traverse_tree(self, callback, **kwargs):
        """
        Traverse tree, invoking callback at each node, with kwargs as arguments
        :param callback:
        :param kwargs:
        :return:
        """
        callback(self, **kwargs)
        for child in self.children:
            child.traverse_tree(callback, **kwargs)

    def traverse_to_string(self, callback, depth=0, **kwargs):
        st = callback(self, depth, **kwargs)
        st += "".join([child.traverse_to_string(callback, depth + 1, **kwargs) for child in self.children])
        return st

    def traverse_to_json(self, callback, depth=0, **kwargs):
        js = callback(self, depth, **kwargs)
        if self.children:
            js[getattr(self, "children_attr")] = [child.traverse_to_json(callback, depth + 1, **kwargs) for child in self.children]
        return js

    def traverse_to_list(self, callback, depth=0, **kwargs):
        listy = callback(self, depth, **kwargs)
        if self.children:
            listy += reduce(lambda a, b: a + b, [child.traverse_to_list(callback, depth + 1, **kwargs) for child in self.children], [])
        return listy

    def serialize(self, **kwargs):
        d = {}
        if self.children:
            d[self.children_attr] = [n.serialize(**kwargs) for n in self.children]

        # Only output nodeType and nodeParameters if there is at least one param. This seems like it may not remain a good measure.
        params = {k: getattr(self, k) for k in self.required_param_keys + self.optional_param_keys if getattr(self, k, None) is not None}
        if any(params):
            d["nodeType"] = self.__class__.__name__
            d.update(params)

        return d

    def copy(self, callback=None):
        children_serial = []
        for child in self.children:
            children_serial.append(child.copy(callback).serialize())
        serial = copy.deepcopy(self.serialize())
        if self.children_attr in serial:
            serial[self.children_attr] = children_serial
        new_node = self.__class__(serial)
        if callback:
            new_node = callback(new_node)
        return new_node

    def all_children(self):
        return self.traverse_to_list(lambda n, i: [n])[1:]

    def get_leaf_nodes_to_depth(self, max_depth = None):
        """
        :param max_depth: How many levels below this one to traverse.
        1 returns only this node's children, 0 returns only this node.
        """
        assert max_depth is not None
        leaves = []

        if not self.children:
            return [self]
        elif max_depth > 0:
            for node in self.children:
                if not node.children:
                    leaves += [node]
                else:
                    leaves += node.get_leaf_nodes_to_depth(max_depth=max_depth - 1)
        return leaves

    def get_leaf_nodes(self):
        """
        :return:
        """
        if not self._leaf_nodes:
            if not self.children:
                self._leaf_nodes = [self]
            else:
                for node in self.children:
                    if not node.children:
                        self._leaf_nodes += [node]
                    else:
                        self._leaf_nodes += node.get_leaf_nodes()
        return self._leaf_nodes

    def get_child_order(self, child):
        """
        Intention is to call this on the root node of a schema, in order to get the order of a child node.
        :param child: TreeNode
        :return: Integer
        """
        if child.parent and child.parent.is_virtual:
            return child.parent.get_child_order(child)
        return self.all_children().index(child) + 1


class TitledTreeNode(TreeNode, AbstractTitledOrTermedObject, HasMatchTemplates):
    """
    A tree node that has a collection of titles - as contained in a TitleGroup instance.
    In this class, node titles, terms, 'default', and combined titles are handled.
    """

    after_title_delimiter_re = r"(?:[,.:\s]|(?:to|\u05d5?\u05d1?(?:\u05e1\u05d5\u05e3|\u05e8\u05d9\u05e9)))+"  # should be an arg?  \r\n are for html matches
    after_address_delimiter_ref = r"[,.:\s]+"
    title_separators = [", "]

    def __init__(self, serial=None, **kwargs):
        super(TitledTreeNode, self).__init__(serial, **kwargs)
        self._load_title_group()

    def _init_defaults(self):
        super(TitledTreeNode, self)._init_defaults()
        self.default = False
        self._primary_title = {}
        self._full_title = {}
        self._full_titles = {}

        self._init_title_defaults()
        self.sharedTitle = None

    def all_tree_titles(self, lang="en"):
        """
        :param lang: "en" or "he"
        :return: list of strings - all possible titles within this subtree
        """
        return list(self.title_dict(lang).keys())

    def title_dict(self, lang="en", baselist=None):
        """
        Recursive function that generates a map from title to node
        :param node: the node to start from
        :param lang: "en" or "he"
        :param baselist: list of starting strings that lead to this node
        :return: map from title to node
        """
        if baselist is None:
            baselist = []

        title_dict = {}
        thisnode = self

        this_node_titles = [title["text"] for title in self.get_titles_object() if title["lang"] == lang and title.get("presentation") != "alone"]
        if (not len(this_node_titles)) and (not self.is_default()):
            error = 'No "{}" title found for schema node: "{}"'.format(lang, self.key)
            error += ', child of "{}"'.format(self.parent.full_title("en")) if self.parent else ""
            raise IndexSchemaError(error)
        if baselist:
            if self.is_default():
                node_title_list = baselist  # doesn't add any titles of its own
            else:
                node_title_list = [baseName + sep + title for baseName in baselist for sep in self.title_separators for title in this_node_titles]
        else:
            node_title_list = this_node_titles

        alone_node_titles = [title["text"] for title in self.get_titles_object() if title["lang"] == lang and title.get("presentation") == "alone" or title.get("presentation") == "both"]
        node_title_list += alone_node_titles

        for child in self.children:
            if child.default:
                thisnode = child
            title_dict.update(child.title_dict(lang, node_title_list))

        for title in node_title_list:
            title_dict[title] = thisnode

        return title_dict

    def full_titles(self, lang="en"):
        if not self._full_titles.get(lang):
            if self.parent:
                self._full_titles[lang] = [parent + sep + local
                                           for parent in self.parent.full_titles(lang)
                                           for sep in self.title_separators
                                           for local in self.all_node_titles(lang)]
            else:
                self._full_titles[lang] = self.all_node_titles(lang)
        return self._full_titles[lang]

    def full_title(self, lang="en", force_update=False):
        """
        :param lang: "en" or "he"
        :return string: The full title of this node, from the root node.
        """
        if not self._full_title.get(lang) or force_update:
            if self.is_default():
                self._full_title[lang] = self.parent.full_title(lang, force_update)
            elif self.parent:
                self._full_title[lang] = self.parent.full_title(lang, force_update) + ", " + self.primary_title(lang)
            else:
                self._full_title[lang] = self.primary_title(lang)
        return self._full_title[lang]

    # todo: replace with a direct call to self.default for speed
    def is_default(self):
        """
        Is this node a default node, meaning, do references to its parent cascade to this node?
        :return bool:
        """
        return self.default

    def has_default_child(self):
        return any([c for c in self.children if c.is_default()])

    def get_default_child(self):
        for child in self.children:
            if child.is_default():
                return child
        return None

    def get_child_by_key(self, key):
        for child in self.children:
            if hasattr(child, 'key') and child.key == key:
                return child

    def has_titled_continuation(self):
        """
        :return: True if any normal forms of this node continue with a title.  Used in regex building.
        """
        return any([c for c in self.children if not c.is_default()])

    def has_numeric_continuation(self):
        """
        True if any of the normal forms of this node continue with numbers.  Used in regex building.
        Overridden in subclasses.
        :return:
        """
        # overidden in subclasses
        for child in self.children:
            if child.is_default():
                if child.has_numeric_continuation():
                    return True
        return False

    def primary_title(self, lang="en"):
        # Retained for backwards compatability.  Could be factored out.
        """
        Return the primary title for this node in the language specified
        :param lang: "en" or "he"
        :return: The primary title string or None
        """
        return self.get_primary_title(lang)

    def all_node_titles(self, lang="en"):
        # Retained for backwards compatability.  Could be factored out.
        """
        :param lang: "en" or "he"
        :return: list of strings - the titles of this node
        """
        return self.get_titles(lang)

    def add_title(self, text, lang, primary=False, replace_primary=False, presentation="combined"):
        """
        :param text: Text of the title
        :param lang:  Language code of the title (e.g. "en" or "he")
        :param primary: Is this a primary title?
        :param replace_primary: must be true to replace an existing primary title
        :param presentation: The "presentation" field of a title indicates how it combines with earlier titles. Possible values:
            "combined" - in referencing this node, earlier titles nodes are prepended to this one (default)
            "alone" - this node is reference by this title alone
            "both" - this node is addressable both in a combined and a alone form.
        :return: the object
        """
        return self.title_group.add_title(text, lang, primary, replace_primary, presentation)

    def validate(self):
        super(TitledTreeNode, self).validate()

        if not self.default and not self.sharedTitle and not self.get_titles_object():
            raise IndexSchemaError("Schema node {} must have titles, a shared title node, or be default".format(self))

        if self.default and (self.get_titles_object() or self.sharedTitle):
            raise IndexSchemaError("Schema node {} - default nodes can not have titles".format(self))

        if not self.default:
            try:
                self.title_group.validate()
            except InputError as e:
                raise IndexSchemaError("Schema node {} has invalid titles: {}".format(self, e))

        if self.children and len([c for c in self.children if c.default]) > 1:
            raise IndexSchemaError("Schema Structure Node {} has more than one default child.".format(self.key))

        if self.sharedTitle and Term().load({"name": self.sharedTitle}).titles != self.get_titles_object():
            raise IndexSchemaError("Schema node {} with sharedTitle can not have explicit titles".format(self))

        # disable this check while data is still not conforming to validation
        if not self.sharedTitle and False:
            special_book_cases = ["Genesis", "Exodus", "Leviticus", "Numbers", "Deuteronomy", "Judges"]
            for title in self.title_group.titles:
                title = title["text"]
                if self.get_primary_title() in special_book_cases:
                    break
                term = Term().load_by_title(title)
                if term:
                    if "scheme" in list(vars(term).keys()):
                        if vars(term)["scheme"] == "Parasha":
                            raise InputError(
                                "Nodes that represent Parashot must contain the corresponding sharedTitles.")

        # if not self.default and not self.primary_title("he"):
        #    raise IndexSchemaError("Schema node {} missing primary Hebrew title".format(self.key))

    def serialize(self, **kwargs):
        d = super(TitledTreeNode, self).serialize(**kwargs)
        if self.default:
            d["default"] = True
        else:
            if self.sharedTitle:
                d["sharedTitle"] = self.sharedTitle
            if not self.sharedTitle or kwargs.get("expand_shared"):
                d["titles"] = self.get_titles_object()
        if kwargs.get("expand_titles"):
            d["title"] = self.title_group.primary_title("en")
            d["heTitle"] = self.title_group.primary_title("he")
        return d

    def get_referenceable_alone_nodes(self):
        """
        Currently almost exact copy of function with same name in Index
        See docstring there
        @return:
        """
        alone_nodes = []
        for child in self.children:
            if child.has_scope_alone_match_template():
                alone_nodes += [child]
            alone_nodes += child.get_referenceable_alone_nodes()
        return alone_nodes

    """ String Representations """
    def __str__(self):
        return self.full_title("en")

    def __repr__(self):
        # Wanted to use orig_tref, but repr can not include Unicode
        # add `repr` around `full_title()` in case there's unicode in the output
        return self.__class__.__name__ + "(" + repr(self.full_title("en")) + ")"


"""
                --------------------------------
                 Alternate Structure Tree Nodes
                --------------------------------
"""


class NumberedTitledTreeNode(TitledTreeNode):
    """
    A :class:`TreeNode` that can address its :class:`TreeNode` children by Integer, or other :class:`AddressType`.
    """
    required_param_keys = ["depth", "addressTypes", "sectionNames"]
    optional_param_keys = ["lengths"]

    def __init__(self, serial=None, **kwargs):
        """
        depth: Integer depth of this JaggedArray
        address_types: A list of length (depth), with string values indicating class names for address types for each level
        section_names: A list of length (depth), with string values of section names for each level
        e.g.:
        {
          "depth": 2,
          "addressTypes": ["Integer","Integer"],
          "sectionNames": ["Chapter","Verse"],
          "lengths": [12, 122]
        }
        """
        super(NumberedTitledTreeNode, self).__init__(serial, **kwargs)

        # Anything else in this __init__ needs to be reflected in JaggedArrayNode.__init__
        self._regexes = {}
        self._init_address_classes()

    def _init_address_classes(self):
        self._addressTypes = []
        for i, atype in enumerate(getattr(self, "addressTypes", [])):
            try:
                klass = globals()["Address" + atype]
            except KeyError:
                raise IndexSchemaError("No matching class for addressType {}".format(atype))

            if i == 0 and getattr(self, "lengths", None) and len(self.lengths) > 0:
                self._addressTypes.append(klass(i, self.lengths[i]))
            else:
                self._addressTypes.append(klass(i))

    def validate(self):
        super(NumberedTitledTreeNode, self).validate()
        for p in ["addressTypes", "sectionNames"]:
            if len(getattr(self, p)) != self.depth:
                raise IndexSchemaError("Parameter {} in {} {} does not have depth {}".format(p, self.__class__.__name__, self.key, self.depth))

        for sec in getattr(self, 'sectionNames', []):
            if any((c in '.-\\/') for c in sec):
                raise InputError("Text Structure names may not contain periods, hyphens or slashes.")

    def address_class(self, depth):
        return self._addressTypes[depth]

    def full_regex(self, title, lang, anchored=True, compiled=True, capture_title=False, escape_titles=True, **kwargs):
        """
        :return: Regex object. If kwargs[for_js] == True, returns the Regex string
        :param for_js: Defaults to False
        :param match_range: Defaults to False
        :param strict: Only match string where all address components match
        :param terminated: Only match string that contains just a valid ref

        A call to `full_regex("Bereishit", "en", for_js=True)` returns the follow regex, expanded here for clarity :
        ```
        Bereishit                       # title
        [,.: \r\n]+                     # a separator (self.after_title_delimiter_re)
        (?:                             # Either:
            (?:                         # 1)
                (\d+)                   # Digits
                (                       # and maybe
                    [,.: \r\n]+         # a separator
                    (\d+)               # and more digits
                )?
            )
            |                           # Or:
            (?:                         # 2: The same
                [[({]                   # With beginning
                (\d+)
                (
                    [,.: \r\n]+
                    (\d+)
                )?
                [])}]                   # and ending brackets or parens or braces around the numeric portion
            )
        )
        (?=                             # and then either
            [.,;?! })<]                 # some kind of delimiting character coming after
            |                           # or
            $                           # the end of the string
        )
        ```
        Different address type / language combinations produce different internal regexes in the innermost portions of the above, where the comments say 'digits'.

        """
        parentheses = kwargs.get("parentheses", False)
        prefixes = '||||||||||||||||||||||||||||||' if lang == 'he' else ''
        prefix_group = rf'(?:{prefixes})?'
        key = (title, lang, anchored, compiled, kwargs.get("for_js"), kwargs.get("match_range"), kwargs.get("strict"), kwargs.get("terminated"), kwargs.get("escape_titles"), parentheses)
        if not self._regexes.get(key):
            if anchored:
                reg = r"^"
            elif parentheses:
                parens_lookbehind = r'(?<=[(\[](?:[^)\]]*?\s)?'
                if kwargs.get("for_js"):
                    reg = rf"{parens_lookbehind}){prefix_group}"  # prefix group should be outside lookbehind in js to be consistent with when parentheses == False
                else:
                    reg = rf"{parens_lookbehind}{prefix_group})"
            else:
                word_break_group = r'(?:^|\s|\(|\[|-|/)' # r'(?:^|\s|\(|\[)'
                if kwargs.get("for_js"):
                    reg = rf"{word_break_group}{prefix_group}"  # safari still does not support lookbehinds (although this issue shows they're working on it https://bugs.webkit.org/show_bug.cgi?id=174931)
                else:
                    reg = rf"(?<={word_break_group}{prefix_group})"
            title_block = regex.escape(title) if escape_titles else title
            if kwargs.get("for_js"):
                reg += r"("  # use capture group to distinguish prefixes from captured ref
            reg += r"(?P<title>" + title_block + r")" if capture_title else title_block
            reg += self.after_title_delimiter_re
            addr_regex = self.address_regex(lang, **kwargs)
            reg += r'(?:(?:' + addr_regex + r')|(?:[\[({]' + addr_regex + r'[\])}]))'  # Match expressions with internal parentheses around the address portion
            if kwargs.get("for_js"):
                reg += r")"  # use capture group to distinguish prefixes from captured ref
            if parentheses:
                reg += r"(?=(?:[)\]])|(?:[.,:;?!\s<][^\])]*?[)\]]))"
            else:
                reg += r"(?=[-/.,:;?!\s})\]<]|$)" if kwargs.get("for_js") else r"(?=\W|$)" if not kwargs.get(
                    "terminated") else r"$"
            self._regexes[key] = regex.compile(reg, regex.VERBOSE) if compiled else reg
        return self._regexes[key]

    def address_regex(self, lang, **kwargs):
        group = "a0"
        reg = self._addressTypes[0].regex(lang, group, **kwargs)
        for i in range(1, self.depth):
            group = "a{}".format(i)
            reg += "(" + self.after_address_delimiter_ref + self._addressTypes[i].regex(lang, group, **kwargs) + ")"
            if not kwargs.get("strict", False):
                reg += "?"

        if kwargs.get("match_range"):
            # TODO there is a potential error with this regex. it fills in toSections starting from highest depth and going to lowest.
            # TODO Really, the depths should be filled in the opposite order, but it's difficult to write a regex to match.
            # TODO However, most false positives will be filtered out in library._get_ref_from_match()

            reg += r"(?:\s*([-\u2010-\u2015\u05be]|to)\s*"  # maybe there's a dash (either n or m dash) and a range
            reg += r"(?=\S)"  # must be followed by something (Lookahead)
            group = "ar0"
            reg += self._addressTypes[0].regex(lang, group, **kwargs)
            reg += "?"
            for i in range(1, self.depth):
                reg += r"(?:(?:" + self.after_address_delimiter_ref + r")?"
                group = "ar{}".format(i)
                reg += "(" + self._addressTypes[i].regex(lang, group, **kwargs) + ")"
                # assuming strict isn't relevant on ranges  # if not kwargs.get("strict", False):
                reg += ")?"
            reg += r")?"  # end range clause
        return reg

    def sectionString(self, sections, lang="en", title=True, full_title=False):
        assert len(sections) <= self.depth

        ret = ""
        if title:
            ret += self.full_title(lang) if full_title else self.primary_title(lang)
            ret += " "
        strs = []
        for i in range(len(sections)):
            strs.append(self.address_class(i).toStr(lang, sections[i]))
        ret += ":".join(strs)

        return ret

    def add_structure(self, section_names, address_types=None):
        self.depth = len(section_names)
        self.sectionNames = section_names
        if address_types is None:
            self.addressTypes = [sec if globals().get("Address{}".format(sec), None) else 'Integer' for sec in section_names]
        else:
            self.addressTypes = address_types

    def serialize(self, **kwargs):
        d = super(NumberedTitledTreeNode, self).serialize(**kwargs)
        if kwargs.get("translate_sections"):
                d["heSectionNames"] = list(map(hebrew_term, self.sectionNames))
        return d

    def is_segment_level_dibur_hamatchil(self) -> bool:
        return getattr(self, 'isSegmentLevelDiburHamatchil', False)


class AltStructNode(TitledTreeNode):
    """
    Structural node for alt structs
    Allows additional attributes for referencing these nodes with the linker
    Note, these nodes can't be the end of a reference since they themselves don't map to a `Ref`. But they are helpful
    being intermediate nodes in a longer reference.
    """
    optional_param_keys = ["match_templates", "numeric_equivalent", 'referenceable']

    def ref(self):
        return None


class ArrayMapNode(NumberedTitledTreeNode):
    """
    A :class:`TreeNode` that contains jagged arrays of references.
    Used as the leaf node of alternate structures of Index records.
    (e.g., Parsha structures of chapter/verse stored Tanach, or Perek structures of Daf/Line stored Talmud)
    """
    required_param_keys = ["depth", "wholeRef"]
    optional_param_keys = ["lengths", "addressTypes", "sectionNames", "refs", "includeSections", "startingAddress", "match_templates", "numeric_equivalent", "referenceableSections", "isSegmentLevelDiburHamatchil", "diburHamatchilRegexes", 'referenceable', "addresses", "skipped_addresses", "isMapReferenceable"]  # "addressTypes", "sectionNames", "refs" are not required for depth 0, but are required for depth 1 +
    has_key = False  # This is not used as schema for content

    def get_ref_from_sections(self, sections):
        if not sections:
            return self.wholeRef
        return reduce(lambda a, i: a[i], [s - 1 for s in sections], self.refs)

    def serialize(self, **kwargs):
        d = super(ArrayMapNode, self).serialize(**kwargs)
        if kwargs.get("expand_refs"):
            if getattr(self, "includeSections", False):
                # We assume that with "includeSections", we're going from depth 0 to depth 1, and expanding "wholeRef"
                from . import text

                refs         = text.Ref(self.wholeRef).split_spanning_ref()
                first, last  = refs[0], refs[-1]
                offset       = first.sections[-2] - 1 if first.is_segment_level() else first.sections[-1] - 1

                d["refs"] = [r.normal() for r in refs]
                d["addressTypes"] = first.index_node.addressTypes[-2:-1]
                d["sectionNames"] = first.index_node.sectionNames[-2:-1]
                d["depth"] += 1
                d["offset"] = offset
            elif getattr(self, "startingAddress", False):
                d["offset"] = self.address_class(0).toIndex("en", self.startingAddress)
            if (kwargs.get("include_previews", False)):
                d["wholeRefPreview"] = self.expand_ref(self.wholeRef, kwargs.get("he_text_ja"), kwargs.get("en_text_ja"))
                if d.get("refs"):
                    d["refsPreview"] = []
                    for r in d["refs"]:
                        d["refsPreview"].append(self.expand_ref(r, kwargs.get("he_text_ja"), kwargs.get("en_text_ja")))
                else:
                    d["refsPreview"] = None
        return d

    # Move this over to Ref and cache it?
    def expand_ref(self, tref, he_text_ja = None, en_text_ja = None):
        from . import text
        from sefaria.utils.util import text_preview

        oref = text.Ref(tref)
        if oref.is_spanning():
            oref = oref.first_spanned_ref()
        if he_text_ja is None and en_text_ja is None:
            t = text.TextFamily(oref, context=0, pad=False, commentary=False)
            preview = text_preview(t.text, t.he) if (t.text or t.he) else []
        else:
            preview = text_preview(en_text_ja.subarray_with_ref(oref).array(), he_text_ja.subarray_with_ref(oref).array())

        return preview

    def validate(self):
        if getattr(self, "depth", None) is None:
            raise IndexSchemaError("Missing Parameter 'depth' in {}".format(self.__class__.__name__))
        if self.depth == 0:
            TitledTreeNode.validate(self)  # Skip over NumberedTitledTreeNode validation, which requires fields we don't have
        elif self.depth > 0:
            for k in ["addressTypes", "sectionNames", "refs"]:
                if getattr(self, k, None) is None:
                    raise IndexSchemaError("Missing Parameter '{}' in {}".format(k, self.__class__.__name__))
            super(ArrayMapNode, self).validate()

    def ref(self):
        from . import text
        return text.Ref(self.wholeRef)


"""
                -------------------------
                 Index Schema Tree Nodes
                -------------------------
"""


class SchemaNode(TitledTreeNode):
    """
    A node in an Index Schema tree.
    Schema nodes form trees which define a storage format.
    At this level, keys, storage addresses, and recursive content constructors are defined.
    Conceptually, there are two types of Schema node:
    - Schema Structure Nodes define nodes which have child nodes, and do not store content.
    - Schema Content Nodes define nodes which store content, and do not have child nodes
    The two are both handled by this class, with calls to "if self.children" to distinguishing behavior.

    """
    is_virtual = False
    optional_param_keys = ["match_templates", "numeric_equivalent", "ref_resolver_context_swaps", 'referenceable']

    def __init__(self, serial=None, **kwargs):
        """
        Construct a SchemaNode
        :param serial: The serialized form of this subtree
        :param kwargs: "index": The Index object that this tree is rooted in.
        :return:
        """
        super(SchemaNode, self).__init__(serial, **kwargs)
        self.index = kwargs.get("index", None)

    def _init_defaults(self):
        super(SchemaNode, self)._init_defaults()
        self.key = None
        self.checkFirst = None
        self._address = []

    def validate(self):
        super(SchemaNode, self).validate()

        if not all(ord(c) < 128 for c in self.title_group.primary_title("en")):
            raise InputError("Primary English title may not contain non-ascii characters")

        if not getattr(self, "key", None):
            raise IndexSchemaError("Schema node missing key")

        if "." in self.key:  # Mongo doesn't like . in keys
            raise IndexSchemaError("'.' is not allowed in key names.")

        if self.default and self.key != "default":
            raise IndexSchemaError("'default' nodes need to have key name 'default'")

    def concrete_children(self):
        return [c for c in self.children if not c.is_virtual]

    def create_content(self, callback=None, *args, **kwargs):
        """
        Tree visitor for building content trees based on this Index tree - used for counts and versions
        Callback is called for content nodes only.
        :param callback:
        :return:
        """
        if self.concrete_children():
            return {node.key: node.create_content(callback, *args, **kwargs) for node in self.concrete_children()}
        else:
            if not callback:
                return None
            return callback(self, *args, **kwargs)

    def create_skeleton(self):
        return self.create_content(lambda n: [])

    def add_primary_titles(self, en_title, he_title, key_as_title=True):
        self.add_title(en_title, 'en', primary=True)
        self.add_title(he_title, 'he', primary=True)
        if key_as_title:
            self.key = en_title

    def visit_content(self, callback, *contents, **kwargs):
        """
        Tree visitor for traversing content nodes of existing content trees based on this Index tree and passing them to callback.
        Outputs a content tree.
        Callback is called for content nodes only.
        :param contents: one tree or many
        :param callback:
        :return:
        """
        if self.children:
            dict = {}
            for node in self.concrete_children():
                # todo: abstract out or put in helper the below reduce
                c = [tree[node.key] for tree in contents]
                dict[node.key] = node.visit_content(callback, *c, **kwargs)
            return dict
        else:
            return self.create_content(callback, *contents, **kwargs)

    def visit_structure(self, callback, content, **kwargs):
        """
        Tree visitor for traversing existing structure nodes of content trees based on this Index and passing them to callback.
        Traverses from bottom up, with intention that this be used to aggregate content from content nodes up.
        Modifies contents in place.
        :param callback:
        :param args:
        :param kwargs:
        :return:
        """
        if self.concrete_children():
            for node in self.concrete_children():
                node.visit_structure(callback, content)
            callback(self, content.content_node(self), **kwargs)

    def as_index_contents(self):
        res = self.index.contents(raw=True)
        res["title"]   = self.full_title("en")
        res["heTitle"] = self.full_title("he")
        res['schema']  = self.serialize(expand_shared=True, expand_titles=True, translate_sections=True)
        res["titleVariants"] = self.full_titles("en")
        if self.all_node_titles("he"):
            res["heTitleVariants"] = self.full_titles("he")
        if self.index.has_alt_structures():
            res['alts'] = {}
            if not self.children: # preload text and pass it down to the preview generation
                from . import text
                he_text_ja = text.TextChunk(self.ref(), "he").ja()
                en_text_ja = text.TextChunk(self.ref(), "en").ja()
            else:
                he_text_ja = en_text_ja = None
            for key, struct in self.index.get_alt_structures().items():
                res['alts'][key] = struct.serialize(expand_shared=True, expand_refs=True, he_text_ja=he_text_ja, en_text_ja=en_text_ja, expand_titles=True)
            del res['alt_structs']
        return res

    def serialize(self, **kwargs):
        """
        :param callback: function applied to dictionary before it's returned.  Invoked on concrete nodes, not the abstract level.
        :return string: serialization of the subtree rooted in this node
        """
        d = super(SchemaNode, self).serialize(**kwargs)
        d["key"] = self.key
        if getattr(self, "checkFirst", None) is not None:
            d["checkFirst"] = self.checkFirst
        return d

    # http://stackoverflow.com/a/14692747/213042
    # http://stackoverflow.com/a/16300379/213042
    def address(self):
        """
        Returns a list of keys to uniquely identify and to access this node.
        :return list:
        """
        if not self._address:
            if self.parent:
                self._address = self.parent.address() + [self.key]
            else:
                self._address = [self.key]

        return self._address

    def version_address(self):
        """
        In a version storage context, the first key is not used.  Traversal starts from position 1.
        :return:
        """
        return self.address()[1:]

    def ref(self, force_update=False):
        from . import text
        d = {
            "index": self.index,
            "book": self.full_title("en", force_update=force_update),
            "primary_category": self.index.get_primary_category(),
            "index_node": self,
            "sections": [],
            "toSections": []
        }
        return text.Ref(_obj=d)

    def first_section_ref(self):
        if self.children:
            return self.ref()
        return self.ref().padded_ref()

    def last_section_ref(self):
        if self.children:
            return self.ref()

        from . import version_state
        from . import text

        sn = version_state.StateNode(snode=self)
        sections = [i + 1 for i in sn.ja("all").last_index(self.depth - 1)]

        d = self.ref()._core_dict()
        d["sections"] = sections
        d["toSections"] = sections
        return text.Ref(_obj=d)

    def find_string(self, regex_str, cleaner=lambda x: x, strict=True, lang='he', vtitle=None):
        """
        See TextChunk.text_index_map
        :param regex_str:
        :param cleaner:
        :param strict:
        :param lang:
        :param vtitle:
        :return:
        """
        def traverse(node):
            matches = []
            if node.children:
                for child in node.children:
                    temp_matches = traverse(child)
                    matches += temp_matches
            else:
                return node.ref().text(lang=lang, vtitle=vtitle).find_string(regex_str, cleaner=cleaner, strict=strict)

        return traverse(self)

    def text_index_map(self, tokenizer=lambda x: re.split(r'\s+',x), strict=True, lang='he', vtitle=None):
        """
        See TextChunk.text_index_map
        :param tokenizer:
        :param strict:
        :param lang:
        :return:
        """
        def traverse(node, callback, offset=0):
            index_list, ref_list, temp_offset = callback(node)
            if node.children:
                for child in node.children:
                    temp_index_list, temp_ref_list, temp_offset = traverse(child, callback, offset)
                    index_list += temp_index_list
                    ref_list += temp_ref_list
                    offset = temp_offset
            else:
                index_list = [i + offset for i in index_list]
                offset += temp_offset
            return index_list, ref_list, offset

        def callback(node):
            if not node.children:
                index_list, ref_list, total_len = node.ref().text(lang=lang, vtitle=vtitle).text_index_map(tokenizer,strict=strict)
                return index_list, ref_list, total_len
            else:
                return [],[], 0

        index_list, ref_list, _ = traverse(self, callback)
        return index_list, ref_list

    def nodes_missing_content(self):
        """
        Used to identify nodes in the tree that have no content
        :return: (bool-> True if node is missing content, list)
        The list is a list of nodes that represent the root of an "empty" tree. If a SchemaNode has three children where
        all three are missing content, only the parent SchemaNode will be in the list.
        """
        if self.is_leaf():
            if self.ref().text('en').is_empty() and self.ref().text('he').is_empty():
                return True, [self]
            else:
                return False, []

        children_results = [child.nodes_missing_content() for child in self.children]

        # If all my children are empty nodes, I am an empty node. Since I am the root of an empty tree, I add myself
        # to the list of empty nodes instead of my children
        if all([result[0] for result in children_results]):
            return True, [self]
        else:
            return False, reduce(lambda x, y: x+y, [result[1] for result in children_results])

    def all_children(self):
        return self.traverse_to_list(lambda n, i: list(n.all_children()) if n.is_virtual else [n])[1:]

    def __eq__(self, other):
        try:
            return self.address() == other.address()
        except AttributeError:
            # in case `other` isn't a SchemaNode
            return False

    def __ne__(self, other):
        return not self.__eq__(other)


class JaggedArrayNode(SchemaNode, NumberedTitledTreeNode):
    """
    A :class:`SchemaNode` that defines JaggedArray content and can be addressed by :class:`AddressType`
    Used both for:
    - Structure Nodes whose children can be addressed by Integer or other :class:`AddressType`
    - Content Nodes that define the schema for JaggedArray stored content
    """
    optional_param_keys = SchemaNode.optional_param_keys + ["lengths", "toc_zoom", "referenceableSections", "isSegmentLevelDiburHamatchil", "diburHamatchilRegexes", 'index_offsets_by_depth']

    def __init__(self, serial=None, **kwargs):
        # call SchemaContentNode.__init__, then the additional parts from NumberedTitledTreeNode.__init__
        SchemaNode.__init__(self, serial, **kwargs)

        # Below are the elements of NumberedTitledTreeNode that go beyond SchemaNode init.
        self._regexes = {}
        self._init_address_classes()

    def validate(self):
        # this is minorly repetitious, at the top tip of the diamond inheritance.
        SchemaNode.validate(self)
        NumberedTitledTreeNode.validate(self)
        self.check_index_offsets_by_depth()

    def check_index_offsets_by_depth(self):
        if hasattr(self, 'index_offsets_by_depth'):
            assert all(int(num) <= self.depth for num in self.index_offsets_by_depth)
            def check_offsets(to_check, depth=0):
                if depth == 0:
                    assert isinstance(to_check, int)
                else:
                    for array in to_check:
                        check_offsets(array, depth-1)
            for k, v in self.index_offsets_by_depth.items():
                check_offsets(v, int(k)-1)

    def has_numeric_continuation(self):
        return True

    def as_index_contents(self):
        res = super(JaggedArrayNode, self).as_index_contents()
        res["sectionNames"] = self.sectionNames
        res["depth"] = self.depth
        return res

    @staticmethod
    def get_index_offset(section_indexes, index_offsets_by_depth):
        current_depth = len(section_indexes) + 1
        if not index_offsets_by_depth or str(current_depth) not in index_offsets_by_depth:
            return 0
        return reduce(lambda x, y: x[y], section_indexes, index_offsets_by_depth[str(current_depth)])

    def trim_index_offsets_by_sections(self, sections, toSections, depths=None):
        """
        Trims `self.index_offsets_by_depth` according to `sections` and `toSections`
        @param sections:
        @param toSections:
        @param depths:
        @return:
        """
        index_offsets_by_depth = copy.deepcopy(getattr(self, 'index_offsets_by_depth', {}))
        if index_offsets_by_depth and sections:
            if not depths:
                depths = sorted([int(x) for x in index_offsets_by_depth.keys()])
            for depth in depths:
                if depth == 1:
                    continue
                if len(sections) > depth - 2:
                    for d in range(depth, max(depths)+1):
                        last = reduce(lambda x, _: x[-1], range(depth-2), index_offsets_by_depth[str(d)])
                        del last[toSections[depth-2]:]
                        first = reduce(lambda x, _: x[0], range(depth-2), index_offsets_by_depth[str(d)])
                        del first[:sections[depth-2]-1]
        return index_offsets_by_depth


class StringNode(JaggedArrayNode):
    """
    A :class:`JaggedArrayNode` with depth 0 - effectively defining a string.
    """
    def __init__(self, serial=None, **kwargs):
        super(StringNode, self).__init__(serial, **kwargs)
        self.depth = 0
        self.addressTypes = []
        self.sectionNames = []

    def serialize(self, **kwargs):
        d = super(StringNode, self).serialize(**kwargs)
        d["nodeType"] = "JaggedArrayNode"
        return d

"""
                -------------------------------------
                 Index Schema Tree Nodes - Virtual
                -------------------------------------
"""


class VirtualNode(TitledTreeNode):

    is_virtual = True    # False on SchemaNode
    entry_class = None

    def __init__(self, serial=None, **kwargs):
        """
        Abstract superclass for SchemaNodes that are not backed by Versions.
        :param serial:
        :param kwargs:
        """
        super(VirtualNode, self).__init__(serial, **kwargs)
        self.index = kwargs.get("index", None)

    def _init_defaults(self):
        super(VirtualNode, self)._init_defaults()
        self.index = None

    def address(self):
        return self.parent.address()

    def create_dynamic_node(self, title, tref):
        return self.entry_class(self, title, tref)

    def first_child(self):
        pass

    def last_child(self):
        pass

    def supports_language(self, lang):
        raise Exception("supports_language needs to be overriden by subclasses")

class DictionaryEntryNode(TitledTreeNode):
    is_virtual = True

    def __init__(self, parent, title=None, tref=None, word=None, lexicon_entry=None):
        """
        A schema node created on the fly, in memory, to correspond to a dictionary entry.
        Created by a DictionaryNode object.
        Can be instantiated with title+tref or word
        :param parent:
        :param title:
        :param tref:
        :param word:
        :param lexicon_entry: LexiconEntry. if you pass this param and don't pass title, tref or word, then this will bootstrap the DictionaryEntryNode and avoid an extra mongo call
        """
        if title and tref:
            self.title = title
            self._ref_regex = regex.compile("^" + regex.escape(title) + r"[, _]*(\S[^0-9.]*)(?:[. ](\d+))?$")
            self._match = self._ref_regex.match(tref)
            self.word = self._match.group(1) or ""
        elif word:
            self.word = word
        elif lexicon_entry:
            self.lexicon_entry = lexicon_entry
            self.has_word_match = bool(self.lexicon_entry)
            self.word = self.lexicon_entry.headword

        super(DictionaryEntryNode, self).__init__({
            "titles": [{
                "lang": "he",
                "text": self.word,
                "primary": True
            },
                {
                "lang": "en",
                "text": self.word,
                "primary": True
            }]
        })

        self.parent = parent
        self.index = self.parent.index
        self.sectionNames = ["Line"]    # Hacky hack
        self.depth = 1
        self.addressTypes = ["Integer"]
        self._addressTypes = [AddressInteger(0)]

        if self.word:
            self.lexicon_entry = self.parent.dictionaryClass().load({"parent_lexicon": self.parent.lexiconName, "headword": self.word})
            self.has_word_match = bool(self.lexicon_entry)

        if not self.word or not self.has_word_match:
            raise DictionaryEntryNotFoundError("Word not found in {}".format(self.parent.full_title()), self.parent.lexiconName, self.parent.full_title(), self.word)

    def __eq__(self, other):
        return self.address() == other.address()

    def __ne__(self, other):
        return not self.__eq__(other)

    def has_numeric_continuation(self):
        return True

    def has_titled_continuation(self):
        return False

    def get_sections(self):
        s = self._match.group(2)
        return [int(s)] if s else []

    def address_class(self, depth):
        return self._addressTypes[depth]

    def get_index_title(self):
        return self.parent.lexicon.index_title

    def get_version_title(self, lang):
        return self.parent.lexicon.version_title

    def get_text(self):
        if not self.has_word_match:
            return ["No Entry for {}".format(self.word)]

        return self.lexicon_entry.as_strings()

    def address(self):
        return self.parent.address() + [self.word]

    def prev_sibling(self):
        return self.prev_leaf()

    def next_sibling(self):
        return self.next_leaf()

    # Currently assumes being called from leaf node
    def next_leaf(self):
        if not self.has_word_match:
            return None
        try:
            return self.__class__(parent=self.parent, word=self.lexicon_entry.next_hw)
        except AttributeError:
            return None

    # Currently assumes being called from leaf node
    def prev_leaf(self):
        if not self.has_word_match:
            return None
        try:
            return self.__class__(parent=self.parent, word=self.lexicon_entry.prev_hw)
        except AttributeError:
            return None

    # This is identical to SchemaNode.ref().  Inherit?
    def ref(self):
        from . import text
        d = {
            "index": self.index,
            "book": self.full_title("en"),
            "primary_category": self.index.get_primary_category(),
            "index_node": self,
            "sections": [],
            "toSections": []
        }
        return text.Ref(_obj=d)


class DictionaryNode(VirtualNode):
    """
    A schema node corresponding to the entirety of a dictionary.
    The parent of DictionaryEntryNode objects, which represent individual entries
    """
    required_param_keys = ["lexiconName", "firstWord", "lastWord"]
    optional_param_keys = ["headwordMap"]
    entry_class = DictionaryEntryNode

    def __init__(self, serial=None, **kwargs):
        """
        Construct a SchemaNode
        :param serial: The serialized form of this subtree
        :param kwargs: "index": The Index object that this tree is rooted in.
        :return:
        """
        super(DictionaryNode, self).__init__(serial, **kwargs)

        from .lexicon import LexiconEntrySubClassMapping, Lexicon

        self.lexicon = Lexicon().load({"name": self.lexiconName})

        try:
            self.dictionaryClass = LexiconEntrySubClassMapping.lexicon_class_map[self.lexiconName]

        except KeyError:
            raise IndexSchemaError("No matching class for {} in DictionaryNode".format(self.lexiconName))

    def _init_defaults(self):
        super(DictionaryNode, self)._init_defaults()

    def validate(self):
        super(DictionaryNode, self).validate()

    def first_child(self):
        try:
            return self.entry_class(self, word=self.firstWord)
        except DictionaryEntryNotFoundError:
            return None

    def last_child(self):
        try:
            return self.entry_class(self, word=self.lastWord)
        except DictionaryEntryNotFoundError:
            return None

    def all_children(self):
        lexicon_entry_set = LexiconEntrySet({"parent_lexicon": self.lexiconName})
        for lexicon_entry in lexicon_entry_set:
            yield self.entry_class(self, lexicon_entry=lexicon_entry)

    def serialize(self, **kwargs):
        """
        :return string: serialization of the subtree rooted in this node
        """
        d = super(DictionaryNode, self).serialize(**kwargs)
        d["nodeType"] = "DictionaryNode"
        d["lexiconName"] = self.lexiconName
        d["headwordMap"] = self.headwordMap
        d["firstWord"] = self.firstWord
        d["lastWord"] = self.lastWord
        return d

    def get_child_order(self, child):
        if isinstance(child, DictionaryEntryNode):
            if hasattr(child.lexicon_entry, "rid"):
                return str(child.lexicon_entry.rid)
            else:
                return child.word
        else:
            return ""

    # This is identical to SchemaNode.ref() and DictionaryEntryNode.ref().  Inherit?
    def ref(self):
        from . import text
        d = {
            "index": self.index,
            "book": self.full_title("en"),
            "primary_category": self.index.get_primary_category(),
            "index_node": self,
            "sections": [],
            "toSections": []
        }
        return text.Ref(_obj=d)

    def supports_language(self, lang):
        return lang == self.lexicon.version_lang


class SheetNode(NumberedTitledTreeNode):
    is_virtual = True
    supported_languages = ["en", "he"]

    def __init__(self, sheet_library_node, title=None, tref=None):
        """
        A node created on the fly, in memory, to correspond to a sheet.
        In the case of sheets, the dynamic nodes created present as the root node, with section info.
        :param parent:
        :param title:
        :param tref:
        :param word:
        """
        assert title and tref

        self.title = title
        self.parent = None
        self.depth = 2
        self.sectionNames = ["Sheet", "Segment"]
        self.addressTypes = ["Integer", "Integer"]
        self.index = sheet_library_node.index
        super(SheetNode, self).__init__(None)

        self._sheetLibraryNode = sheet_library_node
        self.title_group = sheet_library_node.title_group

        self._ref_regex = regex.compile("^" + regex.escape(title) + self.after_title_delimiter_re + "([0-9]+)(?:" + self.after_address_delimiter_ref + "([0-9]+)|$)")
        self._match = self._ref_regex.match(tref)
        if not self._match:
            raise InputError("Could not find sheet ID in sheet ref")
        self.sheetId = int(self._match.group(1))
        if not self.sheetId:
            raise Exception

        self.nodeId = int(self._match.group(2)) if self._match.group(2) else None
        self._sections = [self.sheetId] + ([self.nodeId] if self.nodeId else [])

        self.sheet_object = db.sheets.find_one({"id": int(self.sheetId)})
        if not self.sheet_object:
            raise SheetNotFoundError

    def has_numeric_continuation(self):
        return False  # What about section level?

    def has_titled_continuation(self):
        return False

    def get_sections(self):
        return self._sections

    def get_index_title(self):
        return self.index.title

    def get_version_title(self, lang):
        return "Dummy"

    def return_text_from_sheet_source(self, source):
        if source.get("text"):
            return (source.get("text"))
        elif source.get("outsideText"):
            return (source.get("outsideText"))
        elif source.get("outsideBiText"):
            return (source.get("outsideBiText"))
        elif source.get("comment"):
            return (source.get("comment"))
        elif source.get("media"):
            return (source.get("media"))

    def get_text(self):
        text = []
        for source in self.sheet_object.get("sources"):
            if self.nodeId:
                if self.nodeId == source.get("node"):
                    text.append(self.return_text_from_sheet_source(source))
                    break
            else:
                text.append(self.return_text_from_sheet_source(source))

        return text

    # def address(self):
    #    return self.parent.address() + [self.sheetId]

    def prev_sibling(self):
        return None

    def next_sibling(self):
        return None

    def next_leaf(self):
        return None

    def prev_leaf(self):
        return None

    def ref(self):
        from . import text
        d = {
            "index": self.index,
            "book": self.full_title("en"),
            "primary_category": self.index.get_primary_category(),
            "index_node": self,
            "sections": self._sections,
            "toSections": self._sections[:]
        }
        return text.Ref(_obj=d)


class SheetLibraryNode(VirtualNode):
    entry_class = SheetNode

    # These tree walking methods are needed, currently, so that VersionState doesn't get upset.
    # Seems like there must be a better way to do an end run around VersionState
    def create_content(self, callback=None, *args, **kwargs):
        if not callback:
            return None
        return callback(self, *args, **kwargs)

    def visit_content(self, callback, *contents, **kwargs):
        return self.create_content(callback, *contents, **kwargs)

    def visit_structure(self, callback, content, **kwargs):
        pass

    def serialize(self, **kwargs):
        """
        :return string: serialization of the subtree rooted in this node
        """
        d = super(SheetLibraryNode, self).serialize(**kwargs)
        d["nodeType"] = "SheetLibraryNode"
        return d

    def supports_language(self, lang):
        return True

"""
{
    "title" : "Sheet",
    "schema" : {
        "titles" : [
            {
                "text" : "",
                "primary" : true,
                "lang" : "he"
            },
            {
                "text" : "Sheet",
                "primary" : true,
                "lang" : "en"
            }
        ],
        nodes: [{
            "default" : true,
            "nodeType" : "SheetLibraryNode"
        }]
    }
    "category": ["_unlisted"]    #!!!!!
}

"""





"""
                ------------------------------------
                 Index Schema Trees - Address Types
                ------------------------------------
"""


class AddressType(object):
    """
    Defines a scheme for referencing and addressing a level of a Jagged Array.
    Used by :class:`NumberedTitledTreeNode`
    """
    special_cases = {}
    section_patterns = {
        'he': None,
        'en': None
    }
    reish_samekh_reg = "(?:\u05e1(?:\u05d9\u05e9\\s+)?|\u05e8(?:\u05d5\u05e3\\s+)?)?"  # matches letters reish or samekh or words reish or sof. these are common prefixes for many address types

    def __init__(self, order, length=None):
        self.order = order
        self.length = length

    def regex(self, lang, group_id=None, **kwargs):
        """
        The regular expression part that matches this address reference, wrapped with section names, if provided
        :param lang: "en" or "he"
        :param group_id: The id of the regular expression group the this match will be captured in
        :param kwargs: 'strict' kwarg indicates that section names are required to match
        :return string: regex component
        """
        try:
            if self.section_patterns[lang]:
                strict = kwargs.get("strict", False)
                reg = self.section_patterns[lang]
                if not strict:
                    reg += "?"
                reg += self._core_regex(lang, group_id, **kwargs)
                return reg
            else:
                return self._core_regex(lang, group_id, **kwargs)
        except KeyError:
            raise Exception("Unknown Language passed to AddressType: {}".format(lang))

    def _core_regex(self, lang, group_id=None, **kwargs):
        """
        The regular expression part that matches this address reference
        :param lang: "en" or "he"
        :param group_id: The id of the regular expression group the this match will be captured in
        :return string: regex component
        """
        pass

    @staticmethod
    def hebrew_number_regex():
        """
        Regular expression component to capture a number expressed in Hebrew letters
        :return string:
        \p{Hebrew} ~= [\\u05d0\\u05ea]
        """
        return r"""                                    # 1 of 3 styles:
        ((?=[\u05d0-\u05ea]+(?:"|\u05f4|\u201c|\u201d|'')[\u05d0-\u05ea])    # (1: ") Lookahead:  At least one letter, followed by double-quote, two single quotes, right fancy double quote, or gershayim, followed by  one letter
                \u05ea*(?:"|\u05f4|\u201c|\u201d|'')?				    # Many Tavs (400), maybe dbl quote
                [\u05e7-\u05ea]?(?:"|\u05f4|\u201c|\u201d|'')?	    # One or zero kuf-tav (100-400), maybe dbl quote
                [\u05d8-\u05e6]?(?:"|\u05f4|\u201c|\u201d|'')?	    # One or zero tet-tzaddi (9-90), maybe dbl quote
                [\u05d0-\u05d8]?					    # One or zero alef-tet (1-9)															#
            |[\u05d0-\u05ea]['\u05f3\u2018\u2019]					# (2: ') single letter, followed by a single quote, geresh, or right fancy quote
            |(?=[\u05d0-\u05ea])					    # (3: no punc) Lookahead: at least one Hebrew letter
                \u05ea*								    # Many Tavs (400)
                [\u05e7-\u05ea]?					    # One or zero kuf-tav (100-400)
                [\u05d8-\u05e6]?					    # One or zero tet-tzaddi (9-90)
                [\u05d0-\u05d8]?					    # One or zero alef-tet (1-9)
        )"""

    def toNumber(self, lang, s):
        """
        Return the numerical form of s in this address scheme
        :param s: The address component
        :param lang: "en" or "he"
        :return int:
        """
        pass

    def is_special_case(self, s):
        return s in self.special_cases

    def to_numeric_possibilities(self, lang, s, **kwargs):
        if s in self.special_cases:
            return self.special_cases[s]
        try:
            return [self.toNumber(lang, s)]
        except ValueError:
            return []

    @classmethod
    def can_match_out_of_order(cls, lang, s):
        """
        Can `s` match out of order when parsing sections?
        @param s:
        @return:
        """
        return True

    def toIndex(self, lang, s):
        return self.toNumber(lang, s) - 1

    def format_count(self, name, number):
        return {name: number}

    @classmethod
    def get_all_possible_sections_from_string(cls, lang, s, fromSections=None, strip_prefixes=False):
        """
        For string `s`, parse to sections using all address types that `cls` inherits from
        Useful for parsing ambiguous sections, e.g. for AddressPerek " = 8 but for its superclass AddressInteger, it equals 88.
        :param fromSections: optional. in case of parsing toSections, these represent the sections. Used for parsing edge-case of toSections='b' which is relative to sections
        :param strip_prefixes: optional. if true, consider possibilities when stripping potential prefixes
        """
        from sefaria.utils.hebrew import get_prefixless_inds

        sections = []
        toSections = []
        addr_classes = []
        starti_list = [0]
        if strip_prefixes and lang == 'he':
            starti_list += get_prefixless_inds(s)
        for starti in starti_list:
            curr_s = s[starti:]
            for SuperClass in cls.__mro__:  # mro gives all super classes
                if SuperClass == AddressType: break
                if SuperClass in {AddressInteger, AddressTalmud} and starti > 0: continue  # prefixes don't really make sense on AddressInteger or Talmud (in my opinion)
                addr = SuperClass(0)  # somewhat hacky. trying to get access to super class implementation of `regex` but actually only AddressTalmud implements this function. Other classes just overwrite class fields which modify regex's behavior. Simplest to just instantiate the appropriate address and use it.
                section_str = None
                if addr.is_special_case(curr_s):
                    section_str = curr_s
                else:
                    strict = SuperClass not in {AddressAmud, AddressTalmud, AddressFolio}  # HACK: these address types don't inherit from AddressInteger so it relies on flexibility of not matching "Daf"
                    regex_str = addr.regex(lang, strict=strict, group_id='section', with_roman_numerals=True) + "$"  # must match entire string
                    if regex_str is None: continue
                    reg = regex.compile(regex_str, regex.VERBOSE)
                    match = reg.match(curr_s)
                    if match:
                        section_str = match.group('section')
                if section_str:
                    temp_sections = addr.to_numeric_possibilities(lang, section_str, fromSections=fromSections)
                    temp_toSections = temp_sections[:]
                    if hasattr(cls, "lacks_amud") and cls.lacks_amud(section_str, lang) and not fromSections:
                        temp_toSections = [sec+1 for sec in temp_toSections]
                    sections += temp_sections
                    toSections += temp_toSections
                    addr_classes += [SuperClass]*len(temp_sections)

        if len(sections) > 0:
            # make sure section, toSection pairs are unique. prefer higher level address_types since these are more generic
            section_map = {}
            for i, (sec, toSec, addr) in enumerate(zip(sections, toSections, addr_classes)):
                section_map[(sec, toSec)] = (i, sec, toSec, addr)
            _, sections, toSections, addr_classes = zip(*sorted(section_map.values(), key=lambda x: x[0]))
        return sections, toSections, addr_classes

    @classmethod
    def toStr(cls, lang, i, **kwargs):
        if lang == "en":
            return str(i)
        elif lang == "he":
            punctuation = kwargs.get("punctuation", True)
            return sanitize(encode_small_hebrew_numeral(i), punctuation) if i < 1200 else encode_hebrew_numeral(i, punctuation=punctuation)

    @staticmethod
    def to_str_by_address_type(atype, lang, i):
        """
        Return string verion of `i` given `atype`
        :param str atype: name of address type
        :param str lang: "en" or "he"
        """
        try:
            klass = globals()["Address" + atype]
        except KeyError:
            raise IndexSchemaError("No matching class for addressType {}".format(atype))
        return klass(0).toStr(lang, i)

    @staticmethod
    def to_class_by_address_type(atype):
        """
        Return class that corresponds to 'atype'
        :param atype:
        :return:
        """
        try:
            klass = globals()["Address" + atype]
        except KeyError:
            raise IndexSchemaError("No matching class for addressType {}".format(atype))
        return klass(0)

    # Is this used?
    def storage_offset(self):
        return 0


class AddressDictionary(AddressType):
    # Important here is language of the dictionary, not of the text where the reference is.
    def _core_regex(self, lang, group_id=None, **kwargs):
        if group_id:
            reg = r"(?P<" + group_id + r">"
        else:
            reg = r"("

        reg += r".+"
        return reg

    def toNumber(self, lang, s):
        pass

    def toStr(cls, lang, i, **kwargs):
        pass


class AddressAmud(AddressType):
    section_patterns = {
        "en": r"""(?:[Aa]mud(?:im)?\s+)""",
        "he": f'''(?:{AddressType.reish_samekh_reg}\u05e2(?:"|\u05f4||''|\u05de\u05d5\u05d3\\s+))''' #+ (optional: (optional: samekh or reish for sof/reish) Ayin for amud) + [alef or bet] + (optional: single quote of any type (really only makes sense if there's no Ayin beforehand))
    }

    section_num_patterns = {
        "en": "[aAbB]",
        "he": "(?P<amud_letter>[\u05d0\u05d1])['\u05f3\u2018\u2019]?",
    }

    def _core_regex(self, lang, group_id=None, **kwargs):
        if group_id:
            reg = r"(?P<" + group_id + r">"
        else:
            reg = r"("

        reg += self.section_num_patterns[lang] + r")"

        return reg

    def toNumber(self, lang, s, **kwargs):
        if lang == "en":
            return 1 if s in {'a', 'A'} else 2
        elif lang == "he":
            return decode_hebrew_numeral(s)


class AddressTalmud(AddressType):
    """
    :class:`AddressType` for Talmud style Daf + Amud addresses
    """
    section_patterns = {
        "en": r"""(?:(?:[Ff]olios?|[Dd]af|[Pp](ages?|s?\.))?\s*)""",  # the internal ? is a hack to allow a non match, even if 'strict'
        "he": "((\u05d1?\u05d3\u05b7?\u05bc?[\u05e3\u05e4\u05f3\u2018\u2019'\"]\\s+)|(?:\u05e1|\u05e8)?\u05d3\"?)"			# (Daf, spelled with peh, peh sofit, geresh, gereshayim,  or single or doublequote) OR daled prefix
    }
    _can_match_out_of_order_patterns = None
    he_amud_pattern = AddressAmud(0).regex('he')
    amud_patterns = {
        "en": "[ABab]",
        "he": '''([.:]|[,\\s]+{})'''.format(he_amud_pattern)  # Either (1) period / colon (2) some separator + AddressAmud.section_patterns["he"]
    }
    special_cases = {
        "B": [None],
        "b": [None],
        "": [None],
    }

    @classmethod
    def oref_to_amudless_tref(cls, ref, lang):
        """
        Remove last amud from `ref`. Assumes `ref` ends in a Talmud address.
        This may have undesirable affect if `ref` doesn't end in a Talmud address
        """
        normal_form = ref._get_normal(lang)
        return re.sub(f"{cls.amud_patterns[lang]}$", '', normal_form)


    @classmethod
    def normal_range(cls, ref, lang):
        if ref.sections[-1] % 2 == 1 and ref.toSections[-1] % 2 == 0:  # starts at amud alef and ends at bet?
            start_daf = AddressTalmud.oref_to_amudless_tref(ref.starting_ref(), lang)
            end_daf = AddressTalmud.oref_to_amudless_tref(ref.ending_ref(), lang)
            if start_daf == end_daf:
                return start_daf
            else:
                range_wo_last_amud = AddressTalmud.oref_to_amudless_tref(ref, lang)
                # looking for rest of ref after dash
                end_range = re.search(f'-(.+)$', range_wo_last_amud).group(1)
                return f"{start_daf}-{end_range}"
        else: # range is in the form Shabbat 7b-8a, Shabbat 7a-8a, or Shabbat 7b-8b.  no need to special case it
            return ref._get_normal(lang)

    @classmethod
    def lacks_amud(cls, part, lang):
        if lang == "he":
            return re.search(cls.amud_patterns["he"], part) is None
        else:
            return re.search(cls.amud_patterns["en"] + "{1}$", part) is None
    
    @classmethod
    def parse_range_end(cls, ref, parts, base):
        """
        :param ref: Ref object (example: Zohar 1:2-3)
        :param parts: list of text of Ref; if Ref is a range, list will be of length 2; otherwise, length 1;
        if Ref == Zohar 1:2-3, parts = ["Zohar 1:2", "3"]
        :param base: parts[0] without title; in the above example, base would be "1:2"
        :return:
        """

        if len(parts) == 1:
            # check for Talmud ref without amud, such as Berakhot 2 or Zohar 1:2,
            # we don't want "Berakhot 2a" or "Zohar 1:2a" but "Berakhot 2a-2b" and "Zohar 1:2a-2b"
            # so change toSections if lacks_amud
            if cls.lacks_amud(base, ref._lang):
                ref.toSections[-1] += 1
        elif len(parts) == 2:
            range_parts = parts[1].split(".")  # this was converting space to '.', for some reason.

            he_bet_reg_ex = "^"+cls.he_amud_pattern.replace('[\u05d0\u05d1]', '\u05d1')  # don't want to look for Aleph

            if re.search(he_bet_reg_ex, range_parts[-1]):
                # 'Shabbat 23a-b' or 'Zohar 1:2a-b'
                ref.toSections[-1] = ref.sections[-1] + 1
            else:
                if cls.lacks_amud(parts[0], ref._lang) and cls.lacks_amud(parts[1], ref._lang):
                    # 'Shabbat 7-8' -> 'Shabbat 7a-8b'; 'Zohar 3:7-8' -> 'Zohar 3:7a-8b'
                    range_parts[-1] = range_parts[-1] + ('b' if ref._lang == 'en' else ' ')
                ref._parse_range_end(range_parts)

        # below code makes sure toSections doesn't go pass end of section/book
        if getattr(ref.index_node, "lengths", None):
            end = ref.index_node.lengths[len(ref.sections)-1]
            while ref.toSections[-1] > end:  # Yoma 87-90 should become Yoma 87a-88a, since it ends at 88a
                ref.toSections[-1] -= 1

    def _core_regex(self, lang, group_id=None, **kwargs):
        if group_id and kwargs.get("for_js", False) == False:
            reg = r"(?P<" + group_id + r">"
        else:
            reg = r"("

        if lang == "en":
            reg += r"\d*" if group_id == "ar0" else r"\d+" # if ref is Berakhot 2a:1-3a:4, "ar0" is 3a when group_id == "ar0", we don't want to require digit, as ref could be Berakhot 2a-b
            reg += r"{}?)".format(self.amud_patterns["en"])
        elif lang == "he":
            reg += self.hebrew_number_regex() + r'''{}?)'''.format(self.amud_patterns["he"])


        return reg

    def toNumber(self, lang, s, **kwargs):
        amud_b_list = ['b', 'B', '']
        if lang == "en":
            try:
                if re.search(self.amud_patterns["en"]+"{1}$", s):
                    amud = s[-1]
                    s = self.toStr(lang, kwargs['sections']) if s in amud_b_list else s
                    daf = int(s[:-1])
                else:
                    amud = "a"
                    daf = int(s)
            except ValueError:
                raise InputError("Couldn't parse Talmud reference: {}".format(s))

            if self.length and daf > self.length:
                # todo: Catch this above and put the book name on it.  Proably change Exception type.
                raise InputError("{} exceeds max of {} dafs.".format(daf, self.length))

            indx = daf * 2
            if amud in ["A", "a", ""]:
                indx -= 1
            return indx
        elif lang == "he":
            num = re.split(r"[.:,\s]", s)[0]
            daf = decode_hebrew_numeral(num) * 2
            amud_match = re.search(self.amud_patterns["he"] + "$", s)
            if s[-1] == ':' or (amud_match is not None and amud_match.group("amud_letter") == ''):
                return daf  # amud B
            return daf - 1

            # if s[-1] == "." or (s[-1] == u"\u05d0" and len(s) > 2 and s[-2] in ",\s"):

    @classmethod
    def toStr(cls, lang, i, **kwargs):
        i += 1
        daf_num = i // 2
        daf = ""

        if i > daf_num * 2:
            en_daf = "%db" % daf_num
        else:
            en_daf = "%da" % daf_num

        if lang == "en":
            daf = en_daf

        elif lang == "he":
            dotted = kwargs.get("dotted")
            if dotted:
                daf = encode_hebrew_daf(en_daf)
            else:
                punctuation = kwargs.get("punctuation", True)
                if i > daf_num * 2:
                    daf = ("%s " % sanitize(encode_small_hebrew_numeral(daf_num), punctuation) if daf_num < 1200 else encode_hebrew_numeral(daf_num, punctuation=punctuation)) + "\u05d1"
                else:
                    daf = ("%s " % sanitize(encode_small_hebrew_numeral(daf_num), punctuation) if daf_num < 1200 else encode_hebrew_numeral(daf_num, punctuation=punctuation)) + "\u05d0"

        return daf

    def format_count(self, name, number):
        if name == "Daf":
            return {
                "Amud": number,
                "Daf": number / 2
            }
        else: # shouldn't get here
            return {name: number}

    def storage_offset(self):
        return 2

    def to_numeric_possibilities(self, lang, s, **kwargs):
        """
        Hacky function to handle special case of ranged amud
        @param lang:
        @param s:
        @param kwargs:
        @return:
        """
        fromSections = kwargs['fromSections']
        if s in self.special_cases and fromSections:
            # currently assuming only special case is 'b'
            return [fromSec[-1]+1 for fromSec in fromSections]
        addr_num = self.toNumber(lang, s)
        possibilities = []
        if fromSections and s == '':
            for fromSec in fromSections:
                if addr_num < fromSec[-1]:
                    possibilities += [fromSec[-1]+1]
                else:
                    possibilities += [addr_num]
        else:
            possibilities = [addr_num]
        return possibilities

    @staticmethod
    def _get_can_match_out_of_order_pattern(lang):
        regex_str = AddressInteger(0).regex(lang, strict=True, group_id='section') + "$"
        return regex.compile(regex_str, regex.VERBOSE)

    @classmethod
    def can_match_out_of_order(cls, lang, s):
        if not cls._can_match_out_of_order_patterns:
            cls._can_match_out_of_order_patterns = {
                "en": cls._get_can_match_out_of_order_pattern("en"),
                "he": cls._get_can_match_out_of_order_pattern("he"),
            }
        reg = cls._can_match_out_of_order_patterns[lang]
        return reg.match(s) is None


class AddressFolio(AddressType):
    """
    :class:`AddressType` for Folio style #[abcd] addresses
    """
    section_patterns = {
        "en": r"""(?:(?:[Ff]olios?|[Dd]af|[Pp](ages?|s?\.))?\s*)""",  # the internal ? is a hack to allow a non match, even if 'strict'
        "he": r"(\u05d1?\u05d3\u05b7?\u05bc?[\u05e3\u05e4\u05f3\u2018\u2019'\"]\s+)"			# Daf, spelled with peh, peh sofit, geresh, gereshayim,  or single or doublequote
    }

    def _core_regex(self, lang, group_id=None, **kwargs):
        if group_id:
            reg = r"(?P<" + group_id + r">"
        else:
            reg = r"("

        if lang == "en":
            reg += r"\d+[abcd]?)"
        elif lang == "he":
            # either dots for amud (add dots from amud gimmel and dalet) or ayin followed by some type of quote followed by alef, bet, gimmel, or dalet
            reg += self.hebrew_number_regex() + r'''([.:]|[,\s]+(?:(?:"||''))?[-])?)'''

        return reg

    def toNumber(self, lang, s, **kwargs):
        if lang == "en":
            try:
                if s[-1] in ["a", "b", "c", "d", '', '', '', '']:
                    amud = s[-1]
                    daf = int(s[:-1])
                else:
                    amud = "a"
                    daf = int(s)
            except ValueError:
                raise InputError("Couldn't parse Talmud reference: {}".format(s))

            if self.length and daf > self.length:
                # todo: Catch this above and put the book name on it.  Proably change Exception type.
                raise InputError("{} exceeds max of {} dafs.".format(daf, self.length))

            indx = daf * 4
            if amud == "a" or amud == "":
                indx -= 3
            if amud == "b" or amud == "":
                indx -= 2
            if amud == "c" or amud == "":
                indx -= 1
            return indx
        elif lang == "he":
            num = re.split(r"[.:,\s]", s)[0]
            daf = decode_hebrew_numeral(num) * 4
            rest = s[len(num):]

            # check for each amud letter in reverse order (dalet, gimmel, bet, alef)
            # note: amud_dots for gimmel and dalet are a best guess at what might be used. should be refined as real examples are found.
            # if the amud matches that amud letter, subtract the appropriate offset
            quotes = "(?:''|\"|)"
            for amud_offset, (amud_letter, amud_dots) in enumerate((("", ""), ("", ""), ("", ":"), ("", "."))):
                if re.search(fr"^(?:{amud_dots}|,?\s?(?:{amud_letter}|{quotes}{amud_letter}))$", rest):
                    return daf - amud_offset
            # no match
            raise ValueError(f"Couldn't parse Folio address: {s} for lang {lang}")


    @classmethod
    def toStr(cls, lang, i, **kwargs):
        """

        1 => 1a
        2 => 1b
        3 => 1c
        4 => 1d
        5 => 2a
        6 => 2b
        etc.

        (i // 4) + 1
        """
        daf_num = ((i - 1) // 4) + 1
        mod = i % 4
        folio = "a" if mod == 1 else "b" if mod == 2 else "c" if mod == 3 else "d"
        daf = str(daf_num) + folio

        # todo
        if lang == "he":
            dotted = kwargs.get("dotted")
            if dotted:
                daf = encode_hebrew_daf(daf)
            else:
                punctuation = kwargs.get("punctuation", True)
                if i > daf_num * 2:
                    daf = ("%s " % sanitize(encode_small_hebrew_numeral(daf_num), punctuation) if daf_num < 1200 else encode_hebrew_numeral(daf_num, punctuation=punctuation)) + "\u05d1"
                else:
                    daf = ("%s " % sanitize(encode_small_hebrew_numeral(daf_num), punctuation) if daf_num < 1200 else encode_hebrew_numeral(daf_num, punctuation=punctuation)) + "\u05d0"

        return daf

    def format_count(self, name, number):
        if name == "Daf":
            return {
                "Amud": number,
                "Daf": number / 2
            }
        else:  # shouldn't get here
            return {name: number}

    def storage_offset(self):
        return 0


class AddressInteger(AddressType):
    """
    :class:`AddressType` for Integer addresses
    """
    def _core_regex(self, lang, group_id=None, **kwargs):
        if group_id:
            reg = r"(?P<" + group_id + r">"
        else:
            reg = r"("

        if lang == "en":
            if kwargs.get('with_roman_numerals', False):
                # any char valid in roman numerals (I, V, X, L, C, D, M) + optional trailing period
                reg += r"(?:\d+|[ivxlcdmIVXLCDM]+(?:\s?\.)?))"
            else:
                reg += r"\d+)"
        elif lang == "he":
            reg += self.hebrew_number_regex() + r")"

        return reg

    def toNumber(self, lang, s, **kwargs):
        if lang == "en":
            return int(s)
        elif lang == "he":
            return decode_hebrew_numeral(s)

    def to_numeric_possibilities(self, lang, s, **kwargs):
        import roman
        from roman import InvalidRomanNumeralError

        possibilities = super().to_numeric_possibilities(lang, s, **kwargs)
        if lang == "en":
            try:
                s = re.sub(r"\.$", "", s).strip()  # remove trailing period
                possibilities.append(roman.fromRoman(s.upper()))
            except InvalidRomanNumeralError as e:
                pass
        return possibilities

    @classmethod
    def can_match_out_of_order(cls, lang, s):
        """
        By default, this method should only apply to AddressInteger and no subclasses
        Can still be overriden for a specific class that needs to
        """
        return cls != AddressInteger


''' Never used
class AddressYear(AddressInteger):
    """
    :class: AddressYear stores Hebrew years as numbers, for example 778 for the year 
    To convert to Roman year, add 1240
    """
    def toNumber(self, lang, s):
        if lang == "he":
            return decode_hebrew_numeral(s)
        elif lang == "en":
            return int(s) - 1240

    @classmethod
    def toStr(cls, lang, i, **kwargs):
        if lang == "en":
            return str(i + 1240)
        elif lang == "he":
            punctuation = kwargs.get("punctuation", True)
            return sanitize(encode_small_hebrew_numeral(i), punctuation) if i < 1200 else encode_hebrew_numeral(i, punctuation=punctuation)
'''


class AddressAliyah(AddressInteger):
    en_map = ["First", "Second", "Third", "Fourth", "Fifth", "Sixth", "Seventh"]
    he_map = ["", "", "", "", "", "", ""]

    @classmethod
    def toStr(cls, lang, i, **kwargs):
        if lang == "en":
            return cls.en_map[i - 1]
        if lang == "he":
            return cls.he_map[i - 1]


class AddressPerek(AddressInteger):
    special_cases = {
        " ": [1, 141],
        " ": [1, 141],
        '"': [1, 100],  # this is inherently ambiguous (1 or 100)
        " ": [-1],
        '" ': [-1],
        '" ': [-1],
    }
    section_patterns = {
        "en": r"""(?:(?:[Cc]h(apters?|\.)|[Pp]erek|s\.)?\s*)""",  # the internal ? is a hack to allow a non match, even if 'strict'
        "he": fr"""(?:\u05d1?{AddressType.reish_samekh_reg}\u05e4((?:"|\u05f4|''|'\s|\s)|(?=[\u05d0-\u05ea]+(?:"|\u05f4|''|'\s)))   # Peh (for 'perek') maybe followed by a quote of some sort OR lookahead for some letters followed by a quote (e.g.  for chapter 11)
        |\u05e4\u05bc?\u05b6?\u05e8\u05b6?\u05e7(?:\u05d9\u05b4?\u05dd)?\s*                  # or 'perek(ym)' spelled out, followed by space
        )"""
    }


class AddressPasuk(AddressInteger):
    section_patterns = {
        "en": r"""(?:(?:([Vv](erses?|[vs]?\.)|[Pp]ass?u[kq]))?\s*)""",  # the internal ? is a hack to allow a non match, even if 'strict'
        "he": r"""(?:\u05d1?(?:                                        # optional  in front
            (?:\u05e4\u05b8?\u05bc?\u05e1\u05d5\u05bc?\u05e7(?:\u05d9\u05dd)?\s*)|    #pasuk spelled out, with a space after
            (?:\u05e4\u05e1['\u2018\u2019\u05f3]?\s+)
        ))"""
    }


class AddressMishnah(AddressInteger):
    section_patterns = {
        "en": r"""(?:(?:[Mm](ishnah?|s?\.))?\s*)""",  #  the internal ? is a hack to allow a non match, even if 'strict'
        "he": r"""(?:\u05d1?                                                   # optional  in front
            (?:\u05de\u05b4?\u05e9\u05b0?\u05c1?\u05e0\u05b8?\u05d4\s)			# Mishna spelled out, with a space after
            |(?:\u05de(?:["\u05f4]|'')?)				# or Mem (for 'mishna') maybe followed by a quote of some sort
        )"""
    }


class AddressVolume(AddressInteger):
    """
    :class:`AddressType` for Volume/ addresses
    """

    section_patterns = {
        "en": r"""(?:(?:[Vv](olumes?|\.))?\s*)""",  #  the internal ? is a hack to allow a non match, even if 'strict'
        "he": r"""(?:\u05d1?                                 # optional  in front
        (?:\u05d7\u05b5?(?:\u05dc\u05b6?\u05e7|'|\u05f3|\u2018|\u2019)\s+)  # Helek - spelled out with nikkud possibly or followed by a ' or a geresh - followed by space
         |(?:\u05d7["\u05f4])                     # chet followed by gershayim or double quote
        )
        """
    }
    
class AddressSiman(AddressInteger):
    section_patterns = {
        "en": r"""(?:(?:[Ss]iman)?\s*)""",
        "he": r"""(?:\u05d1?
            (?:[\u05e1\u05e8](?:"|\u05f4|'')\u05e1\s+)  # (reish or samekh) gershayim samekh. (start or end of siman)
            |(?:\u05e1\u05b4?\u05d9\u05de\u05b8?\u05df\s+)			# Siman spelled out with optional nikud, with a space after
            |(?:\u05e1\u05d9(?:["\u05f4'\u05f3\u2018\u2019](?:['\u05f3\u2018\u2019]|\s+)))		# or Samech, Yued (for 'Siman') maybe followed by a quote of some sort
        )"""
    }


class AddressHalakhah(AddressInteger):
    """
    :class:`AddressType` for Halakhah/ addresses
    """
    section_patterns = {
        "en": r"""(?:(?:[Hh]ala[ck]hah?)?\s*)""",  #  the internal ? is a hack to allow a non match, even if 'strict'
        "he": r"""(?:\u05d1?
            (?:\u05d4\u05bb?\u05dc\u05b8?\u05db(?:\u05b8?\u05d4|\u05d5\u05b9?\u05ea)\s+)			# Halakhah spelled out, with a space after
            |(?:\u05d4\u05dc?(?:["\u05f4'\u05f3\u2018\u2019](?:['\u05f3\u2018\u2019\u05db]|\s+)?)?)		# or Haeh and possible Lamed(for 'halakhah') maybe followed by a quote of some sort
        )"""
    }


class AddressSeif(AddressInteger):
    section_patterns = {
        "en": r"""(?:(?:[Ss][ae]if)?\s*)""",  #  the internal ? is a hack to allow a non match, even if 'strict'
        "he": r"""(?:\u05d1?
            (?:\u05e1[\u05b0\u05b8]?\u05e2\u05b4?\u05d9\u05e3\s+)			# Seif spelled out, with a space after
            |(?:\u05e1(?:\u05e2\u05d9)?(?:['\u2018\u2019\u05f3"\u05f4](?:['\u2018\u2019\u05f3]|\s+)?)?)	# or trie of first three letters followed by a quote of some sort
        )"""
    }


class AddressSeifKatan(AddressInteger):
    section_patterns = {
        "en": r"""(?:(?:[Ss][ae]if Katt?an)?\s*)""",  #  the internal ? is a hack to allow a non match, even if 'strict'
        "he": r"""(?:\u05d1?
            (?:\u05e1[\u05b0\u05b8]?\u05e2\u05b4?\u05d9\u05e3\s+\u05e7\u05d8\u05df\s+)			# Seif katan spelled out with or without nikud
            |(?:\u05e1(?:['\u2018\u2019\u05f3"\u05f4](?:['\u2018\u2019\u05f3])?)?\u05e7)(?:['\u2018\u2019\u05f3"\u05f4]['\u2018\u2019\u05f3]?|\s+)?	# or trie of first three letters followed by a quote of some sort
        )"""
    }


class AddressSection(AddressInteger):
    section_patterns = {
        "en": r"""(?:(?:([Ss]ections?|)?\s*))""",  #  the internal ? is a hack to allow a non match, even if 'strict'
        "he": r""""""
    }

```

### sefaria/model/user_profile.py

```
import hashlib
import urllib.request, urllib.parse, urllib.error
import re
import bleach
import sys
import json
import csv
from datetime import datetime
from django.utils.translation import ugettext as _, ungettext_lazy
from random import randint

from sefaria.system.exceptions import InputError, SheetNotFoundError
from functools import reduce

if not hasattr(sys, '_doc_build'):
    from django.contrib.auth.models import User, Group, AnonymousUser
    from emailusernames.utils import get_user, user_exists
    from django.core.mail import EmailMultiAlternatives
    from django.template.loader import render_to_string
    from django.core.validators import URLValidator, EmailValidator
    from django.core.exceptions import ValidationError
    from anymail.exceptions import AnymailRecipientsRefused

from . import abstract as abst
from sefaria.model.following import FollowersSet, FolloweesSet, general_follow_recommendations
from sefaria.model.blocking import BlockersSet, BlockeesSet
from sefaria.model.text import Ref, TextChunk
from sefaria.system.database import db
from sefaria.utils.util import epoch_time
from django.utils import translation

import structlog
logger = structlog.get_logger(__name__)


class UserHistory(abst.AbstractMongoRecord):
    collection = 'user_history'

    required_attrs = [
        "uid",                # user id
        "ref",                # str
        "he_ref",             # str
        "versions",           # dict: {en: str, he: str}
        "time_stamp",         # int: time this ref was read in epoch time
        "server_time_stamp",  # int: time this was saved on the server in epoch time
        "last_place",         # bool: True if this is the last ref read for this user in this book
        "book",               # str: index title
        "saved",              # bool: True if saved
        "secondary"          # bool: True when view is from sidebar
    ]

    optional_attrs = [
        "datetime",  # datetime: converted from time_stamp.  Can move to required once legacy records are converted.
        "context_refs",  # list of ref strings: derived from ref.  Can move to required once legacy records are converted.
        "categories",  # list of str: derived from ref.  Can move to required once legacy records are converted.
        "authors",  # list of str: derived from ref.  Can move to required once legacy records are converted.
        "is_sheet",  # bool: is this a sheet ref?  Can move to required once legacy records are converted.
        "language",           # oneOf(english, hebrew, bilingual) didn't exist in legacy model
        "num_times_read",     # int: legacy for migrating old recent views
        "sheet_title",        # str: for sheet history
        "sheet_owner",        # str: ditto
        "sheet_id",           # int: ditto
        "delete_saved",       # bool: True if this item was saved and but then was deleted
    ]

    def __init__(self, attrs=None, load_existing=False, field_updates=None, update_last_place=False):
        """
        :param attrs:
        :param load_existing: True if you want to load an existing mongo record if it exists to avoid duplicates
        :param field_updates: dict of updates in case load_existing finds a record
        """
        if attrs is None:
            attrs = {}
        # set defaults
        if "saved" not in attrs:
            attrs["saved"] = False
        if "secondary" not in attrs:
            attrs["secondary"] = False
        if "last_place" not in attrs:
            attrs["last_place"] = False
        # remove empty versions
        if not hasattr(attrs.get("versions", None), "items"):
            attrs["versions"] = {}  # if versions doesn't have 'items', make it an empty dict
        for k, v in list(attrs.get("versions", {}).items()):
            if v is None:
                del attrs["versions"][k]
        if load_existing:
            temp = UserHistory().load({"uid": attrs["uid"], "ref": attrs["ref"], "versions": attrs["versions"], "secondary": attrs['secondary']})
            if temp is not None:
                attrs = temp._saveable_attrs()
            # in the race-condition case where you're creating the saved item before the history item, do the update outside the previous if
            if field_updates:
                attrs.update(field_updates)
        if update_last_place:
            temp = UserHistory().load({"uid": attrs["uid"], "book": attrs["book"], "last_place": True})
            if temp is not None:
                temp.last_place = False
                temp.save()
            attrs["last_place"] = True

        super(UserHistory, self).__init__(attrs=attrs)

    def _validate(self):
        if self.secondary and self.saved:
            raise InputError("UserHistory item cannot currently have both saved and secondary flags set at the same time")

    def _normalize(self):
        # Derived values - used to make downstream queries quicker
        self.datetime = datetime.utcfromtimestamp(self.time_stamp)
        try:
            r = Ref(self.ref)
            self.context_refs   = [r.normal() for r in r.all_context_refs()]
            self.categories     = r.index.categories
            self.authors        = getattr(r.index, "authors", [])
            self.is_sheet       = r.index.title == "Sheet"
            if self.is_sheet:
                self.sheet_id = r.sections[0]
            if not self.secondary and not self.is_sheet and getattr(self, "language", None) != "hebrew" and r.is_empty("en"):
                # logically, this would be on frontend, but easier here.
                self.language = "hebrew"
        except SheetNotFoundError:
            self.context_refs   = [self.ref]
            self.categories     = ["_unlisted"]
            self.authors        = []
            self.is_sheet       = True
        except InputError:   # Ref failed to resolve
            self.context_refs   = [self.ref]
            self.categories     = []
            self.authors        = []
            self.is_sheet       = False
        except KeyError:     # is_text_translated() stumbled on a bad version state
            pass

    def contents(self, **kwargs):
        from sefaria.sheets import get_sheet_listing_data
        d = super(UserHistory, self).contents(**kwargs)
        if kwargs.get("for_api", False):
            keys = {
                'ref': '',
                'he_ref': '',
                'book': '',
                'versions': {},
                'time_stamp': 0,
                'saved': False,
                'delete_saved': False,
                'is_sheet': False,
                'sheet_id': -1,
                'sheet_owner': '',
                'sheet_title': '',
            }
            d = {
                key: d.get(key, default) for key, default in list(keys.items())
            }
        if kwargs.get("annotate", False):
            try:
                ref = Ref(d["ref"])
                if ref.is_sheet():
                    d.update(get_sheet_listing_data(d["sheet_id"]))
                else:
                    d["text"] = {
                        "en": TextChunk(ref, "en").as_sized_string(),
                        "he": TextChunk(ref, "he").as_sized_string()
                    }
            except Exception as e:
                logger.warning("Failed to retrieve text for history Ref: {}".format(d['ref']))
                return d
        return d

    def _sanitize(self):
        # UserHistory API is only open to post for your uid
        pass

    @staticmethod
    def save_history_item(uid, hist, action=None, time_stamp=None):
        if time_stamp is None:
            time_stamp = epoch_time()
        hist["uid"] = uid
        if "he_ref" not in hist or "book" not in hist:
            oref = Ref(hist["ref"])
            hist["he_ref"] = oref.he_normal()
            hist["book"] = oref.index.title
        hist["server_time_stamp"] = time_stamp if "server_time_stamp" not in hist else hist["server_time_stamp"]  # DEBUG: helpful to include this field for debugging

        saved = True if action == "add_saved" else (False if action == "delete_saved" else hist.get("saved", False))
        uh = UserHistory(hist, load_existing=(action is not None), update_last_place=(action is None), field_updates={
            "saved": saved,
            "server_time_stamp": hist["server_time_stamp"],
            "delete_saved": action == "delete_saved"
        })
        uh.save()
        return uh

    @staticmethod
    def remove_history_item(uid, hist):
        hist["uid"] = uid
        uh = UserHistory(hist, load_existing=True)
        uh.delete()

    @staticmethod
    def get_user_history(uid=None, oref=None, saved=None, secondary=None, last_place=None, sheets=None, serialized=False, annotate=False, limit=0, skip=0):
        query = {}
        if uid is not None:
            query["uid"] = uid
        if oref is not None:
            regex_list = oref.context_ref().regex(as_list=True)
            ref_clauses = [{"ref": {"$regex": r}} for r in regex_list]
            query["$or"] = ref_clauses
        if saved is not None:
            query["saved"] = saved
        if sheets is not None:
            query["is_sheet"] = sheets
        if secondary is not None:
            query["secondary"] = secondary
        if last_place is not None:
            query["last_place"] = last_place
        if serialized:
            return [uh.contents(for_api=True, annotate=annotate) for uh in UserHistorySet(query, proj={"uid": 0, "server_time_stamp": 0}, sort=[("time_stamp", -1)], limit=limit, skip=skip)]
        return UserHistorySet(query, sort=[("time_stamp", -1)], limit=limit, skip=skip)

    @staticmethod
    def delete_user_history(uid, exclude_saved=True, exclude_last_place=False):
        if not uid:
            raise InputError("Cannot delete user history without an id")
        query = {"uid": uid}
        if exclude_saved:
            query["saved"] = False
        if exclude_last_place:
            query["last_place"] = False
        UserHistorySet(query).delete(bulk_delete=True)

    @staticmethod
    def timeclause(start=None, end=None):
        """
        Returns a time range clause, fit for use in a pymongo query
        :param start: datetime
        :param end: datetime
        :return:
        """
        if start is None and end is None:
            return {}

        timerange = {}
        if start:
            timerange["$gte"] = start
        if end:
            timerange["$lte"] = end
        return {"datetime": timerange}


class UserHistorySet(abst.AbstractMongoSet):
    recordClass = UserHistory

    def hits(self):
        return reduce(lambda agg,o: agg + getattr(o, "num_times_read", 1), self, 0)


"""
Wrapper class for operations on the user object. Currently only for changing primary email.
"""
class UserWrapper(object):
    def __init__(self, email=None, user_obj=None):
        if email:
            self.user = get_user(email)
        elif user_obj:
            self.user = user_obj
        else:
            raise InputError("No user provided")
        self._errors = []

    def set_email(self, new_email):
        self.email = new_email
        self._errors = []

    def validate(self):
        return not self.errors()

    def errors(self):
        if len(self._errors):
            return self._errors[0]
        if user_exists(self.email):
            u = get_user(self.email)
            if u.id != self.user.id:
                self._errors.append(_("A user with that email already exists"))
        email_val = EmailValidator()
        try:
            email_val(self.email)
        except ValidationError as e:
            self._errors.append(_("The email address is not valid."))
        return self._errors[0] if len(self._errors) else None

    def save(self):
        if self.validate():
            self.user.email = self.email
            self.user.username = self.email #this is to conform with our username-as-email library, which doesnt really take care of itself properly as advertised
            self.user.save()
        else:
            raise ValueError(self.errors())

    def has_permission_group(self, group_name):
        try:
            group = Group.objects.get(name=group_name)
            return group in self.user.groups.all()
        except:
            return False


class UserProfile(object):
    def __init__(self, user_obj=None, id=None, slug=None, email=None, user_registration=False):
        """
        :param user_registration: pass during user registration so as to not create an extra profile record as init side effect
        """
        #TODO: Can we optimize the init to be able to load a profile without a call to user db?
        # say in a case where we already have an id and just want some fields from the profile object
        if slug:  # Load profile by slug, if passed
            profile = db.profiles.find_one({"slug": slug})
            if profile:
                self.__init__(id=profile["id"])
                return

        try:
            if user_obj and not isinstance(user_obj, AnonymousUser):
                user = user_obj
                id = user.id
            elif email and not id:  # Load profile by email, if passed.
                user = User.objects.get(email__iexact=email)
                id = user.id
            else:
                user = User.objects.get(id=id)
            self.first_name        = user.first_name
            self.last_name         = user.last_name
            self.email             = user.email
            self.date_joined       = user.date_joined
            self.user              = user
        except:
            # These default values allow profiles to function even
            # if the Django User records are missing (for testing)
            self.first_name        = "User"
            self.last_name         = str(id)
            self.email             = "test@sefaria.org"
            self.date_joined       = None
            self.user              = None

        self._id                   = None  # Mongo ID of profile doc
        self.id                    = id    # user ID
        self.slug                  = ""
        self.position              = ""
        self.organization          = ""
        self.jewish_education      = []
        self.bio                   = ""
        self.website               = ""
        self.location              = ""
        self.public_email          = ""
        self.youtube               = ""
        self.facebook              = ""
        self.twitter               = ""
        self.linkedin              = ""
        self.pinned_sheets         = []
        self.last_sync_web        = 0  # epoch time for last sync of web app
        self.profile_pic_url      = ""
        self.profile_pic_url_small = ""

        self.settings     =  {
            "email_notifications": "daily",
            "interface_language": "english",
            "textual_custom" : "sephardi",
            "reading_history" : True,
            "translation_language_preference": None,
        }
        self.version_preferences_by_corpus = {}

        # dict that stores the last time an attr has been modified
        self.attr_time_stamps = {
            "settings": 0
        }

        # flags that indicate a change needing a cascade after save
        self._name_updated      = False
        self._process_remove_history = False

        # Followers
        self.followers = FollowersSet(self.id)
        self.followees = FolloweesSet(self.id)

        # Blocks
        self.blockees = BlockeesSet(self.id)
        self.blockers = BlockersSet(self.id)

        # Google API token
        self.gauth_token = None
        self.gauth_email = None

        # CRM
        self.nationbuilder_id = None
        self.sf_app_user_id = None

        # Fundraising
        self.is_sustainer = False

        # Update with saved profile doc in MongoDB
        profile = db.profiles.find_one({"id": id})
        if profile: # overwrite if fake profile in db
            # TODO: think about how we want to handle the postgres database not being synced
            # with the mongo database. This is an existing issue; a 'new user' will be populated with 'old user'
            # data from a nonexistent user (in postgres)
            self.update(profile, ignore_flags_on_init=True)
        elif self.exists() and not user_registration:
            # If we encounter a user that has a Django user record but not a profile document
            # create a profile for them. This allows two enviornments to share a user database,
            # while maintaining separate profiles (e.g. Sefaria and S4D).
            self.show_editor_toggle = False
            self.uses_new_editor = True
            self.assign_slug()
            self.save()

    @property
    def full_name(self):
        return self.first_name + " " + self.last_name

    def _set_flags_on_update(self, obj):
        if "first_name" in obj or "last_name" in obj:
            if self.first_name != obj["first_name"] or self.last_name != obj["last_name"]:
                self._name_updated = True

        if "reading_history" in self.settings and self.settings["reading_history"] == True:
            if "settings" in obj and "reading_history" in obj["settings"] and obj["settings"]["reading_history"] == False:
                self._process_remove_history = True

    @staticmethod
    def transformOldRecents(uid, recents):
        from dateutil import parser
        from sefaria.system.exceptions import InputError
        import pytz
        default_epoch_time = epoch_time(
            datetime(2017, 12, 1))  # the Sefaria epoch. approx time since we added time stamps to recent items

        def xformer(recent):
            try:
                return {
                    "uid": uid,
                    "ref": recent[0],
                    "he_ref": recent[1],
                    "book": Ref(recent[0]).index.title,
                    "last_place": True,
                    "time_stamp": epoch_time(parser.parse(recent[2]).replace(tzinfo=None)) if recent[2] is not None else default_epoch_time,
                    "server_time_stamp": epoch_time(parser.parse(recent[2]).replace(tzinfo=None)) if recent[2] is not None else default_epoch_time,
                    "num_times_read": (recent[3] if recent[3] and isinstance(recent[3], int) else 1),  # we dont really know how long they've read this book. it's probably correlated with the number of times they opened the book
                    "versions": {
                        "en": recent[4],
                        "he": recent[5]
                    }
                }
            except InputError:
                return None
            except ValueError:
                return None
            except IndexError:
                return None
            except AttributeError:
                return None
        return [_f for _f in [xformer(r) for r in recents] if _f]

    def update(self, obj, ignore_flags_on_init=False):
        """
        Update this object with the fields in dictionary 'obj'
        """
        if not ignore_flags_on_init:
            self._set_flags_on_update(obj)
        for dict_key in ("settings", "version_preferences_by_corpus"):
            # merge these keys separately since they are themselves dicts.
            # want to allow partial updates to be passed to update.
            from sefaria.utils.util import deep_update
            if dict_key in obj and dict_key in self.__dict__:
                obj[dict_key] = deep_update(self.__dict__[dict_key], obj[dict_key])
        self.__dict__.update(obj)

        return self

    def update_empty(self, obj):
        self._set_flags_on_update(obj)
        for k, v in list(obj.items()):
            if v:
                if k not in self.__dict__ or self.__dict__[k] == '' or self.__dict__[k] == []:
                    self.__dict__[k] = v

    def update_version_preference(self, corpus, vtitle, lang):
        """
        Convenience method to keep update logic in one place
        """

        self.update({"version_preferences_by_corpus": {corpus: {lang: vtitle}}})

    def save(self):
        """
        Save profile to DB, updated Django User object if needed
        """
        # Sanitize & Linkify fields that allow HTML
        self.bio = bleach.linkify(self.bio)

        d = self.to_mongo_dict()
        if self._id:
            db.profiles.replace_one({'_id': self._id}, d, upsert=True)
        else:
            db.profiles.insert_one(d)

        # store name changes on Django User object
        if self._name_updated:
            user = User.objects.get(id=self.id)
            user.first_name = self.first_name
            user.last_name  = self.last_name
            user.save()
            self._name_updated = False

        if self._process_remove_history:
            self.delete_user_history()
            self._process_remove_history = False

        return self

    def errors(self):
        """
        Returns a string with any validation errors,
        or None if the profile is valid.
        """
        # Slug
        if re.search(r"[^a-z0-9\-]", self.slug):
            return "Profile URLs may only contain lowercase letters, numbers and hyphens."

        existing = db.profiles.find_one({"slug": self.slug, "_id": {"$ne": self._id}})
        if existing:
            return "The Profile URL you have requested is already in use."
        # URL Fields: website, facebook, linkedin
        url_val = URLValidator()
        try:
            if self.facebook: url_val(self.facebook)
        except ValidationError as e:
            return "The Facebook URL you entered is not valid."
        try:
            if self.linkedin: url_val(self.linkedin)
        except ValidationError as e:
            return "The LinkedIn URL you entered is not valid."
        try:
            if self.website: url_val(self.website)
        except ValidationError as e:
            return "The Website URL you entered is not valid."
        email_val = EmailValidator()
        try:
            if self.email: email_val(self.email)
        except ValidationError as e:
            return "The email address you entered is not valid."

        return None

    def exists(self):
        """
        Returns True if this is a real existing user, not simply a mock profile.
        """
        return bool(self.user)

    def assign_slug(self):
        """
        Set the slug according to the profile name,
        using the first available number at the end if duplicated exist
        """
        slug = "%s-%s" % (self.first_name, self.last_name)
        slug = slug.lower()
        slug = slug.replace(" ", "-")
        slug = re.sub(r"[^a-z0-9\-]", "", slug)
        self.slug = slug
        dupe_count = 0
        while self.errors():
            dupe_count += 1
            self.slug = "%s%d" % (slug, dupe_count)

        return self

    def join_invited_collections(self):
        """
        Add this user as a editor of any collections for which there is an outstanding invitation.
        """
        from sefaria.model import CollectionSet
        collections = CollectionSet({"invitations.email": self.email})
        for collection in collections:
            collection.add_member(self.id)
            collection.remove_invitation(self.email)

    def follows(self, uid):
        """Returns true if this user follows uid"""
        return uid in self.followees.uids

    def followed_by(self, uid):
        """Returns true if this user is followed by uid"""
        return uid in self.followers.uids

    def recent_notifications(self):
        from sefaria.model.notification import NotificationSet
        return NotificationSet().recent_for_user(self.id)

    def unread_notification_count(self):
        from sefaria.model.notification import NotificationSet
        return NotificationSet().unread_for_user(self.id).count()

    def process_history_item(self, hist, time_stamp):
        action = hist.pop("action", None)
        if self.settings.get("reading_history", True) or action == "add_saved":  # regular case where history enabled, save/unsave saved item etc. or save history in either case
            return UserHistory.save_history_item(self.id, hist, action, time_stamp)
        elif action == "delete_saved":  # user has disabled history and is "unsaving", therefore deleting this item.
            UserHistory.remove_history_item(self.id, hist)
            return None
        else:  # history disabled do nothing.
            return None

    def get_history(self, oref=None, saved=None, secondary=None, sheets=None, last_place=None, serialized=False, annotate=False, limit=0, skip=0):
        """
        personal user history
        :param oref:
        :param saved: True if you only want saved. False if not. None if you dont care
        :param secondary: ditto
        :param last_place: ditto
        :param sheets: ditto
        :param serialized: for return from API call
        :param limit: Passed on to Mongo to limit # of results
        :return:
        """
        if not self.settings.get('reading_history', True) and not saved:
            return [] if serialized else None
        return UserHistory.get_user_history(uid=self.id, oref=oref, saved=saved, secondary=secondary, sheets=sheets, last_place=last_place, serialized=serialized, annotate=annotate, limit=limit, skip=skip)

    def delete_user_history(self, exclude_saved=True, exclude_last_place=False):
        UserHistory.delete_user_history(uid=self.id, exclude_saved=exclude_saved, exclude_last_place=exclude_last_place)

    def follow_recommendations(self, lang="english", n=4):
        """
        Returns a list of users recommended for `self` to follow.
        """
        from random import choices
        options = general_follow_recommendations(lang=lang, n=100)
        if not len(options):
            return []
        filtered_options = [u for u in options if not self.follows(u["uid"])]

        return choices(filtered_options, k=n)

    def to_mongo_dict(self):
        """
        Return a json serializable dictionary which includes all data to be saved in mongo (as opposed to postgres)
        """
        return {
            "id":                    self.id,
            "slug":                  self.slug,
            "position":              self.position,
            "organization":          self.organization,
            "jewish_education":      self.jewish_education,
            "bio":                   self.bio,
            "website":               self.website,
            "location":              self.location,
            "public_email":          self.public_email,
            "facebook":              self.facebook,
            "twitter":               self.twitter,
            "linkedin":              self.linkedin,
            "youtube":               self.youtube,
            "pinned_sheets":         self.pinned_sheets,
            "settings":              self.settings,
            "version_preferences_by_corpus": self.version_preferences_by_corpus,
            "attr_time_stamps":      self.attr_time_stamps,
            "is_sustainer":          self.is_sustainer,
            "tag_order":             getattr(self, "tag_order", None),
            "last_sync_web":         self.last_sync_web,
            "profile_pic_url":       self.profile_pic_url,
            "profile_pic_url_small": self.profile_pic_url_small,
            "gauth_token":           self.gauth_token,
            "nationbuilder_id":      self.nationbuilder_id,
            "sf_app_user_id":        self.sf_app_user_id,
            "gauth_email":           self.gauth_email,
        }

    def to_api_dict(self, basic=False):
        """
        Return a json serializble dictionary this profile which includes fields used in profile API methods
        If basic is True, only return enough data to display a profile listing
        """
        dictionary = {
                "id": self.id,
                "slug": self.slug,
                "profile_pic_url": self.profile_pic_url,
                "full_name": self.full_name,
                "followers": self.followers.uids,
                "followees": self.followees.uids,
                "profile_pic_url": self.profile_pic_url,
                "jewish_education": self.jewish_education,
                "bio": self.bio,
                "website": self.website,
                "location": self.location,
                "public_email": self.public_email,
                "facebook": self.facebook,
                "twitter": self.twitter,
                "linkedin": self.linkedin,
                "youtube": self.youtube,
                "position": self.position,
                "organization": self.organization
            }
        if basic:
            return dictionary
        other_info = {
            "pinned_sheets":         self.pinned_sheets,
            "is_sustainer":          self.is_sustainer,
        }
        dictionary.update(other_info)
        return dictionary


    def to_mongo_json(self):
        """
        Return a json serializable dictionary which includes all data to be saved in mongo (as opposed to postgres)
        """
        return json.dumps(self.to_mongo_dict())


def detect_potential_spam_message_notifications():
    # Get "message" type notifications where one user has sent many messages to multiple users.
    spammers = db.notifications.aggregate(
        [
            {
                "$match": {
                    "read": False,
                    "is_global": False,
                    "type": "message"
                }
            },
            {
                "$group": {
                    "_id": "$content.sender",
                    "countmessages": {
                        "$sum": 1
                    },
                    "uids": {
                        "$addToSet": "$uid"
                    }
                }
            },
            {"$match": {"countmessages": {"$gt": 20}}},
        ])
    suspect_results = []
    for spammer in spammers:
        spammer_id = spammer["_id"]
        if len(spammer["uids"]) >= 5:
            suspect_results.append(spammer_id)
            try:
                spammer_account = User.objects.get(id=spammer_id)
                spammer_account.is_active = False
                spammer_account.save()
            except:
                continue

        print(spammer["_id"])
    # Mark all of these Notifications with these sender ids as suspicious so they dont get sent to the users
    db.notifications.update_many({"content.sender": {"$in": suspect_results}}, {"$set": {"suspected_spam": True}})
    return suspect_results


def email_unread_notifications(timeframe):
    """
    Looks for all unread notifications and sends each user one email with a summary.
    Marks any sent notifications as "read".

    timeframe may be:
    * 'daily'  - only send to users who have the daily email setting
    * 'weekly' - only send to users who have the weekly email setting
    * 'all'    - send all notifications
    """
    from sefaria.model.notification import NotificationSet

    detect_potential_spam_message_notifications()

    users = db.notifications.find({"read": False, "is_global": False}).distinct("uid")

    for uid in users:
        profile = UserProfile(id=uid)
        if profile.settings["email_notifications"] != timeframe and timeframe != 'all':
            continue
        notifications = NotificationSet().unread_personal_for_user(uid)
        if len(notifications) == 0:
            continue
        try:
            user = User.objects.get(id=uid)
        except User.DoesNotExist:
            continue

        if "interface_language" in profile.settings:
            translation.activate(profile.settings["interface_language"][0:2])

        message_html  = render_to_string("email/notifications_email.html", {"notifications": notifications, "recipient": user.first_name})
        actors_string = notifications.actors_string()
        # TODO Hebrew subjects
        if actors_string:
            verb      = "have" if " and " in actors_string else "has"
            subject   = "%s %s new activity on Sefaria" % (actors_string, verb)
        elif notifications.like_count() > 0:
            noun      = "likes" if notifications.like_count() > 1 else "like"
            subject   = "%d new %s on your Source Sheet" % (notifications.like_count(), noun)
        from_email    = "Sefaria Notifications <notifications@sefaria.org>"
        to            = user.email

        msg = EmailMultiAlternatives(subject, message_html, from_email, [to])
        msg.content_subtype = "html"
        try:
            msg.send()
            notifications.mark_read(via="email")
        except AnymailRecipientsRefused:
            print('bad email address: {}'.format(to))

        if "interface_language" in profile.settings:
            translation.deactivate()


public_user_data_cache = {}
def public_user_data(uid, ignore_cache=False):
    """Returns a dictionary with common public data for `uid`"""
    if uid in public_user_data_cache and not ignore_cache:
        return public_user_data_cache[uid]

    profile = UserProfile(id=uid)
    try:
        user = User.objects.get(id=uid)
        is_staff = user.is_staff()
    except:
        is_staff = False

    data = {
        "name": profile.full_name,
        "profileUrl": "/profile/" + profile.slug,
        "imageUrl": profile.profile_pic_url_small,
        "position": profile.position,
        "organization": profile.organization,
        "isStaff": is_staff,
        "uid": uid
    }
    public_user_data_cache[uid] = data
    return data


def user_name(uid):
    """Returns a string of a user's full name"""
    data = public_user_data(uid)
    return data["name"]


def profile_url(uid):
    """Returns url to user's profile"""
    data = public_user_data(uid)
    return data["profileUrl"]


def user_link(uid):
    """Returns a string with an <a> tag linking to a users profile"""
    data = public_user_data(uid)
    link = "<a href='" + data["profileUrl"] + "' class='userLink'>" + data["name"] + "</a>"
    return link


def is_user_staff(uid):
    """
    Returns True if the user with uid is staff.
    """
    data = public_user_data(uid)  #needed?
    try:
        uid  = int(uid)
        user = User.objects.get(id=uid)
        return user.is_staff
    except:
        return False

def annotate_user_list(uids):
    """
    Returns a list of dictionaries giving details (names, profile links)
    for the user ids list in uids.
    """
    annotated_list = []
    for uid in uids:
        data = public_user_data(uid)
        annotated = {
            "userLink": user_link(uid),
            "imageUrl": data["imageUrl"]
        }
        annotated_list.append(annotated)

    return annotated_list


def process_index_title_change_in_user_history(indx, **kwargs):
    print("Cascading User History from {} to {}".format(kwargs['old'], kwargs['new']))

    # ensure that the regex library we're using here is the same regex library being used in `Ref.regex`
    from .text import re as reg_reg
    patterns = [pattern.replace(reg_reg.escape(indx.title), reg_reg.escape(kwargs["old"]))
                for pattern in Ref(indx.title).regex(as_list=True)]
    queries = [{'ref': {'$regex': pattern}} for pattern in patterns]
    objs = UserHistorySet({"$or": queries})
    for o in objs:
        o.ref = o.ref.replace(kwargs["old"], kwargs["new"], 1)
        try:
            o.save()
        except InputError:
            logger.warning("Failed to convert user history from: {} to {}".format(kwargs['old'], kwargs['new']))

```

### sefaria/model/history.py

```
"""
history.py
Writes to MongoDB Collection: history

"add index"     done
"add link"      done
"add note"      done
"add text"      done
"delete link"   done
"delete note"   done
"edit index"    done
"edit link"     done
"edit note"     done
"edit text"     done
"publish sheet"
"revert text"
"review"

"""

import regex as re
from datetime import datetime
from diff_match_patch import diff_match_patch
dmp = diff_match_patch()

from . import abstract as abst
from sefaria.system.database import db


def log_text(user, action, oref, lang, vtitle, old_text, new_text, **kwargs):

    if isinstance(new_text, list):
        if not isinstance(old_text, list):  # is this necessary? the TextChunk should handle it.
            old_text = [old_text]
        maxlength = max(len(old_text), len(new_text))
        for i in reversed(list(range(maxlength))):
            subref = oref.subref(i + 1)
            subold = old_text[i] if i < len(old_text) else [] if isinstance(new_text[i], list) else ""
            subnew = new_text[i] if i < len(new_text) else [] if isinstance(old_text[i], list) else ""
            log_text(user, action, subref, lang, vtitle, subold, subnew, **kwargs)
        return

    if old_text == new_text:
        return

    # create a patch that turns the new version back into the old
    backwards_diff = dmp.diff_main(new_text, old_text)
    patch = dmp.patch_toText(dmp.patch_make(backwards_diff))
    # get html displaying edits in this change.
    forwards_diff = dmp.diff_main(old_text, new_text)
    dmp.diff_cleanupSemantic(forwards_diff)
    diff_html = dmp.diff_prettyHtml(forwards_diff)

    log = {
        "ref": oref.normal(),
        "version": vtitle,
        "language": lang,
        "diff_html": diff_html,
        "revert_patch": patch,
        "user": user,
        "date": datetime.now(),
        #"revision": next_revision_num(),
        "message": kwargs.get("message", ""), # is this used?
        "rev_type": "{} text".format(action),
        "method": kwargs.get("method", "Site")
    }

    History(log).save()


def log_update(user, klass, old_dict, new_dict, **kwargs):
    kind = klass.history_noun
    rev_type = "edit {}".format(kind)
    return _log_general(user, kind, old_dict, new_dict, rev_type, **kwargs)


def log_delete(user, klass, old_dict, **kwargs):
    kind = klass.history_noun
    rev_type = "delete {}".format(kind)
    return _log_general(user, kind, old_dict, None, rev_type, **kwargs)


def log_add(user, klass, new_dict, **kwargs):
    kind = klass.history_noun
    rev_type = "add {}".format(kind)
    return _log_general(user, kind, None, new_dict, rev_type, **kwargs)


def _log_general(user, kind, old_dict, new_dict, rev_type, **kwargs):
    log = {
        #"revision": next_revision_num(),
        "user": user,
        "old": old_dict,
        "new": new_dict,
        "rev_type": rev_type,
        "date": datetime.now(),
    }

    # Need a better way to handle variations in handling of different objects in history
    if kind == "note":
        #Don't log any changes to private notes, even notes that had been previously private - since the old version will be shown in history
        if (new_dict and not new_dict.get("public")) or (old_dict and not old_dict.get("public")):
            return

    # TODO: added just for link, but should check if this can be added for any object
    # Appears to be conflict with text.method
    if kind == 'link':
        log['method'] = kwargs.get("method", "Site")

    if kind == "index":
        log['title'] = new_dict["title"]

    return History(log).save()

'''
def next_revision_num():
    #todo: refactor to use HistorySet? May add expense for no gain.
    last_rev = db.history.find().sort([['revision', -1]]).limit(1)
    revision = last_rev.next()["revision"] + 1 if last_rev.count() else 1
    return revision
'''

class History(abst.AbstractMongoRecord):
    collection = 'history'
    required_attrs = [
        "rev_type",
        "user",
        "date"
    ]
    optional_attrs = [
        "revision",  # do we need this at all? Could use _id
        "message",
        "revert_patch",
        "language",
        "diff_html",
        "version",
        "ref",
        "method",
        "old",
        "new",
        "link_id",
        "title",    # .25%
        "note_id",  # .05%
        "comment",  # rev_type: review
        "score",    # rev_type: review
        "sheet"     # rev_type: publish sheet
    ]

    def _sanitize(self):
        # History should only ever be called internally with clean text. No need to sanitize
        pass

    def pretty_print(self):
        pass


class HistorySet(abst.AbstractMongoSet):
    recordClass = History


def process_index_title_change_in_history(indx, **kwargs):

    def construct_query(attribute, queries):
        query_list = [{attribute: {'$regex': query}} for query in queries]
        return {'$or': query_list}

    print("Cascading History {} to {}".format(kwargs['old'], kwargs['new']))
    """
    Update all history entries which reference 'old' to 'new'.
    """
    from sefaria.model.text import prepare_index_regex_for_dependency_process
    queries = prepare_index_regex_for_dependency_process(indx, as_list=True)
    queries = [query.replace(re.escape(indx.title), re.escape(kwargs["old"])) for query in queries]
    title_pattern = r'(^{}$)'.format(re.escape(kwargs["old"]))

    text_hist = HistorySet(construct_query('ref', queries),  sort=[('ref', 1)])
    print("Cascading Text History {} to {}".format(kwargs['old'], kwargs['new']))
    for h in text_hist:
        h.ref = h.ref.replace(kwargs["old"], kwargs["new"], 1)
        h.save()

    link_hist = HistorySet(construct_query("new.refs", queries), sort=[('new.refs', 1)])
    print("Cascading Link History {} to {}".format(kwargs['old'], kwargs['new']))
    for h in link_hist:
        h.new["refs"] = [r.replace(kwargs["old"], kwargs["new"], 1) for r in h.new["refs"]]
        h.save()

    note_hist = HistorySet(construct_query("new.ref", queries), sort=[('new.ref', 1)])
    print("Cascading Note History {} to {}".format(kwargs['old'], kwargs['new']))
    for h in note_hist:
        h.new["ref"] = h.new["ref"].replace(kwargs["old"], kwargs["new"], 1)
        h.save()

    title_hist = HistorySet({"title": {"$regex": title_pattern}}, sort=[('title', 1)])
    print("Cascading Index History {} to {}".format(kwargs['old'], kwargs['new']))
    for h in title_hist:
        h.title = h.title.replace(kwargs["old"], kwargs["new"], 1)
        h.save()

def process_version_title_change_in_history(ver, **kwargs):
    """
    Rename a text version title in history records
    'old' and 'new' are the version title names.
    """
    query = {
        "ref": {"$regex": r'^%s(?= \d)' % ver.title},
        "version": kwargs["old"],
        "language": ver.language,
    }
    db.history.update(query, {"$set": {"version": kwargs["new"]}}, upsert=False, multi=True)
```

### sefaria/local_settings_example.py

```
# An example of settings needed in a local_settings.py file.
# copy this file to sefaria/local_settings.py and provide local info to run.
from datetime import timedelta
import sys
import structlog
import sefaria.system.logging as sefaria_logging
import os

# These are things you need to change!

################
# YOU ONLY NEED TO CHANGE "NAME" TO THE PATH OF YOUR SQLITE DATA FILE
# If the db.sqlite file does not exist, simply list a path where it can be created.
# You can set the path to /path/to/Sefaria-Project/db.sqlite, since we git-ignore all sqlite files
# (you do not need to create the empty db.sqlite file, as Django will handle that later)
# ########################################
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.
        'NAME': '/path/to/Sefaria-Project/db.sqlite', # Path to where you would like the database to be created including a file name, or path to an existing database file if using sqlite3.
        'USER': '',                      # Not used with sqlite3.
        'PASSWORD': '',                  # Not used with sqlite3.
        'HOST': '',                      # Set to empty string for localhost. Not used with sqlite3.
        'PORT': '',                      # Set to empty string for default. Not used with sqlite3.
    }
}

"""
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'name of db table here',
        'USER': 'name of db user here',
        'PASSWORD': 'password here',
        'HOST': '127.0.0.1',
        'PORT': '',
    }
}"""

# Map domain to an interface language that the domain should be pinned to.
# Leave as {} to prevent language pinning, in which case one domain can serve either Hebrew or English
DOMAIN_LANGUAGES = {
    "http://hebrew.example.org": "hebrew",
    "http://english.example.org": "english",
}


################ These are things you can change! ###########################################################################
#SILENCED_SYSTEM_CHECKS = ['captcha.recaptcha_test_key_error']

ALLOWED_HOSTS = ["localhost", "127.0.0.1","0.0.0.0"]

ADMINS = (
     ('Your Name', 'you@example.com'),
)
ADMIN_PATH = 'somethingsomething' #This will be the path to the admin site, locally it can also be 'admin'

PINNED_IPCOUNTRY = "IL" #change if you want parashat hashavua to be diaspora.

MONGO_REPLICASET_NAME = None # If the below is a list, this should be set to something other than None. 
# This can be either a string of one mongo host server or a list of `host:port` string pairs. So either e.g "localhost" of ["localhost:27017","localhost2:27017" ]
MONGO_HOST = "localhost"
MONGO_PORT = 27017 # Not used if the above is a list
# Name of the MongoDB database to use.
SEFARIA_DB = 'sefaria' # Change if you named your db something else
SEFARIA_DB_USER = '' # Leave user and password blank if not using Mongo Auth
SEFARIA_DB_PASSWORD = ''
APSCHEDULER_NAME = "apscheduler"


""" These are some examples of possible caches. more here: https://docs.djangoproject.com/en/1.11/topics/cache/"""
CACHES = {
    "shared": {
        'BACKEND': 'django.core.cache.backends.dummy.DummyCache',
    },
    "default": {
        'BACKEND': 'django.core.cache.backends.dummy.DummyCache',
    },
}
"""
CACHES = {
    'shared': {
        'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
        'LOCATION': '/home/ephraim/www/sefaria/django_cache/',
    },
    'default': {
        'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
        'LOCATION': '/home/ephraim/www/sefaria/django_cache/',
    }
}
"""

SESSION_CACHE_ALIAS = "default"
USER_AGENTS_CACHE = 'default'
SHARED_DATA_CACHE_ALIAS = 'shared'

"""THIS CACHE DEFINITION IS FOR USE WITH NODE AND SERVER SIDE RENDERING"""
"""
CACHES = {
    "shared": {
        "BACKEND": "django_redis.cache.RedisCache",
        "LOCATION": "redis://127.0.0.1:6379/1", #The URI used to look like this "127.0.0.1:6379:0"
        "OPTIONS": {
            "CLIENT_CLASS": "django_redis.client.DefaultClient",
            #"SERIALIZER": "django_redis.serializers.json.JSONSerializer", #this is the default, we override it to ensure_ascii=False
            "SERIALIZER": "sefaria.system.serializers.JSONSerializer",
        },
        "TIMEOUT": None,
    },
    "default": {
        "BACKEND": "django_redis.cache.RedisCache",
        "LOCATION": "redis://127.0.0.1:6379/0", #The URI used to look like this "127.0.0.1:6379:0"
        "OPTIONS": {
            "CLIENT_CLASS": "django_redis.client.DefaultClient",
            #"PASSWORD": "secretpassword", # Optional
        },
        "TIMEOUT": 60 * 60 * 24 * 30,
    },
}
"""

SITE_PACKAGE = "sites.sefaria"







################ These are things you DO NOT NEED to touch unless you know what you are doing. ##############################
DEBUG = True
ALLOWED_HOSTS = ['localhost', '127.0.0.1', '[::1]']
OFFLINE = False
DOWN_FOR_MAINTENANCE = False
MAINTENANCE_MESSAGE = ""

# Location of Strapi CMS instance
# For local development, Strapi is located at http://localhost:1337 by default
STRAPI_LOCATION = None
STRAPI_PORT = None


MANAGERS = ADMINS

SECRET_KEY = 'insert your long random secret key here !'


EMAIL_HOST = 'localhost'
EMAIL_PORT = 1025
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'

# Example using anymail, replaces block above
# EMAIL_BACKEND = 'anymail.backends.mandrill.EmailBackend'
# DEFAULT_FROM_EMAIL = "Sefaria <hello@sefaria.org>"
# ANYMAIL = {
#    "MANDRILL_API_KEY": "your api key",
# }


# ElasticSearch server
# URL to connect to ES server.
# Set this to https://sefaria.org:443/api/search to connect to production search.
# If ElasticSearch server has a password use the following format: http(s)://{username}:{password}@{base_url}
SEARCH_URL = "http://localhost:9200"

SEARCH_INDEX_ON_SAVE = False  # Whether to send texts and source sheet to Search Host for indexing after save
SEARCH_INDEX_NAME_TEXT = 'text'  # name of the ElasticSearch index to use
SEARCH_INDEX_NAME_SHEET = 'sheet'

# Node Server
USE_NODE = False
NODE_HOST = "http://localhost:4040"
NODE_TIMEOUT = 10

SEFARIA_DATA_PATH = '/path/to/your/Sefaria-Data' # used for Data
SEFARIA_EXPORT_PATH = '/path/to/your/Sefaria-Data/export' # used for exporting texts


GOOGLE_GTAG = 'your gtag id here'
GOOGLE_TAG_MANAGER_CODE = 'you tag manager code here'

HOTJAR_ID = None

# Determine which CRM connection implementations to use
CRM_TYPE = "NONE"  # "SALESFORCE" || "NATIONBUILDER" || "NONE"

# Integration with a NationBuilder list
NATIONBUILDER_SLUG = ""
NATIONBUILDER_TOKEN = ""
NATIONBUILDER_CLIENT_ID = ""
NATIONBUILDER_CLIENT_SECRET = ""

# Integration with Salesforce
SALESFORCE_BASE_URL = ""
SALESFORCE_CLIENT_ID = ""
SALESFORCE_CLIENT_SECRET = ""

# Issue bans to Varnish on update.
USE_VARNISH = False
FRONT_END_URL = "http://localhost:8000"  # This one wants the http://
VARNISH_ADM_ADDR = "localhost:6082" # And this one doesn't
VARNISH_HOST = "localhost"
VARNISH_FRNT_PORT = 8040
VARNISH_SECRET = "/etc/varnish/secret"
# Use ESI for user box in header.
USE_VARNISH_ESI = False

# Prevent modification of Index records
DISABLE_INDEX_SAVE = False

# Turns off search autocomplete suggestions, which are reinitialized on every server reload
# which can be annoying for local development.
DISABLE_AUTOCOMPLETER = False

# Turns on loading of machine learning models to run linker
ENABLE_LINKER = False

# Caching with Cloudflare
CLOUDFLARE_ZONE = ""
CLOUDFLARE_EMAIL = ""
CLOUDFLARE_TOKEN = ""

# Multiserver
MULTISERVER_ENABLED = False
MULTISERVER_REDIS_SERVER = "127.0.0.1"
MULTISERVER_REDIS_PORT = 6379
MULTISERVER_REDIS_DB = 0
MULTISERVER_REDIS_EVENT_CHANNEL = "msync"   # Message queue on Redis
MULTISERVER_REDIS_CONFIRM_CHANNEL = "mconfirm"   # Message queue on Redis

# OAUTH these fields dont need to be filled in. they are only required for oauth2client to __init__ successfully
GOOGLE_OAUTH2_CLIENT_ID = ""
GOOGLE_OAUTH2_CLIENT_SECRET = ""
# This is the field that is actually used
GOOGLE_OAUTH2_CLIENT_SECRET_FILEPATH = ""

GOOGLE_APPLICATION_CREDENTIALS_FILEPATH = ""

GEOIP_DATABASE = 'data/geoip/GeoLiteCity.dat'
GEOIPV6_DATABASE = 'data/geoip/GeoLiteCityv6.dat'

RAW_REF_MODEL_BY_LANG_FILEPATH = {
    "en": None,
    "he": None
}

RAW_REF_PART_MODEL_BY_LANG_FILEPATH = {
    "en": None,
    "he": None
}

# Simple JWT
SIMPLE_JWT = {
    'ACCESS_TOKEN_LIFETIME': timedelta(days=1),
    'REFRESH_TOKEN_LIFETIME': timedelta(days=90),
    'ROTATE_REFRESH_TOKENS': True,
    'SIGNING_KEY': 'a signing key: at least 256 bits',
}

# Celery - the following section defines variables to connect to the celery broker. This can be either redis or redis sentinel
# Either define SENTINEL_HEADLESS_URL if using sentinel or REDIS_URL for a simple redis instance
# If using sentinel, also pass any variables prefixed with SENTINEL. Otherwise, they can be left as default.
# All other variables need to be defined if connecting to either redis or redis sentinel
REDIS_URL = "redis://127.0.0.1"
REDIS_PORT = 6379  # the port exposed on either redis or redis sentinel. default for sentinel is 26379
REDIS_PASSWORD = None
SENTINEL_HEADLESS_URL = None
SENTINEL_PASSWORD = None
SENTINEL_TRANSPORT_OPTS = {}
CELERY_REDIS_BROKER_DB_NUM = 2
CELERY_REDIS_RESULT_BACKEND_DB_NUM = 3
CELERY_QUEUES = {}
CELERY_ENABLED = False
# END Celery

#Slack
SLACK_URL = ''

# Key which identifies the Sefaria app as opposed to a user
# using our API outside of the app. Mainly for registration
MOBILE_APP_KEY = "MOBILE_APP_KEY"

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        "json_formatter": {
            "()": structlog.stdlib.ProcessorFormatter,
            "processor": structlog.processors.JSONRenderer(),
        },
    },
    'handlers': {
        'default': {
            "class": "logging.StreamHandler",
            "formatter": "json_formatter",
        },
    },
    'loggers': {
        '': {
            'handlers': ['default'],
            'propagate': False,
        },
        'django': {
            'handlers': ['default'],
            'propagate': False,
        },
        'django.request': {
            'handlers': ['default'],
            'propagate': False,
        },
    }
}

structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.add_logger_name,
        sefaria_logging.add_severity,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.StackInfoRenderer(),
        sefaria_logging.log_exception_info,
        structlog.processors.UnicodeDecoder(),
        sefaria_logging.decompose_request_info,
        structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
    ],
    context_class=structlog.threadlocal.wrap_dict(dict),
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

SENTRY_DSN = None
CLIENT_SENTRY_DSN = None

# Fail gracefully when decorator conditional_graceful_exception on function. This should be set to True on production
# Example: If a text or ref cannot be properly loaded, fail gracefully and let the server continue to run
FAIL_GRACEFULLY = False
if "pytest" in sys.modules:
    FAIL_GRACEFULLY = False

WEBHOOK_USERNAME = os.getenv("WEBHOOK_USERNAME")
WEBHOOK_PASSWORD = os.getenv("WEBHOOK_PASSWORD")
```

### sefaria/settings.py

```
# Django settings for sefaria project.

import os.path
from django.utils.translation import ugettext_lazy as _

relative_to_abs_path = lambda *x: os.path.join(os.path.dirname(
                               os.path.realpath(__file__)), *x)
# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# On Unix systems, a value of None will cause Django to use the same
# timezone as the operating system.
# If running in a Windows environment this must be set to the same as your
# system time zone.
TIME_ZONE = 'America/Halifax'

# Language code for this installation. All choices can be found here:
# http://www.i18nguy.com/unicode/language-identifiers.html
LANGUAGE_CODE = 'en'

LANGUAGES = (
    ('en', _("English")),
    ('he', _("Hebrew")),
)

SITE_ID = 1

# If you set this to False, Django will make some optimizations so as not
# hereto load the internationalization machinery.
USE_I18N = True

# If you set this to False, Django will not format dates, numbers and
# calendars according to the current locale.
USE_L10N = True

# If you set this to False, Django will not use timezone-aware datetimes.
USE_TZ = True

# Absolute filesystem path to the directory that will hold user-uploaded files.
# Example: "/home/media/media.lawrence.com/media/"
MEDIA_ROOT = ''

# URL that handles the media served from MEDIA_ROOT. Make sure to use a
# trailing slash.
# Examples: "http://media.lawrence.com/media/", "http://example.com/media/"
MEDIA_URL = ''

# Absolute path to the directory static files should be collected to.
# Don't put anything in this directory yourself; store your static files
# in apps' "static/" subdirectories and in STATICFILES_DIRS.
# Example: "/home/media/media.lawrence.com/static/"
STATIC_ROOT = '/app/static-collected'

# URL prefix for static files.
# Example: "http://media.lawrence.com/static/"
STATIC_URL = '/static/'

# List of finder classes that know how to find static files in
# various locations.
STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
    #'django.contrib.staticfiles.finders.DefaultStorageFinder',
)

STATICFILES_DIRS = [
    relative_to_abs_path('../static/'),
]

# Make this unique, and don't share it with anybody.
SECRET_KEY = ''

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [
            relative_to_abs_path('../templates/'),
        ],
        'OPTIONS': {
            'context_processors': [
                    # Insert your TEMPLATE_CONTEXT_PROCESSORS here or use this
                    # list if you haven't customized them:
                    "django.contrib.auth.context_processors.auth",
                    "django.template.context_processors.debug",
                    "django.template.context_processors.i18n",
                    "django.template.context_processors.media",
                    "django.template.context_processors.static",
                    "django.template.context_processors.tz",
                    "django.contrib.messages.context_processors.messages",
                    "django.template.context_processors.request",
                    "sefaria.system.context_processors.global_settings",
                    "sefaria.system.context_processors.cache_timestamp",
                    "sefaria.system.context_processors.large_data",
                    "sefaria.system.context_processors.body_flags",
                    "sefaria.system.context_processors.header_html",
                    "sefaria.system.context_processors.footer_html",
                    "sefaria.system.context_processors.base_props",
            ],
            'loaders': [
                #'django_mobile.loader.Loader',
                'django.template.loaders.filesystem.Loader',
                'django.template.loaders.app_directories.Loader',
            ]
        },
    },
]

MIDDLEWARE = [
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.locale.LocaleMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django_user_agents.middleware.UserAgentMiddleware',
    'sefaria.system.middleware.LocationSettingsMiddleware',
    'sefaria.system.middleware.LanguageCookieMiddleware',
    'sefaria.system.middleware.LanguageSettingsMiddleware',
    'sefaria.system.middleware.ProfileMiddleware',
    'sefaria.system.middleware.CORSDebugMiddleware',
    'sefaria.system.middleware.SharedCacheMiddleware',
    'sefaria.system.multiserver.coordinator.MultiServerEventListenerMiddleware',
    'django_structlog.middlewares.RequestMiddleware',
    #'easy_timezones.middleware.EasyTimezoneMiddleware',
    #'django.middleware.cache.UpdateCacheMiddleware',
    #'django.middleware.cache.FetchFromCacheMiddleware',

]

ROOT_URLCONF = 'sefaria.urls'

# Python dotted path to the WSGI application used by Django's runserver.
WSGI_APPLICATION = 'sefaria.wsgi.application'

INSTALLED_APPS = (
    'adminsortable',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'reader',
    'django.contrib.staticfiles',
    'django.contrib.humanize',
    'emailusernames',
    'guides',
    'sefaria.gauth',
    'django_topics.apps.DjangoTopicsAppConfig',
    'captcha',
    'django.contrib.admin',
    'anymail',
    'webpack_loader',
    'django_user_agents',
    'rest_framework',
    #'easy_timezones'
    # Uncomment the next line to enable admin documentation:
    # 'django.contrib.admindocs',
)

LOGIN_URL = 'login'

LOGIN_REDIRECT_URL = 'table_of_contents'

LOGOUT_REDIRECT_URL = 'table_of_contents'

AUTHENTICATION_BACKENDS = (
    'emailusernames.backends.EmailAuthBackend',
)

REST_FRAMEWORK = {
    'DEFAULT_AUTHENTICATION_CLASSES': (
        'rest_framework_simplejwt.authentication.JWTAuthentication',
        'rest_framework.authentication.SessionAuthentication',
    )
}

SESSION_SAVE_EVERY_REQUEST = True
SESSION_SERIALIZER = 'django.contrib.sessions.serializers.JSONSerializer' # this is the default anyway right now, but make sure

LOCALE_PATHS = (
    relative_to_abs_path('../locale'),
)

# A sample logging configuration. The only tangible logging
# performed by this configuration is to send an email to
# the site admins on every HTTP 500 error when DEBUG=False.
# See http://docs.djangoproject.com/en/dev/topics/logging for
# more details on how to customize your logging configuration.

""" to use logging, in any module:
# import the logging library
import structlog

# Get an instance of a logger
logger = structlog.get_logger(__name__)

#log stuff
logger.critical()
logger.error()
logger.warning()
logger.info()
logger.debug()
"""

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': '%(asctime)s - %(levelname)s %(name)s: %(message)s'
        },
        'simple': {
            'format': '%(levelname)s %(message)s'
        },
        'verbose': {
            'format': '%(asctime)s - %(levelname)s: [%(name)s] %(process)d %(thread)d %(message)s'
        },

    },
    'filters': {
        'require_debug_false': {
            '()': 'django.utils.log.RequireDebugFalse'
        },
        'require_debug_true': {
            '()': 'sefaria.utils.log.RequireDebugTrue'
        },
        'exclude_errors' : {
            '()': 'sefaria.utils.log.ErrorTypeFilter',
            'error_types' : ['BookNameError'],
            'exclude' : True
        },
        'filter_book_name_errors' : {
            '()': 'sefaria.utils.log.ErrorTypeFilter',
            'error_types' : ['BookNameError', 'InputError'],
            'exclude' : False
        }
    },
    'handlers': {
        'default': {
            'level':'WARNING',
            'filters': ['exclude_errors'],
            'class':'logging.handlers.RotatingFileHandler',
            'filename': relative_to_abs_path('../log/sefaria.log'),
            'maxBytes': 1024*1024*5, # 5 MB
            'backupCount': 20,
            'formatter':'verbose',
        },
        'book_name_errors': {
            'level':'ERROR',
            'filters': ['filter_book_name_errors'],
            'class':'logging.handlers.RotatingFileHandler',
            'filename': relative_to_abs_path('../log/sefaria_book_errors.log'),
            'maxBytes': 1024*1024*5, # 5 MB
            'backupCount': 20,
            'formatter':'verbose',
        },
        'null': {
            'level':'INFO',
            'class':'logging.NullHandler',
        },

        'mail_admins': {
            'level': 'ERROR',
            'filters': ['require_debug_false'],
            'class': 'django.utils.log.AdminEmailHandler'
        },
        'request_handler': {
            'level':'WARNING',
            'class':'logging.handlers.RotatingFileHandler',
            'filename': relative_to_abs_path('../log/django_request.log'),
            'maxBytes': 1024*1024*5, # 5 MB
            'backupCount': 20,
            'formatter':'standard',
        }
    },
    'loggers': {
        '': {
            'handlers': ['default', 'book_name_errors'],
            'level': 'INFO',
            'propagate': True
        },
        'django': {
            'handlers': ['null'],
            'propagate': False,
            'level': 'INFO',
        },
        'django.request': {
            'handlers': ['mail_admins', 'request_handler'],
            'level': 'INFO',
            'propagate': True,
        },
    }
}

CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
        'LOCATION': '/var/tmp/django_cache',
    }
}


# Grab environment specific settings from a file which
# is left out of the repo.
try:
    if os.getenv("CI_RUN"):
        from sefaria.local_settings_ci import *
    else:
        from sefaria.local_settings import *
except ImportError:
    from sefaria.local_settings_example import *
if os.getenv("COOLIFY"):
    from sefaria.local_settings_coolify import *

# Listed after local settings are imported so CACHE can depend on DEBUG
WEBPACK_LOADER = {
    'DEFAULT': {
        'BUNDLE_DIR_NAME': 'bundles/client/',  # must end with slash
        'STATS_FILE': relative_to_abs_path('../node/webpack-stats.client.json'),
        'POLL_INTERVAL': 0.1,
        'TIMEOUT': None,
        'CACHE': not DEBUG,
    },
    'SEFARIA_JS': {
        'BUNDLE_DIR_NAME': 'bundles/sefaria/',  # must end with slash
        'STATS_FILE': relative_to_abs_path('../node/webpack-stats.sefaria.json'),
        'POLL_INTERVAL': 0.1,
        'TIMEOUT': None,
        'CACHE': not DEBUG,
    },
    'LINKER': {
        'BUNDLE_DIR_NAME': 'bundles/linker.v3/',  # must end with slash
        'STATS_FILE': relative_to_abs_path('../node/webpack-stats.linker.v3.json'),
        'POLL_INTERVAL': 0.1,
        'TIMEOUT': None,
        'CACHE': not DEBUG,
    }

}

DATA_UPLOAD_MAX_MEMORY_SIZE = 24000000

BASE_DIR = os.path.dirname(os.path.dirname(__file__))

```

### sefaria/site/__init__.py

```

```

### sefaria/site/site_settings.py

```
from sefaria.settings import SITE_PACKAGE

temp = __import__(SITE_PACKAGE + ".site_settings", fromlist=["SITE_SETTINGS"])

SITE_SETTINGS = temp.SITE_SETTINGS

```

### sefaria/site/urls.py

```
from sefaria.settings import SITE_PACKAGE

__temp = __import__(SITE_PACKAGE + ".urls", fromlist=["site_urlpatterns"])
site_urlpatterns = __temp.site_urlpatterns

```

### sefaria/search.py

```
# -*- coding: utf-8 -*-
"""
search.py - full-text search for Sefaria using ElasticSearch

Writes to MongoDB Collection: index_queue
"""
import os
from datetime import datetime, timedelta
import re
import bleach
import pymongo


import structlog
import logging
from logging import NullHandler
from collections import defaultdict
import time as pytime
logger = structlog.get_logger(__name__)

from elasticsearch.client import IndicesClient
from elasticsearch.helpers import bulk
from elasticsearch.exceptions import NotFoundError
from sefaria.model import *
from sefaria.model.text import AbstractIndex, AbstractTextRecord
from sefaria.model.user_profile import user_link, public_user_data
from sefaria.model.collection import CollectionSet
from sefaria.system.database import db
from sefaria.system.exceptions import InputError
from sefaria.utils.util import strip_tags
from .settings import SEARCH_INDEX_NAME_TEXT, SEARCH_INDEX_NAME_SHEET
from sefaria.helper.search import get_elasticsearch_client
from sefaria.site.site_settings import SITE_SETTINGS
from sefaria.utils.hebrew import strip_cantillation
import sefaria.model.queue as qu

es_client = get_elasticsearch_client()
index_client = IndicesClient(es_client)

tracer = structlog.get_logger(__name__)
tracer.setLevel(logging.CRITICAL)
#tracer.addHandler(logging.FileHandler('/tmp/es_trace.log'))
tracer.addHandler(NullHandler())

doc_count = 0


def delete_text(oref, version, lang):
    try:
        curr_index = get_new_and_current_index_names('text')['current']

        id = make_text_doc_id(oref.normal(), version, lang)
        es_client.delete(index=curr_index, id=id)
    except Exception as e:
        logger.error("ERROR deleting {} / {} / {} : {}".format(oref.normal(), version, lang, e))


def delete_version(index, version, lang):
    assert isinstance(index, AbstractIndex)

    refs = []

    if SITE_SETTINGS["TORAH_SPECIFIC"]:
        all_gemara_indexes = library.get_indexes_in_category("Bavli")
        davidson_indexes = all_gemara_indexes[:all_gemara_indexes.index("Horayot") + 1]
        if Ref(index.title).is_bavli() and index.title not in davidson_indexes:
            refs += index.all_section_refs()

    refs += index.all_segment_refs()

    for ref in refs:
        delete_text(ref, version, lang)


def delete_sheet(index_name, id):
    try:
        es_client.delete(index=index_name, id=id)
    except Exception as e:
        logger.error("ERROR deleting sheet {}".format(id))


def make_text_doc_id(ref, version, lang):
    """
    Returns a doc id string for indexing based on ref, versiona and lang.

    [HACK] Since Elasticsearch chokes on non-ascii ids, hebrew titles are converted
    into a number using unicode_number. This mapping should be unique, but actually isn't.
    (any tips welcome)
    """
    if not version.isascii():
        version = str(unicode_number(version))

    id = "%s (%s [%s])" % (ref, version, lang)
    return id


def unicode_number(u):
    """
    Returns a number corresponding to the sum value
    of each unicode character in u
    """
    n = 0
    for i in range(len(u)):
        n += ord(u[i])
    return n

def make_sheet_topics(sheet):
    topics = []
    for t in sheet.get('topics', []):
        topic_obj = Topic.init(t['slug'])
        if not topic_obj:
            continue
        topics += [topic_obj]
    return topics

def index_sheet(index_name, id):
    """
    Index source sheet with 'id'.
    """

    sheet = db.sheets.find_one({"id": id})
    if not sheet: return False

    pud = public_user_data(sheet["owner"])
    topics = make_sheet_topics(sheet)
    collections = CollectionSet({"sheets": id, "listed": True})
    collection_names = [c.name for c in collections]
    try:
        doc = {
            "title": strip_tags(sheet["title"]),
            "content": make_sheet_text(sheet, pud),
            "owner_id": sheet["owner"],
            "owner_name": pud["name"],
            "owner_image": pud["imageUrl"],
            "profile_url": pud["profileUrl"],
            "version": "Source Sheet by " + user_link(sheet["owner"]),
            "topic_slugs": [topic_obj.slug for topic_obj in topics],
            "topics_en": [topic_obj.get_primary_title('en') for topic_obj in topics],
            "topics_he": [topic_obj.get_primary_title('he') for topic_obj in topics],
            "sheetId": id,
            "summary": sheet.get("summary", None),
            "collections": collection_names,
            "datePublished": sheet.get("datePublished", None),
            "dateCreated": sheet.get("dateCreated", None),
            "dateModified": sheet.get("dateModified", None),
            "views": sheet.get("views", 0)
        }
        es_client.create(index=index_name, id=id, body=doc)
        global doc_count
        doc_count += 1
        return True
    except Exception as e:
        print("Error indexing sheet %d" % id)
        print(e)
        return False

def make_sheet_text(sheet, pud):
    """
    Returns a plain text representation of the content of sheet.
    :param sheet: The sheet record
    :param pud: Public User Database record for the author
    """
    text = sheet["title"] + "\n{}".format(sheet.get("summary", ''))
    if pud.get("name"):
        text += "\nBy: " + pud["name"]
    text += "\n"
    if sheet.get("topics"):
        topics = make_sheet_topics(sheet)
        topics_en = [topic_obj.get_primary_title('en') for topic_obj in topics]
        topics_he = [topic_obj.get_primary_title('he') for topic_obj in topics]
        text += " [" + ", ".join(topics_en+topics_he) + "]\n"
    for s in sheet["sources"]:
        text += source_text(s) + " "

    text = bleach.clean(text, strip=True, tags=())

    return text


def source_text(source):
    """
    Recursive function to translate a source dictionary into text.
    """
    str_fields = ["customTitle", "ref", "comment", "outsideText"]
    dict_fields = ["text", "outsideBiText"]
    content = [source.get(field, "") for field in str_fields]
    content += [val for field in dict_fields for val in source.get(field, {}).values()]
    text = " ".join([strip_tags(c) for c in content])

    if "subsources" in source:
        for s in source["subsources"]:
            text += source_text(s)

    return text


def get_exact_english_analyzer():
    return {
        "tokenizer": "standard",
        "char_filter": [
            "icu_normalizer",
        ],
        "filter": [
            "lowercase",
            "icu_folding",
        ],
    }


def get_stemmed_english_analyzer():
    stemmed_english_analyzer = get_exact_english_analyzer()
    stemmed_english_analyzer['filter'] += ["my_snow"]
    return stemmed_english_analyzer


def create_index(index_name, type):
    """
    Clears the indexes and creates it fresh with the below settings.
    """
    clear_index(index_name)

    settings = {
        "index": {
            "blocks": {
                "read_only_allow_delete": False
            },
            "analysis": {
                "analyzer": {
                    "stemmed_english": get_stemmed_english_analyzer(),
                    "exact_english": get_exact_english_analyzer(),
                },
                "filter": {
                    "my_snow": {
                        "type": "snowball",
                        "language": "English"
                    }
                }
            }
        }
    }
    print('Creating index {}'.format(index_name))
    index_client.create(index=index_name, settings=settings)

    if type == 'text':
        put_text_mapping(index_name)
    elif type == 'sheet':
        put_sheet_mapping(index_name)


def put_text_mapping(index_name):
    """
    Settings mapping for the text document type.
    """
    text_mapping = {
        'properties' : {
            'categories': {
                'type': 'keyword',
            },
            "category": {
                'type': 'keyword',
            },
            "he_category": {
                'type': 'keyword',
            },
            "index_title": {
                'type': 'keyword',
            },
            "path": {
                'type': 'keyword',
            },
            "he_index_title": {
                'type': 'keyword',
            },
            "he_path": {
                'type': 'keyword',
            },
            "order": {
                'type': 'keyword',
            },
            "pagesheetrank": {
                'type': 'double',
                'index': False
            },
            "comp_date": {
                'type': 'integer',
                'index': False
            },
            "version_priority": {
                'type': 'integer',
                'index': False
            },
            "exact": {
                'type': 'text',
                'analyzer': 'exact_english'
            },
            "naive_lemmatizer": {
                'type': 'text',
                'analyzer': 'sefaria-naive-lemmatizer',
                'search_analyzer': 'sefaria-naive-lemmatizer-less-prefixes',
                'fields': {
                    'exact': {
                        'type': 'text',
                        'analyzer': 'exact_english'
                    }
                }
            }
        }
    }
    index_client.put_mapping(body=text_mapping, index=index_name)


def put_sheet_mapping(index_name):
    """
    Sets mapping for the sheets document type.
    """
    sheet_mapping = {
        'properties': {
            'owner_name': {
                'type': 'keyword'
            },
            'tags': {
                'type': 'keyword'
            },
            "topics_en": {
                "type": "keyword"
            },
            "topics_he": {
                "type": "keyword"
            },
            "topic_slugs": {
                "type": "keyword"
            },
            'owner_image': {
                'type': 'keyword'
            },
            'datePublished': {
                'type': 'date'
            },
            'dateCreated': {
                'type': 'date'
            },
            'dateModified': {
                'type': 'date'
            },
            'sheetId': {
                'type': 'integer'
            },
            'collections': {
                'type': 'keyword'
            },
            'title': {
                'type': 'keyword'
            },
            'views': {
                'type': 'integer'
            },
            'summary': {
                'type': 'keyword'
            },
            'content': {
                'type': 'text',
                'analyzer': 'stemmed_english'
            },
            'version': {
                'type': 'keyword'
            },
            'profile_url': {
                'type': 'keyword'
            },
            'owner_id': {
                'type': 'integer'
            }
        }
    }
    index_client.put_mapping(body=sheet_mapping, index=index_name)

def get_search_categories(oref, categories):
    toc_tree = library.get_toc_tree()
    cats = oref.index.categories

    indexed_categories = categories  # the default

    # get the full path of every cat along the way.
    # starting w/ the longest,
    # check if they're root swapped.
    paths = [cats[:i] for i in range(len(cats), 0, -1)]
    for path in paths:
        cnode = toc_tree.lookup(path)
        if getattr(cnode, "searchRoot", None) is not None:
            # Use the specified searchRoot, with the rest of the category path appended.
            indexed_categories = [cnode.searchRoot] + cats[len(path) - 1:]
            break
    return indexed_categories


class TextIndexer(object):

    @classmethod
    def clear_cache(cls):
        cls.terms_dict = None
        cls.version_priority_map = None
        cls._bulk_actions = None
        cls.best_time_period = None


    @classmethod
    def create_terms_dict(cls):
        cls.terms_dict = {}
        ts = TermSet()
        for t in ts:
            cls.terms_dict[t.name] = t.contents()

    @classmethod
    def create_version_priority_map(cls):
        toc = library.get_toc()
        cls.version_priority_map = {}

        def traverse(mini_toc):
            if type(mini_toc) == list:
                for t in mini_toc:
                    traverse(t)
            elif "contents" in mini_toc:
                for t in mini_toc["contents"]:
                    traverse(t)
            elif "title" in mini_toc and not mini_toc.get("isCollection", False):
                title = mini_toc["title"]
                try:
                    r = Ref(title)
                except InputError:
                    print("Failed to parse ref, {}".format(title))
                    return
                vlist = cls.get_ref_version_list(r)
                vpriorities = defaultdict(lambda: 0)
                for i, v in enumerate(vlist):
                    lang = v.language
                    cls.version_priority_map[(title, v.versionTitle, lang)] = (vpriorities[lang], mini_toc["categories"])
                    vpriorities[lang] += 1

        traverse(toc)

    @staticmethod
    def get_ref_version_list(oref, tries=0):
        try:
            return oref.index.versionSet().array()
        except InputError as e:
            print(f"InputError: {oref.normal()}")
            return []
        except pymongo.errors.AutoReconnect as e:
            if tries < 200:
                pytime.sleep(5)
                return TextIndexer.get_ref_version_list(oref, tries+1)
            else:
                print("get_ref_version_list -- Tried: {} times. Failed :(".format(tries))
                raise e

    @classmethod
    def get_all_versions(cls, tries=0, versions=None, page=0):
        versions = versions or []
        try:
            version_limit = 10
            temp_versions = []
            first_run = True
            while first_run or len(temp_versions) > 0:
                temp_versions = VersionSet(limit=version_limit, page=page).array()
                versions += temp_versions
                page += 1
                first_run = False
            return versions
        except pymongo.errors.AutoReconnect as e:
            if tries < 200:
                pytime.sleep(5)
                return cls.get_all_versions(tries+1, versions, page)
            else:
                print("Tried: {} times. Got {} versions".format(tries, len(versions)))
                raise e

    @staticmethod
    def excluded_from_search(version):
        return version.versionTitle in [
            "Yehoyesh's Yiddish Tanakh Translation [yi]",
            'Miqra Mevoar, trans. and edited by David Kokhav, Jerusalem 2020'
        ]

    @classmethod
    def index_all(cls, index_name, debug=False, for_es=True, action=None):
        cls.index_name = index_name
        cls.create_version_priority_map()
        cls.create_terms_dict()
        Ref.clear_cache()  # try to clear Ref cache to save RAM

        versions = sorted([x for x in cls.get_all_versions() if (x.title, x.versionTitle, x.language) in cls.version_priority_map], key=lambda x: cls.version_priority_map[(x.title, x.versionTitle, x.language)][0])
        versions_by_index = {}
        # organizing by index for the merged case. There is no longer a merged case but keeping this logic b/c it seems fine
        for v in versions:
            key = (v.title, v.language)
            if key in versions_by_index:
                versions_by_index[key] += [v]
            else:
                versions_by_index[key] = [v]
        print("Beginning index of {} versions.".format(len(versions)))
        vcount = 0
        total_versions = len(versions)
        versions = None  # release RAM
        for title, vlist in list(versions_by_index.items()):
            if len(vlist) == 0:
                continue
            cls.curr_index = vlist[0].get_index()
            if for_es:
                cls._bulk_actions = []
                try:
                    cls.best_time_period = cls.curr_index.best_time_period()
                except ValueError:
                    cls.best_time_period = None
            for v in vlist:
                if cls.excluded_from_search(v):
                    print("skipping version")
                    continue

                cls.index_version(v, action=action)
                print("Indexed Version {}/{}".format(vcount, total_versions))
                vcount += 1
            if for_es:
                bulk(es_client, cls._bulk_actions, stats_only=True, raise_on_error=False)

    @classmethod
    def index_version(cls, version, tries=0, action=None):
        if not action:
            action = cls._cache_action
        try:
            version.walk_thru_contents(action, heTref=cls.curr_index.get_title('he'), schema=cls.curr_index.schema, terms_dict=cls.terms_dict)
        except pymongo.errors.AutoReconnect as e:
            # Adding this because there is a mongo call for dictionary words in walk_thru_contents()
            if tries < 200:
                pytime.sleep(5)
                print("Retrying {}. Try {}".format(version.title, tries))
                cls.index_version(version, tries+1)
            else:
                print("Tried {} times to get {}. I have failed you...".format(tries, version.title))
                raise e
        except StopIteration:
            print("Could not find dictionary node in {}".format(version.title))

    @classmethod
    def index_ref(cls, index_name, oref, version_title, lang, language_family_name, is_primary):
        # slower than `cls.index_version` but useful when you don't want the overhead of loading all versions into cache
        cls.index_name = index_name
        cls.curr_index = oref.index
        try:
            cls.best_time_period = cls.curr_index.best_time_period()
        except ValueError:
            cls.best_time_period = None
        version_priority = 0
        hebrew_version_title = None
        for priority, v in enumerate(cls.get_ref_version_list(oref)):
            if v.versionTitle == version_title:
                version_priority = priority
                hebrew_version_title = getattr(v, 'versionTitleInHebrew', None)
        content = TextChunk(oref, lang, vtitle=version_title).ja().flatten_to_string()
        categories = cls.curr_index.categories
        tref = oref.normal()
        doc = cls.make_text_index_document(tref, oref.he_normal(), version_title, lang, version_priority, content, categories, hebrew_version_title, language_family_name, is_primary)
        id = make_text_doc_id(tref, version_title, lang)
        es_client.index(index_name, doc, id=id)

    @classmethod
    def _cache_action(cls, segment_str, tref, heTref, version):
        # Index this document as a whole
        vtitle = version.versionTitle
        vlang = version.language
        language_family_name = version.languageFamilyName
        is_primary = version.isPrimary
        hebrew_version_title = getattr(version, 'versionTitleInHebrew', None)
        try:
            version_priority, categories = cls.version_priority_map[(version.title, vtitle, vlang)]
            #TODO include sgement_str in this func
            doc = cls.make_text_index_document(tref, heTref, vtitle, vlang, version_priority, segment_str, categories, hebrew_version_title, language_family_name, is_primary)
            # print doc
        except Exception as e:
            logger.error("Error making index document {} / {} / {} : {}".format(tref, vtitle, vlang, str(e)))
            return

        if doc:
            try:
                cls._bulk_actions += [
                    {
                        "_index": cls.index_name,
                        "_id": make_text_doc_id(tref, vtitle, vlang),
                        "_source": doc
                    }
                ]
            except Exception as e:
                logger.error("ERROR indexing {} / {} / {} : {}".format(tref, vtitle, vlang, e))

    @classmethod
    def remove_footnotes(cls, content):
        ftnotes = AbstractTextRecord.find_all_itags(content, only_footnotes=True)[1]
        if len(ftnotes) == 0:
            return content
        else:
            for sup_tag in ftnotes:
                i_tag = sup_tag.next_sibling
                content += f" {sup_tag.text} {i_tag.text}"
            content = AbstractTextRecord.strip_itags(content)
            return content

    @classmethod
    def modify_text_in_doc(cls, content):
        content = AbstractTextRecord.strip_imgs(content)
        content = cls.remove_footnotes(content)
        content = strip_cantillation(content, strip_vowels=False).strip()
        content = re.sub(r'<[^>]+>', ' ', content)     # replace HTML tags with space so that words dont get smushed together
        content = re.sub(r'\([^)]+\)', ' ', content)   # remove all parens
        while "  " in content:                                 # make sure there are not many spaces in a row
            content = content.replace("  ", " ")
        return content
        
    @classmethod
    def make_text_index_document(cls, tref, heTref, version, lang, version_priority, content, categories, hebrew_version_title, language_family_name, is_primary):
        """
        Create a document for indexing from the text specified by ref/version/lang
        """
        # Don't bother indexing if there's no content
        if not content:
            return False
        content = cls.modify_text_in_doc(content)
        if len(content) == 0:
            return False

        oref = Ref(tref)

        indexed_categories = get_search_categories(oref, categories)

        tp = cls.best_time_period
        if tp is not None:
            comp_start_date = int(tp.start)
        else:
            comp_start_date = 3000  # far in the future

        ref_data = RefData().load({"ref": tref})
        pagesheetrank = ref_data.pagesheetrank if ref_data is not None else RefData.DEFAULT_PAGESHEETRANK

        return {
            "ref": tref,
            "heRef": heTref,
            "version": version,
            "lang": lang,
            "version_priority": version_priority if version_priority is not None else 1000,
            "titleVariants": oref.index_node.all_tree_titles("en"),
            "categories": indexed_categories,
            "order": oref.order_id(),
            "path": "/".join(indexed_categories + [cls.curr_index.title]),
            "pagesheetrank": pagesheetrank,
            "comp_date": comp_start_date,
            #"hebmorph_semi_exact": content,
            "exact": content,
            "naive_lemmatizer": content,
            'hebrew_version_title': hebrew_version_title,
            "languageFamilyName": language_family_name,
            "isPrimary": is_primary,
        }


def index_sheets_by_timestamp(timestamp):
    """
    :param timestamp str: index all sheets modified after `timestamp` (in isoformat)
    """

    name_dict = get_new_and_current_index_names('sheet', debug=False)
    curr_index_name = name_dict['current']
    try:
        ids = db.sheets.find({"status": "public", "dateModified": {"$gt": timestamp}}).distinct("id")
    except Exception as e:
        print(e)
        return str(e)

    succeeded = []
    failed = []

    for id in ids:
        did_succeed = index_sheet(curr_index_name, id)
        if did_succeed:
            succeeded += [id]
        else:
            failed += [id]

    return {"succeeded": {"num": len(succeeded), "ids": succeeded}, "failed": {"num": len(failed), "ids": failed}}


def index_public_sheets(index_name):
    """
    Index all source sheets that are publicly listed.
    """
    ids = db.sheets.find({"status": "public"}).distinct("id")
    for id in ids:
        index_sheet(index_name, id)


def index_public_notes():
    """
    Index all public notes.

    TODO
    """
    pass


def clear_index(index_name):
    """
    Delete the search index.
    """
    try:
        index_client.delete(index=index_name)
    except Exception as e:
        print("Error deleting Elasticsearch Index named %s" % index_name)
        print(e)


def add_ref_to_index_queue(ref, version, lang):
    """
    Adds a text to index queue to be indexed later.
    """
    qu.IndexQueue({
        "ref": ref,
        "lang": lang,
        "version": version,
        "type": "ref",
    }).save()

    return True


def index_from_queue():
    """
    Index every ref/version/lang found in the index queue.
    Delete queue records on success.
    """
    index_name = get_new_and_current_index_names('text')['current']
    queue = db.index_queue.find()
    for item in queue:
        try:
            TextIndexer.index_ref(index_name, Ref(item["ref"]), item["version"], item["lang"], item['languageFamilyName'], item['isPrimary'])
            db.index_queue.remove(item)
        except Exception as e:
            logging.error("Error indexing from queue ({} / {} / {}) : {}".format(item["ref"], item["version"], item["lang"], e))


def add_recent_to_queue(ndays):
    """
    Look through the last ndays of the activitiy log,
    add to the index queue any refs that had their text altered.
    """
    cutoff = datetime.now() - timedelta(days=ndays)
    query = {
        "date": {"$gt": cutoff},
        "rev_type": {"$in": ["add text", "edit text"]}
    }
    activity = db.history.find(query)
    refs = set()
    for a in activity:
        refs.add((a["ref"], a["version"], a["language"]))
    for ref in list(refs):
        add_ref_to_index_queue(ref[0], ref[1], ref[2])


def get_new_and_current_index_names(type, debug=False):
    base_index_name_dict = {
        'text': SEARCH_INDEX_NAME_TEXT,
        'sheet': SEARCH_INDEX_NAME_SHEET,
    }
    index_name_a = "{}-a{}".format(base_index_name_dict[type], '-debug' if debug else '')
    index_name_b = "{}-b{}".format(base_index_name_dict[type], '-debug' if debug else '')
    alias_name = "{}{}".format(base_index_name_dict[type], '-debug' if debug else '')
    aliases = index_client.get_alias()
    try:
        a_alias = aliases[index_name_a]['aliases']
        choose_a = alias_name not in a_alias
    except KeyError:
        choose_a = True

    if choose_a:
        new_index_name = index_name_a
        old_index_name = index_name_b
    else:
        new_index_name = index_name_b
        old_index_name = index_name_a
    return {"new": new_index_name, "current": old_index_name, "alias": alias_name}


def index_all(skip=0, debug=False):
    """
    Fully create the search index from scratch.
    """
    start = datetime.now()
    index_all_of_type('text', skip=skip, debug=debug)
    index_all_of_type('sheet', skip=skip, debug=debug)
    end = datetime.now()
    db.index_queue.delete_many({})  # index queue is now stale
    print("Elapsed time: %s" % str(end-start))


def index_all_of_type(type, skip=0, debug=False):
    index_names_dict = get_new_and_current_index_names(type=type, debug=debug)
    print('CREATING / DELETING {}'.format(index_names_dict['new']))
    print('CURRENT {}'.format(index_names_dict['current']))
    for i in range(10):
        print('STARTING IN T-MINUS {}'.format(10 - i))
        pytime.sleep(1)

    index_all_of_type_by_index_name(type, index_names_dict['new'], skip, debug)

    try:
        #index_client.put_settings(index=index_names_dict['current'], body={"index": { "blocks": { "read_only_allow_delete": False }}})
        index_client.delete_alias(index=index_names_dict['current'], name=index_names_dict['alias'])
        print("Successfully deleted alias {} for index {}".format(index_names_dict['alias'], index_names_dict['current']))
    except NotFoundError:
        print("Failed to delete alias {} for index {}".format(index_names_dict['alias'], index_names_dict['current']))

    clear_index(index_names_dict['alias']) # make sure there are no indexes with the alias_name

    #index_client.put_settings(index=index_names_dict['new'], body={"index": { "blocks": { "read_only_allow_delete": False }}})
    index_client.put_alias(index=index_names_dict['new'], name=index_names_dict['alias'])

    if index_names_dict['new'] != index_names_dict['current']:
        clear_index(index_names_dict['current'])


def index_all_of_type_by_index_name(type, index_name, skip=0, debug=False):
    if skip == 0:
        create_index(index_name, type)
    if type == 'text':
        TextIndexer.clear_cache()
        TextIndexer.index_all(index_name, debug=debug)
    elif type == 'sheet':
        index_public_sheets(index_name)
```

### sefaria/tracker.py

```
"""
Object history tracker
Accepts change requests for model objects, passes the changes to the models, and records the changes in history

"""
from functools import reduce

import structlog
logger = structlog.get_logger(__name__)

import sefaria.model as model
from sefaria.system.exceptions import InputError
try:
    from sefaria.settings import USE_VARNISH
except ImportError:
    USE_VARNISH = False
if USE_VARNISH:
    from sefaria.system.varnish.wrapper import invalidate_ref, invalidate_linked


def modify_text(user, oref, vtitle, lang, text, vsource=None, **kwargs):
    """
    Updates a chunk of text, identified by oref, versionTitle, and lang, and records history.
    :param user:
    :param oref:
    :param vtitle:
    :param lang:
    :param text:
    :param vsource:
    :return:
    """
    chunk = model.TextChunk(oref, lang, vtitle)
    if getattr(chunk.version(), "status", "") == "locked" and not model.user_profile.is_user_staff(user):
        raise InputError("This text has been locked against further edits.")
    action = kwargs.get("type") or "edit" if chunk.text else "add"
    old_text = chunk.text
    chunk.text = text
    if vsource:
        chunk.versionSource = vsource  # todo: log this change
    if chunk.save():
        kwargs['skip_links'] = kwargs.get('skip_links', False) or chunk.has_manually_wrapped_refs()
        post_modify_text(user, action, oref, lang, vtitle, old_text, chunk.text, chunk.full_version._id, **kwargs)

    return chunk


def modify_bulk_text(user: int, version: model.Version, text_map: dict, vsource=None, **kwargs) -> dict:
    """
    user: user ID of user making modification
    version: version object of text being modified
    text_map: dict with segment ref keys and text values. Each key/value pair represents a segment that should be modified. Segments that don't have changes will be ignored. The key should be the tref, and the value the text, ex: {'Mishnah Berakhot 1:1': 'Text of the Mishnah goes here'}
    vsource: optional parameter to set the version source of the version. not sure why this is here. I copied it from modify_text.
    """
    def populate_change_map(old_text, en_tref, he_tref, _):
        nonlocal change_map, existing_tref_set
        existing_tref_set.add(en_tref)
        new_text = text_map.get(en_tref, None)
        if new_text is None or new_text == old_text:
            return
        change_map[en_tref] = (old_text, new_text, model.Ref(en_tref))
    change_map = {}
    existing_tref_set = set()
    version.walk_thru_contents(populate_change_map)
    new_ref_set = set(text_map.keys()).difference(existing_tref_set)
    for new_tref in new_ref_set:
        if len(text_map[new_tref].strip()) == 0:
            # this ref doesn't exist for this version. probably exists in a different version
            # no reason to add to change_map if it has not content
            continue
        change_map[new_tref] = ('', text_map[new_tref], model.Ref(new_tref))

    if vsource:
        version.versionSource = vsource  # todo: log this change

    # modify version in place
    error_map = {}
    for _, new_text, oref in change_map.values():
        try:
            version.sub_content_with_ref(oref, new_text)
        except Exception as e:
            error_map[oref.normal()] = f"Ref doesn't match schema of version. Exception: {repr(e)}"
    version.save()

    for old_text, new_text, oref in change_map.values():
        if oref.normal() in error_map: continue
        kwargs['skip_links'] = kwargs.get('skip_links', False) or getattr(version, 'hasManuallyWrappedRefs', False)
        # hard-code `count_after` to False here. It will be called later on the whole index once
        # (which is all that's necessary)
        kwargs['count_after'] = False
        post_modify_text(user, kwargs.get("type"), oref, version.language, version.versionTitle, old_text, new_text, version._id, **kwargs)

    count_segments(version.get_index())
    return error_map


def modify_version(user: int, version_dict: dict, patch=True, **kwargs):

    def modify_node(jagged_array_node):
        address = jagged_array_node.address()[1:]  # first element is the index name
        new_content = reduce(lambda x, y: x.get(y, {}), address, version_dict['chapter'])
        if is_version_new:
            old_content = []
        else:
            old_content = reduce(lambda x, y: x.setdefault(y, {}), address, version.chapter) or []
        if (patch and new_content == {}) or old_content == new_content:
            return
        new_content = new_content or []
        if not is_version_new:
            if address:
                reduce(lambda x, y: x[y], address[:-1], version.chapter)[address[-1]] = new_content
            else:
                version.chapter = new_content
        action = 'add' if not old_content else 'edit'
        changing_texts.append({'action': action, 'oref': jagged_array_node.ref(), 'old_text': old_content, 'curr_text': new_content})

    index_title = version_dict['title']
    lang = version_dict['language']
    version_title = version_dict['versionTitle']
    version = model.Version().load({'title': index_title, 'versionTitle': version_title, 'language': lang})
    changing_texts = []
    if version:
        is_version_new = False
        if not patch:
            for attr in model.Version.required_attrs + model.Version.optional_attrs:
                if hasattr(version, attr) and attr != 'chapter':
                    delattr(version, attr)
        for key, value in version_dict.items():
            if key == 'chapter':
                continue
            else:
                setattr(version, key, value)
    else:
        is_version_new = True
        version = model.Version(version_dict)
    model.Ref(index_title).index_node.visit_content(modify_node)
    version.save()

    for change in changing_texts:
        post_modify_text(user, change['action'], change['oref'], lang, version_title, change['old_text'], change['curr_text'], version._id, **kwargs)
    count_segments(version.get_index())


def post_modify_text(user, action, oref, lang, vtitle, old_text, curr_text, version_id, **kwargs) -> None:
    model.log_text(user, action, oref, lang, vtitle, old_text, curr_text, **kwargs)
    if USE_VARNISH:
        invalidate_ref(oref, lang=lang, version=vtitle, purge=True)
        if oref.next_section_ref():
            invalidate_ref(oref.next_section_ref(), lang=lang, version=vtitle, purge=True)
        if oref.prev_section_ref():
            invalidate_ref(oref.prev_section_ref(), lang=lang, version=vtitle, purge=True)
    if not kwargs.get("skip_links", None):
        from sefaria.helper.link import add_links_from_text
        # Some commentaries can generate links to their base text automatically
        linker = oref.autolinker(user=user)
        if linker:
            linker.refresh_links(**kwargs)
        # scan text for links to auto add
        add_links_from_text(oref, lang, curr_text, version_id, user, **kwargs)

        if USE_VARNISH:
            invalidate_linked(oref)
    # rabbis_move(oref, vtitle)
    count_and_index(oref, lang, vtitle, to_count=kwargs.get("count_after", 1))


def count_and_index(oref, lang, vtitle, to_count=1):
    from sefaria.settings import SEARCH_INDEX_ON_SAVE

    # count available segments of text
    if to_count:
        count_segments(oref.index)
    
    if SEARCH_INDEX_ON_SAVE:
        model.IndexQueue({
            "ref": oref.normal(),
            "lang": lang,
            "version": vtitle,
            "type": "ref",
        }).save()


def count_segments(index):
    from sefaria.settings import MULTISERVER_ENABLED
    from sefaria.system.multiserver.coordinator import server_coordinator

    model.library.recount_index_in_toc(index)
    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "recount_index_in_toc", [index.title])


def add(user, klass, attrs, **kwargs):
    """
    Creates a new instance, saves it, and records the history
    :param klass: The class we are instantiating
    :param attrs: Dictionary with the attributes of the class that we are instantiating
    :param user:  Integer user id
    :return:
    """
    assert issubclass(klass, model.abstract.AbstractMongoRecord)
    obj = None
    if getattr(klass, "criteria_override_field", None) and attrs.get(klass.criteria_override_field):
        obj = klass().load({klass.criteria_field: attrs[klass.criteria_override_field]})
    elif attrs.get(klass.criteria_field):
        if klass.criteria_field == klass.id_field:  # a clumsy way of pushing _id through ObjectId
            obj = klass().load_by_id(attrs[klass.id_field])
        else:
            obj = klass().load({klass.criteria_field: attrs[klass.criteria_field]})
    if obj:
        old_dict = obj.contents(**kwargs)
        obj.load_from_dict(attrs).save()
        model.log_update(user, klass, old_dict, obj.contents(**kwargs), **kwargs)
        return obj
    obj = klass(attrs).save()
    model.log_add(user, klass, obj.contents(**kwargs), **kwargs)
    return obj


def update(user, klass, attrs, **kwargs):
    assert issubclass(klass, model.abstract.AbstractMongoRecord)
    if getattr(klass, "criteria_override_field", None) and attrs.get(klass.criteria_override_field):
        obj = klass().load({klass.criteria_field: attrs[klass.criteria_override_field]})
    else:
        if klass.criteria_field == klass.id_field:  # a clumsy way of pushing _id through ObjectId
            obj = klass().load_by_id(attrs[klass.id_field])
        else:
            obj = klass().load({klass.criteria_field: attrs[klass.criteria_field]})
    old_dict = obj.contents(**kwargs)
    obj.load_from_dict(attrs).save()
    model.log_update(user, klass, old_dict, obj.contents(**kwargs), **kwargs)
    return obj


def delete(user, klass, _id, **kwargs):
    """
    :param user:
    :param klass:
    :param _id:
    :param kwargs:
        "callback" - an optional function that will be run on the object before it's deleted
        All other kwargs are passed to obj.contents()
    :return:
    """
    obj = klass().load_by_id(_id)
    if obj is None:
        return {'error': 'item with id: {} not found'.format(_id)}
    if kwargs.get("callback"):
        kwargs.get("callback")(obj)
        del kwargs["callback"]
    old_dict = obj.contents(**kwargs)
    obj.delete()
    model.log_delete(user, klass, old_dict, **kwargs)
    return {"response": "ok"}


```

### sefaria/pagesheetrank.py

```
# implementation of pagerank with low ram requirements
# source: http://michaelnielsen.org/blog/using-your-laptop-to-compute-pagerank-for-millions-of-webpages/

import re
import math
import numpy
import random
import json
import time
from pymongo.errors import AutoReconnect
from collections import defaultdict, OrderedDict
from sefaria.model import *
from sefaria.system.exceptions import InputError, NoVersionFoundError
from sefaria.system.database import db
from .settings import STATICFILES_DIRS
from functools import reduce

tanach_indexes = set(library.get_indexes_in_category("Tanakh"))


class web:
    def __init__(self, n):
        self.size = n
        self.in_links = {}
        self.number_out_links = {}
        self.dangling_pages = {}
        for j in range(n):
            self.in_links[j] = []
            self.number_out_links[j] = 0
            self.dangling_pages[j] = True


def paretosample(n, power=2.0):
    '''Returns a sample from a truncated Pareto distribution
  with probability mass function p(l) proportional to
  1/l^power.  The distribution is truncated at l = n.'''
    m = n + 1
    while m > n: m = numpy.random.zipf(power)
    return m


def random_web(n=1000, power=2.0):
    '''Returns a web object with n pages, and where each
  page k is linked to by L_k random other pages.  The L_k
  are independent and identically distributed random
  variables with a shifted and truncated Pareto
  probability mass function p(l) proportional to
  1/(l+1)^power.'''
    g = web(n)
    for k in range(n):
        lk = paretosample(n + 1, power) - 1
        values = random.sample(range(n), lk)
        g.in_links[k] = values
        for j in values:
            if g.number_out_links[j] == 0: g.dangling_pages.pop(j)
            g.number_out_links[j] += 1
    return g


def create_empty_nodes(g):
    all_links = set(reduce(lambda a, b: a + b, [list(v.keys()) for v in list(g.values())], []))
    for l in all_links:
        if l not in g:
            g[l] = {}
    return g


def create_web(g):
    n = len(g)
    w = web(n)
    node2index = {r: i for i, r in enumerate([x[0] for x in g])}
    for i, (r, links) in enumerate(g):
        r_ind = node2index[r]
        link_inds = [(node2index[r_temp], count) for r_temp, count in list(links.items())]
        w.in_links[r_ind] = reduce(lambda a, b: a + [b[0]] * int(round(b[1])), link_inds, [])
        for j, count in link_inds:
            if w.number_out_links[j] == 0: w.dangling_pages.pop(j)
            w.number_out_links[j] += count
    return w


def step(w, p, s=0.85):
    '''Performs a single step in the PageRank computation,
    with web g and parameter s.  Applies the corresponding M
    matrix to the vector p, and returns the resulting
    vector.'''
    n = w.size
    v = numpy.matrix(numpy.zeros((n, 1)))
    inner_product = sum(p[j] for j in w.dangling_pages.keys())
    for j in range(n):
        v[j] = s * sum([p[k] / w.number_out_links[k]
                        for k in w.in_links[j]]) + s * inner_product / n + (1 - s) / n
    # We rescale the return vector, so it remains a
    # probability distribution even with floating point
    # roundoff.
    return v / numpy.sum(v)


def pagerank(g, s=0.85, tolerance=0.00001, maxiter=100, verbose=False, normalize=False):
    w = create_web(g)
    n = w.size
    p = numpy.matrix(numpy.ones((n, 1))) / n
    iteration = 1
    change = 2
    while change > tolerance and iteration < maxiter:
        if verbose:
            print("Iteration: %s" % iteration)
        new_p = step(w, p, s)
        change = numpy.sum(numpy.abs(p - new_p))
        if verbose:
            print("Change in l1 norm: %s" % change)
        p = new_p
        iteration += 1
    if normalize:
        # This is interesting and nerdy, but min seems to do the exact same thing
        # dangling_pr_sum = sum(p[j] for j in w.dangling_pages.keys())
        # norm_factor = ((1-s) + s*dangling_pr_sum)/w.size  # see: https://www2007.org/posters/poster893.pdf
        # p /= norm_factor
        try:
            p /= p.min()
        except ValueError:
            pass  # empty list can't calculate min
    pr_list = list(numpy.squeeze(numpy.asarray(p)))
    return {k: v for k, v in zip([x[0] for x in g], pr_list)}


def has_intersection(a, b):
    for temp_a in a:
        if temp_a in b:
            return True
    return False


def init_pagerank_graph(ref_list=None):
    """
    :param ref_list: optional list of refs to use instead of using all links. link graph is built from all links to these refs
    :return: graph which is a double dict. the keys of both dicts are refs. the values are the number of incoming links
    between outer key and inner key
    """

    def is_tanach(r):
        return r.index.title in tanach_indexes

    def recursively_put_in_graph(ref1, ref2, weight=1.0):
        if ref1.is_section_level():
            return  # ignore section level
            seg_refs = ref1.all_segment_refs()
            for ref1_seg in seg_refs:
                recursively_put_in_graph(ref1_seg, ref2, weight / len(seg_refs))
        elif ref2.is_section_level():
            return  # ignore section level
            seg_refs = ref2.all_segment_refs()
            for ref2_seg in seg_refs:
                recursively_put_in_graph(ref1, ref2_seg, weight / len(seg_refs))
        elif ref1.is_range():
            for ref1_seg in ref1.range_list():
                if ref2.is_range():
                    for ref2_seg in ref2.range_list():
                        recursively_put_in_graph(ref1_seg, ref2_seg)
                else:
                    recursively_put_in_graph(ref1_seg, ref2)
        else:
            put_link_in_graph(ref1, ref2, weight)

    def put_link_in_graph(ref1, ref2, weight=1.0):
        str1 = ref1.normal()
        str2 = ref2.normal()
        if str1 not in all_ref_cat_counts:
            all_ref_cat_counts[str1] = set()
        if str2 not in all_ref_cat_counts:
            all_ref_cat_counts[str2] = set()
        # not a typo. add the cat of ref2 to ref1
        all_ref_cat_counts[str1].add(ref2.primary_category)
        all_ref_cat_counts[str2].add(ref1.primary_category)

        if str1 not in graph:
            graph[str1] = {}

        if str2 == str1 or (is_tanach(ref1) and is_tanach(ref2)):
            # self link
            return

        if str2 not in graph[str1]:
            graph[str1][str2] = 0
        graph[str1][str2] += weight

    graph = OrderedDict()
    if ref_list is None:
        all_links = LinkSet()  # LinkSet({"type": re.compile(ur"(commentary|quotation)")}).array()
        len_all_links = all_links.count()
    else:
        link_list = []
        ref_list_seg_set = {rr.normal() for r in ref_list for rr in r.all_segment_refs()}
        for oref in ref_list:
            link_list += list(filter(lambda x: has_intersection(x.expandedRefs0, ref_list_seg_set) and has_intersection(x.expandedRefs1, ref_list_seg_set), oref.linkset()))
        len_all_links = len(link_list)
        all_links = LinkSet()
        all_links.records = link_list
    all_ref_cat_counts = {}
    current_link, page, link_limit = 0, 0, 100000
    if ref_list is None:
        all_links = LinkSet(limit=link_limit, page=page)

    while len(all_links.array()) > 0:
        for link in all_links:  # raw records avoids caching the entire LinkSet into memory
            if current_link % 1000 == 0 and current_link > 0:
                print("{}/{}".format(current_link, len_all_links))

            try:
                # TODO pagerank segments except Talmud. Talmud is pageranked by section
                # TODO if you see a section link, add pagerank to all of its segments
                refs = [Ref(r) for r in link.refs]
                tp1 = refs[0].index.best_time_period()
                tp2 = refs[1].index.best_time_period()
                start1 = int(tp1.determine_year_estimate()) if tp1 else 3000
                start2 = int(tp2.determine_year_estimate()) if tp2 else 3000

                older_ref, newer_ref = (refs[0], refs[1]) if start1 < start2 else (refs[1], refs[0])

                temp_links = []
                older_ref = older_ref.padded_ref()
                newer_ref = newer_ref.padded_ref()
                if start1 == start2:
                    if ref_list is not None:
                        continue  # looks like links at the same time span can cause a big increase in PR. I'm going to disable this right now for small graphs
                    # randomly switch refs that are equally dated
                    older_ref, newer_ref = (older_ref, newer_ref) if random.choice([True, False]) else (
                    newer_ref, older_ref)
                recursively_put_in_graph(older_ref, newer_ref)

            except InputError:
                pass
            except TypeError as e:
                print("TypeError")
                print(link.refs)
            except IndexError:
                pass
            except AssertionError:
                pass
            except ValueError:
                print("ValueError")
                print(link.refs)
                pass
            current_link += 1
        if ref_list is None:
            page += 1
            all_links = LinkSet(limit=link_limit, page=page)
        else:
            break

    for ref in all_ref_cat_counts:
        if ref not in graph:
            graph[ref] = {}

    return graph, all_ref_cat_counts


def pagerank_rank_ref_list(ref_list, normalize=False, seg_ref_map=None):
    """
    :param seg_ref_map: dict with keys that are ranged refs and values are list of segment trefs. pass in order to save from recomputing within this function
    """
    # make unique
    ref_list = [v for k, v in {r.normal(): r for r in ref_list}.items()]
    graph, all_ref_cat_counts = init_pagerank_graph(ref_list)
    pr = pagerank(list(graph.items()), 0.85, verbose=False, tolerance=0.00005, normalize=normalize)

    if not normalize:
        # remove lowest pr value which just means it quoted at least one source but was never quoted
        sorted_ranking = sorted(list(pr.items()), key=lambda x: x[1])
        count = 0
        if len(sorted_ranking) > 0:
            smallest_pr = sorted_ranking[0][1]
            while count < len(sorted_ranking) and (sorted_ranking[count][1] - smallest_pr) < 1e-30:
                count += 1
            if count < len(sorted_ranking) - 1:
                pr = {r: temp_pr for r, temp_pr in sorted_ranking[count:]}
    # map pr values onto ref_list
    if seg_ref_map is None:
        seg_ref_map = {r.normal(): [rr.normal() for rr in r.all_segment_refs()] for r in ref_list}
    # TODO do we always want to choose max segment PR over the range? maybe average is better?
    ref_list_with_pr = sorted([
        (r, max([pr.get(rr, 0.0) for rr in seg_ref_map[r.normal()]])) if len(seg_ref_map[r.normal()]) > 0 else (r, 0.0) for r in
        ref_list
    ], key=lambda x: x[1], reverse=True)
    return ref_list_with_pr


def calculate_pagerank():
    graph, all_ref_cat_counts = init_pagerank_graph()
    # json.dump(graph.items(), open("{}pagerank_graph3.json".format(STATICFILES_DIRS[0]), "wb"))
    ranked = pagerank(list(graph.items()), 0.85, verbose=True, tolerance=0.00005)
    sorted_ranking = sorted(list(dict(ranked).items()), key=lambda x: x[1])
    count = 0
    smallest_pr = sorted_ranking[0][1]
    while count < len(sorted_ranking) and (sorted_ranking[count][1] - smallest_pr) < 1e-30:
        count += 1
    sorted_ranking = sorted_ranking[count:]
    print("Removing {} low pageranks".format(count))

    pagerank_dict = {tref: pr for tref, pr in sorted_ranking}
    return pagerank_dict


def update_pagesheetrank():
    pagerank = calculate_pagerank()
    sheetrank = calculate_sheetrank()
    pagesheetrank = {}
    all_trefs = set(list(pagerank.keys()) + list(sheetrank.keys()))
    for tref in all_trefs:
        temp_pagerank_scaled = math.log(pagerank[tref]) + 20 if tref in pagerank else RefData.DEFAULT_PAGERANK
        temp_sheetrank_scaled = (1.0 + sheetrank[tref] / 5) ** 2 if tref in sheetrank else RefData.DEFAULT_SHEETRANK
        pagesheetrank[tref] = temp_pagerank_scaled * temp_sheetrank_scaled
    from pymongo import UpdateOne
    result = db.ref_data.bulk_write([
        UpdateOne({"ref": tref}, {"$set": {"pagesheetrank": psr}}, upsert=True) for tref, psr in
        list(pagesheetrank.items())
    ])


def cat_bonus(num_cats):
    return 1.0 + (0.04 * num_cats)


def length_penalty(l):
    # min of about -4.4
    # penalize segments less than minLen characters
    minLen, maxLen = 10, 1000
    l = l if l <= maxLen else maxLen
    penalty = 1000 * math.log(1.0 / (-l + 10000)) - math.log(10000 - minLen)
    return -penalty if l <= minLen else penalty


def test_pagerank(a, b):
    g = {
        "a": {
            "b": 0.7
        },
        "b": {
            "c": 0.1
        },
        "c": {}
    }
    ranked = pagerank(g, a, verbose=True, tolerance=b)
    print(ranked)


def get_all_sheets(tries=0, page=0):
    limit = 1000
    has_more = True
    while has_more:
        try:
            temp_sheets = list(db.sheets.find().skip(page * limit).limit(limit))
        except AutoReconnect as e:
            tries += 1
            if tries >= 200:
                print("Tried: {} times".format(tries))
                raise e
            time.sleep(5)
            continue
        has_more = False
        for s in temp_sheets:
            has_more = True
            yield s
        page += 1


def calculate_sheetrank():
    def count_sources(sources):
        temp_sources_count = 0
        for s in sources:
            if "ref" in s and s["ref"] is not None:
                temp_sources_count += 1
                try:
                    oref = Ref(s["ref"])
                    if oref.is_range():
                        oref = oref.range_list()[0]

                    ref_list = []
                    if oref.is_section_level():
                        ref_list = oref.all_subrefs()
                    elif oref.is_segment_level():
                        ref_list = [oref]
                    else:
                        pass

                    for r in ref_list:
                        sheetrank_dict[r.normal()] += 1
                except (InputError, TypeError, AssertionError, KeyError, AttributeError, IndexError):
                    continue

            if "subsources" in s:
                temp_sources_count += count_sources(s["subsources"])
        return temp_sources_count

    sheetrank_dict = defaultdict(int)
    len_sheets = db.sheets.find().count()
    sheets = get_all_sheets()
    sources_count = 0
    for i, sheet in enumerate(sheets):
        if i % 1000 == 0:
            print("{}/{}".format(i, len_sheets))
        if "sources" not in sheet:
            continue
        sources_count += count_sources(sheet["sources"])

    return sheetrank_dict

```

### sefaria/settings_utils.py

```
from http.client import HTTPException
import sentry_sdk
from sentry_sdk.integrations.django import DjangoIntegration
import os

def init_sentry(sentry_dsn, sentry_code_version, sentry_environment):

    def before_send(event, hint):
        # Check if the event has an exception
        if 'exc_info' in hint:
            exc_type, exc_value, tb = hint['exc_info']
            # If it is not an HTTP 500 error, drop the event
            if isinstance(exc_value, HTTPException) and exc_value.code != 500:
                return None

        # If there is no exception, drop the event
        if 'exception' not in event:
            return None

        return event

    sentry_sdk.init(
        dsn=sentry_dsn,
        environment=sentry_environment,
        integrations=[DjangoIntegration()],
        traces_sample_rate=0.01,
        profiles_sample_rate=0.01,
        send_default_pii=False,
        before_send=before_send,
        max_breadcrumbs=30,
        release=sentry_code_version,
        _experiments={
            "continuous_profiling_auto_start": True,
        },
    )
```

### sefaria/urls.py

```
# -*- coding: utf-8 -*-
from rest_framework_simplejwt.views import TokenObtainPairView, TokenRefreshView
from functools import partial
from django.conf.urls import include, url
from django.conf.urls import handler404, handler500
from django.contrib import admin
from django.http import HttpResponseRedirect
import django.contrib.auth.views as django_auth_views
from sefaria.forms import SefariaPasswordResetForm, SefariaSetPasswordForm, SefariaLoginForm
from sefaria.settings import DOWN_FOR_MAINTENANCE, STATIC_URL, ADMIN_PATH

import reader.views as reader_views
import sefaria.views as sefaria_views
import sourcesheets.views as sheets_views
import sefaria.gauth.views as gauth_views
import django.contrib.auth.views as django_auth_views
import api.views as api_views
import guides.views as guides_views

from sefaria.site.urls import site_urlpatterns

admin.autodiscover()
handler500 = 'reader.views.custom_server_error'
handler404 = 'reader.views.custom_page_not_found'


# App Pages
urlpatterns = [
    url(r'^$', reader_views.home, name="home"),
    url(r'^texts/?$', reader_views.texts_list, name="table_of_contents"),
    url(r'^texts/saved/?$', reader_views.saved),
    url(r'^texts/history/?$', reader_views.user_history),
    url(r'^texts/recent/?$', reader_views.old_recent_redirect),
    url(r'^texts/(?P<cats>.+)?$', reader_views.texts_category_list),
    url(r'^search/?$', reader_views.search),
    url(r'^search-autocomplete-redirecter/?$', reader_views.search_autocomplete_redirecter),
    url(r'^calendars/?$', reader_views.calendars),
    url(r'^collections/?$', reader_views.public_collections),
    url(r'^collections/new$', reader_views.edit_collection_page),
    url(r'^collections/(?P<slug>[^.]+)/settings$', reader_views.edit_collection_page),
    url(r'^collections/(?P<slug>[^.]+)$', reader_views.collection_page),
    url(r'^translations/(?P<slug>[^.]+)$', reader_views.translations_page),
    url(r'^community/?$', reader_views.community_page),
    url(r'^notifications/?$', reader_views.notifications),
    url(r'^modtools/?$', reader_views.modtools),
    url(r'^modtools/upload_text$', sefaria_views.modtools_upload_workflowy),
    url(r'^modtools/links$', sefaria_views.links_upload_api),
    url(r'^modtools/links/(?P<tref1>.+)/(?P<tref2>.+)$', sefaria_views.get_csv_links_by_refs_api),
    url(r'^modtools/index_links/(?P<tref1>.+)/(?P<tref2>.+)$', partial(sefaria_views.get_csv_links_by_refs_api, by_segment=True)),
    url(r'^torahtracker/?$', reader_views.user_stats),
]

# People Pages
urlpatterns += [
    url(r'^person/(?P<name>.+)$', reader_views.person_page_redirect),

    url(r'^people/Talmud/?$', reader_views.talmud_person_index_redirect),
    url(r'^people/?$', reader_views.person_index_redirect),
]

# Visualizations / Link Explorer
urlpatterns += [
    url(r'^explore(-(?P<topCat>[\w-]+)-and-(?P<bottomCat>[\w-]+))?(/(?P<book1>[A-Za-z-,\']+))?(/(?P<book2>[A-Za-z-,\']+))?(/(?P<lang>\w\w)/?)?/?$', reader_views.explore),
    url(r'^visualize/library/(?P<lang>[enh]*)/?(?P<cats>.*)/?$', reader_views.visualize_library),
    url(r'^visualize/library/?(?P<cats>.*)/?$', reader_views.visualize_library),
    url(r'^visualize/toc$', reader_views.visualize_toc),
    url(r'^visualize/parasha-colors$', reader_views.visualize_parasha_colors),
    url(r'^visualize/links-through-rashi$', reader_views.visualize_links_through_rashi),
    url(r'^visualize/talmudic-relationships$', reader_views.talmudic_relationships),
    url(r'^visualize/sefer-hachinukh-mitzvot$', reader_views.sefer_hachinukh_mitzvot),
    url(r'^visualize/timeline$', reader_views.visualize_timeline),
    url(r'^visualize/unique-words-by-commentator', reader_views.unique_words_viz),
]

# Source Sheet Builder
urlpatterns += [
    url(r'^sheets/new/?$', sheets_views.new_sheet),
    url(r'^sheets/(?P<sheet_id>\d+)$', sheets_views.view_sheet),
    url(r'^sheets/visual/(?P<sheet_id>\d+)$', sheets_views.view_visual_sheet),
]

# Profiles & Settings
urlpatterns += [
    url(r'^my/profile', reader_views.my_profile),
    url(r'^profile/(?P<username>[^/]+)/?$', reader_views.user_profile),
    url(r'^settings/account?$', reader_views.account_settings),
    url(r'^settings/profile?$', reader_views.edit_profile),
    url(r'^settings/account/user$', reader_views.account_user_update),
    url(r'^interface/(?P<language>english|hebrew)$', reader_views.interface_language_redirect),
    url(r'^api/profile/user_history$', reader_views.user_history_api),
    url(r'^api/profile/sync$', reader_views.profile_sync_api),
    url(r'^api/profile/upload-photo$', reader_views.profile_upload_photo),
    url(r'^api/profile$', reader_views.profile_api),
    url(r'^api/profile/(?P<slug>[^/]+)$', reader_views.profile_api),
    url(r'^api/profile/(?P<slug>[^/]+)/(?P<ftype>followers|following)$', reader_views.profile_follow_api),
    url(r'^api/user_history/saved$', reader_views.saved_history_for_ref),
]

# Topics
urlpatterns += [
    url(r'^topics/category/(?P<topicCategory>.+)?$', reader_views.topics_category_page),
    url(r'^topics/all/(?P<letter>.)$', reader_views.all_topics_page),
    url(r'^topics/?$', reader_views.topics_page),
    url(r'^topics/b/(?P<topic>.+)$', reader_views.topic_page_b),
    url(r'^topics/(?P<topic>.+)$', reader_views.topic_page),
    url(r'^_api/topics/images/secondary/(?P<slug>.+)$', reader_views.topic_upload_photo, {"secondary": True}),
    url(r'^_api/topics/images/(?P<slug>.+)$', reader_views.topic_upload_photo)

]

# Calendar Redirects
urlpatterns += [
    url(r'^parashat-hashavua$', reader_views.parashat_hashavua_redirect),
    url(r'^todays-daf-yomi$', reader_views.daf_yomi_redirect),
]

# Texts Add / Edit / Translate
urlpatterns += [
    url(r'^add/textinfo/(?P<new_title>.+)$', reader_views.edit_text_info),
    url(r'^add/new/?$', reader_views.edit_text),
    url(r'^add/(?P<ref>.+)$', reader_views.edit_text),
    url(r'^translate/(?P<ref>.+)$', reader_views.edit_text),
    url(r'^edit/terms/(?P<term>.+)$', reader_views.terms_editor),
    url(r'^add/terms/(?P<term>.+)$', reader_views.terms_editor),
    url(r'^edit/(?P<ref>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.edit_text),
    url(r'^edit/(?P<ref>.+)$', reader_views.edit_text),
]

# Redirects for legacy URLs
urlpatterns += [
    url(r'^new-home/?$', reader_views.new_home_redirect),
    url(r'^account/?$', reader_views.my_profile),
    url(r'^my/notes/?$', reader_views.my_notes_redirect),
    url(r'^sheets/tags/?$', reader_views.topics_redirect),
    url(r'^sheets/tags/(?P<tag>.+)$', reader_views.topic_page_redirect),
    url(r'^sheets/(?P<type>(public|private))/?$', reader_views.sheets_pages_redirect),
    url(r'^groups/?(?P<group>[^/]+)?$', reader_views.groups_redirect),
    url(r'^contributors/(?P<username>[^/]+)(/(?P<page>\d+))?$', reader_views.profile_redirect),
]

# Texts / Index / Links etc API
urlpatterns += [
    url(r'^api/texts/versions/(?P<tref>.+)$', reader_views.versions_api),
    url(r'^api/texts/version-status/tree/?(?P<lang>.*)?/?$', reader_views.version_status_tree_api),
    url(r'^api/texts/version-status/?$', reader_views.version_status_api),
    url(r'^api/texts/parashat_hashavua$', reader_views.parashat_hashavua_api),
    url(r'^api/texts/translations/?$', reader_views.translations_api),
    url(r'^api/texts/translations/(?P<lang>.+)', reader_views.translations_api),
    url(r'^api/texts/random?$', reader_views.random_text_api),
    url(r'^api/texts/random-by-topic/?$', reader_views.random_by_topic_api),
    url(r'^api/texts/modify-bulk/(?P<title>.+)$', reader_views.modify_bulk_text_api),
    url(r'^api/texts/(?P<tref>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.old_text_versions_api_redirect),
    url(r'^api/texts/(?P<tref>.+)$', reader_views.texts_api),
    url(r'^api/versions/?$', reader_views.complete_version_api),
    url(r'^api/v3/texts/(?P<tref>.+)$', api_views.Text.as_view()),
    url(r'^api/index/?$', reader_views.table_of_contents_api),
    url(r'^api/opensearch-suggestions/?$', reader_views.opensearch_suggestions_api),
    url(r'^api/index/titles/?$', reader_views.text_titles_api),
    url(r'^api/v2/raw/index/(?P<title>.+)$', reader_views.index_api, {'raw': True}),
    url(r'^api/v2/index/(?P<title>.+)$', reader_views.index_api),
    url(r'^api/index/(?P<title>.+)$', reader_views.index_api),
    url(r'^api/links/bare/(?P<book>.+)/(?P<cat>.+)$', reader_views.bare_link_api),
    url(r'^api/links/(?P<link_id_or_ref>.*)$', reader_views.links_api),
    url(r'^api/link-summary/(?P<ref>.+)$', reader_views.link_summary_api),
    url(r'^api/notes/all$', reader_views.all_notes_api),
    url(r'^api/notes/(?P<note_id_or_ref>.*)$', reader_views.notes_api),
    url(r'^api/related/(?P<tref>.*)$', reader_views.related_api),
    url(r'^api/counts/links/(?P<cat1>.+)/(?P<cat2>.+)$', reader_views.link_count_api),
    url(r'^api/counts/words/(?P<title>.+)/(?P<version>.+)/(?P<language>.+)$', reader_views.word_count_api),
    url(r'^api/counts/(?P<title>.+)$', reader_views.counts_api),
    url(r'^api/shape/(?P<title>.+)$', reader_views.shape_api),
    url(r'^api/preview/(?P<title>.+)$', reader_views.text_preview_api),
    url(r'^api/terms/(?P<name>.+)$', reader_views.terms_api),
    url(r'^api/calendars/next-read/(?P<parasha>.+)$', reader_views.parasha_next_read_api),
    url(r'^api/calendars/?$', reader_views.calendars_api),
    url(r'^api/calendars/topics/parasha/?$', reader_views.parasha_data_api),
    url(r'^api/calendars/topics/holiday/?$', reader_views.next_holiday),
    url(r'^api/name/(?P<name>.+)$', reader_views.name_api),
    url(r'^api/category/?(?P<path>.+)?$', reader_views.category_api),
    url(r'^api/tag-category/?(?P<path>.+)?$', reader_views.tag_category_api),
    url(r'^api/words/completion/(?P<word>.+)/(?P<lexicon>.+)$', reader_views.dictionary_completion_api),
    url(r'^api/words/completion/(?P<word>.+)$', reader_views.dictionary_completion_api),   # Search all dicts
    url(r'^api/words/(?P<word>.+)$', reader_views.dictionary_api),
    url(r'^api/notifications/?$', reader_views.notifications_api),
    url(r'^api/notifications/read', reader_views.notifications_read_api),
    url(r'^api/updates/?(?P<gid>.+)?$', reader_views.updates_api),
    url(r'^api/user_stats/(?P<uid>.+)/?$', reader_views.user_stats_api),
    url(r'^api/site_stats/?$', reader_views.site_stats_api),
    url(r'^api/manuscripts/(?P<tref>.+)', reader_views.manuscripts_for_source),
    url(r'^api/background-data', reader_views.background_data_api),

]

# Source Sheets API
urlpatterns += [
    url(r'^api/sheets/?$',                                            sheets_views.save_sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/delete$',                     sheets_views.delete_sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/add$',                        sheets_views.add_source_to_sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/add_ref$',                    sheets_views.add_ref_to_sheet_api),
    url(r'^api/sheets/(?P<parasha>.+)/get_aliyot$',                   sheets_views.get_aliyot_by_parasha_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/copy_source$',                sheets_views.copy_source_to_sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/topics$',                     sheets_views.update_sheet_topics_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)$',                            sheets_views.sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)\.(?P<node_id>\d+)$',          sheets_views.sheet_node_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/like$',                       sheets_views.like_sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/visualize$',                  sheets_views.visual_sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/unlike$',                     sheets_views.unlike_sheet_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/likers$',                     sheets_views.sheet_likers_api),
    url(r'^api/sheets/user/(?P<user_id>\d+)/((?P<sort_by>\w+)/(?P<limiter>\d+)/(?P<offset>\d+))?$',       sheets_views.user_sheet_list_api),
    url(r'^api/sheets/modified/(?P<sheet_id>\d+)/(?P<timestamp>.+)$', sheets_views.check_sheet_modified_api),
    url(r'^api/sheets/create/(?P<ref>[^/]+)(/(?P<sources>.+))?$',     sheets_views.make_sheet_from_text_api),
    url(r'^api/sheets/tag/(?P<tag>[^/]+)?$',                          sheets_views.sheets_by_tag_api),
    url(r'^api/v2/sheets/tag/(?P<tag>[^/]+)?$',                       sheets_views.story_form_sheets_by_tag),
    url(r'^api/v2/sheets/bulk/(?P<sheet_id_list>.+)$',                sheets_views.bulksheet_api),
    url(r'^api/sheets/trending-tags/?$',                              sheets_views.trending_tags_api),
    url(r'^api/sheets/tag-list/?$',                                   sheets_views.tag_list_api),
    url(r'^api/sheets/tag-list/user/(?P<user_id>\d+)?$',              sheets_views.user_tag_list_api),
    url(r'^api/sheets/tag-list/(?P<sort_by>[a-zA-Z\-]+)$',            sheets_views.tag_list_api),
    url(r'^api/sheets/ref/(?P<ref>[^/]+)$',                           sheets_views.sheets_by_ref_api),
    url(r'^api/sheets/all-sheets/(?P<limiter>\d+)/(?P<offset>\d+)$',  sheets_views.all_sheets_api),
    url(r'^api/sheets/(?P<sheet_id>\d+)/export_to_drive$',            sheets_views.export_to_drive),
    url(r'^api/sheets/upload-image$',                                 sheets_views.upload_sheet_media),
    url(r'^api/sheets/next-untagged/?$',                              sheets_views.next_untagged),
    url(r'^api/sheets/next-uncategorized/?$',                         sheets_views.next_uncategorized)
]

# Unlink Google Account Subscribe
urlpatterns += [
    url(r'^unlink-gauth$', sefaria_views.unlink_gauth),
]

# Collections API
urlpatterns += [
    url(r'^api/collections/user-collections/(?P<user_id>\d+)$', sheets_views.user_collections_api),
    url(r'^api/collections/upload$', sefaria_views.collections_image_upload),
    url(r'^api/collections/for-sheet/(?P<sheet_id>\d+)$', sheets_views.collections_for_sheet_api),
    url(r'^api/collections(/(?P<slug>[^/]+))?$', sheets_views.collections_api),
    url(r'^api/collections/(?P<slug>[^/]+)/set-role/(?P<uid>\d+)/(?P<role>[^/]+)$', sheets_views.collections_role_api),
    url(r'^api/collections/(?P<slug>[^/]+)/invite/(?P<uid_or_email>[^/]+)(?P<uninvite>\/uninvite)?$', sheets_views.collections_invite_api),
    url(r'^api/collections/(?P<slug>[^/]+)/(?P<action>(add|remove))/(?P<sheet_id>\d+)', sheets_views.collections_inclusion_api),
    url(r'^api/collections/(?P<slug>[^/]+)/(?P<action>(add|remove))/(?P<sheet_id>\d+)', sheets_views.collections_inclusion_api),
    url(r'^api/collections/(?P<slug>[^/]+)/pin-sheet/(?P<sheet_id>\d+)', sheets_views.collections_pin_sheet_api),
]

# Search API
urlpatterns += [
    url(r'^api/dummy-search$', reader_views.dummy_search_api),
    url(r'^api/search-wrapper/es6$', reader_views.search_wrapper_api, {'es6_compat': True}),
    url(r'^api/search-wrapper/es8$', reader_views.search_wrapper_api),
    url(r'^api/search-wrapper$', reader_views.search_wrapper_api, {'es6_compat': True}),
    url(r'^api/search-path-filter/(?P<book_title>.+)$', reader_views.search_path_filter),
]

# Following API
urlpatterns += [
    url(r'^api/(?P<action>(follow|unfollow))/(?P<uid>\d+)$', reader_views.follow_api),
    url(r'^api/(?P<kind>(followers|followees))/(?P<uid>\d+)$', reader_views.follow_list_api),
]

# Blocking API
urlpatterns += [
    url(r'^api/(?P<action>(block|unblock))/(?P<uid>\d+)$', reader_views.block_api),
]

# Topics API
urlpatterns += [
    url(r'^api/topics$', reader_views.topics_list_api),
    url(r'^api/topics/generate-prompts/(?P<slug>.+)$', reader_views.generate_topic_prompts_api),
    url(r'^api/topics-graph/(?P<topic>.+)$', reader_views.topic_graph_api),
    url(r'^_api/topics/seasonal-topic/?$', reader_views.seasonal_topic_api),
    url(r'^api/topics/pools/(?P<pool_name>.+)$', reader_views.topic_pool_api),
    url(r'^_api/topics/featured-topic/?$', reader_views.featured_topic_api),
    url(r'^api/topics/trending/?$', reader_views.trending_topics_api),
    url(r'^api/ref-topic-links/bulk$', reader_views.topic_ref_bulk_api),
    url(r'^api/ref-topic-links/(?P<tref>.+)$', reader_views.topic_ref_api),
    url(r'^api/v2/topics/(?P<topic>.+)$', reader_views.topics_api, {'v2': True}),
    url(r'^api/topics/(?P<topic>.+)$', reader_views.topics_api),
    url(r'^api/topic/new$', reader_views.add_new_topic_api),
    url(r'^api/topic/delete/(?P<topic>.+)$', reader_views.delete_topic),
    url(r'^api/topic/reorder$', reader_views.reorder_topics),
    url(r'^api/source/reorder$', reader_views.reorder_sources),
    url(r'^api/bulktopics$', reader_views.bulk_topic_api),
    url(r'^api/recommend/topics(/(?P<ref_list>.+))?', reader_views.recommend_topics_api),
]

# Portals API
urlpatterns += [
    url(r'^api/portals/(?P<slug>.+)$', reader_views.portals_api),
]

# History API
urlpatterns += [
    url(r'^api/history/(?P<tref>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.texts_history_api),
    url(r'^api/history/(?P<tref>.+)$', reader_views.texts_history_api),
]

# Edit Locks API (temporary locks on segments during editing)
urlpatterns += [
    url(r'^api/locks/set/(?P<tref>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.set_lock_api),
    url(r'^api/locks/release/(?P<tref>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.release_lock_api),
    url(r'^api/locks/check/(?P<tref>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.check_lock_api),
]

# Lock Text API (permament locking of an entire text)
urlpatterns += [
    url(r'^api/locktext/(?P<title>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.lock_text_api),
    url(r'^api/version/flags/(?P<title>.+)/(?P<lang>\w\w)/(?P<version>.+)$', reader_views.flag_text_api),
]
# SEC-AUDIT: do we also want to maybe move these to 'admin'

# Discussions
urlpatterns += [
    url(r'^discussions/?$', reader_views.discussions),
    url(r'^api/discussions/new$', reader_views.new_discussion_api),
]

# Dashboard Page
urlpatterns += [
    url(r'^dashboard/?$', reader_views.dashboard),
]

# Activity
urlpatterns += [
    url(r'^activity/?$', reader_views.global_activity),
    url(r'^activity/leaderboard?$', reader_views.leaderboard),
    url(r'^activity/(?P<page>\d+)$', reader_views.global_activity),
    url(r'^activity/(?P<slug>[^/]+)/(?P<page>\d+)?$', reader_views.user_activity),
    url(r'^activity/(?P<tref>[^/]+)/(?P<lang>.{2})/(?P<version>.+)/(?P<page>\d+)$', reader_views.segment_history),
    url(r'^activity/(?P<tref>[^/]+)/(?P<lang>.{2})/(?P<version>.+)$', reader_views.segment_history),
    url(r'^api/revert/(?P<tref>[^/]+)/(?P<lang>.{2})/(?P<version>.+)/(?P<revision>\d+)$', reader_views.revert_api),
]

# Random Text
urlpatterns += [
    url(r'^random/link$',        reader_views.random_redirect),
    url(r'^random/?$',           reader_views.random_text_page),
]

# Preview Images
urlpatterns += [
    url(r'^api/img-gen/(?P<tref>.+)$', reader_views.social_image_api),
]


# Registration
urlpatterns += [
    url(r'^login/?$', django_auth_views.LoginView.as_view(authentication_form=SefariaLoginForm), name='login'),
    url(r'^register/?$', sefaria_views.register, name='register'),
    url(r'^logout/?$', django_auth_views.LogoutView.as_view(), name='logout'),
    url(r'^password/reset/?$', django_auth_views.PasswordResetView.as_view(form_class=SefariaPasswordResetForm, email_template_name='registration/password_reset_email.txt', html_email_template_name='registration/password_reset_email.html'), name='password_reset'),
    url(r'^password/reset/confirm/(?P<uidb64>[0-9A-Za-z_\-]+)/(?P<token>[0-9A-Za-z]{1,13}-[0-9A-Za-z]{1,20})/$', django_auth_views.PasswordResetConfirmView.as_view(form_class=SefariaSetPasswordForm), name='password_reset_confirm'),
    url(r'^password/reset/complete/$', django_auth_views.PasswordResetCompleteView.as_view(), name='password_reset_complete'),
    url(r'^password/reset/done/$', django_auth_views.PasswordResetDoneView.as_view(), name='password_reset_done'),
    url(r'^api/register/$', sefaria_views.register_api),
    url(r'^api/login/$', TokenObtainPairView.as_view(), name='token_obtain_pair'),
    url(r'^api/login/refresh/$', TokenRefreshView.as_view(), name='token_refresh'),
    url(r'^api/account/delete$', reader_views.delete_user_account_api),
]

# Compare Page
urlpatterns += [
    url(r'^compare/?((?P<comp_ref>[^/]+)/)?((?P<lang>en|he)/)?((?P<v1>[^/]+)/)?(?P<v2>[^/]+)?$', sefaria_views.compare)
]

# Gardens
urlpatterns += [
    #url(r'^garden/sheets/(?P<key>.+)$', 'sheet_tag_garden_page),
    url(r'^garden/(?P<key>.+)$', reader_views.custom_visual_garden_page),
    url(r'^garden/sheets/(?P<key>.+)$', reader_views.sheet_tag_visual_garden_page),
    url(r'^garden/search/(?P<q>.+)$', reader_views.search_query_visual_garden_page),
    url(r'^vgarden/custom/(?P<key>.*)$', reader_views.custom_visual_garden_page),  # legacy.  Used for "maggid" and "ecology"
]

# Sefaria.js -- Packaged JavaScript
urlpatterns += [
    url(r'^data\.(?:(?:\d+)\.)?js$', sefaria_views.data_js), # Allow for regular data.js and also data.<timestamp>.js for caching
    url(r'^sefaria\.js$', sefaria_views.sefaria_js),
]

# Linker js, text upload & download
urlpatterns += [
    url(r'^linker\.?v?([0-9]+)?\.js$', sefaria_views.linker_js),
    url(r'^api/find-refs/report/?$', sefaria_views.find_refs_report_api),
    url(r'^api/find-refs/?$', sefaria_views.find_refs_api),
    url(r'^api/regexs/(?P<titles>.+)$', sefaria_views.title_regex_api),
    url(r'^api/websites/(?P<domain>.+)$', sefaria_views.websites_api),
    url(r'^api/linker-data/(?P<titles>.+)$', sefaria_views.linker_data_api),
    url(r'^api/bulktext/(?P<refs>.+)$', sefaria_views.bulktext_api),
    url(r'^download/version/(?P<title>.+) - (?P<lang>[he][en]) - (?P<versionTitle>.+)\.(?P<format>plain\.txt)', sefaria_views.text_download_api),
    url(r'^download/version/(?P<title>.+) - (?P<lang>[he][en]) - (?P<versionTitle>.+)\.(?P<format>json|csv|txt)',sefaria_views.text_download_api),
    url(r'^download/bulk/versions/', sefaria_views.bulk_download_versions_api),
    url(r'^api/text-upload$', sefaria_views.text_upload_api),
    url(r'^api/linker-track$', sefaria_views.linker_tracking_api),

]

urlpatterns += [
    url(r'^api/passages/(?P<refs>.+)$', sefaria_views.passages_api),
]

# Send Feedback
urlpatterns += [
    url(r'^api/send_feedback$', sefaria_views.generate_feedback),
]

# Email Newsletter Subscriptions
urlpatterns += [
    url(r'^api/subscribe/(?P<org>.+)/(?P<email>.+)$', sefaria_views.generic_subscribe_to_newsletter_api),
    url(r'^api/subscribe/(?P<email>.+)$', sefaria_views.subscribe_sefaria_newsletter_view),
    url(r'^api/newsletter_mailing_lists/?$', sefaria_views.get_available_newsletter_mailing_lists),
]

# Admin
urlpatterns += [
    url(r'^admin/reset/varnish/(?P<tref>.+)$', sefaria_views.reset_varnish),
    url(r'^admin/reset/cache$', sefaria_views.reset_cache),
    url(r'^admin/reset/cache/(?P<title>.+)$', sefaria_views.reset_index_cache_for_text),
    url(r'^admin/reset/counts/all$', sefaria_views.reset_counts),
    url(r'^admin/reset/counts/(?P<title>.+)$', sefaria_views.reset_counts),
    url(r'^admin/reset/toc$', sefaria_views.rebuild_toc),
    url(r'^admin/reset/ac$', sefaria_views.rebuild_auto_completer),
    url(r'^admin/reset/api/(?P<apiurl>.+)$', sefaria_views.reset_cached_api),
    url(r'^admin/reset/community$', reader_views.community_reset),
    url(r'^admin/reset/(?P<tref>.+)$', sefaria_views.reset_ref),
    url(r'^admin/reset-websites-data', sefaria_views.reset_websites_data),
    url(r'^admin/delete/orphaned-counts', sefaria_views.delete_orphaned_counts),
    url(r'^admin/delete/user-account', sefaria_views.delete_user_by_email, name="delete/user-account"),
    url(r'^admin/delete/sheet$', sefaria_views.delete_sheet_by_id, name="delete/sheet"),
    url(r'^admin/rebuild/auto-links/(?P<title>.+)$', sefaria_views.rebuild_auto_links),
    url(r'^admin/rebuild/citation-links/(?P<title>.+)$', sefaria_views.rebuild_citation_links),
    url(r'^admin/rebuild/shared-cache', sefaria_views.rebuild_shared_cache),
    url(r'^admin/delete/citation-links/(?P<title>.+)$', sefaria_views.delete_citation_links),
    url(r'^admin/cache/stats', sefaria_views.cache_stats),
    url(r'^admin/cache/dump', sefaria_views.cache_dump),
    url(r'^admin/run/tests', sefaria_views.run_tests),
    url(r'^admin/export/all', sefaria_views.export_all),
    url(r'^admin/error', sefaria_views.cause_error),
    url(r'^admin/account-stats', sefaria_views.account_stats),
    url(r'^admin/categorize-sheets', sefaria_views.categorize_sheets),
    url(r'^admin/sheet-stats', sefaria_views.sheet_stats),
    url(r'^admin/untagged-sheets', sefaria_views.untagged_sheets),
    url(r'^admin/spam$', sefaria_views.spam_dashboard),
    url(r'^admin/spam/sheets', sefaria_views.sheet_spam_dashboard),
    url(r'^admin/spam/profiles', sefaria_views.profile_spam_dashboard),
    url(r'^admin/versions-csv', sefaria_views.versions_csv),
    url(r'^admin/index-sheets-by-timestamp', sefaria_views.index_sheets_by_timestamp),
    url(r'^admin/community-preview', reader_views.community_preview),
    url(r'^admin/descriptions/authors/update', sefaria_views.update_authors_from_sheet),
    url(r'^admin/descriptions/categories/update', sefaria_views.update_categories_from_sheet),
    url(r'^admin/descriptions/texts/update', sefaria_views.update_texts_from_sheet),
    url(fr'^{ADMIN_PATH}/?', include(admin.site.urls)),
]

# Stats API - return CSV
urlpatterns += [
    url(r'^api/stats/library-stats', sefaria_views.library_stats),
    url(r'^api/stats/core-link-stats', sefaria_views.core_link_stats),
]

# Google API OAuth 2.0
urlpatterns += [
    url(r'^gauth$', gauth_views.index, name="gauth_index"),
    url(r'^gauth/callback$', gauth_views.auth_return, name="gauth_callback"),
]

# Site specific URLS loaded from
urlpatterns += site_urlpatterns

# Sheets in a reader panel
urlpatterns += [
    url(r'^sheets/(?P<tref>[\d.]+)$', reader_views.catchall, {'sheet': True}),
]

# Guides
urlpatterns += [
    url(r'^api/guides/(?P<guide_key>[^/]+)$', guides_views.guides_api),
]

# add static files to urls
from django.contrib.staticfiles.urls import staticfiles_urlpatterns
urlpatterns += staticfiles_urlpatterns()

# Catch all to send to Reader
urlpatterns += [
    url(r'^(?P<tref>[^/]+)/(?P<lang>\w\w)/(?P<version>.*)$', reader_views.old_versions_redirect),
    url(r'^(?P<tref>[^/]+)(/)?$', reader_views.catchall)
]

if DOWN_FOR_MAINTENANCE:
    # Keep admin accessible
    urlpatterns = [
        url(r'^admin/reset/cache', sefaria_views.reset_cache),
        url(r'^admin/?', include(admin.site.urls)),
        url(r'^healthz/?$', reader_views.application_health_api),  # this oddly is returning 'alive' when it's not.  is k8s jumping in the way?
        url(r'^health-check/?$', reader_views.application_health_api),
        url(r'^healthz-rollout/?$', reader_views.rollout_health_api),
    ]
    # Everything else gets maintenance message
    urlpatterns += [
        url(r'.*', sefaria_views.maintenance_message)
    ]

```

### sefaria/helper/link.py

```
# -*- coding: utf-8 -*-

import csv
import sys
import re
from io import StringIO
import structlog
logger = structlog.get_logger(__name__)

from sefaria.model import *
from sefaria.system.exceptions import DuplicateRecordError, InputError
import sefaria.tracker as tracker
try:
    from sefaria.settings import USE_VARNISH
except ImportError:
    USE_VARNISH = False
if USE_VARNISH:
    from sefaria.system.varnish.wrapper import invalidate_ref

#TODO: should all the functions here be decoupled from the need to enter a userid?


class AbstractAutoLinker(object):
    """
    This abstract class defines the interface/contract for autolinking objects.
    There are four main methods:
    1. build_links creates the links from scratch
    2. refresh_links will update a link set by intelligently (and performance oriented) adding and deleting relevant links
    3. delete_links will delete all the links in the set
    4. rebuild_links will delete and then build the links from scratch
    """
    def __init__(self, oref, auto=True, generated_by_string=None, link_type=None, **kwargs):
        self._requested_oref = oref
        if not getattr(self, '_generated_by_string', None):
            self._generated_by_string = generated_by_string if generated_by_string else self.__class__.__name__
        if not getattr(self, '_auto', None):
            self._auto = auto
        if not getattr(self,'_link_type', None):
            self._link_type = link_type if link_type else getattr(oref.index, 'dependence', 'Commentary').lower()
        self._user = kwargs.get('user', None)
        self._title = self._requested_oref.index.title
        self._links = None

    def build_links(self, **kwargs):
        raise NotImplementedError

    def refresh_links(self, **kwargs):
        """
        :return: Meant for adding links while intelligently removing stale links
        """
        raise NotImplementedError

    def delete_links(self, **kwargs):
        """
        Deletes all of the citation generated links from text 'title'
        """
        links = self._load_links()
        for link in links:
            if USE_VARNISH:
                try:
                    invalidate_ref(Ref(link.refs[0]))
                except InputError:
                    pass
                try:
                    invalidate_ref(Ref(link.refs[1]))
                except InputError:
                    pass
            self._delete_link(link)

    def rebuild_links(self, **kwargs):
        """
        Intended to clean out all existing links and build them anew
        :return:
        """
        self.delete_links()
        return self.build_links()

    def _load_links(self):
        if not self._links:
            ref_regex_list = self._requested_oref.regex(as_list=True)
            queries = [{"refs": {"$regex": ref_regex},
                                   "generated_by": self._generated_by_string,
                                   "auto": self._auto,
                                   "type": self._link_type
                                   } for ref_regex in ref_regex_list]
            self._links = LinkSet({"$or": queries})
        return self._links

    def linkset(self):
        return self._load_links()

    def _save_link(self, tref, base_tref, **kwargs):
        nlink = {
            "refs": [base_tref, tref],
            "type": self._link_type,
            "anchorText": "",
            "auto": self._auto,
            "generated_by": self._generated_by_string
        }
        try:
            if not self._user:
                Link(nlink).save()
            else:
                tracker.add(self._user, Link, nlink, **kwargs)
        except DuplicateRecordError as e:
            pass
        return tref

    def _delete_link(self, link):
        if not self._user:
            link.delete()
        else:
            tracker.delete(self._user, Link, link._id)


class AbstractStructureAutoLinker(AbstractAutoLinker):
    """
    This class is for general linking according to two structurally identical texts.
    """
    # TODO: as error checking there should probs be a validate that checks the structures match.
    def __init__(self, oref, depth_up, linked_oref, default_only=False, **kwargs):
        self._linked_title = linked_oref.index.title
        self._depth_up = depth_up
        self._default_only = default_only
        super(AbstractStructureAutoLinker, self).__init__(oref, **kwargs)

    def _generate_specific_base_tref(self, orig_ref):
        """ This function only works with simple texts:  Rashi, Genesis
        whereas we want
        """
        context_ref = orig_ref.context_ref(self._depth_up)

        # Replacing self._title with self._linked_title only works for simple texts
        # and complex texts where there is one default node.  This won't work in the case
        # where there is a complex text with a node that has a title different from the title
        # of the book
        return context_ref.normal().replace(self._title, self._linked_title)

    def _build_links_internal(self, oref, text=None, **kwargs):
        tref = oref.normal()
        found_links = []
        # This is a special case, where the sections length is 0 and that means this is
        # a whole text or complex text node that has been posted. So we get each leaf node
        if not oref.sections:
            vs = StateNode(tref).versionState
            if not vs.is_new_state:
                vs.refresh()  # Needed when saving multiple nodes in a complex text.  This may be moderately inefficient.
            content_nodes = oref.index_node.get_leaf_nodes()
            if self._default_only:
                content_nodes = [node for node in content_nodes if node.key == "default" and getattr(node, "default", False) is True]
            for r in content_nodes:
                cn_oref = r.ref()
                text = TextFamily(cn_oref, commentary=0, context=0, pad=False).contents()
                length = cn_oref.get_state_ja().length()
                for i, sr in enumerate(cn_oref.subrefs(length)):
                    stext = {"sections": sr.sections,
                            "sectionNames": text['sectionNames'],
                            "text": text["text"][i] if i < len(text["text"]) else "",
                            "he": text["he"][i] if i < len(text["he"]) else ""
                            }
                    found_links += self._build_links_internal(sr, stext, **kwargs)

        else:
            if not text:
                try:
                    text = TextFamily(oref, commentary=0, context=0, pad=False).contents()
                except AssertionError:
                    logger.warning("Structure node passed to add_commentary_links: {}".format(oref.normal()))
                    return

            if self._default_only and (oref.index_node.key != "default" or getattr(oref.index_node, "default", False) is False):
                return

            if len(text["sectionNames"]) > len(text["sections"]) > 0:
                # any other case where the posted ref sections do not match the length of the parent texts sections
                # this is a larger group of comments meaning it needs to be further broken down
                # in order to be able to match the commentary to the basic parent text units,
                # recur on each section
                length = max(len(text["text"]), len(text["he"]))
                for i,r in enumerate(oref.subrefs(length)):
                    stext = {"sections": r.sections,
                            "sectionNames": text['sectionNames'],
                            "text": text["text"][i] if i < len(text["text"]) else "",
                            "he": text["he"][i] if i < len(text["he"]) else ""
                            }
                    found_links += self._build_links_internal(r, stext, **kwargs)

            # this is a single comment, trim the last section number (comment) from ref
            elif len(text["sections"]) == len(text["sectionNames"]):
                if len(text['he']) or len(text['text']): #only if there is actually text
                    #base_tref = base_tref[0:base_tref.rfind(":")]
                    base_tref = self._generate_specific_base_tref(oref)
                    found_links += [tref]
                    self._save_link(tref, base_tref, **kwargs)
        return found_links

    def build_links(self, **kwargs):
        return self._build_links_internal(self._requested_oref)

    def refresh_links(self, **kwargs):
        """
        This function both adds links and deletes pre existing ones that are no longer valid,
        by virtue of the fact that they were not detected as commentary links while iterating over the text.
        :param tref:
        :param user:
        :param kwargs:
        :return:
        """
        existing_links = self._load_links()
        found_links = self._build_links_internal(self._requested_oref)
        for exLink in existing_links:
            for r in exLink.refs:
                if self._title not in r:  #current base ref
                    continue
                if USE_VARNISH:
                    try:
                        invalidate_ref(Ref(r))
                    except InputError:
                        pass
                if r not in found_links:
                    self._delete_link(exLink)
                break

class BaseStructureAutoLinker(AbstractStructureAutoLinker):
    """
    This linker will only allow a text to be linked to it's specified base text (currently assumes one base text)
    """
    def __init__(self, oref, depth_up, **kwargs):
        if not oref.is_dependant():
            raise Exception("Text must have a base text to link to")
        """try:"""
        base_oref = Ref(oref.index.base_text_titles[0])
        super(BaseStructureAutoLinker, self).__init__(oref, depth_up, base_oref, **kwargs)
        """except Exception as e:
            raise Exception('Text must have a base text to link to')"""


class CommentaryDefaultOnlyAutoLinker(AbstractStructureAutoLinker):
    """
    Works exactly the same as the CommentaryAutoLinker, except that only default nodes will be linked
    and other nodes will be ignored.  "Ibn Ezra on Isaiah" is a complex text with three Jagged Arrays:
    a Prelude, a Translator's Foreword, and a default node.
    In this case, the default node will be linked to "Isaiah", but there will be no attempt to link to
    "Isaiah, Prelude".
    The only difference in implementation between this class and the CommentaryAutoLinker is that this class sets
    AbstractStructureAutoLinker's default_only parameter to True, whereas CommentaryAutoLinker sets it to False.
    """
    class_key = "many_to_one_default_only"
    _generated_by_string = 'add_commentary_links'
    def __init__(self, oref, **kwargs):
        if not oref.is_dependant():
            raise Exception("Text must have a base text to link to")
        """try:"""
        base_oref = Ref(oref.index.base_text_titles[0])
        super(CommentaryDefaultOnlyAutoLinker, self).__init__(oref, 1, base_oref, default_only=True, **kwargs)


class MatchBaseTextDepthDefaultOnlyAutoLinker(AbstractStructureAutoLinker):
    """
    Works exactly the same as the MatchBaseTextDepthAutoLinker, except that only default nodes will be linked
    and other nodes will be ignored.
    """
    class_key = "one_to_one_default_only"
    _generated_by_string = "add_commentary_links"
    def __init__(self, oref, **kwargs):
        if not oref.is_dependant():
            raise Exception("Text must have a base text to link to")
        """try:"""
        base_oref = Ref(oref.index.base_text_titles[0])
        super(MatchBaseTextDepthDefaultOnlyAutoLinker, self).__init__(oref, 0, base_oref, default_only=True, **kwargs)


class IncrementBaseTextDepthAutoLinker(BaseStructureAutoLinker):
    def __init__(self, oref, **kwargs):
        super(IncrementBaseTextDepthAutoLinker, self).__init__(oref, 1, **kwargs)


class CommentaryAutoLinker(IncrementBaseTextDepthAutoLinker):
    """
    The classic linker, takes a n-dpeth text and
    links each group of terminal segments to the same n-1 depth terminal segment of the base text
    Used primarily for old style commentaries that are shaped like the base text

    Automatically add links for each comment in the commentary text denoted by 'tref'.
    E.g., for the ref 'Sforno on Kohelet 3:2', automatically set links for
    Kohelet 3:2 <-> Sforno on Kohelet 3:2:1, Kohelet 3:2 <-> Sforno on Kohelet 3:2:2, etc.
    for each segment of text (comment) that is in 'Sforno on Kohelet 3:2'.
    """
    class_key = 'many_to_one'
    _generated_by_string = 'add_commentary_links'



class MatchBaseTextDepthAutoLinker(BaseStructureAutoLinker):
    class_key = 'one_to_one'
    _generated_by_string = "add_commentary_links"
    def __init__(self, oref, **kwargs):
        super(MatchBaseTextDepthAutoLinker, self).__init__(oref, 0, **kwargs)


def rebuild_links_for_title(tref, user=None):
    """
    Utility function, can be called from a view or cli. Takes a ref or a more general title to rebuild auto links
    :param tref:
    :param user:
    :return:
    """
    try:
        oref = Ref(tref)
    except InputError:
        # If not a valid ref, maybe a title of an entire corpus.
        # Allow group work names, eg. Rashi alone, rebuild for each text we have
        #TODO: there might need to be some error checking done on this
        title_indices = library.get_indices_by_collective_title(tref)
        for c in title_indices:
            rebuild_links_for_title(c, user)
        return
    linker = oref.autolinker(user=user)
    if linker:
        linker.rebuild_links()


# TODO: refactor with lexicon class map into abstract
class AutoLinkerFactory(object):
    _class_map = {
        CommentaryAutoLinker.class_key            : CommentaryAutoLinker,
        MatchBaseTextDepthAutoLinker.class_key    : MatchBaseTextDepthAutoLinker,
        CommentaryDefaultOnlyAutoLinker.class_key : CommentaryDefaultOnlyAutoLinker,
        MatchBaseTextDepthDefaultOnlyAutoLinker.class_key: MatchBaseTextDepthDefaultOnlyAutoLinker,
    }
    _key_attr = 'base_text_mapping'
    _default_class = CommentaryAutoLinker

    @classmethod
    def class_factory(cls, name):
        if name in cls._class_map:
            return cls._class_map[name]
        else:
            return cls._default_class

    @classmethod
    def instance_factory(cls, name, *args, **kwargs):
        return cls.class_factory(name)(*args, **kwargs)

    @classmethod
    def instance_from_record_factory(cls, oref):
        try:
            return cls.instance_factory(oref.index[cls._key_attr], oref)
        except KeyError:
            return None


# ------------------------------------------------------------------------------------------ #

def add_links_from_text(oref, lang, text, text_id, user, **kwargs):
    """
    Scan a text for explicit references to other texts and automatically add new links between
    ref and the mentioned text.

    text["text"] may be a list of segments, an individual segment, or None.

    The set of no longer supported links (`existingLinks` - `found`) is deleted.
    If Varnish is used, all linked refs, old and new, are refreshed

    Returns `links` - the list of links added.
    """
    if not text:
        return []
    elif isinstance(text, list):
        subrefs = oref.subrefs(len(text))
        links   = []
        for i in range(len(text)):
            single = add_links_from_text(subrefs[i], lang, text[i], text_id, user, **kwargs)
            links += single
        return links
    elif isinstance(text, str):
        """
            Keeps three lists:
            * existingLinks - The links that existed before the text was rescanned
            * found - The links found in this scan of the text
            * links - The new links added in this scan of the text

            The set of no longer supported links (`existingLinks` - `found`) is deleted.
            The set of all links (`existingLinks` + `Links`) is refreshed in Varnish.
        """
        existingLinks = LinkSet({
            "refs": oref.normal(),
            "auto": True,
            "generated_by": "add_links_from_text",
            "source_text_oid": text_id
        }).array()  # Added the array here to force population, so that new links don't end up in this set

        found = []  # The normal refs of the links found in this text
        links = []  # New link objects created by this processes

        if kwargs.get('citing_only') is not None:
            citing_only = kwargs['citing_only']
        else:
            citing_only = True

        refs = library.get_refs_in_string(text, lang, citing_only=citing_only)

        for linked_oref in refs:
            link = {
                # Note -- ref of the citing text is in the first position
                "refs": [oref.normal(), linked_oref.normal()],
                "type": "",
                "auto": True,
                "generated_by": "add_links_from_text",
                "source_text_oid": text_id,
                "inline_citation": True
            }
            found += [linked_oref.normal()]  # Keep this here, since tracker.add will throw an error if the link exists
            try:
                tracker.add(user, Link, link, **kwargs)
                links += [link]
                if USE_VARNISH:
                    invalidate_ref(linked_oref)
            except InputError as e:
                pass

        # Remove existing links that are no longer supported by the text
        for exLink in existingLinks:
            for r in exLink.refs:
                if r == oref.normal():  # current base ref
                    continue
                if USE_VARNISH:
                    try:
                        invalidate_ref(Ref(r))
                    except InputError:
                        pass
                if r not in found:
                    tracker.delete(user, Link, exLink._id)
                break

        return links


def delete_links_from_text(title, user):
    """
    Deletes all of the citation generated links from text 'title'
    """
    regex    = Ref(title).regex()
    links    = LinkSet({"refs.0": {"$regex": regex}, "generated_by": "add_links_from_text"})
    for link in links:
        if USE_VARNISH:
            try:
                invalidate_ref(Ref(link.refs[0]))
            except InputError:
                pass
            try:
                invalidate_ref(Ref(link.refs[1]))
            except InputError:
                pass
        tracker.delete(user, Link, link._id)


def rebuild_links_from_text(title, user):
    """
    Deletes all of the citation generated links from text 'title'
    then rebuilds them.
    """
    delete_links_from_text(title, user)
    index = library.get_index(title)
    title = index.nodes.primary_title("en")
    versions = index.versionSet()

    def add_links_callback(snode, *contents, **kwargs):
        """
        :param snode: SchemaContentNode
        :param contents: Array of one jagged array - the contents of `snode` in one version
        :param kwargs:
        :return:
        """
        assert len(contents) == 1
        version = kwargs.get("version")
        add_links_from_text(snode.ref(), version.language, contents[0], version._id, user)

    for version in versions:
        index.nodes.visit_content(add_links_callback, version.chapter, version=version)
        # add_links_from_text(Ref(title), version.language, version.chapter, version._id, user)

# --------------------------------------------------------------------------------- #


def create_link_cluster(refs, user, link_type="", attrs=None, exception_pairs=None, exception_range = None):
    total = 0
    for i, ref in enumerate(refs):
        for j in range(i + 1, len(refs)):
            ref_strings = [refs[i].normal(), refs[j].normal()]

            # If this link matches an exception pair, skip it.
            if all([any([r.startswith(name) for r in ref_strings]) for pair in exception_pairs for name in pair]):
                continue
            # If this link matches an exception range, skip it.
            if refs[i].section_ref() == refs[j].section_ref():
                continue

            d = {
                "refs": ref_strings,
                "type": link_type
                }
            if attrs:
                d.update(attrs)
            try:
                tracker.add(user, Link, d)
                print("Created {} - {}".format(d["refs"][0], d["refs"][1]))
                total += 1
            except Exception as e:
                print("Exception: {}".format(e))
    return total

def add_links_from_csv(file, linktype, generated_by, uid):
    csv.field_size_limit(sys.maxsize)
    reader = csv.DictReader(StringIO(file.read().decode()))
    fieldnames = reader.fieldnames
    if len(fieldnames) != 2:
        raise ValueError(f'file has {len(fieldnames)} columns rather than 2')
    output = StringIO()
    errors_writer = csv.DictWriter(output, fieldnames=['ref1', 'ref2', 'error'])
    errors_writer.writeheader()
    success = 0
    for row in reader:
        refs = [row[fieldnames[0]], row[fieldnames[1]]]
        try:
            if any(Ref(ref).is_empty() for ref in refs):
                errors_writer.writerow({'ref1': refs[0],
                               'ref2': refs[1],
                               'error': f'{[r for r in refs if Ref(r).is_empty()][0]} is an empty ref'})
                continue
        except Exception as e:
            errors_writer.writerow({'ref1': refs[0],
                               'ref2': refs[1],
                               'error': f'one or more of {refs[0]} and {refs[1]} is not a valid ref'})
            continue
        link = {
            'refs': refs,
            'type': linktype,
            'generated_by': generated_by,
            'auto': True
        }
        try:
            tracker.add(uid, Link, link)
            success += 1
        except Exception as e:
            errors_writer.writerow({'ref1': refs[0],
                           'ref2': refs[1],
                           'error': f'error with linking refs: {refs[0]}, {refs[1]}: {e}'})
        try:
            if USE_VARNISH:
                for ref in link.refs:
                    invalidate_ref(Ref(ref), purge=True)
        except Exception as e:
            logger.error(e)
    return {'message': f'{success} links successfully saved', 'errors': output.getvalue()}


def remove_links_from_csv(file, uid):
    csv.field_size_limit(sys.maxsize)
    reader = csv.DictReader(StringIO(file.read().decode()))
    links_removed = 0
    output = StringIO()
    errors_writer = csv.DictWriter(output, fieldnames=['ref1', 'ref2'])
    errors_writer.writeheader()
    for row in reader:
        refs = sorted(row.values())
        try:
            link = Link().load({'refs': refs})
            tracker.delete(uid, Link, link._id)
            links_removed += 1
        except Exception as e:
            errors_writer.writerow({'ref1': refs[0], 'ref2': refs[1]})
        try:
            if USE_VARNISH:
                for ref in refs:
                    invalidate_ref(Ref(ref), purge=True)
        except Exception as e:
            logger.error(e)
    return {'message': f'{links_removed} links successfully removed', 'errors': output.getvalue()}


def make_link_query(trefs, **additional_query):
    query = additional_query
    if trefs[1] == 'all':
        regex_list = Ref(trefs[0]).regex(as_list=True)
        query['$or'] = [{"expandedRefs0": {"$regex": r}} for r in regex_list]
        query['$or'] += [{"expandedRefs1": {"$regex": r}} for r in regex_list]
    else:
        query['$or'] = []
        regex_lists = [Ref(tref).regex(as_list=True) for tref in trefs]
        for i in range(2):
            ref_clauses0 = {'$or': [{"expandedRefs0": {"$regex": r}} for r in regex_lists[i]]}
            ref_clauses1 = {'$or': [{"expandedRefs1": {"$regex": r}} for r in regex_lists[1-i]]}
            query['$or'].append({"$and": [ref_clauses0, ref_clauses1]})
    return query

def get_links_per_segment_by_refs(trefs, **additional_query):
    oref = Ref(trefs[0])
    if isinstance(oref.index_node, JaggedArrayNode):
        segments = oref.all_segment_refs()
    else:
        segments = oref.index.all_segment_refs()
    for segment in segments:
        links = LinkSet(make_link_query([segment.normal(), trefs[1]], **additional_query))
        for link in links:
            yield link
        if not links:
            yield segment

def get_csv_links_by_refs(trefs, by_segment=False, **additional_query):
    output = StringIO()
    writer = csv.DictWriter(output, fieldnames=[trefs[0], trefs[1], 'type', 'generated_by'])
    writer.writeheader()
    if by_segment:
        links = get_links_per_segment_by_refs(trefs[:], **additional_query) #copy of trefs for trefs will be sorted and get_links_per_segment_by_refs is generator
    else:
        limit = 15000 if trefs[1] == 'all' else 0
        links = LinkSet(make_link_query(trefs, **additional_query), limit=limit)
    ref0 = trefs[0]
    trefs.sort()
    for element in links:
        if isinstance(element, Ref):
            writer.writerow({ref0: element.normal()})
            continue
        linkrefs = element.refs[:]
        if 'all' in trefs:
            expanded_refs = element.expandedRefs0 if trefs[0] == 'all' else element.expandedRefs1
            if any(re.search(Ref(ref0).regex(), expanded_ref) for expanded_ref in expanded_refs):
                linkrefs.reverse()
        writer.writerow({
            trefs[0]: linkrefs[0],
            trefs[1]: linkrefs[1],
            'type': element.type,
            'generated_by': getattr(element, 'generated_by', '')
        })
    return output.getvalue()

```

### sefaria/helper/llm/tasks.py

```
"""
Celery tasks for the LLM server
"""
from typing import List
from dataclasses import asdict
from celery import signature
from sefaria.settings import CELERY_QUEUES
from sefaria.celery_setup.app import app
from sefaria.model.topic import Topic
from sefaria.model.text import Ref
from sefaria.helper.llm.topic_prompt import save_topic_prompt_output, make_topic_prompt_input
from sefaria_llm_interface.topic_prompt import TopicPromptGenerationOutput


@app.task(name="web.save_topic_prompts")
def save_topic_prompts(raw_output: dict):
    output = TopicPromptGenerationOutput(**raw_output)
    save_topic_prompt_output(output)


def generate_and_save_topic_prompts(lang: str, sefaria_topic: Topic, orefs: List[Ref], contexts: List[str]):
    topic_prompt_input = make_topic_prompt_input(lang, sefaria_topic, orefs, contexts)
    generate_signature = signature('llm.generate_topic_prompts', args=(asdict(topic_prompt_input),), queue=CELERY_QUEUES['llm'])
    save_signature = save_topic_prompts.s().set(queue=CELERY_QUEUES['tasks'])
    chain = generate_signature | save_signature
    return chain()

```

### sefaria/helper/llm/tests/topic_prompt_test.py

```
import pytest
from dataclasses import asdict
from sefaria.helper.llm.topic_prompt import *
from sefaria.helper.llm.topic_prompt import _lang_dict_by_func, _get_commentary_from_link_dict


@pytest.mark.parametrize(('fn', 'expected'), [
    [lambda x: x, {'en': 'en', 'he': 'he'}],
])
def test_lang_dict_by_func(fn, expected):
    assert _lang_dict_by_func(fn) == expected


@pytest.mark.parametrize(('link_dict', 'expected'), [
    [  # simple case
        {
            "category": "Commentary", "sourceHasEn": True, "sourceRef": "Job 1", "text": "Yo", "he": "Jo",
        }, {
           "ref": "Job 1", "text": {"en": "Yo", "he": "Jo"} ,
        }
    ],
    [  # only english
        {
            "category": "Commentary", "sourceHasEn": True, "sourceRef": "Job 1", "text": "Yo"
        }, {
        "ref": "Job 1", "text": {"en": "Yo", "he": ""} ,
    }
    ],
    [  # not commentary
        {
            "category": "Not Commentary", "sourceHasEn": True,
        }, None
    ],
    [  # not english
        {
            "category": "Commentary", "sourceHasEn": False,
        }, None
    ]
])
def test_get_commentary_from_link_dict(link_dict, expected):
    topic_prompt_commentary = _get_commentary_from_link_dict(link_dict)
    try:
        asdict(topic_prompt_commentary) == expected
    except TypeError:
        # None will fail as input to `asdict`
        assert topic_prompt_commentary == expected


@pytest.mark.parametrize(('ref_topic_links', 'ref__context_hints_by_lang'), [
    [  # one context one lang
        [
            {
                'ref': 'Genesis 1:1',
                'descriptions': {
                    'en': {'ai_context': 'test'}
                }
            }
        ], {'en': [(Ref('Genesis 1:1'), 'test')]}
    ],
    [  # no contexts
        [
            {
                'ref': 'Genesis 1:1',
                'descriptions': {}
            }
        ], {}
    ],
    [  # two languages in one link
        [
            {
                'ref': 'Genesis 1:1',
                'descriptions': {
                    'en': {'ai_context': 'test'},
                    'he': {'ai_context': 'he_test'}
                }
            }
        ], {'en': [(Ref('Genesis 1:1'), 'test')], 'he': [(Ref('Genesis 1:1'), 'he_test')]}],
    [  # one language in each link
        [
            {
                'ref': 'Genesis 1:1',
                'descriptions': {
                    'en': {'ai_context': 'test'},
                }
            },
            {
                'ref': 'Genesis 1:2',
                'descriptions': {
                    'he': {'ai_context': 'he_test'},
                }
            }
        ], {'en': [(Ref('Genesis 1:1'), 'test')], 'he': [(Ref('Genesis 1:2'), 'he_test')]}
    ],
    [  # one language in each link but one link has prompt
        [
            {
                'ref': 'Genesis 1:1',
                'descriptions': {
                    'en': {'ai_context': 'test'},
                }
            },
            {
                'ref': 'Genesis 1:2',
                'descriptions': {
                    'he': {'ai_context': 'he_test', 'prompt': 'AIs are cool'},
                }
            }
        ], {'en': [(Ref('Genesis 1:1'), 'test')]}
    ]
])
def test_get_ref_context_hints_by_lang(ref_topic_links, ref__context_hints_by_lang):
    assert get_ref_context_hints_by_lang(ref_topic_links) == ref__context_hints_by_lang



```

### sefaria/helper/llm/__init__.py

```

```

### sefaria/helper/llm/topic_prompt.py

```
from typing import List, Callable, Any, Optional, Dict, Tuple
from collections import defaultdict
import re
from sefaria.model.text import Ref, library, TextChunk
from sefaria.model.passage import Passage
from sefaria.model.topic import Topic, RefTopicLink
from sefaria.client.wrapper import get_links
from sefaria.datatype.jagged_array import JaggedTextArray
from sefaria_llm_interface.topic_prompt import TopicPromptGenerationOutput, TopicPromptInput, TopicPromptSource, TopicPromptCommentary
from sefaria_llm_interface import Topic as LLMTopic
from sefaria.utils.util import deep_update


def _lang_dict_by_func(func: Callable[[str], Any]) -> Dict[str, Any]:
    return {lang: func(lang) for lang in ('en', 'he')}


def _get_commentary_from_link_dict(link_dict: dict) -> Optional[TopicPromptCommentary]:
    if link_dict['category'] not in {'Commentary'}:
        return
    if not link_dict['sourceHasEn']:
        return
    commentary_text = _lang_dict_by_func(lambda lang: JaggedTextArray(link_dict.get('text' if lang == 'en' else 'he', '')).flatten_to_string())
    commentary_text = _lang_dict_by_func(lambda lang: re.sub(r"<[^>]+>", " ", TextChunk.strip_itags(commentary_text[lang])))
    return TopicPromptCommentary(
        ref=link_dict['sourceRef'],
        text=commentary_text
    )


def _get_commentary_for_tref(tref: str) -> List[TopicPromptCommentary]:
    """
    Return list of commentary for tref. Currently only considers English commentary.
    :param tref:
    :return: list where each element represents a single commentary on `tref`. Each element is a dict with keys `en`
    and `he` for the English and Hebrew text.
    """
    library.rebuild_toc()
    commentary = []

    for link_dict in get_links(tref, with_text=True):
        temp_commentary = _get_commentary_from_link_dict(link_dict)
        if not temp_commentary: continue
        commentary += [temp_commentary]
    return commentary


def _get_context_ref(segment_oref: Ref) -> Optional[Ref]:
    """
    Decide if `segment_oref` requires a context ref and if so, return it.
    A context ref is a ref which contains `segment_oref` and provides more context for it.
    E.g. Genesis 1 is a context ref for Genesis 1:13
    :param segment_oref:
    :return:
    """
    if segment_oref.primary_category == "Tanakh":
        return segment_oref.section_ref()
    elif segment_oref.index.get_primary_corpus() == "Bavli":
        try:
            passage = Passage.containing_segment(segment_oref)
            return passage.ref()
        except:
            return None
    return None


def _get_surrounding_text(oref: Ref) -> Optional[Dict[str, str]]:
    """
    Get the surrounding context text for `oref`. See _get_context_ref() for an explanation of what a context ref is.
    :param oref:
    :return: dict with keys "en" and "he" and values the English and Hebrew text of the surrounding text, respectively.
    """
    context_ref = _get_context_ref(oref)
    if context_ref:
        return _lang_dict_by_func(lambda lang: context_ref.text(lang).as_string())


def make_llm_topic(sefaria_topic: Topic) -> LLMTopic:
    """
    Return a dict that can be instantiated as `sefaria_interface.Topic` in the LLM repo.
    This represents the basic metadata of a topic for the LLM repo to process.
    :param sefaria_topic:
    :return:
    """
    return LLMTopic(
        slug=sefaria_topic.slug,
        description=getattr(sefaria_topic, 'description', {}),
        title=_lang_dict_by_func(sefaria_topic.get_primary_title)
    )


def make_topic_prompt_source(oref: Ref, context: str, with_commentary=True, normalize_text=True) -> TopicPromptSource:
    """
    Return a dict that can be instantiated as `sefaria_interface.TopicPromptSource` in the LLM repo.
    This represents the basic metadata of a source for the LLM repo to process.
    :param oref:
    :param context:
    :return:
    """

    index = oref.index
    def get_text_by_lang(lang: str) -> str:
        s = oref.text(lang).as_string()
        if normalize_text:
            s = re.sub(r"<[^>]+>", "", TextChunk.strip_itags(s))
        return s
    text = _lang_dict_by_func(get_text_by_lang)
    book_description = _lang_dict_by_func(lambda lang: getattr(index, f"{lang}Desc", "N/A"))
    book_title = _lang_dict_by_func(index.get_title)
    composition_time_period = index.composition_time_period()
    pub_year = composition_time_period.period_string("en") if composition_time_period else "N/A"
    try:
        author_name = Topic.init(index.authors[0]).get_primary_title("en") if len(index.authors) > 0 else "N/A"
    except AttributeError:
        author_name = "N/A"

    commentary = None
    if index.get_primary_category() == "Tanakh" and with_commentary:
        commentary = _get_commentary_for_tref(oref.normal())
    surrounding_text = _get_surrounding_text(oref)

    return TopicPromptSource(
        ref=oref.normal(),
        categories=index.categories,
        book_description=book_description,
        book_title=book_title,
        comp_date=pub_year,
        author_name=author_name,
        context_hint=context,
        text=text,
        commentary=commentary,
        surrounding_text=surrounding_text,
    )


def make_topic_prompt_input(lang: str, sefaria_topic: Topic, orefs: List[Ref], contexts: List[str]) -> TopicPromptInput:
    """
    Return a dict that can be instantiated as `sefaria_interface.TopicPromptInput` in the LLM repo.
    This represents the full input required for the LLM repo to generate topic prompts.
    :param lang:
    :param sefaria_topic:
    :param orefs:
    :param contexts:
    :return:
    """
    return TopicPromptInput(
        lang=lang,
        topic=make_llm_topic(sefaria_topic),
        sources=[make_topic_prompt_source(oref, context) for oref, context in zip(orefs, contexts)]
    )


def save_topic_prompt_output(output: TopicPromptGenerationOutput) -> None:
    for prompt in output.prompts:
        query = {
            "ref": prompt.ref,
            "toTopic": prompt.slug,
            "dataSource": "learning-team",
            "linkType": "about",
        }
        link = RefTopicLink().load(query)
        if link is None:
            link = RefTopicLink(query)
        curr_descriptions = getattr(link, "descriptions", {})
        description_edits = {output.lang: {
            "title": prompt.title, "ai_title": prompt.title,
            "prompt": prompt.prompt, "ai_prompt": prompt.prompt,
            "published": False, "review_state": "not reviewed"
        }}
        setattr(link, "descriptions", deep_update(curr_descriptions, description_edits))
        link.save()


def get_ref_context_hints_by_lang(ref_topic_links: List[dict]) -> Dict[str, List[Tuple[Ref, str]]]:
    """
    Helper function for topic generation API
    Returns dict where keys are the languages of ref_topic_links that should be generated and the values are the Refs
    and context hints that should be inputs to the generation process
    @param ref_topic_links:
    @return:
    """
    ref__context_hints_by_lang = defaultdict(list)
    for ref_topic_link in ref_topic_links:
        oref = Ref(ref_topic_link['ref'])
        description = ref_topic_link.get('descriptions', {})
        for lang, prompt_dict in description.items():
            context_hint = prompt_dict.get('ai_context', '')
            curr_prompt = prompt_dict.get('prompt', '')
            if context_hint and not curr_prompt:
                ref__context_hints_by_lang[lang] += [(oref, context_hint)]
    return ref__context_hints_by_lang

```

### sefaria/helper/linker/tasks.py

```
"""
Celery tasks for the LLM server
"""

from celery import signature
from sefaria.model import library
from sefaria.celery_setup.app import app
from sefaria.model.marked_up_text_chunk import MarkedUpTextChunk
from sefaria.model.text import Ref
from dataclasses import dataclass

@dataclass(frozen=True)
class LinkingArgs:
    ref: str
    text: str
    lang: str
    vtitle: str



@app.task(name="linker.link_segment_with_worker")
def link_segment_with_worker(linking_args_dict: dict) -> None:
    linking_args = LinkingArgs(**linking_args_dict)
    linker = library.get_linker(linking_args.lang)
    book_ref = Ref(linking_args.ref)
    output = linker.link(linking_args.text, book_context_ref=book_ref)

    spans = _extract_resolved_spans(output.resolved_refs)
    if not spans:
        return
    chunk = MarkedUpTextChunk({
        "ref": linking_args.ref,
        "versionTitle": linking_args.vtitle,
        "language": linking_args.lang,
        "spans": spans,
    })

    _replace_existing_chunk(chunk)
    chunk.save()


def _extract_resolved_spans(resolved_refs):
    spans = []
    for resolved_ref in resolved_refs:
        if resolved_ref.is_ambiguous:
            continue
        entity = resolved_ref.raw_entity
        spans.append({
            "charRange": entity.char_indices,
            "text": entity.text,
            "type": "citation",
            "ref": resolved_ref.ref.normal(),
        })
    return spans


def _replace_existing_chunk(chunk: MarkedUpTextChunk):
    existing = MarkedUpTextChunk().load({
        "ref": chunk.ref,
        "language": chunk.language,
        "versionTitle": chunk.versionTitle,
    })
    if existing:
        existing.delete()
```

### sefaria/helper/linker/__init__.py

```

```

### sefaria/helper/linker/linker.py

```
import dataclasses
import json
from cerberus import Validator
from sefaria.model.linker.ref_part import TermContext, RefPartType
from sefaria.model.linker.ref_resolver import PossiblyAmbigResolvedRef
from sefaria.model import text, library
from sefaria.model.webpage import WebPage
from sefaria.system.cache import django_cache
from api.api_errors import APIInvalidInputException
from typing import List, Optional, Tuple


FIND_REFS_POST_SCHEMA = {
    "text": {
        "type": "dict",
        "required": True,
        "schema": {
            "title": {"type": "string", "required": True},
            "body": {"type": "string", "required": True},
        },
    },
    "metaDataForTracking": {
        "type": "dict",
        "required": False,
        "schema": {
            "url": {"type": "string", "required": False},
            "description": {"type": "string", "required": False},
            "title": {"type": "string", "required": False},
        },
    },
    "lang": {
        "type": "string",
        "allowed": ["he", "en"],
        "required": False,
    },
    "version_preferences_by_corpus": {
        "type": "dict",
        "required": False,
        "nullable": True,
        "keysrules": {"type": "string"},
        "valuesrules": {
            "type": "dict",
            "schema": {
                "type": "string",
                "keysrules": {"type": "string"},
                "valuesrules": {"type": "string"},
            },
        },
    },
}


def make_find_refs_response(request):
    request_text, options, meta_data = _unpack_find_refs_request(request)
    if meta_data:
        _add_webpage_hit_for_url(meta_data.get("url", None))
    return _make_find_refs_response_with_cache(request_text, options, meta_data)


@dataclasses.dataclass
class _FindRefsTextOptions:
    """
    @attr debug: If True, adds field "debugData" to returned dict with debug information for matched refs.
    @attr max_segments: Maximum number of segments to return when `with_text` is true. 0 means no limit.
    @attr version_preferences_by_corpus: dict of dicts of the form { <corpus>: { <lang>: <vtitle> }}
    """

    with_text: bool = False
    debug: bool = False
    max_segments: int = 0
    version_preferences_by_corpus: dict = None


@dataclasses.dataclass
class _FindRefsText:
    title: str
    body: str
    lang: str


def _unpack_find_refs_request(request):
    validator = Validator(FIND_REFS_POST_SCHEMA)
    post_body = json.loads(request.body)
    if not validator.validate(post_body):
        raise APIInvalidInputException(validator.errors)
    meta_data = post_body.get('metaDataForTracking')
    return _create_find_refs_text(post_body), _create_find_refs_options(request.GET, post_body), meta_data


def _create_find_refs_text(post_body) -> _FindRefsText:
    from sefaria.utils.hebrew import is_mostly_hebrew
    title = post_body['text']['title']
    body = post_body['text']['body']
    lang = post_body['lang'] if 'lang' in post_body else 'he' if is_mostly_hebrew(body) else 'en'
    return _FindRefsText(title, body, lang)


def _create_find_refs_options(get_body: dict, post_body: dict) -> _FindRefsTextOptions:
    with_text: bool = bool(int(get_body.get("with_text", False)))
    debug: bool = bool(int(get_body.get("debug", False)))
    max_segments: int = int(get_body.get("max_segments", 0))
    version_preferences_by_corpus: dict = post_body.get("version_preferences_by_corpus")
    return _FindRefsTextOptions(with_text, debug, max_segments, version_preferences_by_corpus)


def _add_webpage_hit_for_url(url):
    if url is None: return
    webpage = WebPage().load(url)
    if not webpage: return
    webpage.add_hit()
    webpage.save()


@django_cache(cache_type="persistent")
def _make_find_refs_response_with_cache(request_text: _FindRefsText, options: _FindRefsTextOptions, meta_data: dict) -> dict:
    response = _make_find_refs_response_linker_v3(request_text, options)

    if meta_data:
        _, webpage = WebPage.add_or_update_from_linker({
            "url": meta_data['url'],
            "title": meta_data['title'],
            "description": meta_data['description'],
            "refs": _get_trefs_from_response(response),
        }, add_hit=False)
        if webpage:
            response['url'] = webpage.url
    return response


def _make_find_refs_response_linker_v3(request_text: _FindRefsText, options: _FindRefsTextOptions) -> dict:
    linker = library.get_linker(request_text.lang)
    title_doc = linker.link(request_text.title, type_filter='citation')
    context_ref = None
    if len(title_doc.resolved_refs) == 1 and not title_doc.resolved_refs[0].is_ambiguous:
        context_ref = title_doc.resolved_refs[0].ref
    body_doc = linker.link_by_paragraph(request_text.body, context_ref, with_failures=True, type_filter='citation')

    response = {
        "title": _make_find_refs_response_inner(title_doc.resolved_refs, options),
        "body": _make_find_refs_response_inner(body_doc.resolved_refs, options),
    }

    return response


def _make_find_refs_response_linker_v2(request_text: _FindRefsText, options: _FindRefsTextOptions) -> dict:
    response = {
        "title": _make_find_refs_response_inner_linker_v2(request_text.lang, request_text.title, options),
        "body": _make_find_refs_response_inner_linker_v2(request_text.lang, request_text.body, options),
    }
    return response


def _make_find_refs_response_inner_linker_v2(lang, text, options: _FindRefsTextOptions):
    import re
    ref_results = []
    ref_data = {}

    def _find_refs_action(ref, match: re.Match):
        nonlocal ref_results, ref_data
        tref = ref.normal()
        ref_results += [{
            "startChar": match.start(0),
            "endChar": match.end(0),
            "text": match.group(0),
            "linkFailed": False,
            "refs": [tref]
        }]
        ref_data[tref] = _make_ref_response_for_linker(ref, options)

    library.apply_action_for_all_refs_in_string(text, _find_refs_action, lang, citing_only=True)
    response = {
        "results": ref_results,
        "refData": ref_data
    }
    if options.debug:
        # debugData has no meaning for linker v2 since there are no ref parts
        response['debugData'] = []

    return response


def _get_trefs_from_response(response):
    trefs = []
    for key, value in response.items():
        if isinstance(value, dict) and 'refData' in value:
            trefs += list(value['refData'].keys())
    return trefs


def _make_find_refs_response_inner(resolved_ref_list: List[PossiblyAmbigResolvedRef], options: _FindRefsTextOptions):
    ref_results = []
    ref_data = {}
    debug_data = []
    for resolved_ref in resolved_ref_list:
        resolved_refs = resolved_ref.resolved_raw_refs if resolved_ref.is_ambiguous else [resolved_ref]
        start_char, end_char = resolved_ref.raw_entity.char_indices
        text = resolved_ref.pretty_text
        link_failed = resolved_refs[0].ref is None
        if not link_failed and resolved_refs[0].ref.is_book_level(): continue
        ref_results += [{
            "startChar": start_char,
            "endChar": end_char,
            "text": text,
            "linkFailed": link_failed,
            "refs": None if link_failed else [rr.ref.normal() for rr in resolved_refs]
        }]
        for rr in resolved_refs:
            if rr.ref is None: continue
            tref = rr.ref.normal()
            if tref in ref_data: continue
            ref_data[tref] = _make_ref_response_for_linker(rr.ref, options)
        if options.debug:
            debug_data += [[_make_debug_response_for_linker(rr) for rr in resolved_refs]]

    response = {
        "results": ref_results,
        "refData": ref_data
    }
    if options.debug:
        response['debugData'] = debug_data

    return response


def _make_ref_response_for_linker(oref: text.Ref, options: _FindRefsTextOptions) -> dict:
    res = {
        'heRef': oref.he_normal(),
        'url': oref.url(),
        'primaryCategory': oref.primary_category,
    }
    he, he_truncated = _get_ref_text_by_lang_for_linker(oref, "he", options)
    en, en_truncated = _get_ref_text_by_lang_for_linker(oref, "en", options)
    if options.with_text:
        res.update({
            'he': he,
            'en': en,
            'isTruncated': he_truncated or en_truncated,
        })

    return res


def _get_preferred_vtitle(oref: text.Ref, lang: str, version_preferences_by_corpus: dict) -> Optional[str]:
    vprefs = version_preferences_by_corpus
    corpus = oref.index.get_primary_corpus()
    # Make sure ref's corpus and current lang are specified in version_preferences_by_corpus.
    # If not, use default version
    if vprefs is None or corpus not in vprefs or lang not in vprefs[corpus]:
        return
    return vprefs[corpus][lang]


def _get_ref_text_by_lang_for_linker(oref: text.Ref, lang: str, options: _FindRefsTextOptions) -> Tuple[List[str], bool]:
    vtitle = _get_preferred_vtitle(oref, lang, options.version_preferences_by_corpus)
    chunk = text.TextChunk(oref, lang=lang, vtitle=vtitle, fallback_on_default_version=True)
    as_array = [chunk.strip_itags(s) for s in chunk.ja().flatten_to_array()]
    was_truncated = 0 < options.max_segments < len(as_array)
    return as_array[:options.max_segments or None], was_truncated


def _make_debug_response_for_linker(resolved_ref: PossiblyAmbigResolvedRef) -> dict:
    debug_data = {
        "orig_part_strs": [p.text for p in resolved_ref.raw_entity.raw_ref_parts],
        "orig_part_types": [p.type.name for p in resolved_ref.raw_entity.raw_ref_parts],
        "final_part_strs": [p.text for p in resolved_ref.raw_entity.parts_to_match],
        "final_part_types": [p.type.name for p in resolved_ref.raw_entity.parts_to_match],
        "resolved_part_strs": [p.term.slug if isinstance(p, TermContext) else p.text for p in resolved_ref.resolved_parts],
        "resolved_part_types": [p.type.name for p in resolved_ref.resolved_parts],
        "resolved_part_classes": [p.__class__.__name__ for p in resolved_ref.resolved_parts],
        "context_ref": resolved_ref.context_ref.normal() if resolved_ref.context_ref else None,
        "context_type": resolved_ref.context_type.name if resolved_ref.context_type else None,
    }
    if RefPartType.RANGE.name in debug_data['final_part_types']:
        range_part = next((p for p in resolved_ref.raw_entity.parts_to_match if p.type == RefPartType.RANGE), None)
        debug_data.update({
            'input_range_sections': [p.text for p in range_part.sections],
            'input_range_to_sections': [p.text for p in range_part.toSections]
        })
    return debug_data

```

### sefaria/helper/marked_up_text_chunk_generator.py

```

import structlog
from typing import List, Tuple
from sefaria.model.text import Ref, TextChunk
from sefaria.system.exceptions import InputError
from sefaria.helper.linker.tasks import link_segment_with_worker, LinkingArgs
from dataclasses import asdict


logger = structlog.get_logger(__name__)


class MarkedUpTextChunkGenerator:


    def __init__(self):
        """Initialize the generator with necessary components."""
        pass

    ## Public methods:

    def generate(self, ref: Ref, lang: str, vtitle: str) -> None:
        try:
            segment_refs = ref.all_segment_refs()
            logger.info(f"Generating MarkedUpTextChunks for {len(segment_refs)} segment refs from {ref.normal()}")

            for segment_ref in segment_refs:
                self._generate_single_segment_version(segment_ref, lang, vtitle)

        except Exception as e:
            logger.error(f"Error generating MarkedUpTextChunks for {ref.normal()}: {e}")
            raise

    def generate_from_ref(self, ref: Ref) -> None:
        try:
            segment_refs = ref.all_segment_refs()
            logger.info(f"Generating MarkedUpTextChunks for {len(segment_refs)} segment refs from {ref.normal()}")

            for segment_ref in segment_refs:
                self._generate_all_versions_for_segment(segment_ref)

        except Exception as e:
            logger.error(f"Error generating MarkedUpTextChunks for {ref.normal()}: {e}")
            raise

    ##  Private methods:

    def _create_and_save_marked_up_text_chunk(self, segment_ref: Ref, vtitle: str, lang: str, text: str) -> None:
        linking_args = LinkingArgs(ref=segment_ref.normal(), text=text, lang=lang, vtitle=vtitle)
        link_segment_with_worker.apply_async(args=[asdict(linking_args)], queue="linker")


    def _generate_all_versions_for_segment(self, segment_ref: Ref) -> None:
        lang_title_pairs = [(version.language, version.versionTitle)
                    for version in segment_ref.versionset().array()]
        for lang, vtitle in lang_title_pairs:
            text_chunk = TextChunk(segment_ref, lang=lang, vtitle=vtitle)
            if not text_chunk.text:
                logger.debug(f"No text found for {segment_ref.normal()}, {vtitle}, {lang}")
                continue
            self.generate(segment_ref, lang, vtitle)

    def _generate_single_segment_version(self, segment_ref: Ref, lang: str, vtitle: str) -> None:
        text_chunk = TextChunk(segment_ref, lang=lang, vtitle=vtitle)
        if not text_chunk.text:
            logger.debug(f"No text found for {segment_ref.normal()}, {vtitle}, {lang}")
            return

        try:
            self._create_and_save_marked_up_text_chunk(segment_ref, vtitle, lang, text_chunk.text)
        except Exception as e:
            logger.error(f"Failed to create/save MarkedUpTextChunk for {segment_ref.normal()}, {vtitle}, {lang}: {e}")
            raise
```

### sefaria/helper/legacy_ref.py

```
from typing import List, Union, Dict
import re

from sefaria.model.abstract import AbstractMongoRecord
from sefaria.model.text import Ref


class LegacyRefParserError(Exception):
    """
    Generic LegacyRefParser error
    """


class NoLegacyRefParserError(LegacyRefParserError):
    pass


class LegacyRefParserMappingKeyError(LegacyRefParserError, KeyError):
    pass


class LegacyRefParsingData(AbstractMongoRecord):
    """
    This class is a mongo backed data store for data to aid legacy ref parsing. 
    It can contain ref mapping or any other data we think of down the line to help in future cases.
    Imagine a structure for e.g. 
    ```
    {
        "index_title" : "Zohar",
        "data": {
            "mapping": { "old_ref 1" : "mapped_ref 1" ...}
        }
    }
    ```
    To be used with LegacyRefParser classes in this module.
    Current assumption is all trefs in the mapping (both old and mapped) are in URL form and are segment level.
    """
    collection = 'legacy_ref_data'
    criteria_field = 'title'
    pkeys = ["index_title"]
    required_attrs = [
        "index_title",
        "data",
    ]

    __slots__ = ['index_title', 'data']


class LegacyRefParser:
    """
    Currently empty super class used for type hints and to make the code more flexible in the future
    """

    def parse(self, legacy_tref: str) -> Ref:
        pass


class NonExistentLegacyRefParser(LegacyRefParser):
    """
    This class acts as a semantic indication of a lack of LegacyRefParser
    Currently used in `LegacyRefParserHandler` as return value when no parser is found
    Doesn't inherit from `LegacyRefParser` since it doesn't define the same contract (or any contract)
    """

    def parse(self, legacy_tref: str) -> Ref:
        raise Exception("Not implemented")


NON_EXISTENT_LEGACY_REF_PARSER = NonExistentLegacyRefParser()


class MappingLegacyRefParser(LegacyRefParser):
    """
    Parses legacy refs using a mapping from old ref -> new ref
    Assumption for now is this class can only map refs that are either
    - segment level
    - ranged segment level but not spanning sections
    """
    
    def __init__(self, data: LegacyRefParsingData):
        self._mapping: Dict[str, str] = data.data['mapping']

    def parse(self, legacy_tref: str) -> Ref:
        """

        @param legacy_tref:
        @return:
        """
        legacy_tref = self.__to_url_form(legacy_tref)
        if self.__is_ranged_ref(legacy_tref):
            return self.__parse_ranged_ref(legacy_tref)
        return self.__parse_segment_ref(legacy_tref)

    @staticmethod
    def __to_url_form(tref: str):
        """
        replace last space before sections with a period
        AND
        then replace remaining spaces with underscore
        @param tref:
        @return:
        """
        return re.sub(r" (?=[\d.:ab]+$)", ".", tref).replace(" ", "_")

    @staticmethod
    def __is_ranged_ref(tref: str) -> bool:
        return "-" in tref

    @staticmethod
    def __range_list(ranged_tref: str) -> List[str]:
        segment_range_match = re.search(r'(\d+)-(\d+)$', ranged_tref)
        if segment_range_match is None:
            return [ranged_tref]
        start_segment = int(segment_range_match.group(1))
        end_segment = int(segment_range_match.group(2))
        base_tref = ranged_tref[:segment_range_match.start(0)]

        range_list = []
        for segment_num in range(start_segment, end_segment+1):
            range_list += [f"{base_tref}{segment_num}"]

        return range_list

    def __get_mapped_tref(self, legacy_tref: str) -> str:
        try:
            return self._mapping[legacy_tref]
        except KeyError as err:
            raise LegacyRefParserMappingKeyError(*err.args)

    def __parse_segment_ref(self, legacy_tref: str) -> Ref:
        converted_tref = self.__get_mapped_tref(legacy_tref)
        converted_ref = Ref(converted_tref)
        converted_ref.legacy_tref = legacy_tref
        return converted_ref

    def __parse_ranged_ref(self, legacy_tref: str) -> Ref:
        parsed_range_list = [self.__parse_segment_ref(temp_tref) for temp_tref in self.__range_list(legacy_tref)]
        parsed_range_list.sort(key=lambda x: x.order_id())  # not assuming mapping is in order
        ranged_oref = parsed_range_list[0].to(parsed_range_list[-1])
        ranged_oref.legacy_tref = legacy_tref
        return ranged_oref


# Python type hint which is either a valid `LegacyRefParser` or a lack of one
PossiblyNonExistentLegacyRefParser = Union[LegacyRefParser, NonExistentLegacyRefParser]


class LegacyRefParserHandler:
    """
    pattern copied from django.core.cache.CacheHandler
    This just makes sure to load the correct legacy ref parser class given an index title
    """
    def __init__(self):
        self._parsers: Dict[str, PossiblyNonExistentLegacyRefParser] = {}

    def __getitem__(self, index_title: str) -> LegacyRefParser:
        parser = self.__get_parser(index_title)
        if isinstance(parser, NonExistentLegacyRefParser):
            raise NoLegacyRefParserError(f"Could not find proper legacy parser matching index title '{index_title}'")
        return parser

    def __get_parser(self, index_title: str) -> PossiblyNonExistentLegacyRefParser:
        try:
            return self._parsers[index_title]
        except KeyError:
            pass
        try:
            parser = self.__create_legacy_parser(index_title)
        except NoLegacyRefParserError as e:
            parser = NON_EXISTENT_LEGACY_REF_PARSER
        self._parsers[index_title] = parser
        return parser


    @staticmethod
    def __load_data(index_title: str) -> LegacyRefParsingData:
        """
        Load mapping from the DB
        @return:
        """
        lrpd = LegacyRefParsingData().load({"index_title": index_title})
        if lrpd is None:
            raise NoLegacyRefParserError(f"No LegacyRefParser for index title '{index_title}'")
        return lrpd

    @staticmethod
    def __create_legacy_parser(index_title: str, **kwargs) -> LegacyRefParser:
        """
        Currently, only returns one type of LegacyRefParser but in the future can determine type from the data
        determine the type from the data
        """
        data = LegacyRefParserHandler.__load_data(index_title)
        return MappingLegacyRefParser(data)


legacy_ref_parser_handler = LegacyRefParserHandler()

```

### sefaria/helper/crm/crm_mediator.py

```
from sefaria.helper.crm.crm_factory import CrmFactory
from sefaria.helper.crm.crm_info_store import CrmInfoStore
from sefaria import settings as sls


# todo: task queue, async

class CrmMediator:
    def __init__(self):
        self._crm_connection = CrmFactory().get_connection_manager()

    def create_crm_user(self, email, first_name, last_name, lang="en", educator=False):
        try:
            crm_id = self._crm_connection.add_user_to_crm(email, first_name, last_name, lang, educator)
            if crm_id:
                CrmInfoStore.save_crm_id(crm_id, email, sls.CRM_TYPE)
                return True
            else:
                return False
        except:
            return False

    def subscribe_to_lists(self, email, first_name, last_name, educator=False, lang="en", mailing_lists=None):
        return self._crm_connection.subscribe_to_lists(email, first_name, last_name, educator, lang, mailing_lists)

    def sync_sustainers(self):
        current_sustainers = CrmInfoStore.get_current_sustainers()
        for crm_sustainer in self._crm_connection.get_sustainers():
            crm_sustainer_profile = CrmInfoStore.find_sustainer_profile(crm_sustainer)
            if current_sustainers.get(crm_sustainer.id) is not None:  # keep current sustainer
                del current_sustainers[crm_sustainer.id]
            else:
                CrmInfoStore.mark_sustainer(crm_sustainer_profile)

        for sustainer_to_remove in current_sustainers:
            CrmInfoStore.mark_sustainer(sustainer_to_remove, False)

    def mark_as_spam_in_crm(self, uid=None, email=None, profile=None):
        """
        Marks user as spam in CRM if user exists
        """
        crm_id = CrmInfoStore.get_crm_id(uid, email, profile)
        if crm_id:
            self._crm_connection.mark_as_spam_in_crm(crm_id)
        else:
            return False

    def mark_for_review_in_crm(self, uid=None, email=None, profile=None):
        """
        Marks user as spam in CRM if user exists
        """
        crm_id = CrmInfoStore.get_crm_id(uid, email, profile)
        if crm_id:
            if self._crm_connection.mark_for_review_in_crm(crm_id):
                return True
        return False

    def update_user_email(self, new_email, uid=None, email=None, profile=None):
        """
        Updates user CRM if user exists
        """
        crm_id = CrmInfoStore.get_crm_id(uid, email, profile)
        if crm_id:
            return self._crm_connection.change_user_email(crm_id, new_email)
        else:
            return False

    def get_and_save_crm_id(self, email=None):
        crm_id = self._crm_connection.find_crm_id(email)
        if crm_id:
            CrmInfoStore.save_crm_id(crm_id, email)
        else:
            return False

    def get_available_lists(self):
        return self._crm_connection.get_available_lists()

```

### sefaria/helper/crm/nationbuilder.py

```
from urllib.parse import unquote
from rauth import OAuth2Service
import time
import json

from sefaria import settings as sls
from sefaria.helper.crm.crm_connection_manager import CrmConnectionManager

base_url = "https://" + sls.NATIONBUILDER_SLUG + ".nationbuilder.com"


class NationbuilderConnectionManager(CrmConnectionManager):
    def __init__(self):
        CrmConnectionManager.__init__(self, base_url)

    def _get_connection(self):
        access_token_url = "http://%s.nationbuilder.com/oauth/token" % sls.NATIONBUILDER_SLUG
        authorize_url = "%s.nationbuilder.com/oauth/authorize" % sls.NATIONBUILDER_SLUG
        service = OAuth2Service(
            client_id=sls.NATIONBUILDER_CLIENT_ID,
            client_secret=sls.NATIONBUILDER_CLIENT_SECRET,
            name="NationBuilder",
            authorize_url=authorize_url,
            access_token_url=access_token_url,
            base_url=base_url
        )
        token = sls.NATIONBUILDER_TOKEN
        session = service.get_session(token)
        return session

    def add_user_to_crm(self, email, first_name=None, last_name=None, lang="en", educator=False, signup=True):
        lists = []
        if lang == "en":
            if educator:
                lists.append("Announcements_Edu")
            lists.append("Announcements_General")
        else:
            if educator:
                lists.append("Announcements_Edu_Hebrew")
            lists.append("Announcements_General_Hebrew")
        if signup:
            lists.append("Signed_Up_on_Sefaria")
        else:
            lists.append("Newsletter_Sign_Up")

        tags = lists
        post = {
            "person": {
                "email": email,
                "tags": tags,
            }
        }
        if first_name:
            post["person"]["first_name"] = first_name
        if last_name:
            post["person"]["last_name"] = last_name

        r = self.session.put("https://" + sls.NATIONBUILDER_SLUG + ".nationbuilder.com/api/v1/people/push",
                             data=json.dumps(post),
                             params={'format': 'json'},
                             headers={'content-type': 'application/json'})

        try:  # add nationbuilder id to user profile
            nationbuilder_user = r.json()
            nationbuilder_id = nationbuilder_user["person"]["id"] if "person" in nationbuilder_user else \
                nationbuilder_user["id"]
            return nationbuilder_id
        except:
            return False

        return True

    def subscribe_to_lists(self, email, first_name=None, last_name=None, lang="en", educator=False):
        CrmConnectionManager.subscribe_to_lists(self,email, first_name, last_name, lang, educator)
        return self.add_user_to_crm(email, first_name, last_name, lang, educator, signup=False)

    def nationbuilder_get_all(self, endpoint_func, args=[]):
        next_endpoint = endpoint_func(*args)
        while next_endpoint:
            # print(next_endpoint)
            for attempt in range(0, 3):
                try:
                    res = self.session.get(base_url + next_endpoint)
                    res_data = res.json()
                    for item in res_data['results']:
                        yield item
                    next_endpoint = unquote(res_data['next']) if res_data['next'] else None
                    if 'nation-ratelimit-remaining' in res.headers and res.headers['nation-ratelimit-remaining'] == '0':
                        time.sleep(10)
                        print('sleeping')
                    break
                except Exception as e:
                    time.sleep(5)
                    print("Trying again to access and process {}. Attempts: {}. Exception: {}".format(next_endpoint,
                                                                                                      attempt + 1, e))
                    print(next_endpoint)

    def get_sustainers(self):
        for nationbuilder_sustainer in self.nationbuilder_get_all(self.get_by_tag, ['sustainer_current_engineering']):
            yield {
                "email": nationbuilder_sustainer["email"]
            }

    def mark_as_spam_in_crm(self, nationbuilder_id):
        """
        Deletes spam users from nationbuilder if they are determined to be spam.
        """
        r = self.session.get(self.update_person(nationbuilder_id))
        try:
            # If user is only signed up for junk tags, delete from CRM
            tags = [x for x in r.json()["person"]["tags"] if
                    x.lower() not in ["announcements_general_hebrew", "announcements_general",
                                      "announcements_edu_hebrew",
                                      "announcements_edu", "signed_up_on_sefaria", "spam"]]
            if len(tags) == 0:
                self.session.delete(self.update_person(nationbuilder_id))
            return True
        except Exception as e:
            print(f"Failed to delete. Error: {e}")
            return False

    def mark_for_review_in_crm(self, crm_id):
        return True

    def find_crm_id(self, email=None):
        # This will not be implemented for NB because it will never be used
        CrmConnectionManager.find_crm_id(self, email=email)
        pass

    def __del__(self):
        self.session.close()

    @staticmethod
    def get_by_tag(tag_name):
        return f"/api/v1/tags/{tag_name}/people"

    @staticmethod
    def update_person(id):
        return f"/api/v1/people/{id}"

```

### sefaria/helper/crm/crm_connection_manager.py

```
import re

class CrmConnectionManager(object):
    def __init__(self, base_url):
        self.base_url = base_url
        self.session = self._get_connection()


    def _get_connection(self):
        """
        Authenticate application & return a Requests session object
        """
        pass

    def add_user_to_crm(self, email, first_name, last_name, lang="en", educator=False):
        """
        Add a new Sefaria app user to the CRM.
        Returns user id if successful
        Returns false if no user added
        """
        pass

    def change_user_email(self, uid, new_email):
        """
        Update a user's email in the CRM.
        """
        CrmConnectionManager.validate_email(new_email)

    def mark_as_spam_in_crm(self, crm_id):
        """
        Do in CRM whatever decision has been made to do about spam users.
        """
        pass

    def mark_for_review_in_crm(self, crm_id):
        """
        Do in CRM whatever decision has been made to do about spam users.
        """
        pass

    def subscribe_to_lists(self, email, first_name=None, last_name=None, educator=False, lang="en"):
        CrmConnectionManager.validate_email(email)
        CrmConnectionManager.validate_name(first_name)
        CrmConnectionManager.validate_name(last_name)

    def find_crm_id(self, email=None):
        """
        Find CRM ID in CRM given certain information
        """
        pass

    @staticmethod
    def validate_name(name):
        if len(name) < 20 and re.fullmatch(r"^[\w\- \u0590-\u05fe]+$", name):
            return True
        else:
            raise ValueError("Invalid Name")

    @staticmethod
    def validate_email(name):
        if re.fullmatch(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b', name):
            return True
        else:
            raise ValueError("Invalid Email")

```

### sefaria/helper/crm/salesforce.py

```
import base64
import requests
import time
import json

from sefaria.helper.crm.crm_connection_manager import CrmConnectionManager
from sefaria import settings as sls

from typing import Any, Optional

class SalesforceNewsletterListRetrievalError(Exception):
    pass

class SalesforceConnectionManager(CrmConnectionManager):
    def __init__(self):
        CrmConnectionManager.__init__(self, sls.SALESFORCE_BASE_URL)
        self.version = "56.0"
        self.resource_prefix = f"services/data/v{self.version}/sobjects/"

    def create_endpoint(self, *args):
        return f"{sls.SALESFORCE_BASE_URL}/{self.resource_prefix}{'/'.join(args)}"

    def make_request(self, request, **kwargs):
        for attempt in range(0,3):
            try:
                return request(**kwargs).json()
            except Exception as e:
                print(e)
                time.sleep(1)

    def get(self, endpoint):
        headers = {'Content-type': 'application/json', 'Accept': 'application/json'}
        return self.session.get(endpoint, headers=headers)

    def post(self, endpoint, **kwargs):
        headers = {'Content-type': 'application/json', 'Accept': 'application/json'}
        return self.session.post(endpoint, headers=headers, **kwargs)

    def patch(self, endpoint, **kwargs):
        headers = {'Content-type': 'application/json', 'Accept': 'application/json'}
        return self.session.patch(endpoint, headers=headers, **kwargs)

    def _get_connection(self):
        access_token_url = "%s/services/oauth2/token?grant_type=client_credentials" % self.base_url
        base64_auth = base64.b64encode((sls.SALESFORCE_CLIENT_ID + ":" + sls.SALESFORCE_CLIENT_SECRET).encode("ascii"))\
            .decode("ascii")
        headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
            'Authorization': 'Basic %s' % base64_auth
        }
        basic_res = requests.post(access_token_url, headers=headers)
        basic_data = basic_res.json()
        session = requests.Session()
        session.headers.update({
            'Authorization': 'Bearer %s' % basic_data['access_token']
        })
        return session

    def add_user_to_crm(self, email, first_name, last_name, lang="en", educator=False):
        """
        Adds a new user to the CRM and subscribes them to the specified lists.
        Returns CRM access info
        """
        CrmConnectionManager.add_user_to_crm(self, email, first_name, last_name, lang, educator)
        if lang == "he":
            language = "Hebrew"
        else:
            language = "English"

        res = self.post(self.create_endpoint("Sefaria_App_User__c"),
                  json={
                      "First_Name__c": first_name,
                      "Last_Name__c": last_name,
                      "Sefaria_App_Email__c": email,
                      "Hebrew_English__c": language,
                      "Educator__c": educator
                  })

        try:  # add salesforce id to user profile
            nationbuilder_user = res.json() # {'id': '1234', 'success': True, 'errors': []}
            return nationbuilder_user['id']

        except:
            return False
        return res

    def change_user_email(self, uid, new_email):
        """
        Changes user email and returns true if successful
        """
        CrmConnectionManager.change_user_email(self, uid, new_email)
        res = self.patch(self.create_endpoint("Sefaria_App_User__c", uid),
                 json={
                     "Sefaria_App_Email__c": new_email
                 })
        try:
            return res.status_code == 204
        except:
            return False
        return res

    def mark_as_spam_in_crm(self, uid):
        CrmConnectionManager.mark_as_spam_in_crm(self, uid)
        res = self.patch(self.create_endpoint("Sefaria_App_User__c", uid),
                         json={
                             "Manual_Review_Required__c": True
                         })
        try:
            return res.status_code == 204
        except:
            return False

    def mark_for_review_in_crm(self, crm_id):
        return self.mark_as_spam_in_crm(crm_id)

    def find_crm_id(self, email=None):
        if email:
            CrmConnectionManager.validate_email(email)
        CrmConnectionManager.find_crm_id(self, email=email)
        res = self.get(self.create_endpoint(f"query?=SELECT+id+FROM+Sefaria_App_User__c+WHERE+Sefaria_App_Email__c='{email}'"))
        try:
            print(res)
            print(res.json())
            return res.json()["records"][0]["Id"]
        except:
            return False

    def subscribe_to_lists(
        self, 
        email: str, 
        first_name: Optional[str] = None, 
        last_name: Optional[str] = None, 
        educator: bool = False,
        lang: str = "en",
        mailing_lists: Optional[list[str]] = None) -> Any:

        mailing_lists = mailing_lists or []

        CrmConnectionManager.subscribe_to_lists(self, email, first_name, last_name, educator, lang)
        if lang == "he":
            language = "Hebrew"
        else:
            language = "English"

        json_string = json.dumps({
                      "Action": "Newsletter",
                      "First_Name__c": first_name,
                      "Last_Name__c": last_name,
                      "Sefaria_App_Email__c": email,
                      "Hebrew_English__c": language,
                      "Educator__c": educator,
                      "Newsletter_Names__c": mailing_lists
                  })
        res = self.post(self.create_endpoint("Sefaria_App_Data__c"),
                        json={
                            "JSON_STRING__c": json_string
                        })
        try:
            return res.status_code == 201
        except:
            return False
        return res

    def get_available_lists(self) -> list[str]:
        try:
            resource_prefix = f"services/data/v{self.version}/query"
            endpoint = f"{sls.SALESFORCE_BASE_URL}/{resource_prefix}/"
            response = self.get(endpoint + "?q=SELECT+Subscriptions__c+FROM+AC_to_SF_List_Mapping__mdt")
            records = response.json()["records"]
            return [record["Subscriptions__c"] for record in records]
        except (requests.RequestException, KeyError, json.JSONDecodeError, AttributeError):
            raise SalesforceNewsletterListRetrievalError("Unable to retrieve newsletter mailing lists from Salesforce CRM")

```

### sefaria/helper/crm/dummy_crm.py

```
from sefaria.helper.crm.crm_connection_manager import CrmConnectionManager


class DummyConnectionManager(CrmConnectionManager):
    def __init__(self):
        CrmConnectionManager.__init__(self, "")

    def _get_connection(self):
        return {}

    def add_user_to_crm(self, email, first_name=None, last_name=None, lang="en", educator=False):
        CrmConnectionManager.add_user_to_crm(self, email, first_name, last_name, lang, educator)
        return True

    def mark_as_spam_in_crm(self, crm_id):
        return True

    def mark_for_review_in_crm(self, crm_id):
        return True

    def change_user_email(self, uid, new_email):
        return True

    def subscribe_to_lists(self, email, first_name=None, last_name=None, educator=False, lang="en"):
        CrmConnectionManager.subscribe_to_lists(self, email, first_name, last_name, educator, lang)
        return True

    def find_crm_id(self, email=None):
        CrmConnectionManager.find_crm_id(self, email=email)
        return True

```

### sefaria/helper/crm/tests/crm_mediator_test.py

```
from unittest import TestCase
from unittest.mock import Mock, patch, MagicMock
from sefaria.helper.crm.crm_factory import CrmFactory
from sefaria.helper.crm.crm_info_store import CrmInfoStore
import sys
import copy
from sefaria.helper.crm.crm_mediator import CrmMediator

crm_factory_stub = Mock()
fake_connection_manager = Mock()
crm_factory_stub.get_connection_manager.return_value = fake_connection_manager


class TestCrmMediatorInit(TestCase):
    def test_sets_crm_connection(self):
        with patch('sefaria.helper.crm.crm_factory.CrmFactory.__new__') as mock_factory:
            mock_factory.return_value = crm_factory_stub
            crm_mediator = CrmMediator()
            assert crm_mediator._crm_connection == fake_connection_manager


class TestCrmMediatorCreate(TestCase):
    def test_returns_true_if_id_returned(self):
        crm_mediator = CrmMediator()
        crm_mediator._crm_connection = Mock()
        crm_mediator._crm_connection.add_user_to_crm.return_value = 1
        assert crm_mediator.create_crm_user("fake@fake.com", "joe", "shmo") is True

    def test_stores_crm_id_if_true(self):
        with patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.save_crm_id') as mock_save:
            crm_mediator = CrmMediator()
            crm_mediator._crm_connection = Mock()
            crm_mediator._crm_connection.add_user_to_crm.return_value = 1
            crm_mediator.create_crm_user("fake@fake.com", "joe", "shmo") is True
            assert mock_save.called is True

    def test_returns_false_if_id_not_returned(self):
        crm_mediator = CrmMediator()
        crm_mediator._crm_connection = Mock()
        crm_mediator._crm_connection.add_user_to_crm.return_value = False
        assert crm_mediator.create_crm_user("fake@fake.com", "joe", "shmoo") is not True


class Sustainer():
    def __init__(self, id):
        self.id = id


class TestSyncSustainers(TestCase):
    def __init__(self, *args, **kwargs):
        super(TestSyncSustainers, self).__init__(*args, **kwargs)
        self.current_sustainers = {
            1: "sustainer_1",
            2: "sustainer_2",
            5: "sustainer_5"
        }

        self.crm_sustainers = [
            Sustainer(1),
            Sustainer(3),
            Sustainer(4)
        ]

    def test_marks_sustainers_that_dont_exist(self):
        with patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.mark_sustainer') as mock_mark, \
                patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.get_current_sustainers') as mock_get_current, \
                patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.find_sustainer_profile') as mock_find:
            crm_mediator = CrmMediator()
            crm_mediator._crm_connection = Mock()
            current_sustainers = copy.copy(self.crm_sustainers)
            crm_mediator._crm_connection.get_sustainers.return_value = current_sustainers
            mock_get_current.return_value = copy.copy(self.current_sustainers)
            mock_get_current.return_value = copy.copy(self.current_sustainers)
            mock_find.side_effect = lambda x: x
            crm_mediator.sync_sustainers()
            mock_mark.assert_any_call(current_sustainers[1])
            mock_mark.assert_any_call(current_sustainers[2])

    def test_removes_no_longer_sustainers(self):
        with patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.mark_sustainer') as mock_mark, \
                patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.get_current_sustainers') as mock_get_current, \
                patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.find_sustainer_profile') as mock_find:
            crm_mediator = CrmMediator()
            crm_mediator._crm_connection = Mock()
            crm_mediator._crm_connection.get_sustainers.return_value = copy.copy(self.crm_sustainers)
            mock_get_current.return_value = copy.copy(self.current_sustainers)
            mock_find.side_effect = lambda x: x
            crm_mediator.sync_sustainers()
            mock_mark.assert_any_call(5, False)


class TestMarkAsSpam(TestCase):
    def test_gets_id_marks_spam(self):
        with patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.get_crm_id') as mock_get_id:
            mock_get_id.return_value = 6
            crm_mediator = CrmMediator()
            crm_mediator._crm_connection = MagicMock()
            crm_mediator.mark_as_spam_in_crm(1, "hi@hi.com", "abc")
            mock_get_id.assert_called_with(1, "hi@hi.com", "abc")
            crm_mediator._crm_connection.mark_as_spam_in_crm.assert_called_with(6)


class TestMarkForReview(TestCase):
    def test_gets_id_marks_for_review(self):
        with patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.get_crm_id') as mock_get_id:
            mock_get_id.return_value = 6
            crm_mediator = CrmMediator()
            crm_mediator._crm_connection = MagicMock()
            marked_for_review = crm_mediator.mark_for_review_in_crm(1, "hi@hi.com", "abc")
            mock_get_id.assert_called_with(1, "hi@hi.com", "abc")
            crm_mediator._crm_connection.mark_for_review_in_crm.assert_called_with(6)
            assert marked_for_review is True

    def test_returns_false_doesnt_call_mark_for_review_if_no_crm_id(self):
        with patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.get_crm_id') as mock_get_id:
            mock_get_id.return_value = False
            crm_mediator = CrmMediator()
            crm_mediator._crm_connection = MagicMock()
            marked_for_review = crm_mediator.mark_for_review_in_crm(1, "hi@hi.com", "abc")
            mock_get_id.assert_called_with(1, "hi@hi.com", "abc")
            crm_mediator._crm_connection.mark_for_review_in_crm.assert_not_called()
            assert marked_for_review is False

    def test_returns_false_if_crm_mark_as_spam_returns_false(self):
        with patch('sefaria.helper.crm.crm_info_store.CrmInfoStore.get_crm_id') as mock_get_id:
            mock_get_id.return_value = 6
            crm_mediator = CrmMediator()
            crm_mediator._crm_connection = MagicMock()
            crm_mediator._crm_connection.mark_for_review_in_crm.return_value = False
            marked_for_review = crm_mediator.mark_for_review_in_crm(1, "hi@hi.com", "abc")
            mock_get_id.assert_called_with(1, "hi@hi.com", "abc")
            assert marked_for_review is False

```

### sefaria/helper/crm/tests/crm_connection_manager_test.py

```
from unittest import TestCase

from sefaria.helper.crm.nationbuilder import NationbuilderConnectionManager
from sefaria.helper.crm.salesforce import SalesforceConnectionManager

"""
class TestConnectionTest(TestCase):
    def __init__(self):
        self.nb_connection = NationbuilderConnectionManager()
        self.sf_connection = SalesforceConnectionManager()
        self.connections = [self.nb_connection, self.sf_connection]

    def test_subscribes_user(self):
        for connection in self.connections:
"""
```

### sefaria/helper/crm/__init__.py

```

```

### sefaria/helper/crm/crm_info_store.py

```
from sefaria.model.user_profile import UserProfile
from sefaria import settings as sls
from sefaria.system.database import db

import structlog

class CrmInfoStore(object):

    @staticmethod
    def save_crm_id(crm_id, email, crm_type, profile=None):
        """
        Saves CRM id to the database with the correct field
        """
        if crm_type == "NATIONBUILDER":
            if profile:
                user_profile = profile
            else:
                user_profile = UserProfile(email=email, user_registration=True)
            if user_profile.id is not None and user_profile.nationbuilder_id != crm_id:
                user_profile.nationbuilder_id = crm_id
                user_profile.save()
                return True
            return False
        elif crm_type == "SALESFORCE":
            if profile:
                user_profile = profile
            else:
                user_profile = UserProfile(email=email, user_registration=True)
            if user_profile.id is not None and user_profile.sf_app_user_id != crm_id:
                user_profile.sf_app_user_id = crm_id
                user_profile.save()
                return True
            return False
        elif crm_type == "NONE" or not crm_type:
            return True

    @staticmethod
    def get_crm_id(uid=None, email=None, profile=None, crm_type=sls.CRM_TYPE):
        if profile:
            user_profile = profile
        elif email:
            user_profile = UserProfile(email=email)
        elif uid:
            user_profile = UserProfile(id=uid)
        else:
            raise RuntimeError("Expected a uid, email, or profile to be provided")

        if crm_type == "NATIONBUILDER":
            return user_profile.nationbuilder_id
        elif crm_type == "SALESFORCE":
            return user_profile.sf_app_user_id
        elif crm_type == "NONE":
            return True

    @staticmethod
    def get_current_sustainers():
        return {profile["id"]: profile for profile in db.profiles.find({"is_sustainer": True})}

    @staticmethod
    def find_sustainer_profile(sustainer):
        if sustainer['email']:
            return UserProfile(email=sustainer['email'])

    @staticmethod
    def mark_sustainer(profile, is_sustainer=True):
        profile.update({"is_sustainer": is_sustainer})
        profile.save()

```

### sefaria/helper/crm/crm_factory.py

```
from sefaria import settings as sls
from sefaria.helper.crm.nationbuilder import NationbuilderConnectionManager
from sefaria.helper.crm.salesforce import SalesforceConnectionManager
from sefaria.helper.crm.dummy_crm import DummyConnectionManager


class CrmFactory(object):
    def __init__(self):
        self.crm_type = sls.CRM_TYPE

    def get_connection_manager(self):
        if self.crm_type == "NATIONBUILDER":
            return NationbuilderConnectionManager()
        elif self.crm_type == "SALESFORCE":
            return SalesforceConnectionManager()
        elif self.crm_type == "NONE" or not self.crm_type:
            return DummyConnectionManager()
        else:
            return DummyConnectionManager()

```

### sefaria/helper/community_page.py

```
import re
import csv
import requests
import random
from datetime import datetime, timedelta
from io import StringIO
from pprint import pprint

from django.utils import timezone

from sefaria.model import Ref, Topic, Collection
from sefaria.sheets import get_sheet, sheet_to_dict
from sefaria.system.database import db
from sefaria.utils.calendars import get_parasha
from sefaria.utils.hebrew import has_hebrew, hebrew_term, hebrew_parasha_name
from sefaria.utils.util import strip_tags
from sefaria.helper.topic import get_topic_by_parasha
from sefaria.system.cache import django_cache, delete_cache_elem, cache_get_key, in_memory_cache


def get_community_page_data(language="english", refresh=False):
  """
  Returns (nearly) raw data from Community page spreadsheet for `language`. Manages caching of date in memeory.
  """
  data = in_memory_cache.get("community-page-data-{}".format(language))
  if data and not refresh:
    return data

  urls = {
    "english": {
      "parashah": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSoHRVY9Z5MNhERjolxXzQ6Efp3SFTniHMgkSORWFPlkwoj5ppYeP8AyTX7yG_LcQ3p165iRNfOpOSZ/pub?output=csv',
      "calendar": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSoHRVY9Z5MNhERjolxXzQ6Efp3SFTniHMgkSORWFPlkwoj5ppYeP8AyTX7yG_LcQ3p165iRNfOpOSZ/pub?gid=1789079733&single=true&output=csv',
      "discover": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSoHRVY9Z5MNhERjolxXzQ6Efp3SFTniHMgkSORWFPlkwoj5ppYeP8AyTX7yG_LcQ3p165iRNfOpOSZ/pub?gid=2070604890&single=true&output=csv',
      "featured": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSoHRVY9Z5MNhERjolxXzQ6Efp3SFTniHMgkSORWFPlkwoj5ppYeP8AyTX7yG_LcQ3p165iRNfOpOSZ/pub?gid=1926549189&single=true&output=csv',
    },
    "hebrew": {
      "parashah": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRefP0BMml1sC6Ic50t2ekkLLIh3SIH9uYEBjWmdRwmBGs0-NDFFhjU3vW_tFzj_ATpK2PwqNdpVwQ4/pub?gid=0&single=true&output=csv',
      "calendar": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRefP0BMml1sC6Ic50t2ekkLLIh3SIH9uYEBjWmdRwmBGs0-NDFFhjU3vW_tFzj_ATpK2PwqNdpVwQ4/pub?gid=1789079733&single=true&output=csv',
      "discover": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRefP0BMml1sC6Ic50t2ekkLLIh3SIH9uYEBjWmdRwmBGs0-NDFFhjU3vW_tFzj_ATpK2PwqNdpVwQ4/pub?gid=2070604890&single=true&output=csv',
      "featured": 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRefP0BMml1sC6Ic50t2ekkLLIh3SIH9uYEBjWmdRwmBGs0-NDFFhjU3vW_tFzj_ATpK2PwqNdpVwQ4/pub?gid=1926549189&single=true&output=csv',
    },
  }

  data = {
    "parashah": load_data_from_sheet(urls[language]["parashah"]),
    "calendar": load_data_from_sheet(urls[language]["calendar"]),
    "discover": load_data_from_sheet(urls[language]["discover"]),
    "featured": load_data_from_sheet(urls[language]["featured"]),
  }

  in_memory_cache.set("community-page-data-{}".format(language), data, timeout=60 * 60)

  return data


def load_data_from_sheet(url):
  """
  Returns data from a single spreadsheet URL, formatted as keyed objects according to sheet headers, translating Hebrew headers into English.
  """
  response = requests.get(url)
  data     = response.content.decode("utf-8")
  cr       = csv.reader(StringIO(data))
  rows     = list(cr)
  fields   = [translate_labels(f) for f in rows[1]]
  data     = rows[2:]

  keyed_data = []
  for row in data:
    row_data = {}
    for i in range(len(fields)):
      row_data[fields[i]] = row[i]
    keyed_data.append(row_data)

  return keyed_data


def get_community_page_items(date=None, language="english", diaspora=True, refresh=False):
  """
  Retruns processed community page items
  """
  try:
    data = get_community_page_data(language=language, refresh=refresh)
  except:
    data = {
      "parashah": None,
      "calendar": None,
      "discover": None,
      "featured": None,
    }

  if date is None:
    datetime_obj = timezone.localtime(timezone.now())
    date = datetime_obj.strftime("%-m/%-d/%y")

  return {
    "parashah": get_parashah_item(data["parashah"], date=date, diaspora=diaspora),
    "calendar": get_featured_item(data["calendar"], date=date),
    "discover": get_featured_item(data["discover"], date=date),
    "featured": get_featured_item(data["featured"], date=date),
  }


def get_parashah_item(data, date=None, diaspora=True, interface_lang="english"):
  todays_data = get_todays_data(data, date) # First we want the row in the sheet representing today. Always.
  weekend_reading = get_parasha(datetime.strptime(date, "%m/%d/%y"), diaspora=diaspora) # What is this week's torah reading on Saturday. Can be a special reading for Holiday.
  parashah_name = weekend_reading["parasha"]
  parashah_topic = get_topic_by_parasha(parashah_name)

  if not todays_data or not (todays_data["Sheet URL"]):
    sheet = None
  else:
    sheet = sheet_with_customization(todays_data)
    if sheet:
      if parashah_topic and todays_data["Parashah"] in [parashah_topic.parasha] + parashah_topic.get_titles():  # We have a matched official parasha to the day's entry in the sheet
        parashah_name = parashah_topic.parasha
        sheet["heading"] = {
          "en": "This Week's Torah Portion: " + parashah_name,
          "he": " : " + hebrew_parasha_name(parashah_name)
        }
      else:  
        # We have a sheet row where the title doesn't match a known parasha. It might be a mid week holiday reading (doesnt appear in our parshiot db).
        # Since this sheet column can now essentially be a custom block title, we dont know how to translate automatically, so
        # this will also mostly rely on the fact that the Hebrew only shows up in Hebrew interface and Heb comms team can enter custom Hebrew to begin with.
        sheet["heading"] = {
          "en": "Torah Reading For: " + todays_data["Parashah"],
          "he": "  :" + todays_data["Parashah"]
        }

  if not parashah_topic and not sheet:
    return None

  return {
    "topic": parashah_topic.contents() if parashah_topic else None,
    "sheet": sheet
  }


def get_featured_item(data, date):
  todays_data = get_todays_data(data, date)
  if not todays_data or not (todays_data["Block Title"] and todays_data["Sheet URL"]):
    return None

  sheet = sheet_with_customization(todays_data)
  if not sheet:
    return None

  return {
    "heading": todays_data["Block Title"], # This is never actually used and is confusing here (since it also appears inside the sheet object below). 
    "sheet": sheet
  }


def get_todays_data(data, date):
  todays_data = None
  for day_data in data:
    if day_data["Date"] == date:
      todays_data = day_data
      break
  return todays_data


def sheet_with_customization(data):
  sheet_id = url_to_sheet_id(data["Sheet URL"])
  sheet = get_sheet(sheet_id)
  if "error" in sheet:
    return None

  sheet = sheet_to_dict(sheet)

  if len(data.get("Custom Title", "")):
    sheet["title"] = data["Custom Title"]

  if len(data.get("Custom Summary", "")):
    sheet["summary"] = data["Custom Summary"]

  if len(data.get("Custom Author", "")):
    sheet["ownerName"] = data["Custom Author"]


  if len(data.get("Block Title", "")):
    sheet["heading"] = {"en": data["Block Title"], "he": data["Block Title"]}

  return sheet


def url_to_sheet_id(url):
  m = re.match(r".+\/sheets\/(\d+)", url)
  return int(m[1])


def topic_from_url(url):
  slug = url_to_topic_slug(url)
  topic = Topic().load({"slug": slug}).contents()
  return topic


def url_to_topic_slug(url):
  m = re.match(r".+\/topics\/([^?]+)", url)
  return m[1]


def get_featured_sheet_from_collection(collection):
  import random
  collection = Collection().load({"slug": collection})
  if not collection:
    return None

  sheets = collection.sheet_contents()

  return random.choice(sheets) if len(sheets) else None


def get_featured_sheet_from_topic(slug):
  import random
  from sefaria.sheets import sheet_list
  from sefaria.model.topic import RefTopicLinkSet
  sheet_links = RefTopicLinkSet({"is_sheet": True, "toTopic": slug})
  sids = [int(s.ref.replace("Sheet ", "")) for s in sheet_links]
  if not len(sids):
    return None

  sheets = sheet_list({
    "id": {"$in": sids},
    "summary": {"$exists": 1},
  })
  sheets = [s for s in sheets if not has_hebrew(s["title"]) and s["summary"] and len(s["summary"]) > 140]
  return random.choice(sheets)


def get_featured_sheet_from_ref(ref):
  import random
  sheets = get_sheets_for_ref(ref)
  sheets = [s for s in sheets if not has_hebrew(s["title"]) and s["summary"] and len(s["summary"]) > 140]
  return random.choice(sheets)  


def translate_labels(label):
  LABEL_TRANSLATIONS = {
    "": "Date",
    "":  "Parashah",
    "  ": "Sheet URL",
    " ": "Custom Title",
    " ": "Custom Summary",
    " ": "Custom Author",
    " ": "Ready",
    "  ": "Custom About Title",
    "  ": "Topic URL",
    " ":  "Displayed Date",
    " ": "Description Title",
    "": "Citation",
    "": "Description",
    "  ": "Block Title",
  }
  return LABEL_TRANSLATIONS.get(label, label)



### Helper function for populting spreadsheet ###

def print_parashah_rows(n=1000, lang="en"):
  """
  Helper for populating Community page Schedule spreadsheet.
  Prints a date and the name of upcoming parashah, to be coped into the spreadsheet.
  Prints multiple rows when Israel/Disapora readings differ.
  """

  d = datetime.now()

  for i in range(n):
    d = d + timedelta(days=1)
    p_diaspora = get_parasha(d, diaspora=True)
    p_israel = get_parasha(d, diaspora=False)

    rows = [p_diaspora["parasha"]]
    if p_diaspora["parasha"] != p_israel["parasha"]:
      rows.append(p_israel["parasha"])

    for row in rows:
      print("{}, {}".format(d.strftime("%-m/%-d/%y"), row))


def print_rows_with_interpolated_dates(rows):
  next_date = lambda d: d + timedelta(days=1)
  parse_date = lambda d: datetime.strptime(d, "%m/%d/%y")
  last_date = parse_date(rows[0][0]) - timedelta(days=1)
  for row in rows:
    this_date = parse_date(row[0])
    while this_date > last_date:
      print(last_date.strftime("%-m/%-d/%y"))
      last_date = next_date(last_date)

    if this_date != last_date: #skip duplicates
      print(", ".join(row))
      last_date = next_date(last_date)


def dupe_rows(rows):
  last = None
  for row in rows:
    if row[0] == last:
      print (", ".join(row))
    last = row[0]


def sheets_with_content_by_category(cat, print_results=True):
  """
  Returns or prints a list of public sheets that include ref within the category `cat`.
  `cat` may be either a string which is compared to each ref's `primary_category`,
  or a list of strings which must match exactly the ref's index's `categories`.
  """
  results = []
  sids = set()
  sheets = db.sheets.find({"status": "public"}, {"title": 1, "id": 1, "owner": 1, "includedRefs": 1, "status":1})
  for sheet in sheets:
    if sheet["owner"] == 101527:
      continue
    refs = []
    for ref in sheet["includedRefs"]:
      try:
        oRef = Ref(ref)
      except:
        continue
      if oRef.primary_category == cat or oRef.index.categories == cat:
        refs.append(ref)
        sids.add(sheet["id"])
    if len(refs):
      for ref in refs:
        title = strip_tags(sheet["title"], remove_new_lines=True).strip()
        results.append([sheet["id"], title, ref])
        if print_results:
          print("www.sefaria.org/sheets/{}\t{}\t{}".format(sheet["id"], title, ref))

  print("\n\n{} Sheet with {}".format(len(sids), cat))

  if not print_results:
    return results


def sheets_by_parashah(print_results=True):
  """
  Returns or prints a list of public sheets that include verses of Torah, aggregated and
  labeled by parashah.
  """
  sheets = sheets_with_content_by_category(["Tanakh", "Torah"], print_results=False)
  parshiot = TermSet({"scheme": "Parasha"})
  for p in parshiot:
    p.oRef = Ref(p.ref)

  def parashah_for_ref(ref):
    oRef = Ref(ref)
    for p in parshiot:
      if p.oRef.overlaps(oRef):
        return p
    print("No parashah found for {}".format(ref))
    return None

  for sheet in sheets:
    p = parashah_for_ref(sheet[2])
    if p:
      titles = [p.get_primary_title(), p.get_primary_title(lang="he")]
    else:
      titles = ["", ""]
    sheet.extend(titles)

  # Aggregate each row by parashah
  results_index = {} 
  for sheet in sheets:
    key = str(sheet[0]) + sheet[3]
    if key in results_index:
      results_index[key][4] += ", " + sheet[2] # add to list of citations
    else:
      results_index[key] = [sheet[3], sheet[4], sheet[0], sheet[1], sheet[2]]

  results = results_index.values()

  if print_results:
    for result in results:
      print("{}\t{}\twww.sefaria.org/sheets/{}\t{}\t{}".format(*result))
  else:
    return sheets
```

### sefaria/helper/tests/normalization_tests.py

```
import pytest
import django
django.setup()
from sefaria.helper.normalization import *


def test_itag_normalizer():
    text = "Yo <sup>3</sup><i class=\"footnote\"> <i> Am </i>. 4:4</i>."
    itn = ITagNormalizer(' ')
    norm_text = itn.normalize(text)
    assert norm_text == "Yo   ."
    text_to_remove = itn.find_text_to_remove(text)
    assert len(text_to_remove) == 2
    (s1, e1), r1 = text_to_remove[0]
    assert (s1, e1) == (3, 15)
    (s2, e2), r2 = text_to_remove[1]
    assert (s2, e2) == (15, len(text)-1)


def test_replace_normalizer():
    text = "Hello, what is up? You is nice"
    rn = ReplaceNormalizer(' is ', ' are ')
    norm_text = rn.normalize(text)
    assert norm_text == text.replace(' is ', ' are ')
    text_to_remove = rn.find_text_to_remove(text)
    assert len(text_to_remove) == 2
    (s1, e1), r1 = text_to_remove[0]
    assert (s1, e1) == (11, 15)
    (s2, e2), r2 = text_to_remove[1]
    assert (s2, e2) == (22, 26)


def test_br_tag_html_composer():
    """
    These two normalizers composed seem to cause issues
    """
    text = """<i>hello</i><br><b>as well as</b> yo"""
    normalized = """ hello   as well as yo"""
    nsc = NormalizerComposer(['br-tag', 'html'])
    assert nsc.normalize(text) == normalized
    text_to_remove = nsc.find_text_to_remove(text)
    assert len(text_to_remove) == 5
    (start1, end1), repl1 = text_to_remove[1]
    assert text[start1:end1] == '</i>'
    (start2, end2), repl2 = text_to_remove[2]
    assert repl2 == ' '
    assert text[start2:end2] == '<br>'
    (start4, end4), repl4 = text_to_remove[4]
    assert repl2 == ' '
    assert text[start4:end4] == '</b> '


def test_simpler_normalizer_composer():
    text = ' [sup'
    normalized = " sup"
    nsc = NormalizerComposer(['brackets', 'double-space'])
    assert nsc.normalize(text) == normalized
    text_to_remove = nsc.find_text_to_remove(text)
    assert len(text_to_remove) == 2
    (start0, end0), repl0 = text_to_remove[0]
    assert text[start0:end0] == " "
    assert repl0 == ' '


def test_complicated_normalizer_composer():
    text = """(<i>hello</i> other stuff) [sup] <b>(this is) a test</b>"""
    normalized = """ sup a test """
    nsc = NormalizerComposer(['html', "parens-plus-contents", 'brackets', 'double-space'])
    assert nsc.normalize(text) == normalized
    text_to_remove = nsc.find_text_to_remove(text)
    assert len(text_to_remove) == 6
    (start0, end0), repl0 = text_to_remove[0]
    assert text[start0:end0] == "(<i>hello</i> other stuff) "
    assert repl0 == ' '


@pytest.mark.parametrize(('unnorm', 'norm', 'normalizer_steps', 'test_word'), [
    ["<b><i> test", " test", ['html', 'double-space'], 'test'],
    ["\n\n\nThe rest of Chapter 1.\n \n", " The rest of Chapter 1. ", ['unidecode', 'html', 'double-space'], 'Chapter 1'],
])
def test_mapping(unnorm, norm, normalizer_steps, test_word):
    nsc = NormalizerComposer(normalizer_steps)
    assert nsc.normalize(unnorm) == norm
    start_norm_ind = norm.index(test_word)
    norm_inds = (start_norm_ind, start_norm_ind+len(test_word))
    unnorm_inds = nsc.norm_to_unnorm_indices(unnorm, [norm_inds])[0]
    assert unnorm[slice(*unnorm_inds)] == norm[slice(*norm_inds)]


def test_html_normalizer_for_empty_prefix():
    text = """It is written<sup>24</sup><i class="footnote"><i>1K</i>. 17:1.</i> <i>Elijah the Tisbite</i>"""
    normalizer = NormalizerComposer(['html'])
    ne = "Elijah the Tisbite"
    norm_text = "It is written 24   1K . 17:1.  Elijah the Tisbite "
    assert normalizer.normalize(text) == norm_text
    ne_start = norm_text.index(ne)
    ne_norm_prefix_inds = (ne_start, ne_start)
    assert norm_text[ne_norm_prefix_inds[0]:ne_norm_prefix_inds[0]+len(ne)] == ne
    ne_inds = normalizer.norm_to_unnorm_indices(text, [ne_norm_prefix_inds])[0]
    # actual test
    assert ne_inds[0] == ne_inds[1]
    assert text[ne_inds[0]:ne_inds[0]+len(ne)] == ne


def test_nested_itag():
    text = """<sup>outer</sup><i class="footnote">bull<sup>nested</sup><i class="footnote">The</i>.</i>"""
    normalizer = ITagNormalizer(' ')
    norm_text = normalizer.normalize(text)
    assert norm_text == "  "
    text_to_remove = normalizer.find_text_to_remove(text)
    assert len(text_to_remove) == 2
    (s, e), r = text_to_remove[0]
    assert text[s:e] == "<sup>outer</sup>"
    (s, e), r = text_to_remove[1]
    assert text[s:e] == """<i class="footnote">bull<sup>nested</sup><i class="footnote">The</i>.</i>"""


@pytest.mark.xfail(reason="not clear we want to support char_indices_from_word_indices as it's unused")
def test_two_steps_normalization():
    test_string = ' This is a {{test}}'

    bracket_normalizer = RegexNormalizer(r'\{\{|}}', r'')
    strip_normalizer = RegexNormalizer(r'^\s*|\s*$', r'')
    normalizer = NormalizerComposer(steps=[bracket_normalizer, strip_normalizer])

    mapping = normalizer.get_mapping_after_normalization(test_string, reverse=True)
    assert mapping == {0: 1, 11: 3, 17: 5}
    orig_inds = [(13, 17)]
    new_start, new_end = normalizer.convert_normalized_indices_to_unnormalized_indices(orig_inds, mapping, reverse=True)[0]
    normalized_string = normalizer.normalize(test_string)
    assert normalized_string[new_start:new_end] == "test"


def test_word_to_char():
    test_string = 'some words go here\n\nhello world'
    words = ['go', 'here', 'hello']
    word_indices = (2, 4)
    result = char_indices_from_word_indices(test_string, [word_indices])[0]
    start, end = result
    assert test_string[start:end] == 'go here\n\nhello'
    assert test_string[start:end].split() == words


class TestTextSanitizer:

    text_to_test = [
        'foo bar <erase me> baz',
        'hello <nonsense> world',
        'my name is <not> Jonathan',
        'out of <good> ideas'
    ]
    sanitization_expression = r'\s*<[^<>]+>\s*'
    dividing_expression = r'\s+'

    @classmethod
    def sanitizer(cls, x):
        return re.sub(cls.sanitization_expression, ' ', x)

    def test_initialization(self):
        sanitizer = TextSanitizer(self.text_to_test, self.dividing_expression)
        assert sanitizer.get_original_segments() == tuple(self.text_to_test)
        assert sanitizer.get_sanitized_segments() is None

    def test_sanitize(self):
        sanitizer = TextSanitizer(self.text_to_test, self.dividing_expression)
        sanitizer.set_sanitizer(self.sanitizer)
        sanitizer.sanitize()
        assert sanitizer.get_sanitized_segments() == (
            'foo bar baz',
            'hello world',
            'my name is Jonathan',
            'out of ideas'
        )

    def test_word_to_segment(self):
        sanitizer = TextSanitizer(self.text_to_test, self.dividing_expression)
        sanitizer.set_sanitizer(self.sanitizer)
        sanitizer.sanitize()
        word_list = sanitizer.get_sanitized_word_list()
        jon = word_list.index('Jonathan')
        assert sanitizer.check_sanitized_index(jon) == 2
```

### sefaria/helper/tests/schema_test.py

```
# encoding=utf-8
import re
import django
django.setup()
from sefaria.model import *
from sefaria.helper import schema
from sefaria.sheets import save_sheet
from sefaria.system.database import db
from sefaria.system.exceptions import BookNameError
import pytest


"""
Here is the text structure for the simple text MigrateBook
[
    [
        'This should eventually end up in MigrateBook, Part 1, 1:1',
        'This text is for 1:2-5',
        'This text is for 1:2-5', 'This text is for 1:2-5',
        'This text is for 1:2-5'
    ],
    [
        '',
        'This should eventually end up in MigrateBook, Part 1, 2:2'
    ],
    [
        '',
        '',
        'This should eventually end up in MigrateBook, Part 2, 3',
        '',
        'This text is just to allow for range 3:1-5'
    ],
    [
        'This should eventually end up in MigrateBook, Part 3, 1'
    ],
    [
        '',
        '',
        '',
        'This will eventually go nowhere'
    ]
]
"""

TEST_SHEET_ID = None


def create_test_sheet(list_of_trefs):
    sheet = {
        'title': 'schema test sheet',
        'status': 'unlisted',
        'tags': [],
        'options': {
            'language': 'english',
            'numbered': False,
        },
        'sources': []
    }
    for tref in list_of_trefs:
        sheet['sources'].append(
            {'ref': tref}
        )
    return sheet


def get_sheet_refs(sheet_id):
    sheet_json = db.sheets.find_one({'id': sheet_id})
    assert sheet_json is not None
    return [source.get('ref', '') for source in sheet_json['sources']]


def get_text_for_simple_text():
    return [
        "This should eventually end up in MigrateBook, Part 1, 1:1",
        "This should eventually end up in MigrateBook, Part 1, 2:2",
        "This should eventually end up in MigrateBook, Part 2, 3",
        "This should eventually end up in MigrateBook, Part 3, 1",
        "This will eventually go nowhere",
        "This text is just to allow for range 3:1-5",
        "This text is for 1:2-5",
    ]


def create_simple_text():
    try:
        library.get_index("Complex MigrateBook").delete()
        library.get_index("MigrateBook").delete()
    except BookNameError:
        pass

    index = Index().load({'title': 'MigrateBook'})
    if index is not None:
        ls = LinkSet(Ref("MigrateBook"))
        ls.delete()
        ns = NoteSet({"ref": {"$regex": "MigrateBook.*"}})
        ns.delete()
        index.delete()

    # Build an index with some nodes
    root = JaggedArrayNode()
    root.add_title('MigrateBook', 'en', primary=True)
    root.add_title('', 'he', primary=True)
    root.key = 'MigrateBook'
    root.depth = 2
    root.addressTypes = ["Integer", "Integer"]
    root.sectionNames = ["Siman", "Paragraph"]
    root.validate()

    index = Index({
        'schema': root.serialize(),
        'title': 'MigrateBook',
        'categories': ['Liturgy'],
    })
    index.save()

    p1, p2, p3, p4, p5, p6, p7 = get_text_for_simple_text()

    chunk = TextChunk(Ref('MigrateBook 1:1'), 'en', 'Schema Test')
    chunk.text = p1
    chunk.save()
    for i in range(4):
        chunk = TextChunk(Ref("MigrateBook 1:{}".format(i+2)), 'en', 'Schema Test')
        chunk.text = p7
        chunk.save()
    chunk = TextChunk(Ref("MigrateBook 2:2"), 'en', 'Schema Test')
    chunk.text = p2
    chunk.save()
    chunk = TextChunk(Ref("MigrateBook 3:3"), 'en', 'Schema Test')
    chunk.text = p3
    chunk.save()
    chunk = TextChunk(Ref("MigrateBook 3:5"), 'en', 'Schema Test')
    chunk.text = p6
    chunk.save()
    chunk = TextChunk(Ref("MigrateBook 4:1"), 'en', 'Schema Test')
    chunk.text = p4
    chunk.save()
    chunk = TextChunk(Ref("MigrateBook 5:4"), 'en', 'Schema Test')
    chunk.text = p5
    chunk.save()

    Link({
        'refs': ['MigrateBook 1:1', 'Guide for the Perplexed, Part 1'],
        'type': 'None'
    }).save()

    Link({
        'refs': ['MigrateBook 2:2', 'Guide for the Perplexed, Part 1 2'],
        'type': 'None'
    }).save()

    Link({
        'refs': ['MigrateBook 3:3', 'Guide for the Perplexed, Part 2 4-8'],
        'type': 'None'
    }).save()

    Link({
        'refs': ['MigrateBook 4:1', 'Guide for the Perplexed, Part 3 1'],
        'type': 'None'
    }).save()

    Link({
        'refs': ['MigrateBook 5:4', 'Guide for the Perplexed, Introduction, Introduction 3'],
        'type': 'None'
    }).save()

    Link({
        'refs': ['MigrateBook 1:2-5', 'Genesis 3'],
        'type': 'None'
    }).save()

    Link({
        'refs': ['MigrateBook 2', 'Genesis 2'],
        'type': 'None'
    }).save()

    VersionState("MigrateBook").refresh()


@pytest.mark.deep
def setup_module():
    print('Creating Dummy Index')

    # ensure dummy index was properly deleted
    index = Index().load({'title': 'Delete Me'})
    if index is not None:
        ls = LinkSet(Ref("Delete Me"))
        ls.delete()
        ns = NoteSet({"ref": {"$regex": "Delete Me.*"}})
        ns.delete()
        index.delete()

    # Build an index with some nodes
    root = SchemaNode()
    root.add_title('Delete Me', 'en', primary=True)
    root.add_title(' ', 'he', primary=True)
    root.key = 'Delete Me'

    part1 = JaggedArrayNode()
    part1.add_title('Part1', 'en', primary=True)
    part1.add_title("Partone", 'en')
    part1.add_title(' 1', 'he', primary=True)
    part1.sectionNames = ['Chapter', 'Verse']
    part1.addressTypes = ['Integer', 'Integer']
    part1.depth = 2
    part1.key = 'Part1'
    root.append(part1)

    part2 = JaggedArrayNode()
    part2.add_title('Part2', 'en', primary=True)
    part2.add_title(' 2', 'he', primary=True)
    part2.sectionNames = ['Section', 'Segment']
    part2.addressTypes = ['Integer', 'Integer']
    part2.depth = 2
    part2.key = 'Part2'
    root.append(part2)

    root.validate()

    alt = ArrayMapNode()
    alt.depth = 0
    alt.wholeRef = 'Delete Me, Part1 1:2-3:1'
    alt.add_title('Something', 'en', True)
    alt.add_title('', 'he', True)

    index = Index({
        'schema': root.serialize(),
        'title': 'Delete Me',
        'categories': ['Tanakh'],
        'alt_structs': {'alt': alt.serialize()}
    })
    index.save()

    # add some text
    v = Version({
        "language": "en",
        "title": "Delete Me",
        "versionSource": "http://foobar.com",
        "versionTitle": "Schema Test",
        "chapter": root.create_skeleton()
    }).save()

    # an empty version
    v = Version({
        "language": "en",
        "title": "Delete Me",
        "versionSource": "http://foobar.com",
        "versionTitle": "Schema Test Blank",
        "chapter": root.create_skeleton()
    }).save()

    p1 = [['Part1 part1', 'Part1'], ['Part1'], ['Part1', '', 'part1']]
    chunk = TextChunk(Ref('Delete Me, Part1'), 'en', 'Schema Test')
    chunk.text = p1
    chunk.save()

    p2 = [['Part2 part2', 'Part2'], ['Part2'], ['Part2', '', 'part2']]
    chunk = TextChunk(Ref('Delete Me, Part2'), 'en', 'Schema Test')
    chunk.text = p2
    chunk.save()

    # add some links
    Link({
        'refs': ['Delete Me, Part1 1:1', 'Shabbat 2a:5'],
        'type': 'None'
    }).save()
    Link({
        'refs': ['Delete Me, Part1 2:1', 'Delete Me, Part2 2:1'],
        'type': 'None'
    }).save()
    Link({
        'refs': ['Delete Me, Part1 3', 'Shabbat 2a:5'],
        'type': 'None'
    }).save()
    Link({
        'refs': ['Delete Me, Part2 1:1', 'Shabbat 2a:5'],
        'type': 'None'
    }).save()
    Link({
        'refs': ['Delete Me, Part2 3', 'Shabbat 2a:5'],
        'type': 'None'
    }).save()

    # add a note
    Note({
        'owner': 23432,
        'public': False,
        'text': 'Some very important text',
        'type': 'note',
        'ref': 'Delete Me, Part1 1:1'

    }).save()

    VersionState("Delete Me").refresh()
    sheet = save_sheet(create_test_sheet([
        'Delete Me, Part1 1:1',
        'MigrateBook 1:1',
        'MigrateBook 4:1'
    ]), 1)
    global TEST_SHEET_ID
    TEST_SHEET_ID = sheet['id']

    # set up the simple text
    create_simple_text()

    print('End of test setup')


@pytest.mark.deep
def teardown_module():
    print('Cleaning Up')
    ls = LinkSet(Ref("Delete Me"))
    ls.delete()
    ns = NoteSet({"ref": {"$regex": "Delete Me.*"}})
    ns.delete()
    v = VersionSet({'title': 'Delete Me'})
    v.delete()
    i = Index().load({'title': 'Delete Me'})
    i.delete()
    global TEST_SHEET_ID
    if TEST_SHEET_ID:
        db.sheets.delete_one({'id': TEST_SHEET_ID})
        TEST_SHEET_ID = None
    try:
        library.get_index("MigrateBook").delete()
    except BookNameError:
        pass


@pytest.mark.deep
def test_migrate_to_complex_structure():
    mappings = {
        "MigrateBook 1-2": "MigrateBook, Part 1",
        "MigrateBook 3:1-3": "MigrateBook, Part 2",
        "MigrateBook 4": "MigrateBook, Part 3"
    }

    new_schema = SchemaNode()
    new_schema.key = "MigrateBook"
    new_schema.add_title("MigrateBook", "en", primary=True)
    new_schema.add_title("", "he", primary=True)

    depths = [2, 1, 2, 1, 1, 1, 1, 1, 1, 1]
    for i in range(10):
        ja = JaggedArrayNode()
        ja.add_title('Part {}'.format(i+1), 'en', primary=True)
        ja.add_title(' {}'.format(i+1), 'he', primary=True)
        ja.key = str(i)
        ja.depth = depths[i]
        ja.addressTypes = ["Integer"] * depths[i]
        ja.sectionNames = ["Paragraph"] * depths[i]
        new_schema.append(ja)

    new_schema.validate()

    schema.migrate_to_complex_structure("MigrateBook", new_schema.serialize(), mappings)
    with pytest.raises(BookNameError):
        library.get_index("Complex MigrateBook")
    children = library.get_index("MigrateBook").nodes.children

    assert children[0].full_title() == "MigrateBook, Part 1"
    assert children[1].full_title() == "MigrateBook, Part 2"

    p1, p2, p3, p4, p5, p6, p7 = get_text_for_simple_text()

    assert TextChunk(children[0].ref(), "en", 'Schema Test').text == [[p1, p7, p7, p7, p7], ["", p2]]
    assert TextChunk(children[1].ref(), "en", "Schema Test").text == ["", "", p3]
    assert TextChunk(children[2].ref(), "en", "Schema Test").text == [[p4]]

    assert isinstance(Link().load({'refs': sorted(['MigrateBook, Part 1 1:1', 'Guide for the Perplexed, Part 1']),}), Link)
    assert isinstance(Link().load({'refs': sorted(['MigrateBook, Part 1 2:2', 'Guide for the Perplexed, Part 1 2']),}), Link)
    assert isinstance(Link().load({'refs': sorted(['MigrateBook, Part 2 3', 'Guide for the Perplexed, Part 2 4-8']),}), Link)
    assert isinstance(Link().load({'refs': sorted(['MigrateBook, Part 3 1', 'Guide for the Perplexed, Part 3 1']),}), Link)
    assert Link().load({'refs': sorted(['MigrateBook 5:4', 'Guide for the Perplexed, Introduction, Introduction, 3']),}) is None
    assert isinstance(Link().load({'refs': sorted(['MigrateBook, Part 1 1:2-5', 'Genesis 3']),}), Link)
    assert isinstance(Link().load({'refs': sorted(['MigrateBook, Part 1 2', 'Genesis 2']),}), Link)

    sheet_sources = get_sheet_refs(TEST_SHEET_ID)
    assert all(not re.match(r'^MigrateBook [0-9]', source) for source in sheet_sources)
    assert any('MigrateBook, Part 1' in source for source in sheet_sources)
    assert any('MigrateBook, Part 3' in source for source in sheet_sources)

    # we don't go from complex back to simple, so we'll delete and recreate the simple index\
    # idempotency is broken in the event of a failed test
    library.get_index("MigrateBook").delete()
    create_simple_text()


@pytest.mark.deep
def test_change_node_title():
    node = library.get_index("Delete Me").nodes.children[0]
    schema.change_node_title(node, "Part1", "en", "1st Part")
    node = library.get_index("Delete Me").nodes.children[0]
    assert node.primary_title() == "1st Part"
    assert len(node.get_titles_object()) == 3
    assert isinstance(Link().load({'refs': ['Delete Me, 1st Part 1:1', 'Shabbat 2a:5']}), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, 1st Part 2:1', 'Delete Me, Part2 2:1']}), Link)
    assert isinstance(Note().load({'ref': 'Delete Me, 1st Part 1:1'}), Note)
    assert Link().load({'refs': ['Delete Me, Part1 2:1', 'Delete Me, Part2 2:1']}) is None
    assert Note().load({'ref': 'Delete Me, Part1 1:1'}) is None
    sheet_refs = get_sheet_refs(TEST_SHEET_ID)
    assert any('Delete Me, 1st Part' in s for s in sheet_refs)
    assert all('Delete Me, Part1' not in s for s in sheet_refs)

    schema.change_node_title(node, "1st Part", "en", "Part1")
    node = library.get_index("Delete Me").nodes.children[0]
    assert node.primary_title() == "Part1"

    schema.change_node_title(node, "Partone", "en", "Part One")
    node = library.get_index("Delete Me").nodes.children[0]
    assert len(node.get_titles_object()) == 3
    assert any([title['text'] == 'Part One' for title in node.get_titles_object()])

    schema.change_node_title(node, "Part One", "en", "Partone")
    assert len(node.get_titles_object()) == 3
    assert any([title['text'] == 'Partone' for title in node.get_titles_object()])

    sheet_refs = get_sheet_refs(TEST_SHEET_ID)
    assert any('Delete Me, Part1' in s for s in sheet_refs)
    assert all('Delete Me, 1st Part' not in s for s in sheet_refs)


@pytest.mark.deep
def test_change_node_structure_complex():

    # increase depth
    node = library.get_index('Delete Me').nodes.children[0]
    schema.change_node_structure(node, ['SuperSection', 'Section', 'Segment'])

    assert node.depth == 3
    chunk = TextChunk(Ref('Delete Me, Part1'), 'en', 'Schema Test')
    assert chunk.text == [[['Part1 part1'], ['Part1']], [['Part1']], [['Part1'], [], ['part1']]]
    blank_chunk = TextChunk(Ref('Delete Me, Part1'), 'en', 'Schema Test Blank')
    assert len(blank_chunk.text) == 0
    assert isinstance(Link().load({'refs': ['Delete Me, Part1 1:1:1', 'Shabbat 2a:5'],}), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part1 2:1:1', 'Delete Me, Part2 2:1'], }), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part1 3:1', 'Shabbat 2a:5'], }), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part2 1:1', 'Shabbat 2a:5'], }), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part2 3', 'Shabbat 2a:5'], }), Link)
    assert isinstance(Note().load({'ref': 'Delete Me, Part1 1:1:1'}), Note)
    assert library.get_index('Delete Me').get_alt_structure('alt').wholeRef == 'Delete Me, Part1 1:2:1-3:1:1'

    sheet_refs = get_sheet_refs(TEST_SHEET_ID)
    assert any('Delete Me, Part1 1:1:1' in s for s in sheet_refs)
    assert all(not re.search(r'Delete Me, Part 1 1:1$', s) for s in sheet_refs)

    # decrease depth
    node = library.get_index('Delete Me').nodes.children[0]
    schema.change_node_structure(node, ['Section', 'Segment'])

    assert node.depth == 2
    chunk = TextChunk(Ref('Delete Me, Part1'), 'en', 'Schema Test')
    assert chunk.text == [['Part1 part1', 'Part1'], ['Part1'], ['Part1', '', 'part1']]
    assert isinstance(Link().load({'refs': ['Delete Me, Part1 1:1', 'Shabbat 2a:5'], }), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part1 2:1', 'Delete Me, Part2 2:1'], }), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part1 3', 'Shabbat 2a:5'], }), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part2 1:1', 'Shabbat 2a:5'], }), Link)
    assert isinstance(Link().load({'refs': ['Delete Me, Part2 3', 'Shabbat 2a:5'], }), Link)
    assert isinstance(Note().load({'ref': 'Delete Me, Part1 1:1'}), Note)
    assert library.get_index('Delete Me').get_alt_structure('alt').wholeRef == 'Delete Me, Part1 1:2-3:1'

    sheet_refs = get_sheet_refs(TEST_SHEET_ID)
    assert all('Delete Me, Part1 1:1:1' not in s for s in sheet_refs)
    assert any(re.search(r'Delete Me, Part1 1:1$', s) for s in sheet_refs)


@pytest.mark.deep
def test_change_node_structure_simple():
    # increase depth
    node = library.get_index("MigrateBook").nodes
    schema.change_node_structure(node, ['SuperSection', 'Section', 'Segment'])

    assert node.depth == 3
    tc = TextChunk(Ref("MigrateBook 2:2:1"), "en", "Schema Test")
    assert tc.text == 'This should eventually end up in MigrateBook, Part 1, 2:2'

    # change depth back to 2
    node = library.get_index("MigrateBook").nodes
    schema.change_node_structure(node, ['Section', 'Segment'])
    assert node.depth == 2
    tc = TextChunk(Ref("MigrateBook 2:2"), "en", "Schema Test")
    assert tc.text == 'This should eventually end up in MigrateBook, Part 1, 2:2'


```

### sefaria/helper/tests/text_test.py

```
# encoding=utf-8
import pytest
from sefaria.model import *
from sefaria.helper.text import modify_text_by_function
from sefaria.datatype.jagged_array import JaggedTextArray


@pytest.mark.deep
def test_modify_text_by_function():

    original = TextChunk(Ref("Job"), vtitle="The Holy Scriptures: A New Translation (JPS 1917)")
    total_spaces = JaggedTextArray(original.text).flatten_to_string(joiner="|").count(" ")

    v = Version({
        "language": "en",
        "title": "Job",
        "versionSource": "http://foobar.com",
        "versionTitle": "TextChangeTest",
        "chapter": original.text
    }).save()

    modify_text_by_function("Job", "TextChangeTest", "en", lambda x, sections: x.replace(" ", "$"), 23432)
    modified = TextChunk(Ref("Job"), vtitle="TextChangeTest")
    total_dollars = JaggedTextArray(modified.text).flatten_to_string(joiner="|").count("$")
    v.delete()
    assert total_dollars > 0
    assert total_spaces == total_dollars

```

### sefaria/helper/tests/linker_test.py

```
from typing import Callable
import json
import pytest
from unittest.mock import patch, Mock
from sefaria.google_storage_manager import GoogleStorageManager
from django.test import RequestFactory
from django.core.handlers.wsgi import WSGIRequest
import tarfile
import io
from sefaria.model.text import Ref, TextChunk
from sefaria.model.webpage import WebPage
from sefaria.settings import ENABLE_LINKER
from api.api_errors import APIInvalidInputException

if not ENABLE_LINKER:
    pytest.skip("Linker not enabled", allow_module_level=True)

from sefaria.helper import linker
from sefaria.model.linker.named_entity_recognizer import load_spacy_model
try:
    import spacy
except ImportError:
    spacy = None


@pytest.fixture
def mock_oref() -> Ref:
    return Ref("Job 17")


@pytest.fixture
def spacy_model() -> spacy.Language:
    return spacy.blank('en')


class TestLoadSpacyModel:

    @staticmethod
    @pytest.fixture
    def tarfile_buffer() -> io.BytesIO:
        tar_buffer = io.BytesIO()
        with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tar:
            tar.addfile(tarfile.TarInfo('test'))
        tar_buffer.seek(0)
        return tar_buffer

    @staticmethod
    @patch('spacy.load')
    def test_load_spacy_model_local(spacy_load_mock: Callable, spacy_model: spacy.Language):
        spacy_load_mock.return_value = spacy_model
        assert load_spacy_model('local/path') == spacy_model

    @staticmethod
    @patch('spacy.load')
    @patch.object(GoogleStorageManager, 'get_filename')
    def test_load_spacy_model_cloud(get_filename_mock: Callable, spacy_load_mock: Callable, spacy_model: spacy.Language,
                                    tarfile_buffer: io.BytesIO):
        get_filename_mock.return_value = tarfile_buffer
        spacy_load_mock.return_value = spacy_model
        assert load_spacy_model('gs://bucket_name/blob_name') == spacy_model

    @staticmethod
    @patch('spacy.load')
    @patch.object(GoogleStorageManager, 'get_filename')
    def test_load_spacy_model_cloud_invalid_path(get_filename_mock: Callable, spacy_load_mock: Callable,
                                                 spacy_model: spacy.Language, tarfile_buffer: io.BytesIO):
        get_filename_mock.return_value = tarfile_buffer
        spacy_load_mock.side_effect = OSError
        with pytest.raises(OSError):
            load_spacy_model('invalid_path')


@pytest.fixture
def mock_request_post_data() -> dict:
    return {
        'text': {'title': 'title', 'body': 'body'},
        'version_preferences_by_corpus': {},
        'metaDataForTracking': {'url': 'https://test.com', 'title': 'title', 'description': 'description'}
    }


@pytest.fixture
def mock_request_post_data_without_meta_data(mock_request_post_data: dict) -> dict:
    del mock_request_post_data['metaDataForTracking']
    return mock_request_post_data


@pytest.fixture
def mock_request_invalid_post_data(mock_request_post_data: dict) -> dict:
    mock_request_post_data['text'] = 'plain text'
    return mock_request_post_data


def make_mock_request(post_data: dict) -> WSGIRequest:
    factory = RequestFactory()
    request = factory.post('/api/find-refs', data=json.dumps(post_data), content_type='application/json')
    request.GET = {'with_text': '1', 'debug': '1', 'max_segments': '10'}
    return request


@pytest.fixture
def mock_request(mock_request_post_data: dict) -> WSGIRequest:
    return make_mock_request(mock_request_post_data)


@pytest.fixture
def mock_find_refs_text(mock_request: WSGIRequest) -> linker._FindRefsText:
    post_body = json.loads(mock_request.body)
    return linker._create_find_refs_text(post_body)


@pytest.fixture
def mock_find_refs_options(mock_request: WSGIRequest) -> linker._FindRefsTextOptions:
    post_body = json.loads(mock_request.body)
    return linker._create_find_refs_options(mock_request.GET, post_body)


@pytest.fixture
def mock_request_without_meta_data(mock_request_post_data_without_meta_data: dict) -> WSGIRequest:
    return make_mock_request(mock_request_post_data_without_meta_data)


@pytest.fixture
def mock_request_invalid(mock_request_invalid_post_data: dict) -> WSGIRequest:
    return make_mock_request(mock_request_invalid_post_data)


@pytest.fixture
def mock_webpage() -> WebPage:
    # Note, the path of WebPage matches the path of the import we want to patch
    # NOT the location of the WebPage class
    with patch('sefaria.helper.linker.linker.WebPage') as MockWebPage:
        mock_webpage = MockWebPage.return_value
        loaded_webpage = Mock()
        mock_webpage.load.return_value = loaded_webpage
        loaded_webpage.url = "test url"
        MockWebPage.add_or_update_from_linker.return_value = (None, loaded_webpage)
        yield loaded_webpage


class TestFindRefsHelperClasses:

    def test_find_refs_text(self):
        find_refs_text = linker._FindRefsText('title', 'body', 'en')
        assert find_refs_text.lang == 'en'

    def test_find_refs_text_options(self):
        find_refs_text_options = linker._FindRefsTextOptions(True, False, 10, {})
        assert not find_refs_text_options.debug
        assert find_refs_text_options.with_text
        assert find_refs_text_options.max_segments == 10
        assert find_refs_text_options.version_preferences_by_corpus == {}

    def test_create_find_refs_text(self, mock_request: WSGIRequest):
        post_body = json.loads(mock_request.body)
        find_refs_text = linker._create_find_refs_text(post_body)
        assert find_refs_text.title == 'title'
        assert find_refs_text.body == 'body'

    def test_create_find_refs_options(self, mock_request: WSGIRequest):
        post_body = json.loads(mock_request.body)
        find_refs_options = linker._create_find_refs_options(mock_request.GET, post_body)
        assert find_refs_options.with_text
        assert find_refs_options.debug
        assert find_refs_options.max_segments == 10
        assert find_refs_options.version_preferences_by_corpus == {}


class TestMakeFindRefsResponse:
    def test_make_find_refs_response_with_meta_data(self, mock_request: WSGIRequest, mock_webpage: Mock):
        response = linker.make_find_refs_response(mock_request)
        mock_webpage.add_hit.assert_called_once()
        mock_webpage.save.assert_called_once()

    def test_make_find_refs_response_without_meta_data(self, mock_request_without_meta_data: dict,
                                                       mock_webpage: Mock):
        response = linker.make_find_refs_response(mock_request_without_meta_data)
        mock_webpage.add_hit.assert_not_called()
        mock_webpage.save.assert_not_called()

    def test_make_find_refs_response_invalid_post_data(self, mock_request_invalid: dict,
                                                       mock_webpage: Mock):
        with pytest.raises(APIInvalidInputException) as exc_info:
            response = linker.make_find_refs_response(mock_request_invalid)
        # assert that the 'text' field had a validation error
        assert 'text' in exc_info.value.args[0]


class TestUnpackFindRefsRequest:
    def test_unpack_find_refs_request(self, mock_request: WSGIRequest):
        text, options, meta_data = linker._unpack_find_refs_request(mock_request)
        assert isinstance(text, linker._FindRefsText)
        assert isinstance(options, linker._FindRefsTextOptions)
        assert meta_data == {'url': 'https://test.com', 'description': 'description', 'title': 'title'}

    def test_unpack_find_refs_request_without_meta_data(self, mock_request_without_meta_data: dict):
        text, options, meta_data = linker._unpack_find_refs_request(mock_request_without_meta_data)
        assert isinstance(text, linker._FindRefsText)
        assert isinstance(options, linker._FindRefsTextOptions)
        assert meta_data is None


class TestAddWebpageHitForUrl:
    def test_add_webpage_hit_for_url(self, mock_webpage: Mock):
        linker._add_webpage_hit_for_url('https://test.com')
        mock_webpage.add_hit.assert_called_once()
        mock_webpage.save.assert_called_once()

    def test_add_webpage_hit_for_url_no_url(self, mock_webpage: Mock):
        linker._add_webpage_hit_for_url(None)
        mock_webpage.add_hit.assert_not_called()
        mock_webpage.save.assert_not_called()


class TestFindRefsResponseLinkerV3:

    @pytest.fixture
    def mock_get_linker(self, spacy_model: spacy.Language):
        from sefaria.model.text import library
        from sefaria.model.linker.linker import LinkedDoc
        with patch.object(library, 'get_linker') as mock_get_linker:
            mock_linker = Mock()
            mock_get_linker.return_value = mock_linker
            mock_linker.link.return_value = LinkedDoc('', [], [], [])
            mock_linker.link_by_paragraph.return_value = LinkedDoc('', [], [], [])
            yield mock_get_linker

    def test_make_find_refs_response_linker_v3(self, mock_get_linker: WSGIRequest,
                                               mock_find_refs_text: linker._FindRefsText,
                                               mock_find_refs_options: linker._FindRefsTextOptions):
        response = linker._make_find_refs_response_linker_v3(mock_find_refs_text, mock_find_refs_options)
        assert 'title' in response
        assert 'body' in response


class TestFindRefsResponseInner:
    @pytest.fixture
    def mock_resolved(self):
        return []

    def test_make_find_refs_response_inner(self, mock_resolved: Mock, mock_find_refs_options: linker._FindRefsTextOptions):
        response = linker._make_find_refs_response_inner(mock_resolved, mock_find_refs_options)
        assert 'results' in response
        assert 'refData' in response


class TestRefResponseForLinker:

    def test_make_ref_response_for_linker(self, mock_oref: Ref, mock_find_refs_options: linker._FindRefsTextOptions):
        response = linker._make_ref_response_for_linker(mock_oref, mock_find_refs_options)
        assert 'heRef' in response
        assert 'url' in response
        assert 'primaryCategory' in response


class TestPreferredVtitle:
    @pytest.mark.parametrize(('oref', 'vprefs_by_corpus', 'expected_vpref'), [
        [Ref("Job 17"), None, None],
        [Ref("Job 17"), {"Tanakh": {"en": "vtitle1"}}, "vtitle1"],
        [Ref("Shabbat 2a"), {"Tanakh": {"en": "vtitle1"}}, None],
        [Ref("Shabbat 2a"), {"Bavli": {"en": "vtitle1"}}, "vtitle1"],
        [Ref("Shabbat 2a"), {"Bavli": {"he": "vtitle1"}}, None],
    ])
    def test_get_preferred_vtitle(self, oref: Ref, vprefs_by_corpus: dict, expected_vpref: str):
        vpref = linker._get_preferred_vtitle(oref, 'en', vprefs_by_corpus)
        assert vpref == expected_vpref


class TestRefTextByLangForLinker:

    @pytest.fixture
    def mock_ja(self):
        return Mock()

    @pytest.fixture
    def mock_text_chunk(self, mock_ja: Mock):
        with patch('sefaria.model.text.TextChunk') as MockTC:
            mock_tc = MockTC.return_value
            mock_tc.ja.return_value = mock_ja
            mock_tc.strip_itags.side_effect = lambda x: x
            yield mock_tc

    @pytest.mark.parametrize(('options', 'text_array', 'expected_text_array', 'expected_was_truncated'), [
        ({"max_segments": 4}, ['a'], ['a'], False),
        ({"max_segments": 2}, ['a', 'b'], ['a', 'b'], False),
        ({"max_segments": 2}, ['a', 'b', 'c'], ['a', 'b'], True),
    ])
    def test_get_ref_text_by_lang_for_linker(self, mock_text_chunk: TextChunk, mock_ja: Mock, mock_oref: Ref, options: dict,
                                             text_array: list, expected_text_array: list, expected_was_truncated: bool):
        mock_ja.flatten_to_array.return_value = text_array
        find_refs_options = linker._FindRefsTextOptions(**options)
        actual_text_array, actual_was_truncated = linker._get_ref_text_by_lang_for_linker(mock_oref, 'en', find_refs_options)
        assert actual_text_array == expected_text_array
        assert actual_was_truncated == expected_was_truncated

```

### sefaria/helper/tests/legacy_ref_test.py

```
# -*- coding: utf-8 -*-

import pytest
import re
from sefaria.model import *
from sefaria.helper.legacy_ref import legacy_ref_parser_handler, MappingLegacyRefParser, NoLegacyRefParserError, LegacyRefParsingData, LegacyRefParserMappingKeyError
from sefaria.system.exceptions import PartialRefInputError


@pytest.fixture(scope="module", autouse=True)
def test_zohar_index(test_index_title):
    """
    Creates depth 2 Zohar index which will not be able to parse depth 3 refs
    @return:
    """
    en_title = test_index_title
    schema = {
        "key": en_title,
        "titles": [
            {
                "lang": "en",
                "text": en_title,
                "primary": True
            },
            {
                "lang": "en",
                "text": "Alt Title Yo yo"
            },
            {
                "lang": "he",
                "text": '  ',
                "primary": True
            }
        ],
        "nodeType": "JaggedArrayNode",
        "depth": 2,
        "addressTypes": ["Integer", "Integer"],
        "sectionNames": ["Chapter","Verse"]
    }
    index_dict = {
        "schema": schema,
        "title": en_title,
        "categories": ["Kabbalah"],
    }
    i = Index(index_dict)
    i.save()

    yield i

    i.delete()


@pytest.fixture(scope="module", autouse=True)
def test_zohar_mapping_data(test_index_title, url_test_index_title):
    lrpd = LegacyRefParsingData({
        "index_title": test_index_title,
        "data": {
            "mapping": {
                f"{url_test_index_title}.1.15a.1": f"{url_test_index_title}.1.42",
                f"{url_test_index_title}.1.15a.2": f"{url_test_index_title}.1.42",
                f"{url_test_index_title}.1.15a.3": f"{url_test_index_title}.1.43",
            },
        },
    })
    lrpd.save()

    yield lrpd

    lrpd.delete()


@pytest.fixture(scope="module")
def test_index_title():
    return "Test Zohar"


@pytest.fixture(scope="module")
def url_test_index_title(test_index_title):
    return test_index_title.replace(" ", "_")


@pytest.fixture(scope="module")
def old_and_new_trefs(request, url_test_index_title):
    old_ref, new_ref = request.param
    # if new_ref is None, means mapping doesn't exist
    new_ref = new_ref and f"{url_test_index_title}.{new_ref}"
    return f"{url_test_index_title}.{old_ref}", new_ref


def get_book(tref):
    return Ref(tref).index.title


def get_partial_ref_error(tref):
    try:
        Ref(tref)
    except PartialRefInputError as err:
        return err


@pytest.mark.parametrize("old_and_new_trefs", [
    ["1.15a.1", "1.42"],
    ["1.15a.2", "1.42"],
    ["1.15a.3", "1.43"],
    ["1.15a.1-2", "1.42"],
    ["1.15a.1-3", "1.42-43"],
    ["123.456.789", None],
    ["1.15a.1-4", None],
], indirect=True)
class TestLegacyRefsTestIndex:

    def test_old_zohar_ref_fail(self, old_and_new_trefs):
        """
        old Zohar ref fails

        @param old_and_new_trefs:
        @return:
        """
        old_tref, _ = old_and_new_trefs
        with pytest.raises(PartialRefInputError):
            Ref(old_tref)

    def test_old_zohar_partial_ref(self, test_index_title, old_and_new_trefs):
        """
        tests that once a ranged ref fails that its partial ref exception contains the appropriate data

        @param test_index_title:
        @param segment_level_zohar_tref:
        @return:
        """
        old_tref, _ = old_and_new_trefs
        err = get_partial_ref_error(old_tref)
        book = get_book(err.matched_part)
        assert book == test_index_title

    def test_old_zohar_partial_ref_legacy_loader(self, old_and_new_trefs):
        old_tref, _ = old_and_new_trefs
        err = get_partial_ref_error(old_tref)
        book = get_book(err.matched_part)
        assert type(legacy_ref_parser_handler[book] == MappingLegacyRefParser)

    def test_old_zohar_partial_ref_legacy_parsing(self, old_and_new_trefs):
        old_ref, new_ref = old_and_new_trefs
        err = get_partial_ref_error(old_ref)
        book = get_book(err.matched_part)
        parser = legacy_ref_parser_handler[book]

        if new_ref is None:
            with pytest.raises(LegacyRefParserMappingKeyError):
                parser.parse(old_ref)
        else:
            converted_ref = parser.parse(old_ref)
            assert converted_ref.legacy_tref == old_ref
            assert converted_ref.normal() == Ref(new_ref).normal()

    def test_instantiate_ref_with_legacy_parse_fallback(self, url_test_index_title, old_and_new_trefs):
        old_tref, new_tref = old_and_new_trefs
        instantiate_legacy_refs_tester(url_test_index_title, old_tref, new_tref)


@pytest.mark.parametrize(('url_index_title', 'input_title', 'input_sections', 'output_tref'), [
    ["Test_Zohar", "Alt Title Yo yo", "1:15a:1", "Test_Zohar.1.42"],
    ["Test_Zohar", "Alt_Title_Yo_yo", "1:15a:1", "Test_Zohar.1.42"],
])
def test_instantiate_legacy_refs_parametrized(url_index_title, input_title, input_sections, output_tref):
    old_tref = f"{input_title}.{input_sections}"
    instantiate_legacy_refs_tester(url_index_title, old_tref, output_tref, old_title=input_title)


def instantiate_legacy_refs_tester(url_index_title, old_tref, new_tref, old_title=None):
    oref = Ref.instantiate_ref_with_legacy_parse_fallback(old_tref)
    if new_tref is None:
        assert oref.url() == url_index_title
        assert getattr(oref, 'legacy_tref', None) is None
    else:
        assert oref.url() == new_tref
        expected_legacy_tref = old_tref.replace(':', '.')
        if old_title:
            expected_legacy_tref = expected_legacy_tref.replace(old_title, url_index_title)
        assert oref.legacy_tref == expected_legacy_tref

    if new_tref is not None:
        oref = Ref.instantiate_ref_with_legacy_parse_fallback(new_tref)
        assert oref.url() == new_tref
        assert getattr(oref, 'legacy_tref', None) is None


class TestLegacyRefsRandomIndex:

    @pytest.fixture
    def tref_no_legacy_parser(self):
        return "Genesis, Vayelech 3"

    def test_random_partial_ref_legacy_parsing(self, tref_no_legacy_parser):
        err = get_partial_ref_error(tref_no_legacy_parser)
        with pytest.raises(NoLegacyRefParserError):
            legacy_ref_parser_handler[Ref(err.matched_part).index.title]

```

### sefaria/helper/tests/search_test.py

```
# encoding=utf-8
import json
from sefaria.helper.search import *


def test_query_obj():
    # stam query
    s = get_query_obj("moshe", "text", "exact", False, 0, 0, 10, [], [], [], "sort", ['comp_date', 'order'])
    t = json.loads("""{"_source": false, "from":0,"size":10,"highlight":{"fields":{"exact":{"fragment_size":200,"pre_tags":["<b>"],"post_tags":["</b>"]}}},"sort":["comp_date","order"],"query":{"match_phrase":{"exact":{"query":"moshe","slop":0}}}}""")
    assert ordered(t) == ordered(s.to_dict())
    # text query sorted by pagerank and non-exact
    s = get_query_obj("moshe", "text", "naive_lemmatizer", False, 10, 0, 10, [], [], [], "score", ['pagesheetrank'], sort_score_missing=0.04)
    t = json.loads("""{"_source": false, "size":10,"from":0,"highlight":{"fields":{"naive_lemmatizer":{"fragment_size":200,"pre_tags":["<b>"],"post_tags":["</b>"]}}},"query":{"function_score":{"functions":[{"field_value_factor":{"field":"pagesheetrank","missing":0.04}}],"query":{"match_phrase":{"naive_lemmatizer":{"query":"moshe","slop":10}}}}}}""")
    assert ordered(t) == ordered(s.to_dict())
    # text query sorted by pagerank, non-exact and with aggs
    s = get_query_obj("moshe", "text", "naive_lemmatizer", False, 10, 0, 10, [], [], ["path"], "score", ['pagesheetrank'], sort_score_missing=0.04)
    t = json.loads("""{"_source": false, "size":10,"from":0,"highlight":{"fields":{"naive_lemmatizer":{"fragment_size":200,"pre_tags":["<b>"],"post_tags":["</b>"]}}},"query":{"function_score":{"functions":[{"field_value_factor":{"field":"pagesheetrank","missing":0.04}}],"query":{"match_phrase":{"naive_lemmatizer":{"query":"moshe","slop":10}}}}},"aggs":{"path":{"terms":{"field":"path","size":10000}}}}""")
    assert ordered(t) == ordered(s.to_dict())
    # sheet query sorted by views and with multiple aggs
    s = get_query_obj("moshe", "sheet", "content", False, 10, 0, 10, [], [], ['collections', 'tags'], "sort", ['views'], sort_reverse=True)
    t = json.loads("""{"_source": false, "from": 0, "size":10,"highlight":{"fields":{"content":{"fragment_size":200,"pre_tags":["<b>"],"post_tags":["</b>"]}}},"sort":[{"views":{"order":"desc"}}],"aggs":{"collections":{"terms":{"field":"collections","size":10000}},"tags":{"terms":{"field":"tags","size":10000}}},"query":{"match_phrase":{"content":{"query":"moshe","slop":10}}}}""")
    assert ordered(t) == ordered(s.to_dict())
    # text query sorted by pagerank and with multiple applied filters
    s = get_query_obj("moshe", "text", "naive_lemmatizer", False, 10, 0, 10, ["Tanakh/Targum/Targum Jonathan", "Mishnah/Seder Zeraim/Mishnah Peah", "Talmud/Bavli/Seder Moed/Pesachim"], ["path", "path", "path"], [], "score", ['pagesheetrank'], sort_score_missing=0.04)
    t = json.loads("""{"_source": false, "from":0,"size":10,"highlight":{"fields":{"naive_lemmatizer":{"fragment_size":200,"pre_tags":["<b>"],"post_tags":["</b>"]}}},"query":{"function_score":{"functions":[{"field_value_factor":{"field":"pagesheetrank","missing":0.04}}],"query":{"bool":{"must":[{"match_phrase":{"naive_lemmatizer":{"query":"moshe","slop":10}}}],"filter":[{"bool":{"must":[{"bool":{"should":[{"regexp":{"path":"Tanakh/Targum/Targum\\\\ Jonathan|Tanakh/Targum/Targum\\\\ Jonathan/.*"}},{"regexp":{"path":"Mishnah/Seder\\\\ Zeraim/Mishnah\\\\ Peah|Mishnah/Seder\\\\ Zeraim/Mishnah\\\\ Peah/.*"}},{"regexp":{"path":"Talmud/Bavli/Seder\\\\ Moed/Pesachim|Talmud/Bavli/Seder\\\\ Moed/Pesachim/.*"}}]}}]}}]}}}}}""")
    assert ordered(t) == ordered(s.to_dict())
    # sheet query sorted by relevance, with a collections agg and collections/tag filters
    s = get_query_obj("moshe", "sheet", "content", False, 10, 0, 10, ["", "Moses", "Passover"], ["collections", "tags", "tags"], ['collections'], "score", [])
    t = json.loads("""{"_source": false, "size":10,"from":0,"highlight":{"fields":{"content":{"fragment_size":200,"pre_tags":["<b>"],"post_tags":["</b>"]}}},"aggs":{"collections":{"terms":{"field":"collections","size":10000}}},"query":{"bool":{"must":[{"match_phrase":{"content":{"query":"moshe","slop":10}}}],"filter":[{"bool":{"must":[{"bool":{"must":[{"term":{"tags":"Moses"}},{"term":{"tags":"Passover"}}]}},{"bool":{"must":[{"term":{"collections":""}}]}}]}}]}}}""")
    assert ordered(t) == ordered(s.to_dict())


def ordered(obj):
    if isinstance(obj, dict):
        return sorted((str(k), ordered(v)) for k, v in list(obj.items()))
    if isinstance(obj, list):
        return sorted(ordered(x) for x in obj)
    else:
        return obj

```

### sefaria/helper/tests/topic_test.py

```
import pytest
from sefaria.model.topic import Topic, IntraTopicLink
from sefaria.model.text import library
from sefaria.model.place import Place
from sefaria.helper import topic


@pytest.fixture(autouse=True, scope='module') 
def root_with_self_link():
	# create branch of tree starting with root_with_self_link
	t = Topic({'slug': "", "isTopLevelDisplay": True, "data_source": "sefaria", "numSources": 30, "displayOrder": 10})
	title = "Root Topic With Link to Itself"
	he_title = title[::-1]
	t.add_primary_titles(title, he_title)
	t.set_slug_to_primary_title()
	t.save()
	l = IntraTopicLink({"linkType": "displays-under", "fromTopic": t.slug,
						"toTopic": t.slug, "dataSource": "sefaria",
						"class": "intraTopic"}).save()
	yield {"topic": t, "link": l}
	t.delete()
	l.delete()


@pytest.fixture(autouse=True, scope='module')
def child_of_root_with_self_link(root_with_self_link):
	t = Topic({'slug': "", "isTopLevelDisplay": False, "data_source": "sefaria", "numSources": 0})
	title = "Second Level"
	he_title = title[::-1]
	t.add_primary_titles(title, he_title)
	t.set_slug_to_primary_title()
	t.save()
	l = IntraTopicLink({"linkType": "displays-under", "fromTopic": t.slug,
						"toTopic": root_with_self_link["topic"].slug, "dataSource": "sefaria",
						"class": "intraTopic"}).save()
	yield {"topic": t, "link": l}
	t.delete()
	l.delete()


@pytest.fixture(autouse=True, scope='module')
def grandchild_of_root_with_self_link(child_of_root_with_self_link):
	t = Topic({'slug': "", "isTopLevelDisplay": False, "data_source": "sefaria", "numSources": 0})
	title = "Second Level With Leaf Node"
	he_title = title[::-1]
	t.add_primary_titles(title, he_title)
	t.set_slug_to_primary_title()
	t.save()
	l = IntraTopicLink({"linkType": "displays-under", "fromTopic": t.slug,
						"toTopic": child_of_root_with_self_link["topic"].slug, "dataSource": "sefaria",
						"class": "intraTopic"}).save()
	yield {"topic": t, "link": l}
	t.delete()
	l.delete()


@pytest.fixture(autouse=True, scope='module')
def author_root(django_db_setup, django_db_blocker):
	# create second branch of tree starting with author_root
	with django_db_blocker.unblock():
		t = Topic({'slug': "", "isTopLevelDisplay": True, "data_source": "sefaria", "numSources": 0})
		title = "Authors"
		he_title = title[::-1]
		t.add_primary_titles(title, he_title)
		t.set_slug_to_primary_title()
		t.save()
		l = None
		yield {"topic": t, "link": l}
		t.delete()

@pytest.fixture(autouse=True, scope='module')
def some_topic():
	t = Topic({'slug': "abcd_test", "data_source": "sefaria", "numSources": 0})
	title = "title in English"
	he_title = " "
	t.add_primary_titles(title, he_title)
	t.set_slug_to_primary_title()
	t.save()
	yield t
	t.delete()


@pytest.fixture(autouse=True, scope='module')
def actual_author(author_root):
	t = Topic({'slug': "", "isTopLevelDisplay": False, "data_source": "sefaria", "numSources": 0})
	title = "Author Dude"
	he_title = title[::-1]
	t.add_primary_titles(title, he_title)
	t.set_slug_to_primary_title()
	t.save()
	l = IntraTopicLink({"linkType": "displays-under", "fromTopic": t.slug,
						"toTopic": author_root["topic"].slug, "dataSource": "sefaria",
						"class": "intraTopic"}).save()  # author_root has child leaf_node
	yield {"topic": t, "link": l}
	t.delete()
	l.delete()


def test_title_and_desc(author_root, actual_author, root_with_self_link, child_of_root_with_self_link, grandchild_of_root_with_self_link):
	for count, t in enumerate([author_root, actual_author, root_with_self_link, child_of_root_with_self_link, grandchild_of_root_with_self_link]):
		en_primary_title = {"text": f"new title {count+1}", "primary": True, "lang": 'en'}
		he_primary_title = {"lang": "he", "text": f"new hebrew title {count+1}", "primary": True}

		en_alt_title = {"lang": "en",  "text": f"New Alt title {count+1}"}
		he_alt_title = {"lang": "he", "text": f"New He Alt Title {count+1}"}

		new_values = {"titles": [en_primary_title, en_alt_title, he_alt_title, he_primary_title],
					  "description": {"en": f"new desc", "he": "new hebrew desc"}}
		topic.update_topic(t["topic"], **new_values)
		assert t["topic"].description == new_values["description"]
		assert t["topic"].get_primary_title('he') == he_primary_title['text']
		assert t["topic"].get_titles('en') == [en_primary_title['text'], en_alt_title['text']]

def test_author_root(author_root, actual_author):
	new_values = {"category": "authors", "titles": [
		{'text': actual_author["topic"].get_primary_title('en'), "lang": 'en', 'primary': True},
		{"text": actual_author["topic"].get_primary_title('he'), "lang": 'he', 'primary': True}],
	  	"birthPlace": "Kyoto, Japan", "birthYear": 1300}
	assert Place().load({'key': new_values["birthPlace"]}) is None
	topic.update_topic(actual_author["topic"], **new_values)
	assert Place().load({'key': new_values["birthPlace"]})
	assert actual_author["topic"].properties["birthYear"]["value"] == 1300
	Place().load({'key': new_values["birthPlace"]}).delete()

def test_change_categories_and_titles(author_root, root_with_self_link):
	# tests moving both root categories down the tree and back up and asserting that moving down the tree changes the tree
	# and assert that moving it back to the root position yields the original tree.
	orig_tree_from_normal_root = library.get_topic_toc_json_recursive(author_root["topic"])
	orig_tree_from_root_with_self_link = library.get_topic_toc_json_recursive(root_with_self_link["topic"])
	orig_trees = [orig_tree_from_normal_root, orig_tree_from_root_with_self_link]
	roots = [author_root["topic"], root_with_self_link["topic"]]
	orig_titles = [{'text': roots[0].get_primary_title('en'), 'lang':'en', 'primary': True}, {'text': roots[1].get_primary_title('en'), 'lang':'en', 'primary': True}]
	orig_he_titles = [{'text': roots[0].get_primary_title('he'), 'lang':'he', 'primary': True}, {'text': roots[1].get_primary_title('he'), 'lang':'he', 'primary': True}]
	for i, root in enumerate(roots):
		other_root = roots[1 - i]
		topic.update_topic(root, titles=[{'text': f"fake new title {i+1}", 'lang': 'he', 'primary': True},
										 {'text': f"fake new he title {i+1}", 'lang': 'he', 'primary': True}], category=other_root.slug)  # move root to be child of other root
		new_tree = library.get_topic_toc_json_recursive(other_root)
		assert new_tree != orig_trees[i]  # assert that the changes in the tree have occurred
		assert root.get_titles('en') != [orig_titles[i]['text']]
		assert root.get_titles('he') != [orig_he_titles[i]['text']]
		topic.update_topic(root, titles=[orig_titles[i], orig_he_titles[i]], category=Topic.ROOT)  # move it back to the main menu
		assert root.get_titles('en') == [orig_titles[i]['text']]
		assert root.get_titles('he') == [orig_he_titles[i]['text']]


	final_tree_from_normal_root = library.get_topic_toc_json_recursive(roots[0])
	final_tree_from_root_with_self_link = library.get_topic_toc_json_recursive(roots[1])
	assert final_tree_from_normal_root == orig_tree_from_normal_root  # assert that the tree is back to normal
	assert final_tree_from_root_with_self_link == orig_tree_from_root_with_self_link


def test_change_categories(author_root, actual_author, root_with_self_link, child_of_root_with_self_link, grandchild_of_root_with_self_link):
	# tests moving topics across the tree to a different root

	orig_tree_from_normal_root = library.get_topic_toc_json_recursive(author_root["topic"])
	orig_tree_from_root_with_self_link = library.get_topic_toc_json_recursive(root_with_self_link["topic"])

	topic.topic_change_category(child_of_root_with_self_link["topic"], author_root["topic"].slug)
	topic.topic_change_category(actual_author["topic"], root_with_self_link["topic"].slug)

	new_tree_from_normal_root = library.get_topic_toc_json_recursive(author_root["topic"])
	new_tree_from_root_with_self_link = library.get_topic_toc_json_recursive(root_with_self_link["topic"])
	assert new_tree_from_normal_root != orig_tree_from_normal_root
	assert new_tree_from_root_with_self_link != orig_tree_from_root_with_self_link

	topic.topic_change_category(child_of_root_with_self_link["topic"], root_with_self_link["topic"].slug)
	topic.topic_change_category(actual_author["topic"], author_root["topic"].slug)

	new_tree_from_normal_root = library.get_topic_toc_json_recursive(author_root["topic"])
	new_tree_from_root_with_self_link = library.get_topic_toc_json_recursive(root_with_self_link["topic"])
	assert new_tree_from_normal_root == orig_tree_from_normal_root
	assert new_tree_from_root_with_self_link == orig_tree_from_root_with_self_link


@pytest.mark.parametrize(('current', 'requested', 'was_ai_generated', 'merged'), [
	['not reviewed', 'not reviewed', True, 'not reviewed'],
	['reviewed', 'not reviewed', True, 'reviewed'],
	['edited', 'not reviewed', True, 'edited'],
	['edited', 'edited', True, 'edited'],
	['not reviewed', 'edited', True, 'edited'],
	['reviewed', 'edited', True, 'reviewed'],
	[None, 'edited', True, 'edited'],
	[None, None, True, None],
	['not reviewed', 'not reviewed', False, None],
	[None, 'edited', False, None],
	[None, None, False, None],
])
def test_calculate_approved_review_state(current, requested, was_ai_generated, merged):
	assert topic._calculate_approved_review_state(current, requested, was_ai_generated) == merged

@pytest.mark.parametrize(('current', 'requested', 'merged'), [
	[{'en': {}}, {'en': {}}, {'en': {}}],
	[{'en': {'review_state': 'not reviewed'}}, {'en': {}}, {'en': {'review_state': 'not reviewed'}}],
	[{'en': {}, 'he': {'title': 'yo'}}, {'en': {'title': 'eng'}}, {'en': {'title': 'eng'}, 'he': {'title': 'yo'}}],

])
def test_get_merged_descriptions(current, requested, merged):
	assert topic._get_merged_descriptions(current, requested) == merged


def test_update_topic(some_topic):
	topic.update_topic(some_topic, titles=[{"text": "Tamar", "lang": "en", "primary": True},
							 {"text": "", "lang": "he", "primary": True, "disambiguation": ""}])
	assert some_topic.titles == [{"text": "Tamar", "lang": "en", "primary": True},
						 {"text": "", "lang": "he", "primary": True, "disambiguation": ""}]

	topic.update_topic(some_topic, description={"en": "abcdefg"})
	assert some_topic.description == {"en": "abcdefg"}

	with pytest.raises(Exception):
		topic.update_topic(some_topic, titles=[{"a": "Tamar", "b": "en"},
								 {"c": "", "lang": "d", "disambiguation": ""}])
	with pytest.raises(Exception):
		topic.update_topic(some_topic, slug='abc')

```

### sefaria/helper/tests/auto_linking_test.py

```
# -*- coding: utf-8 -*-

from sefaria.model import *
from sefaria.helper.link import rebuild_links_for_title, AutoLinkerFactory
import sefaria.tracker as tracker
from sefaria.system.exceptions import InputError
from sefaria.helper.schema import convert_simple_index_to_complex, insert_first_child


class Test_AutoLinker(object):
    link_set_lambda = lambda x: LinkSet({"refs": {"$regex": Ref(x).regex()}, "auto": True, "generated_by": "add_commentary_links"})
    rashi_on_genesis_links = link_set_lambda("Rashi on Genesis")
    kos_eliyahu_links = link_set_lambda("Kos Eliyahu on Pesach Haggadah")
    desired_link_counts = {
        'Rashi on Genesis': rashi_on_genesis_links.count(),
        'Kos Eliyahu on Pesach Haggadah': kos_eliyahu_links.count(),
    }

    @classmethod
    def setup_class(cls):
        # create dummy indexes: "Many to One on Genesis" and "One to One on Genesis"
        # ensure dummy index was properly deleted
        index = Index().load({'title': 'Many to One on Genesis'})
        if index is not None:
            ls = LinkSet(Ref("Many to One on Genesis"))
            ls.delete()
            index.delete()

        # Build an index with some nodes
        root = SchemaNode()
        root.add_title('Many to One on Genesis', 'en', primary=True)
        root.add_title('   ', 'he', primary=True)
        root.key = 'Delete Me'

        intro = JaggedArrayNode()
        intro.add_shared_term("Introduction")
        intro.add_structure(['Chapter', 'Verse'])
        intro.depth = 2
        intro.key = 'intro'
        root.append(intro)

        default = JaggedArrayNode()
        default.key = "default"
        default.default = True
        default.add_structure(["Chapter", "Verse", "Comment"])
        root.append(default)

        root.validate()

        index = Index({
            'schema': root.serialize(),
            'title': 'Many to One on Genesis',
            'dependence': "Commentary",
            "base_text_titles": ["Genesis"],
            "base_text_mapping": "many_to_one_default_only",
            'categories': ['Tanakh', "Rishonim on Tanakh"],
        })
        index.save()

        # add some text
        v = Version({
            "language": "en",
            "title": "Many to One on Genesis",
            "versionSource": "http://foobar.com",
            "versionTitle": "Schema Test",
            "chapter": root.create_skeleton()
        }).save()

        p1 = [['intro intro', 'intro'], ['intro'], ['intro', '', 'intro']]
        chunk = TextChunk(Ref('Many to One on Genesis, Introduction'), 'en', 'Schema Test')
        chunk.text = p1
        chunk.save()

        p2 = [[['Default default']], [['default', 'default!']], [['default', '', 'default']]]
        chunk = TextChunk(Ref('Many to One on Genesis'), 'en', 'Schema Test')
        chunk.text = p2
        chunk.save()

        # add some links
        Link({
            'refs': ['Many to One on Genesis, Introduction 1:1', 'Shabbat 2a:5'],
            'type': 'commentary',
            "generated_by": "intro_parser",
            "auto": True
        }).save()
        Link({
            'refs': ['Many to One on Genesis, Introduction 2:1', 'Many to One on Genesis 2:1:2'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "intro_parser"
        }).save()
        Link({
            'refs': ['Many to One on Genesis 3:1:3', 'Genesis 3:1'],
            'type': 'commentary',
            "generated_by": "add_commentary_links",
            "auto": True
        }).save()
        Link({
            'refs': ['Many to One on Genesis 3:1:1', 'Genesis 3:1'],
            'type': 'commentary',
            "generated_by": "add_commentary_links",
            "auto": True
        }).save()
        Link({
            'refs': ['Many to One on Genesis 1:1:1', 'Genesis 1:1'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()
        Link({
            'refs': ['Many to One on Genesis 2:1:1', 'Genesis 2:1'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()
        Link({
            'refs': ['Many to One on Genesis 2:1:2', 'Genesis 2:1'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()

        # ensure dummy index was properly deleted
        index = Index().load({'title': 'One to One on Genesis'})
        if index is not None:
            ls = LinkSet(Ref("One to One on Genesis"))
            ls.delete()
            index.delete()

        # Build an index with some nodes
        root = SchemaNode()
        root.add_title('One to One on Genesis', 'en', primary=True)
        root.add_title('   ', 'he', primary=True)
        root.key = 'Delete Me'

        intro = JaggedArrayNode()
        intro.add_shared_term("Introduction")
        intro.add_structure(['Chapter', 'Verse'])
        intro.depth = 2
        intro.key = 'intro'
        root.append(intro)

        default = JaggedArrayNode()
        default.key = "default"
        default.default = True
        default.add_structure(["Chapter", "Verse"])
        root.append(default)

        root.validate()

        index = Index({
            'schema': root.serialize(),
            'title': 'One to One on Genesis',
            'dependence': 'Commentary',
            'base_text_titles': ['Genesis'],
            'base_text_mapping': 'one_to_one_default_only',
            'categories': ['Tanakh', 'Rishonim on Tanakh'],
        })
        index.save()

        # add some text
        v = Version({
            "language": "en",
            "title": "One to One on Genesis",
            "versionSource": "http://foobar.com",
            "versionTitle": "Schema Test",
            "chapter": root.create_skeleton()
        }).save()

        p1 = [['intro intro', 'intro'], ['intro'], ['intro', '', 'intro']]
        chunk = TextChunk(Ref('One to One on Genesis, Introduction'), 'en', 'Schema Test')
        chunk.text = p1
        chunk.save()

        p2 = [['Default default'], ['default', 'default!'], ['default', '', 'default']]
        chunk = TextChunk(Ref('One to One on Genesis'), 'en', 'Schema Test')
        chunk.text = p2
        chunk.save()

        # add some links
        Link({
            'refs': ['One to One on Genesis, Introduction 1:1', 'Shabbat 2a:5'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "intro_parser"
        }).save()
        Link({
            'refs': ['One to One on Genesis, Introduction 2:1', 'One to One on Genesis 2:1'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "intro_parser"
        }).save()
        Link({
            'refs': ['One to One on Genesis, Introduction 3:1', 'Shabbat 2a:5'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "intro_parser"
        }).save()
        Link({
            'refs': ['One to One on Genesis 1:1', 'Genesis 1:1'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()
        Link({
            'refs': ['One to One on Genesis 2:2', 'Genesis 2:2'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()
        Link({
            'refs': ['One to One on Genesis 2:1', 'Genesis 2:1'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()
        Link({
            'refs': ['One to One on Genesis 3:3', 'Genesis 3:3'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()
        Link({
            'refs': ['One to One on Genesis 3:1', 'Genesis 3:1'],
            'type': 'commentary',
            "auto": True,
            "generated_by": "add_commentary_links"
        }).save()

        VersionState("One to One on Genesis").refresh()

        link_set_lambda = lambda x: LinkSet({"refs": {"$regex": Ref(x).regex()}, "auto": True, "generated_by": "add_commentary_links"})
        cls.desired_link_counts["Many to One on Genesis"] = link_set_lambda("Many to One on Genesis").count()
        cls.desired_link_counts["One to One on Genesis"] = link_set_lambda("One to One on Genesis").count()

        title = 'Kos Eliyahu on Pesach Haggadah'
        i = library.get_index(title)
        Version({
            "chapter": i.nodes.create_skeleton(),
            "versionTitle": "test",
            "versionSource": "blabla",
            "language": "he",
            "title": i.title
        }).save()
        #print 'End of test setup'

    @classmethod
    def teardown_class(cls):
        try:
            ls = LinkSet(Ref("Many to One on Genesis"))
            ls.delete()
        except InputError:
            pass

        try:
            ls = LinkSet(Ref("One to One on Genesis"))
            ls.delete()
        except InputError:
            pass

        v = Version().load({'title': 'Many to One on Genesis'})
        if v:
            v.delete()
        v = Version().load({'title': 'One to One on Genesis'})
        if v:
            v.delete()
        i = Index().load({'title': 'Many to One on Genesis'})
        if i:
            i.delete()
        i = Index().load({"title": "One to One on Genesis"})
        if i:
            i.delete()
        v = Version().load({"title": 'Kos Eliyahu on Pesach Haggadah', "versionTitle": "test", "language": "he"})
        if v:
            v.delete()

    def test_rebuild_commentary_links(self):
        #test simple adding links
        title = 'Rashi on Genesis'
        rf = Ref(title)
        desired_link_count = self.desired_link_counts[title]
        linker = rf.autolinker()
        found = linker.rebuild_links()
        assert len(found) == desired_link_count

    def test_rebuild_commentary_links_complex(self):
        title = 'Kos Eliyahu on Pesach Haggadah'
        rf = Ref(title)
        linker = rf.autolinker(user=1)
        desired_link_count = self.desired_link_counts[title]
        found = linker.rebuild_links()
        assert len(found) == desired_link_count

    def test_rebuild_same_quantity_of_links_for_many_to_one_default_only(self):
        title = 'Many to One on Genesis'
        rf = Ref(title)
        desired_link_count = self.desired_link_counts[title]
        linker = rf.autolinker()
        found = linker.rebuild_links()
        assert len(found) == desired_link_count

    def test_rebuild_same_quantity_of_links_for_one_to_one_default_only(self):
        title = 'One to One on Genesis'
        rf = Ref(title)
        desired_link_count = self.desired_link_counts[title]
        linker = rf.autolinker()
        found = linker.rebuild_links()
        assert len(found) == desired_link_count

    def test_rebuild_same_link_content_for_many_to_one_default_only(self):
        title_ref = 'Many to One on Genesis 3:1:3'
        base_ref = "Genesis 3:1"
        linker = Ref("Many to One on Genesis").autolinker()

        # load an existing link, delete it, then rebuild links and
        # assert that the link exists again
        query = {"$and": [{"refs": {"$regex": "^{}".format(title_ref)}},
                          {"refs": {"$regex": "^{}".format(base_ref)}}]}

        # now load the link and delete it
        existing_link = Link().load(query)
        existing_link.delete()

        # rebuild links and assert old link successfully rebuilt
        found = linker.rebuild_links()
        new_link = Link().load(query)
        assert new_link

    def test_rebuild_same_link_content_for_one_to_one_default_only(self):
        title_ref = "One to One on Genesis 3:3"
        base_ref = "Genesis 3:3"
        linker = Ref("One to One on Genesis").autolinker()

        # load an existing link, delete it, then rebuild links and
        # assert that the link exists again
        query = {"$and": [{"refs": {"$regex": "^{}".format(title_ref)}},
                          {"refs": {"$regex": "^{}".format(base_ref)}}]}

        # now load the link and delete it
        existing_link = Link().load(query)
        existing_link.delete()

        # rebuild links and assert old link successfully rebuilt
        found = linker.rebuild_links()
        new_link = Link().load(query)
        assert new_link

    def test_refresh_commentary_links(self):
        #test that there are the same number of links before and after
        title = 'Rashi on Genesis'
        rf = Ref(title)
        regex = rf.regex()
        desired_link_count = self.desired_link_counts[title]
        linker = rf.autolinker()
        linker.refresh_links()
        link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()
        assert desired_link_count == link_count

    def test_refresh_commentary_links_complex(self):
        #test that there are the same number of links before and after
        title = 'Kos Eliyahu on Pesach Haggadah'
        rf = Ref(title)
        regex = rf.regex()
        desired_link_count = self.desired_link_counts[title]
        linker = rf.autolinker()
        linker.refresh_links()
        link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()
        assert desired_link_count == link_count

    def test_refresh_commentary_links_one_to_one_default_node(self):
        title = "One to One on Genesis"
        base = "Genesis"
        rf = Ref(title)
        regex = rf.regex()
        desired_link_count = self.desired_link_counts[title]
        linker = rf.autolinker()

        # add a link to the intro even though there shouldn't be such a link
        comm_ref = "{}, Introduction 1:2".format(title)
        base_ref = "{} 1:2".format(base)
        Link({"refs": [base_ref, comm_ref], "generated_by": linker._generated_by_string, "type": "commentary", "auto": True}).save()

        # test that refreshing the links causes the new link above to be deleted.
        # the result should be the same number of links as before the link was created
        linker.refresh_links()
        link_count = LinkSet({"generated_by": linker._generated_by_string, "refs": {"$regex": regex}}).count()
        assert desired_link_count == link_count

    def test_refresh_commentary_links_many_to_one_default_node(self):
        title = "Many to One on Genesis"
        base = "Genesis"
        rf = Ref(title)
        regex = rf.regex()
        desired_link_count = self.desired_link_counts[title]
        linker = rf.autolinker()

        # add a link to the intro even though there shouldn't be such a link
        comm_ref = "{}, Introduction 1:2".format(title)
        base_ref = "{} 1".format(base)
        Link({"refs": [base_ref, comm_ref], "generated_by": linker._generated_by_string, "type": "commentary", "auto": True}).save()

        # test that refreshing the links causes the new link above to be deleted.
        # the result should be the same number of links as before the link was created
        linker.refresh_links()
        link_count = LinkSet({"generated_by": linker._generated_by_string, "refs": {"$regex": regex}}).count()
        assert desired_link_count == link_count

    def test_refresh_links_with_text_save(self):
        title = 'Rashi on Genesis'
        section_tref = 'Rashi on Genesis 18:22'
        stext = ["     ", "     ", "     "]
        lang = 'he'
        vtitle = "test"
        oref = Ref(section_tref)
        rf = Ref(title)
        regex = rf.regex()
        #original count
        desired_link_count = self.desired_link_counts[title]
        # add some text (adding one more comment than there is already)
        tracker.modify_text(1, oref, vtitle, lang, stext)
        higher_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()

        # now delete
        chunk = TextChunk(oref, lang, vtitle)
        chunk.text = chunk.text[:-1]
        tracker.modify_text(1, oref, vtitle, lang, chunk.text)
        lower_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()
        # Assert both of these after the removal - lest the first one failing prevent the text removal
        assert higher_link_count == (desired_link_count+1)
        assert lower_link_count == desired_link_count

    def test_refresh_links_with_text_save_many_to_one_default_node(self):
        title_ref = "Many to One on Genesis 1:9"
        title = Ref(title_ref).index.title
        base = Ref(title_ref).index.base_text_titles[0]
        desired_link_count = self.desired_link_counts[title]
        rf = Ref(title)
        regex = rf.regex()

        # add another segment to intro text to show that it won't affect link count
        stext = ["Intro first segment text", "Intro second segment text", "Intro third segment text"]
        oref = Ref("{}, Introduction 1".format(title))
        tracker.modify_text(1, oref, "test", "en", stext)
        base_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()

        # now add 2 segments to default node and check that exactly 2 more links exist than
        lang = 'he'
        vtitle = "test"
        oref = Ref(title_ref)
        stext = TextChunk(oref, lang=lang).text
        stext += ["", ""]
        tracker.modify_text(1, oref, vtitle, lang, stext)
        higher_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()

        # now delete 2 segments
        chunk = TextChunk(oref, lang, vtitle)
        chunk.text = chunk.text[:-2]
        tracker.modify_text(1, oref, vtitle, lang, chunk.text)
        lower_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()

        # Assert both of these after the removal - lest the first one failing prevent the text removal
        assert base_link_count == desired_link_count
        assert higher_link_count == (desired_link_count+2)
        assert lower_link_count == desired_link_count

    def test_refresh_links_with_text_save_one_to_one_default_node(self):
        title_ref = "One to One on Genesis 1"
        title = Ref(title_ref).index.title
        base = Ref(title_ref).index.base_text_titles[0]
        desired_link_count = self.desired_link_counts[title]
        rf = Ref(title)
        regex = rf.regex()

        # add another segment to intro text to show that it won't affect link count
        stext = ["Intro first segment text", "Intro second segment text", "Intro third segment text"]
        oref = Ref("{}, Introduction 1".format(title))
        tracker.modify_text(1, oref, "test", "en", stext)
        base_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()

        # now add 2 segments to default node and check that exactly 2 more links exist than
        lang = 'en'
        vtitle = "test"
        oref = Ref(title_ref)
        stext = TextChunk(oref, lang=lang).text
        stext += ["new", "new"]
        tracker.modify_text(1, oref, vtitle, lang, stext)
        higher_link_count = LinkSet(
            {"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()

        # now delete 2 segments
        chunk = TextChunk(oref, lang, vtitle)
        chunk.text = chunk.text[:-2]
        tracker.modify_text(1, oref, vtitle, lang, chunk.text)
        lower_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()

        # Assert both of these after the removal - lest the first one failing prevent the text removal
        assert base_link_count == desired_link_count
        assert higher_link_count == (desired_link_count+2)
        assert lower_link_count == desired_link_count

    def test_refresh_links_with_text_save_complex(self):
        title = 'Kos Eliyahu on Pesach Haggadah'
        section_tref = 'Kos Eliyahu on Pesach Haggadah, Kadesh 1'
        stext = ["thlerkawje alkejal ekjlkej", "eaflkje arheahrlka jhklajdhkl ADJHKL"]
        lang = 'he'
        vtitle = "test"
        oref = Ref(section_tref)
        rf = Ref(title)
        regex = rf.regex()
        #original count
        desired_link_count = self.desired_link_counts[title]
        # add some text (adding two more comment than there is already)
        tracker.modify_text(1, oref, vtitle, lang, stext)
        higher_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()
        # now delete
        chunk = TextChunk(oref, lang, vtitle)
        chunk.text = chunk.text[:-2]
        tracker.modify_text(1, oref, vtitle, lang, chunk.text)
        lower_link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()
        # Assert both of these after the removal - lest the first one failing prevent the text removal
        assert higher_link_count == (desired_link_count+2)
        assert lower_link_count == desired_link_count

    def test_bulk_refresh_links_with_text_save_complex(self):
        title = 'Kos Eliyahu on Pesach Haggadah'
        section_tref = 'Kos Eliyahu on Pesach Haggadah, Kadesh 1'
        lang = 'he'
        vtitle = "test"
        rf = Ref(title)
        regex = rf.regex()
        version = Version().load({"title": title, "versionTitle": vtitle, "language": lang})
        text_map = {
            f"{section_tref}:1": "more text",
            f"{section_tref}:2": "even more text",
            f"{section_tref}:3": "blah text",
            f"{section_tref}:5": "skip one!"
        }
        # original count
        desired_link_count = self.desired_link_counts[title]
        # add some text
        tracker.modify_bulk_text(1, version, text_map)
        link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()
        assert link_count == (desired_link_count+4)
        for tref, mod_text in text_map.items():
            seg_index = int(tref.split(':')[-1])-1
            version.chapter["Kadesh"][0][seg_index] = mod_text
        assert version.chapter["Kadesh"][0][3] == ""  # 4th element should have been padded

        # now delete
        for k in text_map.keys():
            text_map[k] = ''  # set all values to empty string. which should trigger trim_ending_whitespace to delete them
        tracker.modify_bulk_text(1, version, text_map)
        link_count = LinkSet({"refs": {"$regex": regex}, "auto": True, "generated_by": "add_commentary_links"}).count()
        assert link_count == desired_link_count


```

### sefaria/helper/__init__.py

```

```

### sefaria/helper/trend_manager.py

```
class TrendManager:
    def __init__(self, name, key, period, valueThresholdMin=5):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
        self.name = name
        self.key = key
        self.period = period
        self.valueThresholdMin = valueThresholdMin

    def getPersonInfo(self, trends):
        person_info = {
            "key": self.key,
            "name": self.name+"_"+self.period,
            "period": self.period
        }
        try:
            if trends.get(self.key, {}).get(self.period) >= self.valueThresholdMin:
                value = True
            else:
                value = False
            person_info["value"] = value
        except:
            person_info["value"] = False
        return person_info


class CategoryTrendManager(TrendManager):
    def __init__(self, categoryName, period="alltime", valueThresholdMin=5):
        key = "ReadInCategory" + categoryName
        name = "read_in_category_" + categoryName.replace(" ","_")
        TrendManager.__init__(self,name,key,period, valueThresholdMin)

class SheetReaderManager(TrendManager):
    def __init__(self, period="alltime"):
        TrendManager.__init__(self,"source_sheet_reader","SheetsRead",period)

class ParashaLearnerManager(TrendManager):
    def __init__(self, period="currently"):
        TrendManager.__init__(self,"parasha_learner", "ParashaLearner", period)

    def getPersonInfo(self, trends):
        person_info = {
            "key": self.key,
            "name": self.name+"_"+self.period,
            "period": self.period
        }
        try:
            if trends.get(self.key, {}).get(self.period):
                value = True
            else:
                value = False
            person_info["value"] = value
        except:
            person_info["value"] = False
        return person_info

class SheetCreatorManager(TrendManager):
    def __init__(self, period="alltime", public=False, valueThresholdMin=3):
        name = "source_sheet_creator_over_" + str(valueThresholdMin) + "_sheets"
        TrendManager.__init__(self,name,"SheetsCreated",period, valueThresholdMin=valueThresholdMin)
        self.public = public

    def getPersonInfo(self, trends):
        if(self.public == False):
            return TrendManager.getPersonInfo(self, trends)
        else:
            person_info_public = {
                "key": self.key+"Public",
                "name": self.name+"_public_"+self.period,
                "period": self.period,
                "value": False
            } if self.public else {}
            if (self.public == True):
                try:
                    if trends.get(self.key+"Public", {}).get(self.period) >= 1:
                        value = True
                    else:
                        value = False
                    person_info_public["value"] = value
                except:
                    person_info_public["value"] = False
            if(person_info_public["value"] == False):
                return person_info_public 
        
            person_info_public["value"] = TrendManager.getPersonInfo(self,trends)["value"]
            return person_info_public

class CustomTraitManager(TrendManager):
    def __init__(self, customTraitName, customTraitKey, period="alltime"):
        TrendManager.__init__(self,customTraitName,customTraitKey,period)
    
    def getPersonInfo(self, trends):
        person_info = {
            "key": self.key,
            "name": self.name,
            "period": self.period
        }
        try:
            value = trends.get(self.key, {}).get(self.period)
        except:
            value = None
        person_info["value"] = value
        return person_info

```

### sefaria/helper/file.py

```
# -*- coding: utf-8 -*-
from PIL import Image
from io import BytesIO
import requests

import structlog
logger = structlog.get_logger(__name__)


def get_resized_file(image, size, to_format="PNG"):
    resized_image = image.resize(size, resample=Image.LANCZOS)
    #resized_image.convert('RGB')
    resized_image_file = BytesIO()
    resized_image.save(resized_image_file, format=to_format)
    resized_image_file.seek(0)
    return resized_image_file


def thumbnail_image_file(image, size, to_format="PNG"):
    image.thumbnail(size, resample=Image.BICUBIC)
    resized_image_file = BytesIO()
    image.save(resized_image_file, format=to_format)
    resized_image_file.seek(0)
    return resized_image_file


def scrape_file(url):
    r = requests.get(url, allow_redirects=True)
    fileobj = BytesIO(r.content)
    fileobj.seek(0)
    return fileobj


def scrape_image(url):
    try:
        return Image.open(scrape_file(url))
    except Exception as e:
        raise e

```

### sefaria/helper/texts/tasks.py

```
import traceback
import uuid
import structlog
import django
from celery import chord
from collections import Counter
from sefaria.client.util import celeryResponse, jsonResponse
from sefaria.system.exceptions import DuplicateRecordError

django.setup()
from sefaria.model import *
import sefaria.tracker as tracker
from sefaria.client.wrapper import format_object_for_client
from sefaria.settings import CELERY_QUEUES, CELERY_ENABLED
from sefaria.celery_setup.app import app
from sefaria.settings import USE_VARNISH
from sefaria.helper.slack.send_message import send_message
if USE_VARNISH:
    from sefaria.system.varnish.wrapper import invalidate_ref

logger = structlog.get_logger(__name__)


def should_run_with_celery(from_api):
    return CELERY_ENABLED and from_api

def save_changes(changes, func, method, task_title=''):
    if should_run_with_celery(method == 'API'):
        main_task_id = str(uuid.uuid4())
        tasks = [save_change.s(func.__name__, c).set(queue=CELERY_QUEUES['tasks']) for c in changes]
        job = chord(tasks, inform.s(main_task_id=main_task_id, task_title=task_title).set(queue=CELERY_QUEUES['tasks']))(task_id=main_task_id)
        tasks_ids = [task.id for task in job.parent.results]
        return celeryResponse(job.id, tasks_ids)
    else:
        results = []
        for change in changes:
            try:
                func(change)
            except Exception as e:
                results.append({'error': f'Object: {change}. Error: {e}'})
            else:
                results.append({'status': 'ok'})
        return jsonResponse(results)

@app.task(name="web.save_change", acks_late=True, ignore_result=True)
def save_change(func_name, raw_history_change):
    function_names = {'save_link': save_link, 'save_version': save_version}
    func = function_names[func_name]
    try:
        func(raw_history_change)
        return 'Success'
    except Exception as e:
        logger.error(f'''Error:
            change: {raw_history_change}
            {traceback.format_exc()}''')
        if isinstance(e, DuplicateRecordError):
            return 'DuplicateRecordError'
        else:
            return repr(e)

@app.task(name="web.inform", acks_late=True)
def inform(results, main_task_id, task_title):
    title = f'{task_title} (celery main task id {main_task_id})'
    results = '\n'.join([f'{k}: {v}.' for k, v in Counter(results).items()])
    send_message('#engineering-signal', 'Text Upload', title, results, icon_emoji=':leafy_green:')

def save_link(raw_link_change: dict):
    link = raw_link_change['raw_link']
    uid = raw_link_change['uid']
    kwargs = {}
    if raw_link_change['method'] == 'API':
        kwargs['method'] = raw_link_change['method']
    func = tracker.update if "_id" in link else tracker.add
    # use the correct function if params indicate this is a note save
    obj = func(uid, Link, link, **kwargs)
    try:
        if USE_VARNISH:
            for ref in link.refs:
                invalidate_ref(Ref(ref), purge=True)
    except Exception as e:
        logger.error(e)
    return format_object_for_client(obj)

def save_version(raw_version_change: dict):
    version = raw_version_change['raw_version']
    uid = raw_version_change['uid']
    patch = raw_version_change['patch']
    kwargs = {'skip_links': raw_version_change['skip_links'], 'count_after': raw_version_change['count_after']}
    tracker.modify_version(uid, version, patch, **kwargs)

```

### sefaria/helper/text.py

```
# encoding=utf-8
import unicodecsv as csv
import io
import re

import pymongo

from sefaria.model import *
from sefaria.system.database import db
from sefaria.datatype.jagged_array import JaggedTextArray
from diff_match_patch import diff_match_patch
from functools import reduce
from sefaria.system.exceptions import InputError

import regex as re
import pprint
try:
    import xml.etree.cElementTree as ET
except ImportError:
    import xml.etree.ElementTree as ET
from sefaria.model import *
from sefaria.utils.hebrew import has_hebrew

def add_spelling(category, old, new, lang="en"):
    """
    For a given category, on every index in that title that matches 'old' create a new title with 'new' replacing 'old'
    :param category:
    :param old:
    :param new:
    :return:
    """
    indxs = library.get_indexes_in_category(category)
    for ind in indxs:
        i = library.get_index(ind)
        print()
        assert isinstance(i, Index)
        schema = i.nodes
        assert isinstance(schema, JaggedArrayNode)
        for title in schema.all_node_titles(lang):
            if old in title:
                new_title = title.replace(old, new)
                print(new_title)
                schema.add_title(new_title, lang)
                i.save()


def rename_category(old, new):
    """
    Walk through all index records, replacing every category instance
    called 'old' with 'new'.
    """
    indices = IndexSet({"categories": old})

    assert len(indices), "No categories named {}".format(old)

    for i in indices:
        i.categories = [new if cat == old else cat for cat in i.categories]
        i.save()

    # Not multiserver aware
    library.rebuild_toc()


def resize_text(title, new_structure, upsize_in_place=False):
    # todo: Needs to be converted to objects, but no usages seen in the wild.
    """
    Change text structure for text named 'title'
    to 'new_structure' (a list of strings naming section names)

    Changes index record as well as restructuring any text that is currently saved.

    When increasing size, any existing text will become the first segment of the new level
    ["One", "Two", "Three"] -> [["One"], ["Two"], ["Three"]]

    If upsize_in_place==True, existing text will stay in tact, but be wrapped in new depth:
    ["One", "Two", "Three"] -> [["One", "Two", "Three"]]

    When decreasing size, information is lost as any existing segments are concatenated with " "
    [["One1", "One2"], ["Two1", "Two2"], ["Three1", "Three2"]] - >["One1 One2", "Two1 Two2", "Three1 Three2"]

    """
    index = db.index.find_one({"title": title})
    if not index:
        return False

    old_structure = index["sectionNames"]
    index["sectionNames"] = new_structure
    db.index.replace_one({"_id": index["_id"]}, index)

    delta = len(new_structure) - len(old_structure)
    if delta == 0:
        return True

    texts = db.texts.find({"title": title})
    for text in texts:
        if delta > 0 and upsize_in_place:
            resized = text["chapter"]
            for i in range(delta):
                resized = [resized]
        else:
            resized = JaggedTextArray(text["chapter"]).resize(delta).array()

        text["chapter"] = resized
        db.texts.replace_one({"_id": text["_id"]}, text)

    # TODO Rewrite any existing Links
    # TODO Rewrite any exisitng History items

    library.refresh_index_record_in_cache(index)

    return True


def merge_indices(title1, title2):
    """
    Merges two similar index records
    """
    #merge the index,
    #merge history refsscript to compare mishnah vers
    #TODO: needs more error checking that the indices and versions are of the same shape. Look nto comparing two (new format) index records
    idx1 = Index().load({"title":title1})
    if not idx1:
        return {"error": "Index not found: %s" % title1 }
    idx2 = Index().load({"title":title2})
    if not idx2:
        return {"error": "Index not found: %s" % title2 }
    #we're just going to trash idx2, but make sure all it's related objects move to idx1
    text.process_index_title_change_in_versions(idx1, old=title2, new=title1)
    link.process_index_title_change_in_links(idx1, old=title2, new=title1)
    history.process_index_title_change_in_history(idx1, old=title2, new=title1)
    idx2.delete()


def merge_text_versions(version1, version2, text_title, language, warn=False):
    """
    Merges the contents of two distinct text versions.
    version2 is merged into version1 then deleted.
    Preference is giving to version1 - if both versions contain content for a given segment,
    only the content of version1 will be retained.


    History entries are rewritten for version2.
    NOTE: the history of that results will be incorrect for any case where the content of
    version2 is overwritten - the history of those overwritten edits will remain.
    To end with a perfectly accurate history, history items for segments which have been overwritten
    would need to be identified and deleted.
    """
    v1 = Version().load({"title": text_title, "versionTitle": version1, "language": language})
    if not v1:
        return {"error": "Version not found: %s" % version1 }
    v2 = Version().load({"title": text_title, "versionTitle": version2, "language": language})
    if not v2:
        return {"error": "Version not found: %s" % version2 }

    if isinstance(v1.chapter, dict) or isinstance(v2.chapter, dict):
        #raise Exception("merge_text_versions doesn't yet handle complex records")
        i1 = v1.get_index()
        i2 = v2.get_index()
        assert i1 == i2

        def content_node_merger(snode, *contents, **kwargs):
            """
            :param snode: SchemaContentNode
            :param contents: Length two array of content.  Second is merged into first and returned.
            :param kwargs: "sources": array of source names
            :return:
            """
            assert len(contents) == 2
            if warn and JaggedTextArray(contents[0]).overlaps(JaggedTextArray(contents[1])):
                raise Exception("WARNING - overlapping content in {}".format(snode.full_title()))
            merged_text, sources = merge_texts([contents[0], contents[1]], kwargs.get("sources"))
            return merged_text

        merged_text = i1.nodes.visit_content(content_node_merger, v1.chapter, v2.chapter, sources=[version1, version2])

    else:  #this could be handled with the visitor and callback, above.
        if warn and v1.ja().overlaps(v2.ja()):
            print("WARNING - %s & %s have overlapping content. Aborting." % (version1, version2))

        merged_text, sources = merge_texts([v1.chapter, v2.chapter], [version1, version2])

    v1.chapter = merged_text
    v1.save()
    history.process_version_title_change_in_history(v1, old=version2, new=version1)

    v2.delete()

    return {"status": "ok"}


def merge_multiple_text_versions(versions, text_title, language, warn=False):
    """
    Merges contents of multiple text versions listed in 'versions'
    Versions listed first in 'versions' will receive priority if there is overlap.
    """
    count = 0

    v1 = versions.pop(0)
    for v2 in versions:
        r = merge_text_versions(v1, v2, text_title, language)
        if r["status"] == "ok":
            count += 1
    return {"status": "ok", "merged": count + 1}


def merge_text_versions_by_source(text_title, language, warn=False):
    """
    Merges all texts of text_title in langauge that share the same value for versionSource.
    """
    v = VersionSet({"title": text_title, "language": language})

    for s in v.distinct("versionSource"):
        versions = VersionSet({"title": text_title, "versionSource": s, "language": language}).distinct("versionTitle")
        merge_multiple_text_versions(versions, text_title, language)


def merge_text_versions_by_language(text_title, language, warn=False):
    """
    Merges all texts of text_title in langauge.
    """
    versions = VersionSet({"title": text_title, "language": language}).distinct("versionTitle")
    merge_multiple_text_versions(versions, text_title, language)


# No usages found
def merge_text(a, b):
    """
    Merge two lists representing texts, giving preference to a, but keeping
    values froms b when a position in a is empty or non existant.

    e.g merge_text(["", "Two", "Three"], ["One", "Nope", "Nope", "Four]) ->
        ["One", "Two" "Three", "Four"]
    """
    length = max(len(a), len(b))
    out = [a[n] if n < len(a) and (a[n] or not n < len(b)) else b[n] for n in range(length)]
    return out


def modify_text_by_function(title, vtitle, lang, rewrite_function, uid, needs_rewrite_function=lambda x: True, **kwargs):
    """
    Walks ever segment contained in title, calls rewrite_function on the text and saves the result.
    rewrite_function should accept two parameters: 1) text of current segment 2) zero-indexed indices of segment
    """
    from sefaria.tracker import modify_text

    leaf_nodes = library.get_index(title).nodes.get_leaf_nodes()
    for leaf in leaf_nodes:
        oref = leaf.ref()
        ja = oref.text(lang, vtitle).ja()
        assert isinstance(ja, JaggedTextArray)
        modified_text = ja.modify_by_function(rewrite_function)
        if needs_rewrite_function(ja.array()):
            modify_text(uid, oref, vtitle, lang, modified_text, **kwargs)


def modify_many_texts_and_make_report(rewrite_function, versions_query=None, return_zeros=False):
    """
    Uses pymongo because iterating and saving all texts as Version is heavy.
    That means - be CAREFUL with that.

    :param rewrite_function(string) -> (string, int of times that string has been replaced)
    :param versions_query - query dict for VersionSet, or None for all
    :param return_zeros - bool whether you want cases of 0 replaces in report
    :returns a csv writer with index title, versionTitle, and number of replacements
    """
    def replace_in_text_object(text_obj):
        total = 0
        if isinstance(text_obj, dict):
            for key in text_obj:
                text_obj[key], num = replace_in_text_object(text_obj[key])
                total += num
        elif isinstance(text_obj, list):
            for i, _ in enumerate(text_obj):
                text_obj[i], num = replace_in_text_object(text_obj[i])
                total += num
        elif isinstance(text_obj, str):
            return rewrite_function(text_obj)
        return text_obj, total
    texts_collection = db.texts
    versions_to_change = texts_collection.find(versions_query)
    bulk_operations = []
    output = io.BytesIO()
    report = csv.writer(output)
    report.writerow(['index', 'versionTitle', 'replaces number'])
    for version in versions_to_change:
        new_text, replaces = replace_in_text_object(version['chapter'])
        if replaces or return_zeros:
            report.writerow([version['title'], version['versionTitle'], replaces])
        if replaces:
            bulk_operations.append(pymongo.UpdateOne(
                {'_id': version['_id']},
                {'$set': {'chapter': new_text}}
            ))
    if bulk_operations:
        texts_collection.bulk_write(bulk_operations)
    return output.getvalue()


def split_text_section(oref, lang, old_version_title, new_version_title):
    """
    Splits the text in `old_version_title` so that the content covered by `oref` now appears in `new_version_title`.
    Rewrites history for affected content. 

    NOTE: `oref` cannot be ranging (until we implement saving ranging refs on TextChunk). Spanning refs are handled recursively.
    """
    if oref.is_spanning():
        for span in oref.split_spanning_ref():
            split_text_section(span, lang, old_version_title, new_version_title)
        return

    old_chunk = TextChunk(oref, lang=lang, vtitle=old_version_title)
    new_chunk = TextChunk(oref, lang=lang, vtitle=new_version_title)

    # Copy content to new version
    new_chunk.versionSource = old_chunk.version().versionSource
    new_chunk.text = old_chunk.text
    new_chunk.save()

    # Rewrite History
    ref_regex_queries = [{"ref": {"$regex": r}, "version": old_version_title, "language": lang} for r in oref.regex(as_list=True)]
    query = {"$or": ref_regex_queries}
    db.history.update(query, {"$set": {"version": new_version_title}}, upsert=False, multi=True)

    # Remove content from old version
    old_chunk.text = JaggedTextArray(old_chunk.text).constant_mask(constant="").array()
    old_chunk.save()


def find_and_replace_in_text(title, vtitle, lang, find_string, replace_string, uid):
    """
    Replaces all instances of `find_string` with `replace_string` in the text specified by `title` / `vtitle` / `lang`.
    Changes are attributed to the user with `uid`. 
    """
    def replacer(text, sections):
        return text.replace(find_string, replace_string)

    modify_text_by_function(title, vtitle, lang, replacer, uid)


def replace_roman_numerals(text, allow_lowercase=False, only_lowercase=False):
    """
    Replaces any roman numerals in 'text' with digits.
    Currently only looks for a roman numeral followed by a comma or period, then a space, then a digit.
    e.g. (Isa. Iv. 10) --> (Isa. 4:10)

    WARNING: we've seen e.g., "(v. 15)" used to mean "Verse 15". If run with allow_lowercase=True, this will
    be rewritten as "(5:15)". 
    """
    import roman
    if only_lowercase:
        regex = re.compile(r"((^|[{\[( ])[{\[( ]*)(m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3}))(\. ?)(\d)?")
    else:
        flag = re.I if allow_lowercase else 0
        regex = re.compile(r"((^|[{\[( ])[{\[( ]*)(M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))($|[.,;\])}: ]+)(\d)?", flag)

    def replace_roman_numerals_in_match(m):
        s = m.group(3)
        s = s.upper()
        try:
            if s:
                if m.group(8):    
                    return "{}{}:{}".format(m.group(1), roman.fromRoman(s), m.group(8))
                else:
                    return "{}{}{}".format(m.group(1), roman.fromRoman(s), m.group(7))
            else:
                return m.group(0)
        except:
            return m.group(0)

    return re.sub(regex, replace_roman_numerals_in_match, text)


def replace_roman_numerals_including_lowercase(text):
    """
    Returns `text` with Roman numerals replaced by Arabic numerals, including Roman numerals in lowercase.
    """
    return replace_roman_numerals(text, allow_lowercase=True)


def make_versions_csv():
    """
    Returns a CSV of all text versions in the DB.
    """
    output = io.BytesIO()
    writer = csv.writer(output)
    fields = [
        "title",
        "versionTitle",
        "language",
        "versionSource",
        "status",
        "priority",
        "license",
        "versionNotes",
        "digitizedBySefaria",
        "method",
    ]
    writer.writerow(fields)
    vs = VersionSet()
    for v in vs:
        writer.writerow([str(getattr(v, f, "")).encode("utf-8") for f in fields])

    return output.getvalue()


def get_core_link_stats():
    from sefaria.model.link import get_category_category_linkset
    output = io.BytesIO()
    writer = csv.writer(output)
    titles = [
        "Category 1",
        "Category 2",
        "Count"
    ]
    writer.writerow(titles)
    sets = [
        ("Tanakh", "Tanakh"),
        ("Tanakh", "Bavli"),
        ("Bavli", "Tosefta"),
        ("Tosefta", "Mishnah"),
        ("Bavli", "Yerushalmi"),
        ("Bavli", "Bavli"),
        ("Bavli", "Mishneh Torah"),
        ("Bavli", "Shulchan Arukh"),
        ("Bavli", "Midrash"),
        ("Bavli", "Mishnah")
    ]
    for set in sets:
        writer.writerow([set[0], set[1], get_category_category_linkset(set[0], set[1]).count()])

    return output.getvalue()


def get_library_stats():
    def aggregate_stats(toc_node, path):
        simple_nodes = []
        for x in toc_node:
            node_name = x.get("category", None) or x.get("title", None)
            node_path = path + [node_name]
            simple_node = {
                "name": node_name,
                "path": " ".join(node_path)
            }
            if "category" in x:
                simple_node["type"] = "category"
                simple_node["children"] = aggregate_stats(x["contents"], node_path)
                simple_node["en_version_count"] = reduce(lambda x, v: x + v["en_version_count"], simple_node["children"], 0)
                simple_node["he_version_count"] = reduce(lambda x, v: x + v["he_version_count"], simple_node["children"], 0)
                simple_node["en_index_count"] = reduce(lambda x, v: x + v["en_index_count"], simple_node["children"], 0)
                simple_node["he_index_count"] = reduce(lambda x, v: x + v["he_index_count"], simple_node["children"], 0)
                simple_node["en_word_count"] = reduce(lambda x, v: x + v["en_word_count"], simple_node["children"], 0)
                simple_node["he_word_count"] = reduce(lambda x, v: x + v["he_word_count"], simple_node["children"], 0)
                simple_node["all_index_count"] = reduce(lambda x, v: x + v["all_index_count"], simple_node["children"], 0)
                simple_node["all_word_count"] = simple_node["en_word_count"] + simple_node["he_word_count"]
                simple_node["all_version_count"] = simple_node["en_version_count"] + simple_node["he_version_count"]

            elif "title" in x:
                query = {"title": x["title"]}
                simple_node["type"] = "index"
                simple_node["children"] = [{
                       "name": "{} ({})".format(v.versionTitle, v.language),
                       "path": " ".join(node_path + ["{} ({})".format(v.versionTitle, v.language)]),
                       "size": v.word_count(),
                       "type": "version",
                       "language": v.language,
                       "en_version_count": 1 if v.language == "en" else 0,
                       "he_version_count": 1 if v.language == "he" else 0,
                       "en_word_count": v.word_count() if v.language == "en" else 0,
                       "he_word_count": v.word_count() if v.language == "he" else 0,
                   } for v in VersionSet(query)]
                simple_node["en_version_count"] = reduce(lambda x, v: x + v["en_version_count"], simple_node["children"], 0)
                simple_node["he_version_count"] = reduce(lambda x, v: x + v["he_version_count"], simple_node["children"], 0)
                simple_node["en_index_count"] = 1 if any(v["language"] == "en" for v in simple_node["children"]) else 0
                simple_node["he_index_count"] = 1 if any(v["language"] == "he" for v in simple_node["children"]) else 0
                simple_node["en_word_count"] = reduce(lambda x, v: x + v["en_word_count"], simple_node["children"], 0)
                simple_node["he_word_count"] = reduce(lambda x, v: x + v["he_word_count"], simple_node["children"], 0)
                simple_node["all_word_count"] = simple_node["en_word_count"] + simple_node["he_word_count"]
                simple_node["all_index_count"] = 1
                simple_node["all_version_count"] = simple_node["en_version_count"] + simple_node["he_version_count"]

            simple_nodes.append(simple_node)
        return simple_nodes
    tree = aggregate_stats(library.get_toc(), [])

    from operator import sub
    output = io.BytesIO()
    writer = csv.writer(output)
    titles = [
        "Category",
        "#Titles (all)",
        "#Titles (he)",
        "#Titles (en)",
        "#Versions (all)",
        "#Versions (he)",
        "#Versions (en)",
        "#Words (all)",
        "#Words (he)",
        "#Words (en)"
    ]
    writer.writerow(titles)
    fields = [
        "path",
        "all_index_count",
        "he_index_count",
        "en_index_count",
        "all_version_count",
        "he_version_count",
        "en_version_count",
        "all_word_count",
        "he_word_count",
        "en_word_count",
    ]

    with_commentary = ["Tanakh", "Mishnah", "Talmud", "Halakhah"]
    for n in tree:
        row = [n.get(field) for field in fields]
        if n["name"] in with_commentary:
            if n["name"] == "Tanakh":
                cn = next(filter(lambda x: x["name"] == "Commentary", n["children"]))
                c_row = [cn.get(field) for field in fields]
                tn = next(filter(lambda x: x["name"] == "Targum", n["children"]))
                t_row = [tn.get(field) for field in fields]
                row[1:] = list(map(sub, list(map(sub, row[1:], c_row[1:])), t_row[1:]))
                writer.writerow(row)
                writer.writerow(c_row)
                writer.writerow(t_row)
            else:
                cn = next(filter(lambda x: x["name"] == "Commentary", n["children"]))
                c_row = [cn.get(field) for field in fields]
                row[1:] = list(map(sub, row[1:], c_row[1:]))
                writer.writerow(row)
                writer.writerow(c_row)
        else:
            writer.writerow(row)

    return output.getvalue()


def dual_text_diff(seg1, seg2, edit_cb=None, css_classes=False):
    """
    Make a diff of seg1 on seg2 and return two html strings displaying the differences between each one. Takes an
    optional callback that can edit the texts before the diff is made
    :param seg1:
    :param seg2:
    :param edit_cb: callback
    :param bool css_classes: Set to True to style diffs with css classes. Classes will be "ins" and "del". If False
     will set an inline style tag.
    :return: (str, str)
    """
    def side_by_side_diff(diffs, change_from=True):
        """
        Used to render an html display of a diff from the diff_match_patch library
        :param diffs: list of tuples as produced by diff_match_patch.diff_main()
        :param change_from: diff_match_patch.diff_main() gives a diff that shows how to change from stringA to stringB. This
          flag should be true if you wish to see only the additions that need to be made to textA (first string fed to
          diff_main). If the inserts to the second are to be diplayed, set to False.
        :return: html string
        """
        diff_delete, diff_insert, diff_equal = -1, 1, 0
        html = []
        if css_classes:
            ins, dell = 'class="ins"', 'class="del"'
        else:
            ins, dell = 'style="background:#e6ffe6;"', 'style="background:#ffe6e6;"'

        for (op, data) in diffs:
            my_text = (data.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace("\n", "&para;<br>"))
            if op == diff_insert:
                if change_from:
                    continue
                else:
                    html.append("<span {}>{}</span>".format(ins, my_text))
            elif op == diff_delete:
                if change_from:
                    html.append("<span {}>{}</span>".format(dell, my_text))
                else:
                    continue
            elif op == diff_equal:
                html.append("<span>%s</span>" % my_text)
        return "".join(html)

    if edit_cb is not None:
        seg1, seg2 = edit_cb(seg1), edit_cb(seg2)
    diff = diff_match_patch().diff_main(seg1, seg2)
    return side_by_side_diff(diff), side_by_side_diff(diff, False)


def word_frequency_for_text(title, lang="en"):
    """
    Returns an ordered list of word/count tuples for occurences of words inside the 
    text `title`.
    """
    import string
    from collections import defaultdict
    from sefaria.export import make_text, prepare_merged_text_for_export
    from sefaria.utils.util import strip_tags 
    text = make_text(prepare_merged_text_for_export(title, lang=lang))

    text = strip_tags(text)
    text = text.lower()
    text = re.sub(r'[^a-z ]', " ", text)
    text = re.sub(r' +', " ", text)
    text = text.translate(str.maketrans(dict.fromkeys(string.punctuation)))

    count = defaultdict(int)
    words = text.split(" ")
    for word in words:
        count[word] += 1

    counts = sorted(iter(count.items()), key=lambda x: -x[1])

    return counts


class WorkflowyParser(object):

    title_lang_delim = r"/"
    alt_title_delim = r"|"
    comment_delim = r'#'
    categories_delim = "%"

    def __init__(self, schema_file, uid, term_scheme=None, c_index=False, c_version=False, delims=None):
        self._schema_outline_file = schema_file
        self._uid = uid
        self._term_scheme = term_scheme
        self._c_index = c_index
        self._c_version = c_version
        tree = ET.parse(self._schema_outline_file)
        self.outline = tree.getroot().find("./body/outline/outline")
        self.comment_strip_re = re.compile(r"</b>|<b>|" + self.comment_delim + ".*" + self.comment_delim,
                                           re.UNICODE)
        self.parsed_schema = None
        self.version_info = None
        self.categories = None
        if delims:
            delims = delims.split()
            self.title_lang_delim = delims[0] if len(delims) >= 1 else self.title_lang_delim
            self.alt_title_delim = delims[1] if len(delims) >= 2 else self.alt_title_delim
            self.categories_delim = delims[2] if len(delims) >= 3 else self.categories_delim

    def parse(self):
        # tree = tree.getroot()[1][0]
        # for element in tree.iter('outline'):
        #     print parse_titles(element)["enPrim"]
        self.categories = self.extract_categories_from_title()
        self.version_info = {'info': self.extract_version_info(), 'text': []}
        self.parsed_schema = self.build_index_schema(self.outline)
        self.parsed_schema.validate()
        idx = self.create_index_from_schema()
        if self._c_index:
            idx_obj = Index(idx).save()
            res = "Index record [{}] created.".format(self.parsed_schema.primary_title())
            if self._c_version:
                self.save_version_from_outline_notes()
                res += " Version record created."
            else:
                self.save_version_default(idx_obj)
                res += " No text, Default empty Version record created."
        else:
            res = "Returning index outline without saving."
        return {"message": res, "index": idx}

    # object tree of each with jagged array nodes at the lowest level (recursive)
    def build_index_schema(self, element):
        if self._term_scheme and isinstance(self._term_scheme, str):
            self.create_term_scheme()
        # either type of node:
        ja_sections = self.parse_implied_depth(element)
        titles = self.parse_titles(element)  # an array of titles
        if len(element) == 0:  # length of child nodes
            n = JaggedArrayNode()
            n.depth = len(ja_sections['section_names']) if ja_sections else 1
            n.sectionNames = ja_sections['section_names'] if ja_sections else ['Paragraph']
            n.addressTypes = ja_sections['address_types'] if ja_sections else ['Integer']
            if titles:
                n.key = titles["enPrim"]
                n = self.add_titles_to_node(n, titles)
            else:
                n.key = 'default'
                n.default = True
        else:  # yes child nodes >> schema node
            n = SchemaNode()
            n.key = titles["enPrim"]
            n = self.add_titles_to_node(n, titles)
            for child in element:
                n.append(self.build_index_schema(child))

        if self._term_scheme and element != self.outline:  # add the node to a term scheme
            self.create_shared_term_for_scheme(n.title_group)

        if self._c_version and element != self.outline:  # get the text in the notes and store it with the proper Ref
            text = self.parse_text(element)
            if text:
                self.version_info['text'].append({'node': n, 'text': text})
        return n

    # en & he titles for each element > dict
    def parse_titles(self, element):
        title = element.get("text")
        if '**default**' in title:
            return None
        # print title
        # title = re.sub(ur"</b>|<b>|#.*#|'", u"", title)
        title = self.comment_strip_re.sub("", title)
        spl_title = title.split(self.title_lang_delim)
        titles = {}
        if len(spl_title) == 2:
            he_pos = 1 if has_hebrew(spl_title[1]) else 0
            he = spl_title[he_pos].split(self.alt_title_delim)
            titles["hePrim"] = he[0].strip()
            titles["heAltList"] = [t.strip() for t in he[1:]]
            del spl_title[he_pos]
        en = spl_title[0].split(self.alt_title_delim)
        titles["enPrim"] = en[0].strip()
        titles["enAltList"] = [t.strip() for t in en[1:]]
        # print node.attrib
        return titles

    # appends primary, alternate, hebrew, english titles to node.
    def add_titles_to_node(self, n, titles):
        term = Term()
        # check if the primary title is a "shared term"
        if term.load({"name": titles["enPrim"]}):
            n.add_shared_term(titles["enPrim"])

        else:  # manual add if not a shared term
            n.add_title(titles["enPrim"], 'en', primary=True)
            # print titles["enPrim"]
            if "hePrim" in titles:
                n.add_title(titles["hePrim"], 'he', primary=True)
                # print titles["hePrim"]
            if "enAltList" in titles:
                for title in titles["enAltList"]:
                    n.add_title(title, 'en')
            if "heAltList" in titles:
                for title in titles["heAltList"]:
                    n.add_title(title, 'he')
        return n

    def extract_categories_from_title(self):
        category_pattern = self.categories_delim + r"(.*)" + self.categories_delim
        title = self.outline.get("text")
        category_str = re.search(category_pattern, title)
        if category_str:
            categories = [s.strip() for s in category_str.group(1).split(",")]
            self.outline.set('text', re.sub(category_pattern, "", title))
            return categories
        raise InputError("Categories must be supplied on the Workflowy outline according to specifications")

    def parse_implied_depth(self, element):
        ja_depth_pattern = r"\[(\d)\]$"
        ja_sections_pattern = r"\[(.*)\]$"
        title_str = element.get('text').strip()

        depth_match = re.search(ja_depth_pattern, title_str)
        if depth_match:
            depth = int(depth_match.group(1))
            placeholder_sections = ['Volume', 'Chapter', 'Section', 'Paragraph']
            element.set('text', re.sub(ja_depth_pattern, "", title_str))
            return {'section_names': placeholder_sections[(-1 * depth):], 'address_types': ['Integer'] * depth}

        sections_match = re.search(ja_sections_pattern, title_str)
        if sections_match:
            sections = [s.strip() for s in sections_match.group(1).split(",")]
            element.set('text', re.sub(ja_sections_pattern, "", title_str))
            section_names = []
            address_types = []
            for s in sections:
                tpl = s.split(":")
                section_names.append(tpl[0])
                address_types.append(tpl[1] if len(tpl) > 1 else 'Integer')

            return {'section_names': section_names, 'address_types': address_types}
        else:
            return None

    def extract_version_info(self):
        vinfo_str = self.outline.get("_note")
        if vinfo_str:
            vinfo_dict = {elem.split(":", 1)[0].strip(): elem.split(":", 1)[1].strip() for elem in
                          vinfo_str.split(",")}
        else:
            vinfo_dict = {'language': 'he',
                          'versionSource': 'not available',
                          'versionTitle': 'pending'
                          }
        return vinfo_dict

    def create_index_from_schema(self):
        return {
            "title": self.parsed_schema.primary_title(),
            "categories": self.categories,
            "schema": self.parsed_schema.serialize()
        }

    def create_term_scheme(self):
        if not TermScheme().load({"name": self._term_scheme}):
            print("Creating Term Scheme object")
            ts = TermScheme()
            ts.name = self._term_scheme
            ts.save()
            self._term_scheme = ts

    def create_shared_term_for_scheme(self, title_group):
        # TODO: This might be a silly method, since for most cases we do not want to blindly create terms from ALL thre nodes of a schema
        if not Term().load({"name": title_group.primary_title()}):
            print("Creating Shared Term for Scheme from outline")
            term = Term()
            term.name = title_group.primary_title()
            term.scheme = self._term_scheme.name
            term.title_group = title_group
            term.save()

    # divides text into paragraphs and sentences > list
    def parse_text(self, element):
        if "_note" in element.attrib:
            n = (element.attrib["_note"])
            n = re.sub(r'[/]', '<br>', n)
            n = re.sub(r'[(]', '<em><small>', n)
            n = re.sub(r'[)]', '</small></em>', n)
            text = n.strip().splitlines()
            return text
        return None

    # builds and posts text to api
    def save_version_from_outline_notes(self):
        from sefaria.tracker import modify_text
        for text_ref in self.version_info['text']:
            node = text_ref['node']
            ref = Ref(node.full_title(force_update=True))
            text = text_ref['text']
            user = self._uid
            vtitle = self.version_info['info']['versionTitle']
            lang = self.version_info['info']['language']
            vsource = self.version_info['info']['versionSource']
            modify_text(user, ref, vtitle, lang, text, vsource)

    def save_version_default(self, idx):
        Version(
            {
                "chapter": idx.nodes.create_skeleton(),
                "versionTitle": self.version_info['info']['versionTitle'],
                "versionSource": self.version_info['info']['versionSource'],
                "language": self.version_info['info']['language'],
                "title": idx.title
            }
        ).save()


```

### sefaria/helper/topic.py

```
import re
from tqdm import tqdm
from pymongo import UpdateOne, InsertOne
from typing import Optional, Union
from collections import defaultdict
from functools import cmp_to_key, partial
from sefaria.model import *
from sefaria.model.place import process_topic_place_change
from sefaria.system.exceptions import InputError
from sefaria.model.topic import TopicLinkHelper
from sefaria.system.database import db
from sefaria.system.cache import django_cache
from django_topics.models import Topic as DjangoTopic
import structlog
from sefaria import tracker
from sefaria.helper.descriptions import create_era_link
logger = structlog.get_logger(__name__)

def get_topic(v2, topic, lang, with_html=True, with_links=True, annotate_links=True, with_refs=True, group_related=True, annotate_time_period=False, ref_link_type_filters=None, with_indexes=True):
    """
    Helper function for api/topics/<slug>
    TODO fill in rest of parameters
    @param v2:
    @param topic: slug of topic to get data for
    @param lang: the language of the user to sort the ref links by
    @param with_html: True if description should be returned with HTML. If false, HTML is stripped.
    @param with_links: Should intra-topic links be returned. If true, return dict has a `links` key
    @param annotate_links:
    @param with_refs:
    @param group_related:
    @param annotate_time_period:
    @param ref_link_type_filters:
    @param with_indexes:
    @return:
    """
    topic_obj = Topic.init(topic)
    if topic_obj is None:
        return {}
    response = topic_obj.contents(annotate_time_period=annotate_time_period, with_html=with_html)
    response['primaryTitle'] = {
        'en': topic_obj.get_primary_title('en'),
        'he': topic_obj.get_primary_title('he')
    }
    response['primaryTitleIsTransliteration'] = {
        'en': topic_obj.title_is_transliteration(response['primaryTitle']['en'], 'en'),
        'he': topic_obj.title_is_transliteration(response['primaryTitle']['he'], 'he')
    }
    if not response.get("description_published", False) and "description" in response:
        del response["description"]
    if with_links and with_refs:
        # can load faster by querying `topic_links` query just once
        all_links = topic_obj.link_set(_class=None)
        intra_links = [l.contents() for l in all_links if isinstance(l, IntraTopicLink)]
        ref_links = [l.contents() for l in all_links if isinstance(l, RefTopicLink) and (len(ref_link_type_filters) == 0 or l.linkType in ref_link_type_filters)]
    else:
        if with_links:
            intra_links = [l.contents() for l in topic_obj.link_set(_class='intraTopic')]
        if with_refs:
            query_kwargs = {"linkType": {"$in": list(ref_link_type_filters)}} if len(ref_link_type_filters) > 0 else None
            ref_links = [l.contents() for l in topic_obj.link_set(_class='refTopic', query_kwargs=query_kwargs)]
    if with_links:
        response['links'] = group_links_by_type('intraTopic', intra_links, annotate_links, group_related)
    if with_refs:
        ref_links = sort_and_group_similar_refs(ref_links, lang)
        if v2:
            ref_links = group_links_by_type('refTopic', ref_links, False, False)
        response['refs'] = ref_links
    if with_indexes and isinstance(topic_obj, AuthorTopic):
        response['indexes'] = topic_obj.get_aggregated_urls_for_authors_indexes()

    if getattr(topic_obj, 'isAmbiguous', False):
        possibility_links = topic_obj.link_set(_class="intraTopic", query_kwargs={"linkType": TopicLinkType.possibility_type})
        possibilities = []
        for link in possibility_links:
            possible_topic = Topic.init(link.topic)
            if possible_topic is None:
                continue
            possibilities += [possible_topic.contents(annotate_time_period=annotate_time_period)]
        response['possibilities'] = possibilities
    return response


def group_links_by_type(link_class, links, annotate_links, group_related):
    link_dups_by_type = defaultdict(set)  # duplicates can crop up when group_related is true
    grouped_links = {}
    agg_field = 'links' if link_class == 'intraTopic' else 'refs'

    if link_class == 'intraTopic' and len(links) > 0 and annotate_links:
        link_topic_dict = {other_topic.slug: other_topic for other_topic in TopicSet({"$or": [{"slug": link['topic']} for link in links]})}
    else:
        link_topic_dict = {}
    for link in links:
        is_inverse = link.get('isInverse', False)
        link_type = library.get_topic_link_type(link['linkType'])
        if group_related and link_type.get('groupRelated', is_inverse, False):
            link_type = library.get_topic_link_type(TopicLinkType.related_type)
        link_type_slug = link_type.get('slug', is_inverse)

        if link_class == 'intraTopic':
            if link['topic'] in link_dups_by_type[link_type_slug]:
                continue
            link_dups_by_type[link_type_slug].add(link['topic'])
            if annotate_links:
                link = annotate_topic_link(link, link_topic_dict)
                if link is None:
                    continue

        del link['linkType']
        del link['class']

        if link_type_slug in grouped_links:
            grouped_links[link_type_slug][agg_field] += [link]
        else:
            grouped_links[link_type_slug] = {
                agg_field: [link],
                'title': link_type.get('displayName', is_inverse),
                'shouldDisplay': link_type.get('shouldDisplay', is_inverse, False)
            }
            if link_type.get('pluralDisplayName', is_inverse, False):
                grouped_links[link_type_slug]['pluralTitle'] = link_type.get('pluralDisplayName', is_inverse)
    return grouped_links

def merge_props_for_similar_refs(curr_link, new_link):
    # when grouping similar refs, make sure the source to display has the maximum curatedPrimacy of all the similar refs,
    # as well as datasource and descriptions of all the similar refs
    data_source = new_link.get('dataSource', None)
    if data_source:
        curr_link = update_refs(curr_link, new_link)
        curr_link = update_data_source_in_link(curr_link, new_link, data_source)

    if not curr_link['is_sheet']:
        curr_link = update_descriptions_in_link(curr_link, new_link)
        curr_link = update_curated_primacy(curr_link, new_link)
    return curr_link

def update_data_source_in_link(curr_link, new_link, data_source):
    data_source_obj = library.get_topic_data_source(data_source)
    curr_link['dataSources'][data_source] = data_source_obj.displayName
    del new_link['dataSource']
    return curr_link

def update_refs(curr_link, new_link):
    # use whichever ref covers a smaller range
    if len(curr_link['expandedRefs']) > len(new_link['expandedRefs']):
        curr_link['ref'] = new_link['ref']
        curr_link['expandedRefs'] = new_link['expandedRefs']
    return curr_link

def update_descriptions_in_link(curr_link, new_link):
    # merge new link descriptions into current link
    new_description = new_link.get('descriptions', {})
    if new_description:
        curr_link_description = curr_link.get('descriptions', {})
        for lang in ['en', 'he']:
            if lang not in curr_link_description and lang in new_description:
                curr_link_description[lang] = new_description[lang]
        curr_link['descriptions'] = curr_link_description
    return curr_link

def update_curated_primacy(curr_link, new_link):
    # make sure curr_link has the maximum curated primacy value of itself and new_link
    new_curated_primacy = new_link.get('order', {}).get('curatedPrimacy', {})
    if new_curated_primacy:
        if 'order' not in curr_link:
            curr_link['order'] = new_link['order']
            return
        curr_curated_primacy = curr_link['order'].get('curatedPrimacy', {})
        for lang in ['en', 'he']:
            # front-end sorting considers 0 to be default for curatedPrimacy
            curr_curated_primacy[lang] = max(curr_curated_primacy.get(lang, 0), new_curated_primacy.get(lang, 0))
        curr_link['order']['curatedPrimacy'] = curr_curated_primacy
    return curr_link

def is_learning_team(dataSource):
    return dataSource == 'learning-team' or dataSource == 'learning-team-editing-tool'

def iterate_and_merge(new_ref_links, new_link, subset_ref_map, temp_subset_refs):
    # temp_subset_refs contains the refs within link's expandedRefs that overlap with other refs
    # subset_ref_map + new_ref_links contain mappings to get from the temp_subset_refs to the actual link objects
    for seg_ref in temp_subset_refs:
        for index in subset_ref_map[seg_ref]:
            new_ref_links[index]['similarRefs'] += [new_link]
            curr_link_learning_team = any([is_learning_team(dataSource) for dataSource in new_ref_links[index]['dataSources']])
            if not curr_link_learning_team:  # if learning team, ignore source with overlapping refs
                new_ref_links[index] = merge_props_for_similar_refs(new_ref_links[index], new_link)
    return new_ref_links

def sort_and_group_similar_refs(ref_links, lang):
    ref_links.sort(key=cmp_to_key(partial(sort_refs_by_relevance, lang=lang)))
    subset_ref_map = defaultdict(list)
    new_ref_links = []
    for link in ref_links:
        del link['topic']
        temp_subset_refs = subset_ref_map.keys() & set(link.get('expandedRefs', []))
        new_data_source = link.get("dataSource", None)
        should_merge = len(temp_subset_refs) > 0 and not is_learning_team(new_data_source) # learning team links should be handled separately from one another and not merged
        if should_merge:
            new_ref_links = iterate_and_merge(new_ref_links, link, subset_ref_map, temp_subset_refs)
        else:
            link['similarRefs'] = []
            link['dataSources'] = {}
            if link.get('dataSource', None):
                data_source = library.get_topic_data_source(link['dataSource'])
                link['dataSources'][link['dataSource']] = data_source.displayName
                del link['dataSource']
            new_ref_links += [link]
            for seg_ref in link.get('expandedRefs', []):
                subset_ref_map[seg_ref] += [len(new_ref_links) - 1]
    return new_ref_links


def annotate_topic_link(link: dict, link_topic_dict: dict) -> Union[dict, None]:
    # add display information
    topic = link_topic_dict.get(link['topic'], None)
    if topic is None:
        logger.warning(f"Topic slug {link['topic']} doesn't exist")
        return None
    link["title"] = {
        "en": topic.get_primary_title('en'),
        "he": topic.get_primary_title('he')
    }
    link['titleIsTransliteration'] = {
        'en': topic.title_is_transliteration(link["title"]['en'], 'en'),
        'he': topic.title_is_transliteration(link["title"]['he'], 'he')
    }
    if getattr(topic, "description_published", False):
        link['description'] = getattr(topic, 'description', {})
    else:
        link['description'] = {}
    if not topic.should_display():
        link['shouldDisplay'] = False
    link['order'] = link.get('order', None) or {}
    link['order']['numSources'] = getattr(topic, 'numSources', 0)
    return link


@django_cache(timeout=24 * 60 * 60)
def get_all_topics(limit=1000, displayableOnly=True):
    query = {"shouldDisplay": {"$ne": False}, "numSources": {"$gt": 0}} if displayableOnly else {}
    return TopicSet(query, limit=limit, sort=[('numSources', -1)]).array()

@django_cache(timeout=24 * 60 * 60)
def get_num_library_topics():
    all_topics = DjangoTopic.objects.get_topic_slugs_by_pool('library')
    return len(all_topics)


def get_topic_by_parasha(parasha:str) -> Topic:
    """
    Returns topic corresponding to `parasha`
    :param parasha: as spelled in `parshiot` collection
    :return Topic:
    """
    return Topic().load({"parasha": parasha})


def sort_refs_by_relevance(a, b, lang="english"):
    """
    This function should mimic behavior of `refSort` in TopicPage.jsx.
    It is a comparison function that takes two items from the list and returns the corresponding integer to indicate which should go first. To be used with `cmp_to_key`.
    It considers the following criteria in order:
    - If one object has an `order` key and another doesn't, the one with the `order` key comes first
    - curatedPrimacy, higher comes first
    - pagerank, higher comes first
    - numDatasource (how many distinct links have this ref/topic pair) multiplied by tfidf (a bit complex, in short how "central" to this topic is the vocab used in this ref), higher comes first
    @param lang: language to sort by. Defaults to "english".
    @return:
    """
    aord = a.get('order', {})
    bord = b.get('order', {})
    def curated_primacy(order_dict, lang):
        return order_dict.get("curatedPrimacy", {}).get(lang, 0)

    if not aord and not bord:
        return 0
    if bool(aord) != bool(bord):
        return int(bool(bord)) - int(bool(aord))
    short_lang = lang[:2]
    aprimacy = curated_primacy(aord, short_lang)
    bprimacy = curated_primacy(bord, short_lang)
    if aprimacy > 0 or bprimacy > 0:
        return bprimacy - aprimacy
    if aord.get('pr', 0) != bord.get('pr', 0):
        return bord.get('pr', 0) - aord.get('pr', 0)
    return (bord.get('numDatasource', 0) * bord.get('tfidf', 0)) - (aord.get('numDatasource', 0) * aord.get('tfidf', 0))


def get_random_topic(pool=None) -> Optional[Topic]:
    """
    :param pool: name of the pool from which to select the topic. If `None`, all topics are considered.
    :return: Returns a random topic from the database. If you provide `pool`, then the selection is limited to topics in that pool.
    """
    from django_topics.models import Topic as DjangoTopic
    random_topic_slugs = DjangoTopic.objects.sample_topic_slugs('random', pool, limit=1)
    if len(random_topic_slugs) == 0:
        return None

    return Topic.init(random_topic_slugs[0])


def get_random_topic_source(topic:Topic) -> Optional[Ref]:
    random_source_dict = list(db.topic_links.aggregate([
        {"$match": {"toTopic": topic.slug, 'linkType': 'about', 'class': 'refTopic', 'is_sheet': False, 'order.pr': {'$gt': 0}}},
        {"$sample": {"size": 1}}
    ]))
    if len(random_source_dict) == 0:
        return None
    try:
        oref = Ref(random_source_dict[0]['ref'])
    except InputError:
        return None

    return oref


def get_bulk_topics(topic_list: list) -> TopicSet:
    return TopicSet({'$or': [{'slug': slug} for slug in topic_list]})


def recommend_topics(refs: list) -> list:
    """Returns a list of topics recommended for the list of string refs"""
    seg_refs = []
    for tref in refs:
        try:
           oref = Ref(tref)
        except InputError:
            continue
        seg_refs += [r.normal() for r in oref.all_segment_refs()]
    topic_count = defaultdict(int)
    ref_links = RefTopicLinkSet({"expandedRefs": {"$in": seg_refs}})
    for link in ref_links:
        topic_count[link.toTopic] += 1

    recommend_topics = []
    for slug in topic_count.keys():
        topic = Topic.init(slug)
        recommend_topics.append({
            "slug": slug,
            "titles": {
                "en": topic.get_primary_title(lang="en"),
                "he": topic.get_primary_title(lang="he")
            },
            "count": topic_count[slug]
        })

    return sorted(recommend_topics, key=lambda x: x["count"], reverse=True)

def ref_topic_link_prep(link):
    link['anchorRef'] = link['ref']
    link['anchorRefExpanded'] = link['expandedRefs']
    del link['ref']
    del link['expandedRefs']
    if link.get('dataSource', None):
        data_source_slug = link['dataSource']
        data_source = library.get_topic_data_source(data_source_slug)
        link['dataSource'] = data_source.displayName
        link['dataSource']['slug'] = data_source_slug
    return link

def get_topics_for_ref(tref, lang="english", annotate=False):
    serialized = [l.contents() for l in Ref(tref).topiclinkset()]
    if annotate:
        if len(serialized) > 0:
            link_topic_dict = {topic.slug: topic for topic in TopicSet({"$or": [{"slug": link['topic']} for link in serialized]})}
        else:
            link_topic_dict = {}
        serialized = list(filter(None, (annotate_topic_link(link, link_topic_dict) for link in serialized)))
    for link in serialized:
        ref_topic_link_prep(link)

    serialized.sort(key=cmp_to_key(partial(sort_refs_by_relevance, lang=lang)))
    return serialized

@django_cache(timeout=24 * 60 * 60)
def get_trending_topics(num_topics=10):
    from google.analytics.data_v1beta import BetaAnalyticsDataClient
    from google.analytics.data_v1beta.types import DateRange, Metric, Dimension, RunReportRequest, FilterExpression, Filter, OrderBy
    from sefaria.settings import GOOGLE_APPLICATION_CREDENTIALS_FILEPATH
    import urllib.parse
    sefaria_analytics_mobile_and_web_property_id = 204095655
    PROPERTY_ID = sefaria_analytics_mobile_and_web_property_id
    client = BetaAnalyticsDataClient.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS_FILEPATH)
    request = RunReportRequest(
        property=f"properties/{PROPERTY_ID}",
        date_ranges=[DateRange(start_date="28daysAgo", end_date="yesterday")],
        dimensions=[Dimension(name="pagePath")],
        metrics=[Metric(name="screenPageViews")],
        dimension_filter=FilterExpression(
            filter=Filter(
                field_name="pagePath",
                string_filter=Filter.StringFilter(
                    match_type=Filter.StringFilter.MatchType.BEGINS_WITH,
                    value="/topics/"
                )
            )
        ),
        order_bys=[OrderBy(metric=OrderBy.MetricOrderBy(metric_name="screenPageViews"), desc=True)],
        limit=num_topics,
    )
    response = client.run_report(request)
    slugs = [urllib.parse.unquote(row.dimension_values[0].value.removeprefix("/topics/")) for row in response.rows]
    filtered_slugs = [slug for slug in slugs if not slug.startswith("category/")]
    return filtered_slugs

@django_cache(timeout=24 * 60 * 60, cache_prefix="get_topics_for_book")
def get_topics_for_book(title: str, annotate=False, n=18) -> list:
    all_topics = get_topics_for_ref(title, annotate=annotate)

    topic_counts = defaultdict(int)
    topic_data   = {}
    for topic in all_topics:
        if topic["topic"].startswith("parashat-"):
            continue # parasha topics aren't useful here
        topic_counts[topic["topic"]] += topic["order"].get("user_votes", 1)
        topic_data[topic["topic"]] = {"slug": topic["topic"], "title": topic["title"]}

    topic_order = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)

    return [topic_data[topic[0]] for topic in topic_order][:n]


"""
    SECONDARY TOPIC DATA GENERATION
"""

def generate_all_topic_links_from_sheets(topic=None):
    """
    Processes all public source sheets to create topic links.
    """
    from sefaria.recommendation_engine import RecommendationEngine
    from statistics import mean, stdev
    import math

    OWNER_THRESH = 3
    TFIDF_CUTOFF = 0.15
    STD_DEV_CUTOFF = 2

    all_related_topics = defaultdict(lambda: defaultdict(set))
    all_related_refs = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
    topic_ref_counts = defaultdict(lambda: defaultdict(int))
    # ignore sheets that are copies or were assignments
    query = {"status": "public", "viaOwner": {"$exists": 0}, "assignment_id": {"$exists": 0}}
    if topic:
        query['topics.slug'] = topic
    projection = {"topics": 1, "expandedRefs": 1, "owner": 1}
    sheet_list = db.sheets.find(query, projection)
    for sheet in tqdm(sheet_list, desc="aggregating sheet topics"):
        sheet_topics = sheet.get("topics", [])
        for topic_dict in sheet_topics:
            slug = topic_dict['slug']
            for tref in sheet.get("expandedRefs", []):
                value = all_related_refs[tref][slug].get(sheet['owner'], 0)
                all_related_refs[tref][slug][sheet['owner']] = max(1/len(sheet_topics), value)
                topic_ref_counts[slug][tref] += 1
            for related_topic_dict in sheet_topics:
                if slug != related_topic_dict['slug']:
                    all_related_topics[slug][related_topic_dict['slug']].add(sheet['owner'])

    already_created_related_links = {}
    related_links = []
    source_links = []
    for slug, related_topics_to_slug in tqdm(all_related_topics.items(), desc="creating sheet related topic links"):
        if topic is not None and slug != topic:
            continue

        # filter related topics with less than 2 users who voted for it
        related_topics = [related_topic for related_topic in related_topics_to_slug.items() if len(related_topic[1]) >= 2]
        for related_topic, user_votes in related_topics:
            if related_topic == slug:
                continue
            key = (related_topic, slug) if related_topic > slug else (slug, related_topic)
            if already_created_related_links.get(key, False):
                continue
            already_created_related_links[key] = True
            related_links += [{
                'a': related_topic,
                'b': slug,
                'user_votes': len(user_votes)
            }]
    topic_idf_dict = {slug: math.log2(len(all_related_refs) / len(ref_dict)) for slug, ref_dict in topic_ref_counts.items()}
    raw_topic_ref_links = defaultdict(list)
    for tref, related_topics_to_tref in tqdm(all_related_refs.items(), desc="creating sheet related ref links"):
        # filter sources with less than 3 users who added it and tfidf of at least 0.15
        numerator_list = []
        owner_counts = []
        for slug, owner_map in related_topics_to_tref.items():
            numerator = sum(owner_map.values())
            owner_counts += [len(owner_map)]
            numerator_list += [numerator]
        denominator = sum(numerator_list)
        topic_scores = [(slug, (numerator / denominator) * topic_idf_dict[slug], owners) for slug, numerator, owners in
                  zip(related_topics_to_tref.keys(), numerator_list, owner_counts)]
        # transform data to more convenient format
        oref = get_ref_safely(tref)
        if oref is None:
            continue
        for slug, _, owners in filter(lambda x: x[1] >= TFIDF_CUTOFF and x[2] >= OWNER_THRESH, topic_scores):
            raw_topic_ref_links[slug] += [(oref, owners)]

    for slug, sources in tqdm(raw_topic_ref_links.items()):
        # cluster refs that are close to each other and break up clusters where counts differ by more than 2 standard deviations
        temp_sources = []
        if len(sources) == 0:
            continue
        refs, counts = zip(*sources)
        clustered = RecommendationEngine.cluster_close_refs(refs, counts, 2)
        for cluster in clustered:
            counts = [(x['ref'], x['data']) for x in cluster]
            curr_range_start = 0
            for icount, (_, temp_count) in enumerate(counts):
                temp_counts = [x[1] for x in counts[curr_range_start:icount]]
                if len(temp_counts) < 2:
                    # variance requires two data points
                    continue
                count_xbar = mean(temp_counts)
                count_std = max(1/STD_DEV_CUTOFF, stdev(temp_counts, count_xbar))
                if temp_count > (STD_DEV_CUTOFF*count_std + count_xbar) or temp_count < (count_xbar - STD_DEV_CUTOFF*count_std):
                    temp_range = counts[curr_range_start][0].to(counts[icount-1][0])
                    temp_sources += [(temp_range.normal(), [r.normal() for r in temp_range.range_list()], count_xbar)]
                    curr_range_start = icount
            temp_counts = [x[1] for x in counts[curr_range_start:]]
            count_xbar = mean(temp_counts)
            temp_range = counts[curr_range_start][0].to(counts[-1][0])
            temp_sources += [(temp_range.normal(), [r.normal() for r in temp_range.range_list()], count_xbar)]
        sources = temp_sources

        # create links
        if not topic:
            for source in sources:
                source_links += [{
                    "class": "refTopic",
                    "toTopic": slug,
                    "ref": source[0],
                    "expandedRefs": source[1],
                    "linkType": "about",
                    "is_sheet": False,
                    "dataSource": "sefaria-users",
                    "generatedBy": "sheet-topic-aggregator",
                    "order": {"user_votes": source[2]}
                }]

    if not topic:
        related_links = calculate_tfidf_related_sheet_links(related_links)
        sheet_links = generate_sheet_topic_links()

        # convert to objects
        source_links = [RefTopicLink(l) for l in source_links]
        related_links = [IntraTopicLink(l) for l in related_links]
        sheet_links = [RefTopicLink(l) for l in sheet_links]
        return source_links, related_links, sheet_links


def generate_sheet_topic_links():
    projection = {"topics": 1, "id": 1, "status": 1}
    sheet_list = db.sheets.find({"status": "public", "assignment_id": {"$exists": 0}}, projection)
    sheet_links = []
    for sheet in tqdm(sheet_list, desc="getting sheet topic links"):
        if sheet.get('id', None) is None:
            continue
        sheet_topics = sheet.get("topics", [])
        for topic_dict in sheet_topics:
            slug = topic_dict['slug']
            sheet_links += [{
                "class": "refTopic",
                "toTopic": slug,
                "ref": f"Sheet {sheet['id']}",
                "expandedRefs": [f"Sheet {sheet['id']}"],
                "linkType": "about",
                "is_sheet": True,
                "dataSource": "sefaria-users",
                "generatedBy": "sheet-topic-aggregator"
            }]
    return sheet_links


def calculate_tfidf_related_sheet_links(related_links):
    import math

    MIN_SCORE_THRESH = 0.1  # min tfidf score that will be saved in db

    docs = defaultdict(dict)
    for l in tqdm(related_links, desc='init'):
        docs[l['a']][l['b']] = {"dir": 'to', 'count': l['user_votes'], 'id': '{}|{}'.format(l['a'], l['b'])}
        docs[l['b']][l['a']] = {"dir": 'from', 'count': l['user_votes'], 'id': '{}|{}'.format(l['a'], l['b'])}

    # idf
    doc_topic_counts = defaultdict(int)
    doc_len = defaultdict(int)
    for slug, topic_counts in docs.items():
        for temp_slug, counts in topic_counts.items():
            doc_topic_counts[temp_slug] += 1
            doc_len[temp_slug] += counts['count']
    idf_dict = {slug: math.log2(len(docs)/count) for slug, count in doc_topic_counts.items()}

    # tf-idf
    id_score_map = defaultdict(dict)
    for slug, topic_counts in docs.items():
        for temp_slug, counts in topic_counts.items():
            id_score_map[counts['id']][counts['dir']] = {
                "tfidf": (counts['count'] * idf_dict[temp_slug]) / doc_len[slug],
            }

    # filter
    final_related_links = []
    for l in tqdm(related_links, desc='save'):
        score_dict = id_score_map['{}|{}'.format(l['a'], l['b'])]
        for dir, inner_score_dict in score_dict.items():
            if inner_score_dict['tfidf'] >= MIN_SCORE_THRESH:
                is_inverse = dir == 'from'
                final_related_links += [{
                    "class": "intraTopic",
                    'fromTopic': l['b'] if is_inverse else l['a'],
                    'toTopic': l['a'] if is_inverse else l['b'],
                    "linkType": "sheets-related-to",
                    "dataSource": "sefaria-users",
                    "generatedBy": "sheet-topic-aggregator",
                    "order": {"tfidf": inner_score_dict['tfidf']}
                }]
    return final_related_links


def tokenize_words_for_tfidf(text, stopwords):
    from sefaria.utils.hebrew import strip_cantillation

    try:
        text = TextChunk.strip_itags(text)
    except AttributeError:
        pass
    text = strip_cantillation(text, strip_vowels=True)
    text = re.sub(r'<[^>]+>', ' ', text)
    for match in re.finditer(r'\(.*?\)', text):
        if len(match.group().split()) <= 5:
            text = text.replace(match.group(), " ")
    text = re.sub(r'', ' ', text)
    text = re.sub(r'\[[^\[\]]{1,7}\]', '',
                  text)  # remove kri but dont remove too much to avoid messing with brackets in talmud
    text = re.sub(r'[A-Za-z.,"?!:]', '', text)
    # replace common hashem replacements with the tetragrammaton
    text = re.sub("(^|\s)([\u05de\u05e9\u05d5\u05db\u05dc\u05d1]?)(?:\u05d4['\u05f3]|\u05d9\u05d9)($|\s)",
                  "\\1\\2\u05d9\u05d4\u05d5\u05d4\\3", text)
    # replace common elokim replacement with elokim
    text = re.sub(
        "(^|\s)([\u05de\u05e9\u05d5\u05db\u05dc\u05d1]?)(?:\u05d0\u05dc\u05e7\u05d9\u05dd)($|\s)",
        "\\1\\2\u05d0\u05dc\u05d4\u05d9\u05dd\\3", text)
    words = []
    if len(text) != 0:
        # text = requests.post('https://prefix.dicta.org.il/api', data=json.dumps({'data': text})).text
        # text = re.sub(r'(?<=\s|"|\(|\[|-)[\u05d0-\u05ea]+\|', '', ' ' + text)  # remove prefixes
        text = re.sub('[^\u05d0-\u05ea"]', ' ', text)
        words = list(filter(lambda w: w not in stopwords, [re.sub('^\u05d5', '', w.replace('"', '')) for w in text.split()]))
    return words


def calculate_mean_tfidf(ref_topic_links):
    import math
    with open('data/hebrew_stopwords.txt', 'r') as fin:
        stopwords = set()
        for line in fin:
            stopwords.add(line.strip())

    ref_topic_map = defaultdict(list)
    ref_words_map = {}
    for l in tqdm(ref_topic_links, total=len(ref_topic_links), desc='process text'):
        ref_topic_map[l.toTopic] += [l.ref]
        if l.ref not in ref_words_map:
            oref = get_ref_safely(l.ref)
            if oref is None:
                continue

            ref_words_map[l.ref] = tokenize_words_for_tfidf(oref.text('he').as_string(), stopwords)

    # idf
    doc_word_counts = defaultdict(int)
    for topic, ref_list in tqdm(ref_topic_map.items(), desc='idf'):
        unique_words = set()
        for tref in ref_list:
            try:
                words = ref_words_map[tref]
            except KeyError:
                print("Dont have {}".format(tref))
                continue
            for w in words:
                if w not in unique_words:
                    doc_word_counts[w] += 1
                    unique_words.add(w)
    idf_dict = {}
    for w, count in doc_word_counts.items():
        idf_dict[w] = math.log2(len(ref_topic_map)/count)

    # tf-idf
    topic_tref_score_map = {}
    top_words_map = {}
    for topic, ref_list in ref_topic_map.items():
        total_tf = defaultdict(int)
        tref_tf = defaultdict(lambda: defaultdict(int))
        for tref in ref_list:
            words = ref_words_map.get(tref, [])
            for w in words:
                total_tf[w] += 1
                tref_tf[tref][w] += 1
        tfidf_dict = {}
        for w, tf in total_tf.items():
            tfidf_dict[w] = tf * idf_dict[w]
        for tref in ref_list:
            words = ref_words_map.get(tref, [])
            if len(words) == 0:
                topic_tref_score_map[(topic, tref)] = 0
                continue
            # calculate avg tfidf - tfidf for words that appear in this tref
            # so that tref can't influence score
            topic_tref_score_map[(topic, tref)] = sum((tfidf_dict[w] - tref_tf[tref].get(w, 0)*idf_dict[w]) for w in words)/len(words)
            # top_words_map[(topic, tref)] = [x[0] for x in sorted([(w, (tfidf_dict[w] - tref_tf[tref].get(w, 0)*idf_dict[w])) for w in words], key=lambda x: x[1], reverse=True)[:10]]
    return topic_tref_score_map, ref_topic_map


def calculate_pagerank_scores(ref_topic_map):
    from sefaria.pagesheetrank import pagerank_rank_ref_list
    from statistics import mean
    pr_map = {}
    pr_seg_map = {}  # keys are (topic, seg_tref). used for sheet relevance
    for topic, ref_list in tqdm(ref_topic_map.items(), desc='calculate pr'):
        oref_list = []
        for tref in ref_list:
            oref = get_ref_safely(tref)
            if oref is None:
                continue
            oref_list += [oref]
        seg_ref_map = {r.normal(): [rr.normal() for rr in r.all_segment_refs()] for r in oref_list}
        oref_pr_list = pagerank_rank_ref_list(oref_list, normalize=True, seg_ref_map=seg_ref_map)
        for oref, pr in oref_pr_list:
            pr_map[(topic, oref.normal())] = pr
            for seg_tref in seg_ref_map[oref.normal()]:
                pr_seg_map[(topic, seg_tref)] = pr
    return pr_map, pr_seg_map


def calculate_other_ref_scores(ref_topic_map):
    LANGS_CHECKED = ['en', 'he']
    num_datasource_map = {}
    langs_available = {}
    comp_date_map = {}
    order_id_map = {}
    for topic, ref_list in tqdm(ref_topic_map.items(), desc='calculate other ref scores'):
        seg_ref_counter = defaultdict(int)
        tref_range_lists = {}
        for tref in ref_list:
            oref = get_ref_safely(tref)
            if oref is None:
                continue
            tref_range_lists[tref] = [seg_ref.normal() for seg_ref in oref.range_list()]
            try:
                tp = oref.index.best_time_period()
                year = int(tp.start) if tp else 3000
            except (ValueError, AttributeError):
                year = 3000
            comp_date_map[(topic, tref)] = year
            order_id_map[(topic, tref)] = oref.order_id()
            langs_available[(topic, tref)] = [lang for lang in LANGS_CHECKED if oref.is_text_fully_available(lang)]
            for seg_ref in tref_range_lists[tref]:
                seg_ref_counter[seg_ref] += 1
        for tref in ref_list:
            range_list = tref_range_lists.get(tref, None)
            num_datasource_map[(topic, tref)] = 0 if (range_list is None or len(range_list) == 0) else max(seg_ref_counter[seg_ref] for seg_ref in range_list)
    return num_datasource_map, langs_available, comp_date_map, order_id_map


def update_ref_topic_link_orders(source_links, sheet_topic_links):
    """

    @param source_links: Links between sources and topics (as opposed to sheets and topics)
    @param sheet_topic_links: Links between sheets and topics
    """
    topic_tref_score_map, ref_topic_map = calculate_mean_tfidf(source_links)
    num_datasource_map, langs_available, comp_date_map, order_id_map = calculate_other_ref_scores(ref_topic_map)
    pr_map, pr_seg_map = calculate_pagerank_scores(ref_topic_map)
    sheet_cache = {}

    def get_sheet_order(topic_slug, sheet_id):
        if sheet_id in sheet_cache:
            sheet = sheet_cache[sheet_id]
        else:
            sheet = db.sheets.find_one({"id": sheet_id}, {"views": 1, "includedRefs": 1, "dateCreated": 1, "options": 1, "title": 1, "topics": 1})
            includedRefs = []
            for tref in sheet['includedRefs']:
                try:
                    oref = get_ref_safely(tref)
                    if oref is None:
                        continue
                    includedRefs += [[sub_oref.normal() for sub_oref in oref.all_segment_refs()]]
                except InputError:
                    continue
                except AssertionError:
                    print("Assertion Error", tref)
                    continue
            sheet['includedRefs'] = includedRefs
            sheet_cache[sheet_id] = sheet

        # relevance based on average pagerank personalized to this topic
        total_pr = 0
        for ref_range in sheet['includedRefs']:
            if len(ref_range) == 0:
                continue
            total_pr += sum([pr_seg_map.get((topic_slug, ref), 1e-5) for ref in ref_range]) / len(ref_range)  # make default pr epsilon so that relevance can tell difference between sheets that have sources on topic and those that dont
        avg_pr = 0 if len(sheet['includedRefs']) == 0 else total_pr / len(sheet['includedRefs'])

        # relevance based on other topics on this sheet
        other_topic_slug_set = {t['slug'] for t in sheet.get('topics', []) if t['slug'] != topic_slug}
        total_tfidf = 0
        for other_topic_slug in other_topic_slug_set:
            intra_topic_link = IntraTopicLink().load({'$or': [
                {'fromTopic': topic_slug, 'toTopic': other_topic_slug},
                {'fromTopic': other_topic_slug, 'toTopic': topic_slug}]})
            if intra_topic_link:
                is_inverse = intra_topic_link.toTopic == topic_slug
                tfidfDir = 'fromTfidf' if is_inverse else 'toTfidf'
                total_tfidf += getattr(intra_topic_link, 'order', {}).get(tfidfDir, 0)
        avg_tfidf = 1 if len(other_topic_slug_set) == 0 else (total_tfidf / len(other_topic_slug_set)) + 1

        relevance = 0 if avg_pr == 0 else avg_pr + avg_tfidf  # TODO: test this equation again
        sheet_title = sheet.get('title', 'a')
        if not isinstance(sheet_title, str):
            title_lang = 'english'
        else:
            title_lang = 'english' if re.search(r'[a-zA-Z]', re.sub(r'<[^>]+>', '', sheet_title)) is not None else 'hebrew'
        return {
            'views': sheet.get('views', 0),
            'dateCreated': sheet['dateCreated'],
            'relevance': relevance,
            'avg_ref_pr': avg_pr,
            'avg_topic_tfidf': avg_tfidf,
            'language': sheet.get('options', {}).get('language', 'bilingual'),
            'titleLanguage': title_lang
        }

    all_ref_topic_links_updated = []
    all_ref_topic_links = sheet_topic_links + source_links
    for l in tqdm(all_ref_topic_links, desc='update link orders'):
        if l.is_sheet:
            setattr(l, 'order', get_sheet_order(l.toTopic, int(l.ref.replace("Sheet ", ""))))
        else:
            key = (l.toTopic, l.ref)
            try:
                order = getattr(l, 'order', {})
                order.update({
                    'tfidf': topic_tref_score_map[key],
                    'numDatasource': num_datasource_map[key],
                    'availableLangs': langs_available[key],
                    'comp_date': comp_date_map[key],
                    'order_id': order_id_map[key],
                    'pr': pr_map[key],
                })
                setattr(l, 'order', order)
            except KeyError:
                print("KeyError", key)
                continue
        all_ref_topic_links_updated += [l]

    return all_ref_topic_links_updated


def update_intra_topic_link_orders(sheet_related_links):
    """
    add relevance order to intra topic links in sidebar
    :return:
    """
    import math
    from itertools import chain

    uncats = Topic.get_uncategorized_slug_set()
    topic_link_dict = defaultdict(lambda: defaultdict(lambda: []))
    other_related_links = IntraTopicLinkSet({"generatedBy": {"$ne": TopicLinkHelper.generated_by_sheets}})
    for link in tqdm(chain(other_related_links, sheet_related_links), desc="update intra orders"):
        if link.fromTopic in uncats or link.toTopic in uncats:
            continue
        topic_link_dict[link.fromTopic][link.toTopic] += [{'link': link, 'dir': 'to'}]
        topic_link_dict[link.toTopic][link.fromTopic] += [{'link': link, 'dir': 'from'}]

    # idf
    idf_dict = {}
    N = len(topic_link_dict)  # total num documents
    for topic_slug, topic_links in topic_link_dict.items():
        idf_dict[topic_slug] = 0 if len(topic_links) == 0 else math.log2(N/len(topic_links))

    def link_id(temp_link):
        return f"{temp_link.fromTopic}|{temp_link.toTopic}|{temp_link.linkType}"

    updated_related_link_dict = {}
    for topic_slug, topic_links in topic_link_dict.items():
        for other_topic_slug, link_list in topic_links.items():
            other_topic_links = topic_link_dict.get(other_topic_slug, None)
            if other_topic_links is None:
                continue
            in_common = len(topic_links.keys() & other_topic_links.keys())
            for link_dict in link_list:
                link = link_dict['link']
                temp_order = getattr(link, 'order', {})
                tfidf = in_common * idf_dict[other_topic_slug]
                temp_order[f"{link_dict['dir']}Tfidf"] = tfidf
                lid = link_id(link_dict['link'])
                if lid in updated_related_link_dict:
                    curr_order = getattr(updated_related_link_dict[lid], 'order', {})
                    temp_order.update(curr_order)
                else:
                    updated_related_link_dict[lid] = link
                updated_related_link_dict[lid].order = temp_order

    return list(updated_related_link_dict.values())


def get_top_topic(sheet):
    """
    Chooses the "top" topic of a sheet out of all the topics the sheet was tagged with
    based on the relevance parameter of the topics in regard to the sheet
    :param sheet: Sheet() obj
    :return: Topic() obj
    """
    # get all topics on the sheet (learn the candidates)
    topics = sheet.get("topics", [])  # mongo query on sheet

    def topic_score(t):
        rtl = RefTopicLink().load({"toTopic": t["slug"], "ref": "Sheet {}".format(sheet.get("id"))})
        if rtl is None:
            return t["slug"], 0
        avg_pr = rtl.contents().get("order", {}).get("avg_ref_pr", 0)
        norm_abg_pr = 0.5 if avg_pr == 0 else 1000*avg_pr
        avg_tfidf = rtl.contents().get("order", {}).get("avg_topic_tfidf", 0)
        score = norm_abg_pr + avg_tfidf
        return t["slug"], score

    if len(topics) == 1:
        max_topic_slug = topics[0].get("slug")
    elif len(topics) > 1:
        topic_dict = defaultdict(lambda: [(0, 0), 0])
        for t in topics:
            topic_dict[t.get("slug")][1] += 1
            topic_dict[t.get("slug")][0] = topic_score(t)
        scores = dict([(k, v[0][1] * v[1]) for k, v in topic_dict.items()])
        max_topic_slug = max(scores, key=scores.get)
    else:
        return None
    top_topic = Topic.init(max_topic_slug)
    return top_topic


def add_num_sources_to_topics():
    updates = [{"numSources": RefTopicLinkSet({"toTopic": t.slug, "linkType": {"$ne": "mention"}}).count(), "_id": t._id} for t in TopicSet()]
    db.topics.bulk_write([
        UpdateOne({"_id": t['_id']}, {"$set": {"numSources": t['numSources']}}) for t in updates
    ])


def make_titles_unique():
    ts = TopicSet()
    for t in ts:
        unique = {tuple(tit.values()): tit for tit in t.titles}
        if len(unique) != len(t.titles):
            t.titles = list(unique.values())
            t.save()

def get_ref_safely(tref):
    try:
        oref = Ref(tref)
        return oref
    except InputError:
        print("Input Error", tref)
    except IndexError:
        print("IndexError", tref)
    except AssertionError:
        print("AssertionError", tref)
    return None

def calculate_popular_writings_for_authors(top_n, min_pr):
    RefTopicLinkSet({"generatedBy": "calculate_popular_writings_for_authors"}).delete()
    rds = RefDataSet()
    by_author = defaultdict(list)
    for rd in tqdm(rds, total=rds.count()):
        try:
            tref = rd.ref.replace('&amp;', '&')  # TODO this is a stopgap to prevent certain refs from failing
            oref = Ref(tref)
        except InputError as e:
            continue
        if getattr(oref.index, 'authors', None) is None: continue
        for author in oref.index.authors:
            by_author[author] += [rd.contents()]
    for author, rd_list in by_author.items():
        rd_list = list(filter(lambda x: x['pagesheetrank'] > min_pr, rd_list))
        if len(rd_list) == 0: continue
        top_rd_indexes = sorted(range(len(rd_list)), key=lambda i: rd_list[i]['pagesheetrank'])[-top_n:]
        top_rds = [rd_list[i] for i in top_rd_indexes]
        for rd in top_rds:
            RefTopicLink({
                "toTopic": author,
                "ref": rd['ref'],
                "linkType": "popular-writing-of",
                "dataSource": "sefaria",
                "generatedBy": "calculate_popular_writings_for_authors",
                "order": {"custom_order": rd['pagesheetrank']}
            }).save()

def recalculate_secondary_topic_data():
    source_links = RefTopicLinkSet({'is_sheet': False})
    sheet_links = [RefTopicLink(l) for l in generate_sheet_topic_links()]

    related_links = update_intra_topic_link_orders(IntraTopicLinkSet())
    all_ref_links = update_ref_topic_link_orders(source_links.array(), sheet_links)

    RefTopicLinkSet({"is_sheet": True}).delete()

    db.topic_links.bulk_write([
        UpdateOne({"_id": l._id}, {"$set": {"order": l.order}})
        if getattr(l, "_id", False) else
        InsertOne(l.contents(for_db=True))
        for l in (all_ref_links + related_links)
    ])

def set_all_slugs_to_primary_title():
    # reset all slugs to their primary titles, if they have drifted away
    # no-op if slug already corresponds to primary title
    for t in TopicSet():
        t.set_slug_to_primary_title()

def get_path_for_topic_slug(slug):
    path = []
    while slug in library.get_topic_toc_category_mapping().keys():
        if library.get_topic_toc_category_mapping()[slug] == slug:
            break  # this case occurs when we are at a top level node which has a child with the same name
        path.append(slug)
        slug = library.get_topic_toc_category_mapping()[slug]  # get parent's slug
    path.append(slug)
    return path

def get_node_in_library_topic_toc(path):
    curr_level_in_library_topic_toc = {"children": library.get_topic_toc(), "slug": ""}
    while len(path) > 0:
        curr_node_slug = path.pop()
        for x in curr_level_in_library_topic_toc.get("children", []):
            if x["slug"] == curr_node_slug:
                curr_level_in_library_topic_toc = x
                break

    return curr_level_in_library_topic_toc

def topic_change_category(topic_obj, new_category, old_category="", rebuild=False):
    """
        This changes a topic's category in the topic TOC.  The IntraTopicLink to the topic's parent category
        will be updated to its new parent category.  This function also handles special casing for topics that have
        IntraTopicLinks to themselves and for topics are moved to or from the Main Menu of the TOC.  In cases where
        the Main Menu is involved, the topic_obj's isTopLevelDisplay field is modified.

        :param topic_obj: (model.Topic) the Topic object
        :param new_category: (String) slug of the new Topic category
        :param old_category: (String, optional) slug of old Topic category
        :param rebuild: (bool, optional) whether the topic TOC should be rebuilt
        :return: (model.Topic) the new topic object on success, or None in the case where old_category == new_category
        """
    assert new_category != topic_obj.slug, f"{new_category} should not be the same as {topic_obj.slug}"
    orig_link = IntraTopicLink().load({"linkType": "displays-under", "fromTopic": topic_obj.slug, "toTopic": {"$ne": topic_obj.slug}})
    if old_category == "":
        old_category = orig_link.toTopic if orig_link else Topic.ROOT
        if old_category == new_category:
            logger.warning("To change the category of a topic, new and old categories should not be equal.")
            return None

    link_to_itself = IntraTopicLink().load({"fromTopic": topic_obj.slug, "toTopic": topic_obj.slug, "linkType": "displays-under"})
    had_children_before_changing_category = IntraTopicLink().load({"linkType": "displays-under", "toTopic": topic_obj.slug, "fromTopic": {"$ne": topic_obj.slug}}) is not None
    new_link_dict = {"fromTopic": topic_obj.slug, "toTopic": new_category, "linkType": "displays-under",
                     "dataSource": "sefaria"}

    if old_category != Topic.ROOT and new_category != Topic.ROOT:
        orig_link.load_from_dict(new_link_dict).save()
    elif new_category != Topic.ROOT:
        # old_category is Topic.ROOT, so we are moving down the tree from the Topic.ROOT
        IntraTopicLink(new_link_dict).save()
        topic_obj.isTopLevelDisplay = False
        topic_obj.save()
        if old_category == Topic.ROOT and not had_children_before_changing_category and link_to_itself:
            # suppose a topic had been put at the Topic.ROOT and a self-link was created because the topic had sources.
            # if it now were moved out of the Topic.ROOT, it no longer needs the link to itself
            link_to_itself.delete()
    elif new_category == Topic.ROOT:
        if orig_link:
            # top of the tree doesn't need an IntraTopicLink to its previous parent
            orig_link.delete()

        topic_obj.isTopLevelDisplay = True
        topic_obj.save()

        if getattr(topic_obj, "numSources", 0) > 0 and not had_children_before_changing_category and not link_to_itself:
            # if topic has sources and we dont create an IntraTopicLink to itself, the sources wont be accessible
            # from the topic TOC
            IntraTopicLink({"fromTopic": topic_obj.slug, "toTopic": topic_obj.slug,
                            "dataSource": "sefaria", "linkType": "displays-under"}).save()

    if rebuild:
        rebuild_topic_toc(topic_obj, category_changed=True)
    return topic_obj

def update_authors_place_and_time(topic, dataSource='learning-team-editing-tool', **kwargs):
    # update place info added to author, then update year and era info
    if not hasattr(topic, 'properties'):
        topic.properties = {}
    process_topic_place_change(topic, **kwargs)
    return update_author_era(topic, dataSource=dataSource, **kwargs)

def update_properties(topic_obj, dataSource, k, v):
    if v == '':
        topic_obj.properties.pop(k, None)
    else:
        topic_obj.properties[k] = {'value': v, 'dataSource': dataSource}

def update_author_era(topic_obj, dataSource='learning-team-editing-tool', **kwargs):
    for k in ["birthYear", "deathYear"]:
        if kwargs.get(k, False):   # only change property value if key exists, otherwise it indicates no change
            year = kwargs[k]
            update_properties(topic_obj, dataSource, k, year)

    if kwargs.get('era', False):    # only change property value if key is in data, otherwise it indicates no change
        prev_era = topic_obj.properties.get('era', {}).get('value')
        era = kwargs['era']
        update_properties(topic_obj, dataSource, 'era', era)
        if era != '':
            create_era_link(topic_obj, prev_era_to_delete=prev_era)
    return topic_obj


def update_topic(topic, titles=None, category=None, origCategory=None, categoryDescritpion=None, description=None,
                 birthPlace=None, deathPlace=None, birthYear=None, deathYear=None, era=None,
                 rebuild_toc=True, manual=False, image=None, **kwargs):
    """
    Can update topic object's titles, category, description, and categoryDescription fields
    :param topic: (Topic) The topic to update
    :param **kwargs can be titles, category, description, categoryDescription, and rebuild_toc where `titles` is a list
     of title objects as they are represented in the database, and `category` is a string. `description` and `categoryDescription` are dictionaries where the fields are `en` and `he`.
         The `category` parameter should be the slug of the new category. `rebuild_topic_toc` is a boolean and is assumed to be True
    :return: (model.Topic) The modified topic
    """
    old_category = ""
    orig_slug = topic.slug

    if titles:
        topic.set_titles(titles)
    if category == 'authors':
        topic = update_authors_place_and_time(topic, birthPlace=birthPlace, birthYear=birthYear, deathPlace=deathPlace, deathYear=deathYear, era=era)

    if category and origCategory and origCategory != category:
        orig_link = IntraTopicLink().load({"linkType": "displays-under", "fromTopic": topic.slug, "toTopic": {"$ne": topic.slug}})
        old_category = orig_link.toTopic if orig_link else Topic.ROOT
        if old_category != category:
            topic = topic_change_category(topic, category, old_category=old_category)  # can change topic and intratopiclinks

    if manual:
        topic.data_source = "sefaria"  # any topic edited manually should display automatically in the TOC and this flag ensures this
        topic.description_published = True

    if description or categoryDescritpion:
        topic.change_description(description, categoryDescritpion)

    if image:
        if image["image_uri"] != "":
            topic.image = image
        elif hasattr(topic, 'image'):
            # we don't want captions without image_uris, so if the image_uri is blank, get rid of the caption too
            del topic.image

    topic.save()

    if rebuild_toc:
        rebuild_topic_toc(topic, orig_slug=orig_slug, category_changed=(old_category != category))
    return topic


def rebuild_topic_toc(topic_obj, orig_slug="", category_changed=False):
    if category_changed:
        library.get_topic_toc(rebuild=True)
    else:
        # if just title or description changed, don't rebuild entire topic toc, rather edit library._topic_toc directly
        path = get_path_for_topic_slug(orig_slug)
        old_node = get_node_in_library_topic_toc(path)
        if orig_slug != topic_obj.slug:
            return f"Slug {orig_slug} not found in library._topic_toc."
        old_node.update({"en": topic_obj.get_primary_title(), "slug": topic_obj.slug, "description": topic_obj.description})
        old_node["he"] = topic_obj.get_primary_title('he')
        if hasattr(topic_obj, "categoryDescription"):
            old_node["categoryDescription"] = topic_obj.categoryDescription
    library.get_topic_toc_json(rebuild=True)
    library.get_topic_toc_category_mapping(rebuild=True)

def _calculate_approved_review_state(current, requested, was_ai_generated):
    "Calculates the review state of a description of topic link. Review state of a description can only 'increase'"
    if not was_ai_generated:
        return None
    state_to_num = {
        None: -1,
        "not reviewed": 0,
        "edited": 1,
        "reviewed": 2
    }
    if state_to_num[requested] > state_to_num[current]:
        return requested
    else:
        return current

def _description_was_ai_generated(description: dict) -> bool:
    return bool(description.get('ai_title', ''))

def _get_merged_descriptions(current_descriptions, requested_descriptions):
    from sefaria.utils.util import deep_update
    for lang, requested_description_in_lang in requested_descriptions.items():
        current_description_in_lang = current_descriptions.get(lang, {})
        current_review_state = current_description_in_lang.get("review_state")
        requested_review_state = requested_description_in_lang.get("review_state")
        merged_review_state = _calculate_approved_review_state(current_review_state, requested_review_state, _description_was_ai_generated(current_description_in_lang))
        if merged_review_state:
            requested_description_in_lang['review_state'] = merged_review_state
        else:
            requested_description_in_lang.pop('review_state', None)
    return deep_update(current_descriptions, requested_descriptions)


def edit_topic_source(slug, orig_tref, new_tref="", creating_new_link=True,
                      interface_lang='en', linkType='about', description=None):
    """
    API helper function used by SourceEditor for editing sources associated with topics which are stored as RefTopicLink
    Slug, orig_tref, and linkType define the original RefTopicLink if one existed.
    :param slug: (str) String of topic whose source we are editing
    :param orig_tref (str) String representation of original reference of source.
    :param new_tref: (str) String representation of new reference of source.
    :param linkType: (str) 'about' is used for most topics, except for 'authors' case
    :param description: (dict) Dictionary of title and prompt corresponding to `interface_lang`
    """
    description = description or {}
    topic_obj = Topic.init(slug)
    if topic_obj is None:
        return {"error": "Topic does not exist."}
    ref_topic_dict = {"toTopic": slug, "linkType": linkType, "ref": orig_tref, "dataSource": "learning-team"}
    link = RefTopicLink().load(ref_topic_dict)
    link_already_existed = link is not None
    if not link_already_existed:
        link = RefTopicLink(ref_topic_dict)

    if not hasattr(link, 'order'):
        link.order = {}
    if 'availableLangs' not in link.order:
        link.order['availableLangs'] = []
    if interface_lang not in link.order['availableLangs']:
        link.order['availableLangs'] += [interface_lang]
    link.dataSource = 'learning-team'
    link.ref = new_tref

    current_descriptions = getattr(link, 'descriptions', {})
    link.descriptions = _get_merged_descriptions(current_descriptions, {interface_lang: description})

    if hasattr(link, 'generatedBy') and getattr(link, 'generatedBy', "") == TopicLinkHelper.generated_by_sheets:
        del link.generatedBy  # prevent link from getting deleted when topic cronjob runs

    if not creating_new_link and link is None:
        return {"error": f"Can't edit link because link does not currently exist."}
    elif creating_new_link:
        if not link_already_existed:
            num_sources = getattr(topic_obj, "numSources", 0)
            topic_obj.numSources = num_sources + 1
            topic_obj.save()
        if interface_lang not in link.order.get('curatedPrimacy', {}) and linkType == 'about':
            # this will evaluate to false when (1) creating a new link though the link already exists and has a curated primacy
            # or (2) topic is an author (which means linkType is not 'about') as curated primacy is irrelevant to authors
            # this code sets the new source at the top of the topic page, because otherwise it can be hard to find.
            # curated primacy's default value for all links is 0 so set it to 1 + num of links in this language
            curated_lang_query = {
                "toTopic": slug,
                "linkType": linkType,
                f"order.curatedPrimacy.{interface_lang}": {"$exists": True}
            }
            links = RefTopicLinkSet(curated_lang_query)
            primacies = [
                link.order["curatedPrimacy"][interface_lang]
                for link in links
            ]
            new_primacy = max(primacies, default=0) + 1
            if 'curatedPrimacy' not in link.order:
                link.order['curatedPrimacy'] = {}
            link.order['curatedPrimacy'][interface_lang] = new_primacy

    link.save()
    # process link for client-side, especially relevant in TopicSearch.jsx
    ref_topic_dict = ref_topic_link_prep(link.contents())
    return annotate_topic_link(ref_topic_dict, {slug: topic_obj})

def update_order_of_topic_sources(topic, sources, uid, lang='en'):
    """
    Used by ReorderEditor.  Reorders sources of topics.
    :param topic: (str) Slug of topic
    :param sources: (List) A list of topic sources with ref and order fields.  The first source in the list will have
    its order field modified so that it appears first on the relevant Topic Page
    :param uid: (int) UID of user modifying categories and/or books
    :param lang: (str) 'en' or 'he'
    """

    if AuthorTopic.init(topic):
        return {"error": "Author topic sources can't be reordered as they have a customized order."}
    if Topic.init(topic) is None:
        return {"error": f"Topic {topic} doesn't exist."}
    results = []
    ref_to_link = {}

    # first validate data
    for s in sources:
        try:
             Ref(s['ref']).normal()
        except InputError as e:
            return {"error": f"Invalid ref {s['ref']}"}
        link = RefTopicLink().load({"toTopic": topic, "linkType": "about", "ref": s['ref'], "dataSource": "learning-team"})
        if link is None:
            # for now, we are focusing on learning team links and the lack of existence isn't considered an error
            continue
            # return {"error": f"Link between {topic} and {s['ref']} doesn't exist."}
        order = getattr(link, 'order', {})
        ref_to_link[s['ref']] = link

    # now update curatedPrimacy data
    for display_order, s in enumerate(sources[::-1]):
        link = ref_to_link.get(s['ref'])
        if not link:
            continue
        order = getattr(link, 'order', {})
        curatedPrimacy = order.get('curatedPrimacy', {})
        curatedPrimacy[lang] = display_order
        order['curatedPrimacy'] = curatedPrimacy
        link.order = order
        link.save()
        results.append(link.contents())
    return {"sources": results}


def delete_ref_topic_link(tref, to_topic, link_type, lang):
    """
    :param type: (str) Can be 'ref' or 'intra'
    :param tref: (str) tref of source
    :param to_topic: (str) Slug of topic
    :param lang: (str) 'he' or 'en'
    """
    if Topic.init(to_topic) is None:
        return {"error": f"Topic {to_topic} doesn't exist."}

    topic_link = {"toTopic": to_topic, "linkType": link_type, 'ref': tref, "dataSource": "learning-team"}
    link = RefTopicLink().load(topic_link)
    if link is None:
        return {"error": f"A learning-team link between {tref} and {to_topic} doesn't exist. If you are trying to delete a non-learning-team link, reach out to the engineering team."}

    if lang in link.order.get('curatedPrimacy', []):
        link.order['curatedPrimacy'].pop(lang)
    if lang in getattr(link, 'descriptions', {}):
        link.descriptions.pop(lang)

    # Note, using curatedPrimacy as a proxy here since we are currently only allowing deletion of learning-team links.
    if len(link.order.get('curatedPrimacy', [])) > 0:
        link.save()
        return {"status": "ok"}
    else:   # deleted in both hebrew and english so delete link object
        if link.can_delete():
            link.delete()
            return {"status": "ok"}
        else:
            return {"error": f"Cannot delete link between {tref} and {to_topic}."}


def add_image_to_topic(topic_slug: str, image_uri: str, en_caption: str = "", he_caption: str =""):
    """
    A function to add an image to a Topic in the database. Helper for data migration.
    This function queries the desired Topic, adds the image data, and then saves.
    :param topic_slug String: A valid slug for a Topic
    :param image_uri String: The URI of the image stored in the GCP images bucket, in the topics subdirectory.
                             NOTE: Incorrectly stored, or external images, will not pass validation for save
    :param en_caption String: The English caption for a Topic image
    :param he_caption String: The Hebrew caption for a Topic image
    """
    topic = Topic.init(topic_slug)
    if not hasattr(topic, "image"):
        topic.image = {"image_uri": image_uri, "image_caption": {"en": en_caption, "he": he_caption}}
    else:
        topic.image["image_uri"] = image_uri
        if en_caption:
            topic.image["image_caption"]["en"] = en_caption
        if he_caption:
            topic.image["image_caption"]["he"] = he_caption
    topic.save()


def add_secondary_image_to_topic(topic_slug: str, image_uri: str):
    topic = Topic.init(topic_slug)
    topic.secondary_image_uri = image_uri
    topic.save()


def delete_image_from_topic(topic_slug: str):
    topic = Topic.init(topic_slug)
    if hasattr(topic, "image"):
        del topic.image
        topic.save()


def delete_secondary_image_from_topic(topic_slug: str):
    topic = Topic.init(topic_slug)
    if hasattr(topic, "secondary_image_uri"):
        del topic.secondary_image_uri
        topic.save()

```

### sefaria/helper/search.py

```
from functools import wraps
from elasticsearch_dsl import Q, Search
from elasticsearch_dsl.query import Bool, Regexp, Term
import re


def default_list(param):
    if param is None:
        return []
    return param


def default_bool(param):
    if param is None:
        return False
    return param


def default_search(param):
    if param is None:
        return Search()
    return param


def param_fixer(func):

    @wraps(func)
    def wrapper(*args, **kwargs):
        func_params = func.__code__.co_varnames[:func.__code__.co_argcount]
        extra_params = set(kwargs.keys()) - set(func_params)
        for extra in extra_params:
            kwargs.pop(extra)
        args = list(args)
        params_with_defaults = {
            "source_proj": default_bool,
            "filters": default_list,
            "filter_fields": default_list,
            "aggs": default_list,
            "sort_fields": default_list,
            "sort_reverse": default_bool,
            "search_obj": default_search
        }
        for param, setter in list(params_with_defaults.items()):
            i = func_params.index(param)
            if len(args) > i:
                # in args
                args[i] = setter(args[i])
            else:
                # maybe in kwargs
                kwargs[param] = setter(kwargs.get(param, None))
        return func(*args, **kwargs)
    return wrapper


@param_fixer
def get_query_obj(
        query,
        type="text",
        field="exact",
        source_proj=False,
        slop=0,
        start=0,
        size=100,
        filters=None,
        filter_fields=None,
        aggs=None,
        sort_method="sort",
        sort_fields=None,
        sort_reverse=False,
        sort_score_missing=0,
        search_obj=None):
    """

    :param query :str:
    :param type :str: one_of("text", "sheet")
    :param field :str: which field do you want to query? usually either "exact", "naive_lemmatizer" or "content"
    :param source_proj :str or list(str) or bool: if False, don't return _source. o/w only return fields specified
    :param slop :int: max distance allowed b/w words in the query. 0 is an exact match
    :param start :int: pagination start
    :param size :int: page size
    :param filters :list(str): list of filters you've applied
    :param filter_fields :list(str): list of fields each filter is filtering on. must be same size as `filters` usually "path", "collections" or "tags"
    :param aggs :list(str): list of fields to aggregate on. usually "path", "collections" or "tags"
    :param sort_method :str: how to sort. either "sort" or "score"
    :param sort_fields :list(str): which fields to sort on. sorts are applied in order stably
    :param sort_reverse :bool: should the sorting be reversed?
    :param sort_score_missing :float: in the case of `sort_method = "score"` what value to use if `sort_fields` doesn't exist on a doc
    :param search_obj :Search: object to add the query, sorting, filters etc. optional
    :return: Search object with all the stuff ready to execute
    """
    search_obj = search_obj.source(source_proj)
    query = re.sub(r"(\S)\"(\S)", "\\1\u05f4\\2", query)  # Replace internal quotes with gershaim.
    core_query = Q("match_phrase", **{field: {"query": query, "slop": slop}})

    # sort
    if sort_method == "sort":
        search_obj = search_obj.sort(*["{}{}".format("-" if sort_reverse else "", f) for f in sort_fields])

    # aggregations
    if len(aggs) > 0:
        for a in aggs:
            search_obj.aggs.bucket(a, "terms", field=a, size=10000)

    # filters
    if len(filters) == 0:
        inner_query = core_query
    else:
        inner_query = Bool(must=core_query, filter=get_filter_obj(type, filters, filter_fields))

    # finish up
    if sort_method == "score" and len(sort_fields) == 1:
        search_obj.query = {
            "function_score": {
                "query": inner_query.to_dict(),
                "field_value_factor": {
                    "field": sort_fields[0],
                    "missing": sort_score_missing
                }
            }
        }
    else:
        search_obj.query = inner_query
    search_obj = search_obj.highlight(field, fragment_size=200, pre_tags=["<b>"], post_tags=["</b>"])
    return search_obj[start:start + size]


def get_filter_obj(type, filters, filter_fields):
    if len(filter_fields) == 0:
        filter_fields = [None] * len(filters)  # use default filter_field for query type (defined in make_filter())
    unique_fields = set(filter_fields)
    outer_bools = []
    for agg_type in unique_fields:
        type_filters = [x for x in zip(filters, filter_fields) if x[1] == agg_type]
        bool_type = 'should' if type == 'text' else 'must'  # in general we want filters to be AND (union) but for text filters, we want them to be OR (intersection)
        inner_bool = Bool(**{bool_type: [make_filter(type, agg_type, f) for f, t in type_filters]})
        outer_bools += [inner_bool]
    return Bool(must=outer_bools)


def make_filter(type, agg_type, agg_key):
    if type == "text":
        # filters with '/' might be leading to books. also, very unlikely they'll match an false positives
        agg_key = agg_key.rstrip('/')
        agg_key = re.escape(agg_key)
        reg = f"{agg_key}|{agg_key}/.*"
        return Regexp(path=reg)
    elif type == "sheet":
        return Term(**{agg_type: agg_key})


def get_elasticsearch_client():
    from elasticsearch import Elasticsearch
    from sefaria.settings import SEARCH_URL
    return Elasticsearch(SEARCH_URL)

```

### sefaria/helper/slack/send_message.py

```
import requests
from sefaria.settings import SLACK_URL


def send_message(channel, username, pretext, text, fallback=None, icon_emoji=':robot_face:', color="#a30200"):
    post_object = {
            "icon_emoji": icon_emoji,
            "username": username,
            "channel": channel,
            "attachments": [
                {
                    "fallback": fallback or pretext,
                    "color": color,
                    "pretext": pretext,
                    "text": text
                }
            ]
        }

    requests.post(SLACK_URL, json=post_object)

```

### sefaria/helper/slack/__init__.py

```

```

### sefaria/helper/normalization.py

```
import regex as re
from typing import Dict, List, Callable
from functools import reduce, lru_cache
from bisect import bisect_right
from bs4 import BeautifulSoup, Tag

"""
Tools for normalizing text
"""

UNIDECODE_TABLE = {
    "": "h",
    "": "H",
    "": "a",
    "": "a",
    "g": "g",
    "": "h",
    "": "k",
    "": "K",
    "": "o",
    "": "z",
    "": "Z",
    "": "S",
    "": "s",
    "": "T",
    "": "t",
    "i": "i",
    "": "i",
    "i": "i",
    "": "e",
    "": "'",
    '\u05f3': "'",
    "\u05f4": '"',
    "\u0323": "",  # chirik-like dot
    "": '"',
    "": '"'
}


class AbstractNormalizer:
    """
    Defines signature for normalizers
    Subclasses should implement find_text_to_remove() and optionally normalize()
    Default implementation of normalize() works, but is not optimized. Consider implementing if speed is important.
    """
    def __init__(self):
        pass

    def normalize(self, s: str, **kwargs) -> str:
        """
        Returns a modification of the string s.
        Usually this will remove unwanted characters like HTML or nikkud.
        """
        text_to_remove = self.find_text_to_remove(s, **kwargs)
        schars = list(s)
        # make sure to iterate backwards b/c you're changing indices
        for (start, end), repl in reversed(text_to_remove):
            schars[start:end] = repl
        return ''.join(schars)

    def find_text_to_remove(self, s:str, **kwargs) -> list:
        """
        Returns a list of text to remove when applying normalizer to string s.
        Each item in the list is of form ((start, end), replacement) where start and end are indices in s of text to replace with string `replacement`
        E.g. ((1, 3), " ") means s[1:3] should be replaced with " "
        """
        return []

    @staticmethod
    def remove_subsets(text_to_remove):
        """
        Assumes there are no overlapping or equal length ranges
        Removes strict subsets from list
        """
        def remove_subsets_reducer(curr_text_to_remove: list, next: tuple) -> list:
            (next_start, next_end), _ = next
            for (start, end), _ in curr_text_to_remove:
                if next_start > start and next_end < end:
                    # next is a subset. dont append
                    return curr_text_to_remove
            return curr_text_to_remove + [next]

        text_to_remove.sort(key=lambda x: x[0][1] - x[0][0], reverse=True)
        return reduce(remove_subsets_reducer, text_to_remove, [])

    def get_mapping_after_normalization(self, text, removal_list=None, reverse=False, **kwargs):
        """
        Prefer norm_to_unnorm_indices() over this function since the former is simpler.
        Use this function when you need more control over the mapping outputs.
        It also can be useful to store the mapping and reuse it as an optimization.
        text - unnormalized text
        removal_list - instead of passing `find_text_to_remove`, you can pass an already calculated list of tuples. should be in same format as return value of find_text_to_remove
        reverse - bool. If True, then will return mapping from unnormalized string to normalized string

        returns - dictionary where keys are indices in normalized string and values are how many characters were removed (or added if negative number) by that point from unnormalized string. If reverse=True, indices are in unnormalized string

        Example.
            text = "a###b##c" find_text_to_remove = lambda x: [(m, '') for m in re.finditer(r'#+', x)]
            will return {1: 3, 2: 5}
            meaning by the 2nd index, 5 chars have been removed
            then if you have a range (0,3) in the normalized string "abc" you will know that maps to (0, 8) in the original string
        """
        removal_list = removal_list or self.find_text_to_remove(text, **kwargs)
        total_removed = 0
        removal_map = {}
        subst_end_indexes = set()
        for (start, end), subst in removal_list:
            normalized_text_index = start if reverse else (start + min(len(subst), end-start) - total_removed)
            curr_removed = end - start - len(subst)
            if curr_removed != 0:
                total_removed += curr_removed
                removal_map[normalized_text_index] = total_removed
                if len(subst) > 0:
                    subst_end_indexes.add(normalized_text_index + 1)
        return removal_map, subst_end_indexes

    def norm_to_unnorm_indices(self, text, normalized_indices, removal_list=None, reverse=False, **kwargs):
        """
        text - unnormalized text
        normalized_indices - list of tuples where each tuple is (x, y) x being start index, y is end index + 1
        reverse - if True, normalized_indices are actually unnormalized indices and removal_map was calculated using reverse=True in get_mapping_after_normalization()
        """
        removal_map, subst_end_indices = self.get_mapping_after_normalization(text, removal_list, reverse, **kwargs)
        return self.norm_to_unnorm_indices_with_mapping(normalized_indices, removal_map, subst_end_indices, reverse)

    @staticmethod
    def norm_to_unnorm_indices_with_mapping(normalized_indices, removal_map, subst_end_indices, reverse=False):
        """
        Prefer norm_to_unnorm_indices() over this function since the former is simpler.
        Use this function when you need more control over the mapping inputs.
        It also can be useful to store the mapping and reuse it as an optimization.
        normalized_indices - list of tuples where each tuple is (x, y) x being start index, y is end index + 1
        removal_map - first return value of get_mapping_after_normalization()
        subst_end_indices - second return value from get_mapping_after_normalization()
        reverse - if True, normalized_indices are actually unnormalized indices and removal_map was calculated using reverse=True in get_mapping_after_normalization()
        """
        removal_keys = sorted(removal_map.keys())
        unnormalized_indices = []
        sign = -1 if reverse else 1
        for start, end in normalized_indices:
            unnorm_start_index = bisect_right(removal_keys, start) - 1

            bisect_end_index = end if (start == end or end in subst_end_indices) else end - 1
            unnorm_end_index = bisect_right(removal_keys, bisect_end_index) - 1

            unnorm_start = start if unnorm_start_index < 0 else start + (sign * removal_map[removal_keys[unnorm_start_index]])
            unnorm_end = end if unnorm_end_index < 0 else end + (sign * removal_map[removal_keys[unnorm_end_index]])
            unnormalized_indices += [(unnorm_start, unnorm_end)]
        return unnormalized_indices


class ITagNormalizer(AbstractNormalizer):

    def __init__(self, repl):
        super().__init__()
        self.repl = repl

    @staticmethod
    def _find_itags(tag):
        from sefaria.model.text import AbstractTextRecord
        return AbstractTextRecord._find_itags(tag)

    @staticmethod
    def _get_all_itags(s):
        """
        Very similar to sefaria.model.text.AbstractTextRecord
        Originally called `_strip_itags`
        """
        from sefaria.model.text import AbstractTextRecord

        all_itags = []
        soup = BeautifulSoup(f"<root>{s}</root>", 'lxml')
        itag_list = soup.find_all(ITagNormalizer._find_itags)
        for itag in itag_list:
            all_itags += [itag]
            try:
                if AbstractTextRecord._itag_is_footnote(itag):
                    all_itags += [itag.next_sibling]  # it's a footnote
            except AttributeError:
                pass  # it's an inline commentator
        return all_itags, soup

    @staticmethod
    def _find_itag_start(itag_text: str, s: str, search_start: int) -> int:
        """
        There can be minor differences in itag created by bs4
        Try to find start of itag regardless
        """
        start = -1
        for end_char in range(len(itag_text), round(len(itag_text)/2), -10):
            truncated_itag = itag_text[:end_char]
            start = s.find(truncated_itag, search_start)
            if start != -1:
                break
        return start

    def find_text_to_remove(self, s:str, **kwargs) -> list:
        lenient = kwargs.get('lenient', False)  # if lenient, fail gracefully when you can't find an itag
        all_itags, _ = ITagNormalizer._get_all_itags(s)
        next_start = 0
        text_to_remove = []
        for itag in all_itags:
            itag_text = itag.decode()
            start = self._find_itag_start(itag_text, s, next_start)
            end = start+len(itag_text)
            if start == -1:
                exception_text = f"Couldn't find itag with text '{itag_text}' in\n{s}\nnext_start = {next_start}"
                if lenient:
                    print(exception_text)
                    continue
                else:
                    raise Exception(exception_text)
            text_to_remove += [((start, end), self.repl)]
            next_start = start + 1

        text_to_remove = self.remove_subsets(text_to_remove)
        text_to_remove.sort(key=lambda x: x[0][0])
        return text_to_remove

class ReplaceNormalizer(AbstractNormalizer):

    def __init__(self, old, new):
        super().__init__()
        self.old = old
        self.new = new

    def normalize(self, s, **kwargs):
        return s.replace(self.old, self.new)

    def find_text_to_remove(self, s, **kwargs):
        return [((m.start(), m.end()), self.new) for m in re.finditer(re.escape(self.old), s)]


class RegexNormalizer(AbstractNormalizer):

    def __init__(self, reg, repl) -> None:
        super().__init__()
        self.reg = reg
        self.repl = repl

    def normalize(self, s, **kwargs):
        return re.sub(self.reg, self.repl, s)

    def find_text_to_remove(self, s, **kwargs):
        return [((m.start(), m.end()), self.repl) for m in re.finditer(self.reg, s)]


class NormalizerComposer(AbstractNormalizer):

    def __init__(self, step_keys: List[str]=None, steps: List[AbstractNormalizer]=None) -> None:
        """
        Combines multiple normalizers as if they are one normalizer.
        Should pass either step_keys or steps.
        :param step_keys: list of keys in NormalizerFactory.key_normalizer_map of normalizers to apply in order
        :param steps: list of AbstractNormalizers to apply in order
        """
        super().__init__()
        if steps is not None:
            self.steps = steps
        else:
            self.steps = NormalizerFactory.get_all(step_keys)

    def normalize(self, s, **kwargs):
        for step in self.steps:
            s = step.normalize(s)
        return s

    def find_text_to_remove(self, s, **kwargs):
        """
        this is a bit mind-boggling.
        apply normalization steps one-by-one and keep track of mapping from one step to the next
        iteratively apply mappings (in reverse) on each step's removal inds to get inds in original string
        """
        final_text_to_remove = []
        mappings = []
        snorm = s
        for step in self.steps:
            curr_text_to_remove = step.find_text_to_remove(snorm, **kwargs)
            if len(curr_text_to_remove) == 0:
                text_to_remove_inds, text_to_remove_repls = [], []
            else:
                text_to_remove_inds, text_to_remove_repls = zip(*curr_text_to_remove)
            for mapping, subst_end_indices in reversed(mappings):
                text_to_remove_inds = step.norm_to_unnorm_indices_with_mapping(text_to_remove_inds, mapping,
                                                                               subst_end_indices)
            curr_text_to_remove = list(zip(text_to_remove_inds, text_to_remove_repls))

            # merge any overlapping ranges
            # later edits should override earlier ones
            final_text_to_remove = self.merge_removal_inds(final_text_to_remove, curr_text_to_remove)
            mappings += [step.get_mapping_after_normalization(snorm, **kwargs)]
            snorm = step.normalize(snorm, **kwargs)
        final_text_to_remove.sort(key=lambda x: x[0])
        return final_text_to_remove

    @staticmethod
    def merge_removal_inds(*all_removal_inds):
        combined_removal_inds = reduce(lambda a, b: a + b, all_removal_inds, [])
        combined_removal_inds.sort(key=lambda x: x[0][0])
        merged_removal_inds = []
        for curr_inds, curr_repl in combined_removal_inds:
            if len(merged_removal_inds) == 0:
                merged_removal_inds += [(curr_inds, curr_repl)]
                continue
            last_inds, last_repl = merged_removal_inds[-1]
            if curr_inds[0] >= last_inds[1]:
                # If current interval doesn't overlap with the last interval in result, append it
                merged_removal_inds += [(curr_inds, curr_repl)]
            else:
                # some sort of overlap
                curr_merged_inds = (last_inds[0], max(last_inds[1], curr_inds[1]))
                if curr_inds[0] == last_inds[0] and last_inds[1] <= curr_inds[1]:
                    # last is subset. use curr_repl
                    curr_merged_repl = curr_repl
                elif curr_inds[0] >= last_inds[0] and curr_inds[1] <= last_inds[1]:
                    # curr is subset. use last_repl
                    curr_merged_repl = last_repl
                else:
                    raise Exception(f"partial overlap. not sure how to reconcile. curr_inds: {curr_inds}. last_inds: {last_inds}")
                merged_removal_inds[-1] = (curr_merged_inds, curr_merged_repl)

        return merged_removal_inds


class TableReplaceNormalizer(AbstractNormalizer):

    def __init__(self, table: Dict[str, str]):
        """
        :param table: will replace every key with value in string
        """
        super().__init__()
        replace_pairs = sorted(table.items(), key=lambda x: len(x[0]), reverse=True)
        steps = [ReplaceNormalizer(old, new) for old, new in replace_pairs]
        self.step_composer = NormalizerComposer(steps=steps)

    def normalize(self, s, **kwargs):
        return self.step_composer.normalize(s)

    def find_text_to_remove(self, s, **kwargs):
        return self.step_composer.find_text_to_remove(s)


class FunctionNormalizer(AbstractNormalizer):
    """
    Normalize by arbitrary find_text_to_remove
    """

    def __init__(self, custom_find_text_to_remove):
        super().__init__()
        self.custom_find_text_to_remove = custom_find_text_to_remove

    def find_text_to_remove(self, s: str, **kwargs) -> list:
        return self.custom_find_text_to_remove(s, **kwargs)


class NormalizerFactory:
    key_normalizer_map = {
        "html": RegexNormalizer(r"\s*<[^>]+>\s*", " "),
        "cantillation": RegexNormalizer("[\u0591-\u05bd\u05bf-\u05c5\u05c7]+", ""),
        "parens-plus-contents": RegexNormalizer(r"\([^)]+\)", " "),
        "brackets": RegexNormalizer(r"[\[\]]", ""),
        "kri-ktiv": RegexNormalizer(r'\[[^\[\]]{1,7}\]', ""),  # approximation for length of ktiv
        "english": RegexNormalizer(r'[A-Za-z]+', ""),
        "punctuation": RegexNormalizer(r'[.,"?!:]+', ""),
        "hashem": RegexNormalizer(r"(^|\s)([\u05de\u05e9\u05d5\u05db\u05dc\u05d1]?)(?:\u05d4['\u05f3]|\u05d9\u05d9)($|\s)", "\1\2\u05d9\u05d4\u05d5\u05d4\3"),
        "elokim": RegexNormalizer(r"(^|\s)([\u05de\u05e9\u05d5\u05db\u05dc\u05d1]?)(?:\u05d0\u05dc\u05e7\u05d9\u05dd)($|\s)", "\1\2\u05d0\u05dc\u05d4\u05d9\u05dd\3"),
        "unidecode": TableReplaceNormalizer(UNIDECODE_TABLE),
        "maqaf": ReplaceNormalizer('', ' '),
        "itag": ITagNormalizer(' '),
        "br-tag": ReplaceNormalizer('<br>', '<br/>'),
        "double-space": RegexNormalizer(r"\s+", " "),
    }

    @classmethod
    def get(cls, normalizer_key: str) -> AbstractNormalizer:
        return cls.key_normalizer_map[normalizer_key]

    @classmethod
    def get_all(cls, step_keys: List[str]) -> List[AbstractNormalizer]:
        cls.validate_keys(step_keys)
        return [cls.get(key) for key in step_keys]

    @classmethod
    def validate_keys(cls, step_keys: List[str]):
        if step_keys is None:
            raise Exception("step_keys and steps cannot both be None")
        nonexistant_keys = []
        for key in step_keys:
            if key not in cls.key_normalizer_map:
                nonexistant_keys += [key]
        if len(nonexistant_keys) > 0:
            raise Exception(f"Couldn't find the following keys in NormalizerComposer.key_normalizer_map:", ", ".join(nonexistant_keys))


class NormalizerByLang(AbstractNormalizer):

    def __init__(self, normalizers_by_lang: Dict[str, AbstractNormalizer]):
        """
        :param normalizers_by_lang: dict with keys that are letter lang codes (usually "en" or "he") and values that are subclasses of AbstractNormalizer
        """
        super().__init__()
        self.normalizers_by_lang = normalizers_by_lang

    def normalize(self, s: str, **kwargs) -> str:
        """
        :param lang: passed through kwargs. two letter lang code (usually "en" or "he") indicating which normalizer to apply
        """
        lang = kwargs.get('lang')
        if lang not in self.normalizers_by_lang: return s
        return self.normalizers_by_lang[lang].normalize(s, **kwargs)

    def find_text_to_remove(self, s:str, **kwargs) -> list:
        """
        :param lang: passed through kwargs. two letter lang code (usually "en" or "he") indicating which normalizer to apply
        """
        lang = kwargs.get('lang')
        if lang not in self.normalizers_by_lang: return []
        return self.normalizers_by_lang[lang].find_text_to_remove(s, **kwargs)


"""
Normalization tools for mapping chars to words and vice versa
"""


def char_indices_from_word_indices(input_string, word_ranges, split_regex=None):
    """
    ***Important***
    We use regular expression matching to solve this problem. We use the regex \s+ as default. This *should* replicate
    the behavior of str.split(), but use this with caution. It would be advisable to send the exact regex that was used
    to split the string in the first place.

    :param input_string: Original string that was split into a word list

    :param word_ranges: list of tuples, where each tuple represents a range of words from the word list.
    (first_word, last_word) where last_word is the actual index of the last word
    (the range of words would be word_list[first_word:last_word+1]).
    This matches the results returned from dibbur_hamtchil_matcher.match_text

    :param split_regex: Regular expression pattern to split. If none is supplied will use r'\s+'. see note above.
    :return:
    """

    if not split_regex:
        split_regex = r'\s+'
    regex = re.compile(split_regex)
    regex_normalizer = RegexNormalizer(split_regex, '')
    split_words = regex.split(input_string)
    count, word_indices = 0, []
    for word in split_words:
        start = count
        count += len(word)
        end = count
        word_indices.append((start, end))
    normalized_char_indices = []
    for i, words in enumerate(word_ranges):
        first_word, last_word = [w if w < len(word_indices) else -1 for w in words]
        normalized_char_indices.append(
            (
                word_indices[first_word][0] if first_word >=0 else -1,
                word_indices[last_word][1] if last_word >= 0 else -1
            )
        )
    return regex_normalizer.norm_to_unnorm_indices(input_string, normalized_char_indices)


@lru_cache(maxsize=32)
def get_word_indices(input_string, split_regex=r'\s+'):
    """
    helper method for word_index_from_char_index. Broken out for memoization purposes
    """
    return [r.end() for r in re.finditer(split_regex, input_string)]


def word_index_from_char_index(full_string, char_index, split_regex=r'\s+'):
    word_indices = get_word_indices(full_string, split_regex)
    return bisect_right(word_indices, char_index) if char_index >= 0 else -1


def sanitized_words_to_unsanitized_words(input_string, sanitized_string, sanitization_method, sanitized_word_ranges):
    normalizer = FunctionNormalizer(sanitization_method)
    sanitized_char_ranges = char_indices_from_word_indices(sanitized_string, sanitized_word_ranges)
    unsanitzied_char_ranges = normalizer.norm_to_unnorm_indices(input_string, sanitized_char_ranges)
    # for char_range in unsanitied_char_ranges:
    #     word_range = tuple(word_index_from_char_index(input_string, i) for i in char_range)
    #     stuff.append(word_range)
    return [tuple(word_index_from_char_index(input_string, i) for i in char_range)
            for char_range in unsanitzied_char_ranges]


class TextSanitizer:
    """
    This class is designed so we can easily move from a list of segments to the flat list of words necessary
    for use in dibbur_hamatchil_matcher.match_text. It is primarily helpful when we need to keep track of text before and after edits were
    made to said text that were necessary for improving text matching.
    """
    def __init__(self, section: List[str], divider_pattern: str):
        self._original_segments = tuple(section)
        self._sanitized_segments = None
        self.sanitizer = None
        self._dividing_expression = divider_pattern

        # these variables hold the indices of the first word for each segment
        self._sanitzed_word_indices = None
        self._unsanitized_word_indices = None
        self._set_unsanitzed_word_indices()

    def get_original_segments(self):
        return self._original_segments

    def set_sanitizer(self, sanitizer: Callable[[str], str]):
        self.sanitizer = sanitizer

    def sanitize(self):
        if not self.sanitizer:
            raise AttributeError("no sanitization method set for this instance")
        self._sanitized_segments = tuple(self.sanitizer(x) for x in self._original_segments)
        self._set_sanitized_word_indices()

    def get_sanitized_segments(self):
        if self.sanitizer and not self._sanitized_segments:
            self.sanitize()
        return self._sanitized_segments

    def _set_unsanitzed_word_indices(self):
        self._unsanitized_word_indices = self.get_segment_start_indices(
            self._original_segments, self._dividing_expression)

    def _set_sanitized_word_indices(self):
        self._sanitzed_word_indices = self.get_segment_start_indices(
            self._sanitized_segments, self._dividing_expression
        )

    def get_unsanitized_word_indices(self):
        return tuple(self._unsanitized_word_indices)

    def get_sanitized_word_indices(self):
        if self._sanitzed_word_indices:
            return tuple(self._sanitzed_word_indices)
        elif self.sanitizer:
            self.sanitize()
            return tuple(self._sanitzed_word_indices)
        else:
            raise AttributeError('Cannot get sanitied word indices: No sanitizer set')

    def set_dividing_expression(self, regex_pattern: str):
        self._dividing_expression = regex_pattern

    @staticmethod
    def make_word_list(section, dividing_expression):
        word_list = []
        for segment in section:
            segment_list = re.split(dividing_expression, segment)
            word_list.extend(segment_list)
        return word_list

    def get_sanitized_word_list(self):
        if not self._sanitized_segments:
            if self.sanitizer:
                self.sanitize()
            else:
                raise AttributeError("Sanitizer not set")
        return self.make_word_list(self._sanitized_segments, self._dividing_expression)

    def get_unsanitized_word_list(self):
        return self.make_word_list(self._original_segments, self._dividing_expression)

    @staticmethod
    def get_segment_start_indices(segment_list, divider_pattern):
        """
        Calculates the word number at which each segment starts. Helpful if trying to move from a flat list of words
        back to a segment division.
        :param segment_list:
        :param divider_pattern:
        :return:
        """
        segment_start_indices = []
        word_count = 0
        for segment in segment_list:
            segment_start_indices.append(word_count)
            word_count += len(re.split(divider_pattern, segment))

        return segment_start_indices

    @staticmethod
    def get_segment_index_from_word_index(word_index, start_segment_list):
        return bisect_right(start_segment_list, word_index) - 1

    def check_sanitized_index(self, word_index: int):
        """
        given a word index from a sanitized word list, find what segment it originated from
        """
        return self.get_segment_index_from_word_index(word_index, self._sanitzed_word_indices)

    def check_unsanitized_word_index(self, word_index:int):
        return self.get_segment_index_from_word_index(word_index, self._unsanitized_word_indices)
```

### sefaria/helper/linker_index_converter.py

```
from functools import partial
import re
import unicodedata
from pymongo import InsertOne
from tqdm import tqdm
from sefaria.model.text import library, Index, IndexSet, Ref, VersionSet
from sefaria.model.schema import Term, NonUniqueTerm, TitleGroup
from sefaria.system.exceptions import InputError
from sefaria.model.version_state import StateNode
from sefaria.system.database import db
from sefaria.utils.hebrew import strip_cantillation, has_hebrew

"""
Utility classes for converting Indexes so they are discoverable by Linker.v3
"""


class ReusableTermManager:
    """
    Handles creation of NonUniqueTerms and stores them for reuse
    """

    def __init__(self):
        self.context_and_primary_title_to_term = {}
        self.num_to_perek_term_map = {}
        self.old_term_map = {}

    def get_term_by_primary_title(self, context, title):
        return self.context_and_primary_title_to_term.get((context, title))

    def get_term_by_old_term_name(self, old_term_name):
        return self.old_term_map.get(old_term_name)

    def get_perek_term_by_num(self, perek_num):
        return self.num_to_perek_term_map.get(perek_num)

    def create_term(self, **kwargs):
        """

        @param kwargs:
            'en'
            'he'
            'alt_en'
            'alt_he'
            'context'
        @return:
        """
        slug = kwargs.get('en', kwargs.get('he'))
        term = NonUniqueTerm({
            "slug": slug,
            "titles": []
        })
        for lang in ('en', 'he'):
            if kwargs.get(lang, False):
                term.title_group.add_title(kwargs.get(lang), lang, primary=True)
            for title in kwargs.get(f"alt_{lang}", []):
                term.title_group.add_title(title, lang)

        if kwargs.get('delete_if_existing'):
            slug = NonUniqueTerm.normalize_slug(term.slug)
            existing_term = NonUniqueTerm.init(slug)
            if existing_term:
                existing_term.delete()
        term.save()
        self.context_and_primary_title_to_term[(kwargs.get('context'), term.get_primary_title('en'))] = term
        return term

    def get_or_create_term_for_titled_obj(self, obj, context=None, new_alt_titles=None, title_modifier=None, title_adder=None):
        term = self.get_existing_term_for_titled_obj(obj, new_alt_titles, title_modifier, title_adder)
        if not term:
            return self.create_term_from_titled_obj(obj, context, new_alt_titles, title_modifier, title_adder)
        return term

    def get_existing_term_for_titled_obj(self, obj, new_alt_titles=None, title_modifier=None, title_adder=None):
        en_title, he_title, alt_en_titles, alt_he_titles = self._make_titles_for_term(obj, new_alt_titles,
                                                                                      title_modifier, title_adder)
        return NonUniqueTerm().load({"titles.text": {"$all": [en_title, he_title] + alt_en_titles + alt_he_titles}})

    def create_term_from_titled_obj(self, obj, context=None, new_alt_titles=None, title_modifier=None, title_adder=None):
        """
        Create a NonUniqueTerm from 'titled object' (see explanation of `obj` param)
        Accepts params to modify or add new alt titles
        @param obj: either of instance `TitleGroup` or has an attribute `title_group` (e.g. a `Term` or `SchemaNode` has this field)
        @param context: Optional string (or any hashable object) to distinguish terms with the same primary title. For use with `get_term_by_primary_title()`
        @param new_alt_titles: list[str]. optional list of alt titles to add. will auto-detect language of title.
        @param title_modifier: function(lang, str) -> str. given lang and current alt title, replaces alt title with return value. Useful for removing common prefixes such as "Parshat" or "Mesechet"
        @param title_adder: function(lang, str) -> str. given lang and current alt title, returns new alt title. If returns None, no alt title is added for given title. Useful for creating variations on existing alt titles.
        @return: new NonUniqueTerm

        Example:

        .. highlight:: python
        .. code-block:: python

            # make NonUniqueTerm based on index node of "Genesis"
            # remove leading "Sefer " from all alt titles
            # add new titles that replace "sh" with ""

            def title_modifier(lang, title):
                if lang == "en":
                    return re.sub(r"^Sefer ", "", title)
                return title

            def title_adder(lang, title):
                if "sh" in title:
                    return title.repalce("sh", "")

            index = library.get_index("Genesis")
            gen_term = ReusableTermManager.create_term_from_titled_obj(
                index.nodes, "structural", ["Bershis", "Breiis"],
                title_modifier, title_adder
            )

        ...

        """
        en_title, he_title, alt_en_titles, alt_he_titles = self._make_titles_for_term(obj, new_alt_titles,
                                                                                      title_modifier, title_adder)
        term = self.create_term(en=en_title, he=he_title, context=context, alt_en=alt_en_titles, alt_he=alt_he_titles)
        if isinstance(obj, Term):
            self.old_term_map[obj.name] = term
        return term

    @staticmethod
    def _make_titles_for_term(obj, new_alt_titles=None, title_modifier=None, title_adder=None):
        new_alt_titles = new_alt_titles or []
        title_group = obj if isinstance(obj, TitleGroup) else obj.title_group
        en_title = title_group.primary_title('en')
        he_title = title_group.primary_title('he')
        if not (en_title and he_title):
            raise InputError("title group has no primary titles. can't create term.")
        alt_en_titles = [title for title in title_group.all_titles('en') if title != en_title]
        alt_he_titles = [title for title in title_group.all_titles('he') if title != he_title]
        if title_modifier:
            en_title = title_modifier('en', en_title)
            he_title = title_modifier('he', he_title)
        for new_alt in new_alt_titles:
            if has_hebrew(new_alt):
                alt_he_titles += [new_alt]
            else:
                alt_en_titles += [new_alt]
        for alt_title_list, lang in zip((alt_en_titles + [en_title], alt_he_titles + [he_title]), ('en', 'he')):
            if title_adder:
                new_alt_titles = [title_adder(lang, alt_title) for alt_title in alt_title_list]
                alt_title_list += list(filter(None, new_alt_titles))
            if title_modifier:
                alt_title_list[:] = [title_modifier(lang, t) for t in alt_title_list]
        # make unique
        alt_en_titles = list(set(alt_en_titles))
        alt_he_titles = list(set(alt_he_titles))
        return en_title, he_title, alt_en_titles, alt_he_titles


class LinkerCategoryConverter:
    """
    Manager which handles converting all indexes in a category or corpus.
    """

    def __init__(self, title, is_corpus=False, is_index=False, include_dependant=False, **linker_index_converter_kwargs):
        index_getter = library.get_indexes_in_corpus if is_corpus else library.get_indexes_in_category
        self.titles = [title] if is_index else index_getter(title, include_dependant=include_dependant)
        self.linker_index_converter_kwargs = linker_index_converter_kwargs

    def convert(self):
        for title in self.titles:
            index_converter = LinkerIndexConverter(title, **self.linker_index_converter_kwargs)
            index_converter.convert()


class LinkerCommentaryConverter:
    """
    Handles conversion of all commentaries on a base text
    """

    def __init__(self, base_text_title, get_match_template_suffixes, get_commentary_term, **linker_index_converter_kwargs):
        self.titles = [index.title for index in IndexSet({"base_text_titles": base_text_title})]
        self.linker_index_converter_kwargs = linker_index_converter_kwargs
        self.get_match_template_suffixes = get_match_template_suffixes
        self.get_commentary_term = get_commentary_term
        self.get_match_templates_inner = linker_index_converter_kwargs['get_match_templates']
        base_index = library.get_index(base_text_title)
        linker_index_converter_kwargs['get_match_templates'] = partial(self.get_match_templates_wrapper, base_index)

    def get_match_templates_wrapper(self, base_index, node, depth, isibling, num_siblings, is_alt_node):
        if self.get_match_templates_inner:
            match_templates = self.get_match_templates_inner(base_index, node, depth, isibling, num_siblings, is_alt_node)
            if match_templates is not None:
                return match_templates

        # otherwise, use default implementation
        if is_alt_node or not node.is_root(): return "NO-OP"
        try: comm_term = self.get_commentary_term(node.index.collective_title)
        except: return "NO-OP"
        if comm_term is None: return "NO-OP"
        if self.get_match_template_suffixes is None: return "NO-OP"

        match_templates = [template.clone() for template in self.get_match_template_suffixes(base_index)]
        for template in match_templates:
            template.term_slugs = [comm_term.slug] + template.term_slugs
        return match_templates

    def convert(self):
        for title in self.titles:
            index_converter = LinkerIndexConverter(title, **self.linker_index_converter_kwargs)
            index_converter.convert()


class DiburHamatchilAdder:
    """
    Handles extraction of dibur hamatchils from indexes and saves them to dibur_hamatchils collection
    """

    BOLD_REG = "^<b>(.+?)</b>"
    DASH_REG = '^(.+?)[\-]'

    def __init__(self):
        self.indexes_with_dibur_hamatchils = []
        self.dh_reg_map = {
            "Rashi|Bavli": [self.DASH_REG, '\.(.+?)$', "^(?:(?:'|')\s?)?(.+)$"],
            "Ran|Bavli": [self.DASH_REG, "^(?:(?:'|')\s?)?(.+)$"],
            "Tosafot|Bavli": [self.DASH_REG, "^(?:(?:'|')\s?)?(.+)$"],
        }
        self._dhs_to_insert = []

    def get_dh_regexes(self, collective_title, context=None, use_default_reg=True):
        if collective_title is None:
            return
        key = collective_title + ("" if context is None else f"|{context}")
        dh_reg = self.dh_reg_map.get(key)
        if not dh_reg and use_default_reg:
            return [self.BOLD_REG, self.DASH_REG]
        return dh_reg

    def add_index(self, index):
        self.indexes_with_dibur_hamatchils += [index]

    @staticmethod
    def get_dh(s, regexes, oref):
        matched_reg = False
        s = s.strip()
        for reg in regexes:
            match = re.search(reg, s)
            if not match: continue
            matched_reg = True
            s = match.group(1)
        if not matched_reg: return
        s = s.strip()
        s = unicodedata.normalize('NFKD', s)
        s = strip_cantillation(s, strip_vowels=True)
        s = re.sub(r"[.,:;\-]", "", s)
        words = s.split()
        return " ".join(words[:5])  # DH is unlikely to give more info if longer than 5 words

    def get_container_refs(self, title, segment_ref, perek_refs):
        curr_ref = segment_ref.top_section_ref()
        container_refs = [title]
        is_first = True
        for section, is_referenceable in zip(segment_ref.sections[:-1], getattr(segment_ref.index_node, "referenceableSections", [True]*len(segment_ref.sections))[:-1]):
            if is_first:
                # avoid issues with default nodes. start at top_section_ref()
                is_first = False
            else:
                curr_ref = curr_ref.subref(section)
            if is_referenceable:
                container_refs += [curr_ref.normal()]
        perek_ref = None
        for temp_perek_ref in perek_refs:
            assert isinstance(temp_perek_ref, Ref)
            if temp_perek_ref.contains(segment_ref):
                perek_ref = temp_perek_ref
                break
        if perek_ref:
            container_refs += [perek_ref.normal()]

        return container_refs

    def add_dh_for_seg(self, perek_refs, title, segment_text, en_tref, he_tref, version):
        try:
            oref = Ref(en_tref)
        except:
            print("not a valid ref", en_tref)
            return
        if not getattr(oref.index_node, "diburHamatchilRegexes", None): return
        dh = self.get_dh(segment_text, oref.index_node.diburHamatchilRegexes, oref)
        if not dh: return
        container_refs = self.get_container_refs(title, oref, perek_refs)
        self._dhs_to_insert += [
            {
                "dibur_hamatchil": dh,
                "container_refs": container_refs,
                "ref": en_tref
            }
        ]

    def add_dibur_hamatchil_to_index(self, index):

        index = Index().load({"title": index.title})  # reload index to make sure perek nodes are correct
        perek_refs = []
        for perek_node in index.get_alt_struct_leaves():
            perek_ref = Ref(perek_node.wholeRef)
            perek_refs += [perek_ref]

        versions = VersionSet({"title": index.title, "language": "he"}).array()
        if len(versions) == 0:
            print("No versions for", index.title, ". Can't search for DHs.")
            return
        primary_version = versions[0]
        action = partial(self.add_dh_for_seg, perek_refs, index.title)
        primary_version.walk_thru_contents(action)

    def add_all_dibur_hamatchils(self):
        db.dibur_hamatchils.delete_many({})
        for index in tqdm(self.indexes_with_dibur_hamatchils, desc='add dibur hamatchils'):
            self.add_dibur_hamatchil_to_index(index)
        db.dibur_hamatchils.bulk_write([InsertOne(d) for d in self._dhs_to_insert])


class LinkerIndexConverter:
    """
    Handles conversion of single Index to Linker.v3 format
    """

    def __init__(self, title, get_other_fields=None, get_match_templates=None, get_alt_structs=None,
                 fast_unsafe_saving=True, get_commentary_match_templates=None, get_commentary_other_fields=None,
                 get_commentary_match_template_suffixes=None, get_commentary_alt_structs=None, get_commentary_term=None):
        """

        @param title: title of index to convert
        @param get_other_fields: function of form
            (node: SchemaNode, depth: int, isibling: int, num_siblings: int, is_alt_node: bool) -> dict.
            Returns a dict where keys are other fields to modify. These can be any valid key on `node`
            Some common examples are below. Many of them are documented at the top of this file.
                - isSegmentLevelDiburHamatchil
                - referenceableSections
                - diburHamatchilRegexes
                - numeric_equivalent
                - ref_resolver_context_swaps
            Can return None for any of these
            See top of file for documentation for these fields
        @param get_match_templates:
            function of form
                (node: SchemaNode, depth: int, isibling: int, num_siblings: int, is_alt_node: bool) -> List[MatchTemplate].
            Callback that is run on every node in index including alt struct nodes. Receives callback params as specified above.
            Needs to return a list of MatchTemplate objects corresponding to that node.
        @param get_alt_structs:
            function of form
                (index: Index) -> Dict[String, TitledTreeNode]
            Returns a dict with keys being names of new alt struct and values being alt struct root nodes
        @param get_commentary_match_templates:
            function of form
                (index: Index) -> List[MatchTemplate]
            Callback that is run on every commentary index of this base text.
            Return value is equivalent to that of `get_match_templates()`
        @param get_commentary_other_fields:
            function of form
                (index: Index) -> dict
            Callback that is run on every commentary index of this base text.
            Return value is equivalent to that of `get_other_fields()`
        @param fast_unsafe_saving: If true, skip Python dependency checks and save directly to Mongo (much faster but potentially unsafe)
        """
        self.index = library.get_index(title)
        self.get_other_fields = get_other_fields
        self.get_match_templates = get_match_templates
        self.get_alt_structs = get_alt_structs
        self.get_commentary_match_templates = get_commentary_match_templates
        self.get_commentary_other_fields = get_commentary_other_fields
        self.get_commentary_match_template_suffixes = get_commentary_match_template_suffixes
        self.get_commentary_alt_structs = get_commentary_alt_structs
        self.get_commentary_term = get_commentary_term
        self.fast_unsafe_saving = fast_unsafe_saving

    @staticmethod
    def _traverse_nodes(node, callback, depth=0, isibling=0, num_siblings=0, is_alt_node=False, **kwargs):
        callback(node, depth, isibling, num_siblings, is_alt_node, **kwargs)
        [LinkerIndexConverter._traverse_nodes(child, callback, depth + 1, jsibling, len(node.children), is_alt_node, **kwargs) for (jsibling, child) in enumerate(node.children)]

    def _update_lengths(self):
        if self.index.is_complex(): return
        sn = StateNode(self.index.title)
        ac = sn.get_available_counts("he")
        # really only outer shape is checked. including rest of shape even though it's technically only a count of what's available and skips empty sections
        shape = sn.var('all', 'shape')
        outer_shape = shape if isinstance(shape, int) else len(shape)
        if getattr(self.index, 'dependence', None) == 'Commentary' and getattr(self.index, 'base_text_titles', None):
            if self.index.base_text_titles[0] == 'Shulchan Arukh, Even HaEzer':
                outer_shape = 178
            else:
                sn = StateNode(self.index.base_text_titles[0])
                shape = sn.var('all', 'shape')
                base_outer_shape = shape if isinstance(shape, int) else len(shape)
                if base_outer_shape > outer_shape:
                    outer_shape = base_outer_shape
        self.index.nodes.lengths = [outer_shape] + ac[1:]

    @staticmethod
    def get_all_alt_struct_nodes(index):
        def alt_struct_nodes_helper(node, nodes):
            nodes.append(node)
            for child in node.children:
                alt_struct_nodes_helper(child, nodes)

        nodes = []
        for node in index.get_alt_struct_roots():
            alt_struct_nodes_helper(node, nodes)
        return nodes

    def convert(self):
        if self.get_alt_structs:
            alt_struct_dict = self.get_alt_structs(self.index)
            if alt_struct_dict:
                for name, root in alt_struct_dict.items():
                    self.index.set_alt_structure(name, root)
        self._traverse_nodes(self.index.nodes, self.node_visitor, is_alt_node=False)
        alt_nodes = self.get_all_alt_struct_nodes(self.index)
        for inode, node in enumerate(alt_nodes):
            self.node_visitor(node, 1, inode, len(alt_nodes), True)
        self._update_lengths()  # update lengths for good measure
        if self.get_commentary_match_templates or self.get_commentary_match_template_suffixes or self.get_commentary_other_fields:
            temp_get_comm_fields = partial(self.get_commentary_other_fields, self.index) \
                if self.get_commentary_other_fields else None
            temp_get_alt_structs = partial(self.get_commentary_alt_structs, self.index) \
                if self.get_commentary_alt_structs else None
            comm_converter = LinkerCommentaryConverter(self.index.title, self.get_commentary_match_template_suffixes,
                                                       self.get_commentary_term,
                                                       get_match_templates=self.get_commentary_match_templates,
                                                       get_other_fields=temp_get_comm_fields,
                                                       get_alt_structs=temp_get_alt_structs)
            comm_converter.convert()
        self.save_index()

    def save_index(self):
        if self.fast_unsafe_saving:
            props = self.index._saveable_attrs()
            db.index.replace_one({"_id": self.index._id}, props, upsert=True)
        else:
            self.index.save()

    def node_visitor(self, node, depth, isibling, num_siblings, is_alt_node):
        if self.get_match_templates:
            templates = self.get_match_templates(node, depth, isibling, num_siblings, is_alt_node)
            if templates == "NO-OP":
                pass
            elif templates is not None:
                node.match_templates = [template.serialize() for template in templates]
            else:
                # None
                try:
                    delattr(node, 'match_templates')
                except:
                    pass

        if self.get_other_fields:
            other_fields_dict = self.get_other_fields(node, depth, isibling, num_siblings, is_alt_node)
            if other_fields_dict is not None:
                for key, val in other_fields_dict.items():
                    if val is None: continue
                    if val == "DELETE!":
                        delattr(node, key)
                    else:
                        setattr(node, key, val)

```

### sefaria/helper/descriptions.py

```
# encoding=utf-8


import re
import csv
import requests
from io import StringIO
from collections import defaultdict

from sefaria.system.database import db
from sefaria.model import *
from sefaria.system.exceptions import DuplicateRecordError
from sefaria.model.abstract import SluggedAbstractMongoRecord

def create_era_link(topic, prev_era_to_delete=None):
    era_slug_map = {
        "GN": "geon-person",
        "RI": "rishon-person",
        "AH": "achron-person",
        "CO": "modern-person",
        "KG": "mishnaic-people",
        "PT": "mishnaic-people",
        "T": "mishnaic-people",
        "A": "talmudic-people",
    }

    isa_object_aggregate_map = {
        'tosafot': 'group-of-rishon-people'
    }

    if prev_era_to_delete:
        to_topic = isa_object_aggregate_map.get(topic.slug, era_slug_map[prev_era_to_delete])
        prev_link = IntraTopicLink().load({'toTopic': to_topic, 'fromTopic': topic.slug, 'linkType': 'is-a'})
        if prev_link:
            prev_link.delete()

    to_topic = isa_object_aggregate_map.get(topic.slug, era_slug_map[topic.get_property('era')])
    itl = IntraTopicLink({
        "toTopic": to_topic,
        "fromTopic": topic.slug,
        "linkType": "is-a",
        "dataSource": "sefaria",
        "generatedBy": "update_authors_data"
    })
    itl.save()



def update_authors_data():
    """
    0 key
    1 'Primary English Name'
    2 'Secondary English Names'
    3 'Primary Hebrew Name'
    4 'Secondary Hebrew Names'
    5 'Birth Year '
    6 'Birth Place'
    7 'Death Year'
    8 'Death Place'
    9 'Halachic Era'
    10'English Biography'
    11'Hebrew Biography'
    12'English Wikipedia Link'
    13'Hebrew Wikipedia Link'
    14'Jewish Encyclopedia Link'
    ...
    24 'Sex'"
    """

    eras = {
        "Gaonim": "GN",
        "Rishonim": "RI",
        "Achronim": "AH",
        "Contemporary": "CO"
    }

    url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSx60DLNs8Dp0l2xpsPjrxD3dBpIKASXSBiE-zjq74SvUIc-hD-mHwCxsuJpQYNVHIh7FDBwx7Pp9zR/pub?gid=0&single=true&output=csv'
    response = requests.get(url)
    data = response.content.decode("utf-8")
    cr = csv.reader(StringIO(data))
    rows = list(cr)[4:]
    response_texts = []
    error_texts = []

    # Validate every slug is unique and doesn't exist as a non-author
    internal_slug_count = defaultdict(int)
    has_slug_issues = False
    for l in rows:
        slug = l[0].encode('utf8').decode()
        primary_title = l[1].strip() if len(l[1].strip()) > 0 else l[3].strip()
        if re.search(fr'^{re.escape(SluggedAbstractMongoRecord.normalize_slug(primary_title))}\d*$', slug) is None:
            error_texts.append(f"ERROR: slug '{slug}' does not match primary title '{primary_title}'. Expected slug '{SluggedAbstractMongoRecord.normalize_slug(primary_title)}'")
            has_slug_issues = True
        if len(l[9]) == 0:
            error_texts.append(f"ERROR: slug '{slug}' must have column 'Halachic Era' filled in.")
            has_slug_issues = True
        if len(slug.strip()) == 0: continue
        internal_slug_count[slug] += 1
    for slug, count in internal_slug_count.items():
        if count > 1:
            error_texts.append(f"ERROR: slug {slug} appears {count} times on this sheet. Please update slug in sheet to be internally unique")
            has_slug_issues = True
        non_author = Topic().load({"slug": slug, "subclass": {"$ne": "author"}})
        if non_author is not None:
            error_texts.append(f"ERROR: slug {slug} exists as a non-author. Please update slug in sheet to be globally unique.")
            has_slug_issues = True
        if SluggedAbstractMongoRecord.normalize_slug(slug) != slug:
            error_texts.append(f"ERROR: slug '{slug}' does not match slugified version which is '{SluggedAbstractMongoRecord.normalize_slug(slug)}'. Please slugify in the sheet.")
            has_slug_issues = True
    if has_slug_issues:
        return ["Errors:"] + error_texts + ["Please Correct these errors and re-run the update:"]

    response_texts.append("*** Deleting old authorTopic relationships ***")
    link_query = {"generatedBy": "update_authors_data"}
    db.topic_links.delete_many(link_query)
    response_texts.append(f"Links Deleted '{db.topic_links.count_documents(link_query)}'", )

    # Dependencies take too long here.  Getting rid of relationship dependencies above.  Assumption is that we'll import works right after to handle those dependencies.

    def _(p: Topic, attr, value):
        if value:
            p.set_property(attr, value, "sefaria")

    response_texts.append("*** Updating authorTopic records ***")
    for irow, l in enumerate(rows):
        slug = l[0].encode('utf8').decode()
        if len(slug.strip()) == 0: continue
        # print(slug)
        p = AuthorTopic.init(slug) or AuthorTopic()
        p.slug = slug
        p.title_group.add_title(l[1].strip(), "en", primary=True, replace_primary=True)
        p.title_group.add_title(l[3].strip(), "he", primary=True, replace_primary=True)
        for x in l[2].split(","):
            x = x.strip()
            if len(x):
                p.title_group.add_title(x, "en")
        for x in l[4].split(","):
            x = x.strip()
            if len(x):
                p.title_group.add_title(x, "he")
        if len(l[5]) > 0:
            if "c" in l[5]:
                _(p, 'birthYearIsApprox', True)
            else:
                _(p, 'birthYearIsApprox', False)
            m = re.search(r"\d+", l[5])
            if m:
                _(p, 'birthYear', m.group(0))
        if len(l[7]) > 0:
            if "c" in l[7]:
                _(p, 'deathYearIsApprox', True)
            else:
                _(p, 'deathYearIsApprox', False)
            m = re.search(r"\d+", l[7])
            if m:
                _(p, 'deathYear', m.group(0))
        _(p, "birthPlace", l[6])
        _(p, "deathPlace", l[8])
        _(p, "era", eras.get(l[9]))
        _(p, "enBio", l[10])
        _(p, "heBio", l[11])
        _(p, "enWikiLink", l[12])
        _(p, "heWikiLink", l[13])
        _(p, "jeLink", l[14])
        _(p, "sex", l[24])
        if p.get_property('enBio') or p.get_property('heBio'):
            p.description = {
                'en': p.get_property('enBio'),
                'he': p.get_property('heBio')
            }
            p.description_published = True
        p.save()

        # metadata links
        try:
            IntraTopicLink({
                "toTopic": 'authors',
                "fromTopic": p.slug,
                "generatedBy": "update_authors_data",
                "dataSource": "sefaria",
                "linkType": "displays-under"
            }).save()
        except DuplicateRecordError as e:
            error_texts.append(str(e))

        if p.get_property('era'):
            try:
                create_era_link(p)
            except DuplicateRecordError as e:
                error_texts.append(str(e))

    # Second Pass
    rowmap = {
        16: 'child-of',
        17: 'grandchild-of',
        18: 'child-in-law-of',
        19: 'sibling-in-law-of',
        20: 'taught',
        21: 'member-of',
        22: 'corresponded-with',
        23: 'opposed',
        24: 'cousin-of',
    }
    flip_link_dir = {'taught'}
    response_texts.append("\n*** Adding relationships ***\n")
    for l in rows:
        from_slug = l[0].encode('utf8').decode()
        p = AuthorTopic.init(from_slug)
        for i, link_type_slug in rowmap.items():
            if l[i]:
                for pkey in l[i].split(","):
                    to_slug = pkey.strip().encode('utf8').decode()
                    to_slug, from_slug = (from_slug, to_slug) if link_type_slug in flip_link_dir else (to_slug, from_slug)
                    # print("{} - {}".format(from_slug, to_slug))
                    if AuthorTopic.init(to_slug) and AuthorTopic.init(from_slug):
                        try:
                            IntraTopicLink({
                                "toTopic": to_slug,
                                "fromTopic": from_slug,
                                "linkType": link_type_slug,
                                "dataSource": "sefaria",
                                "generatedBy": "update_authors_data",
                            }).save()
                        except DuplicateRecordError:
                            continue

    link_query = {"generatedBy": "update_authors_data"}
    response_texts.append(f"links created '{db.topic_links.count_documents(link_query)}' ")
    
    return ["Errors:"] + error_texts + ["Updates:"] + response_texts 

def update_categories_data():
    """
    0 Category
    1 English Description
    2 Hebrew Description
    3 Short English Description
    4 Short Hebrew Description
    """
    response_texts = []
    error_texts = []
    url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSx60DLNs8Dp0l2xpsPjrxD3dBpIKASXSBiE-zjq74SvUIc-hD-mHwCxsuJpQYNVHIh7FDBwx7Pp9zR/pub?gid=1537266127&single=true&output=csv'
    response = requests.get(url)
    data = response.content.decode("utf-8")
    cr = csv.reader(StringIO(data))

    next(cr)
    for l in cr:
        updated = False
        path = l[0].split(",")
        c = Category().load({"path": path})
        if not c:
            error_texts.append("Unknown Category: {}".format(path))
            continue
        update_dict = {
            "enDesc"      : l[1].strip(),
            "heDesc"      : l[2].strip(),
            "enShortDesc" : l[3].strip(),
            "heShortDesc" : l[4].strip()
        }
        for key, value in update_dict.items():
            if value != getattr(c, key, ""):
                updated = True
                break
        if updated:
            c.load_from_dict(update_dict)
            c.save(override_dependencies=True)
            response_texts.append("Updated: {}".format(path))
    return ["Errors:"] + error_texts + ["Updates:"] + response_texts


def update_texts_data():
    response_texts = []
    error_texts = []

    """
    0  Primary English Title
    1  Author
    2  English Description
    3  Hebrew Description
    4  English Short Description 
    5  Hebrew Short Description
    6  Composition Year (loazi)
    7  Composition Year Margin of Error (+/- years)
    8  Place composed
    9  Year of first publication
    10 Place of first publication
    11 Era
    """
    eras = {
        "Gaonim": "GN",
        "Rishonim": "RI",
        "Achronim": "AH",
        "Tannaim": "T",
        "Amoraim": "A",
        "Contemporary": "CO"
    }

    url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSx60DLNs8Dp0l2xpsPjrxD3dBpIKASXSBiE-zjq74SvUIc-hD-mHwCxsuJpQYNVHIh7FDBwx7Pp9zR/pub?gid=480609494&single=true&output=csv'
    response = requests.get(url)
    data = response.content.decode("utf-8")
    cr = csv.reader(StringIO(data))

    rows = list(cr)[2:]
    indexes_handled = [row[0].strip() for row in rows]

    unhandled = set([i.primary_title() for i in library.get_index_forest()]) - set(indexes_handled)
    if len(unhandled) > 0:
        error_texts.append("Indexes not covered in the sheet:")
        for a in sorted(unhandled):
            error_texts.append(a)
        error_texts.append("\n******************\n")

    for l in rows:
        updated = False
        try:
            i = library.get_index(l[0].strip())
        except Exception as e:
            error_texts.append("Could not load {}. {}".format(l[0], e))
            continue
        try:
            current_authors = set(getattr(i, "authors", []) or [])
        except TypeError:
            current_authors = set()
        sheet_authors = set([a.strip() for a in l[1].split(",") if AuthorTopic.is_author(a.strip())])
        if sheet_authors != current_authors:
            setattr(i, "authors", list(sheet_authors))
            updated = True
        attrs = [("enDesc", l[2].strip()),
                 ("heDesc", l[3].strip()),
                 ("enShortDesc", l[4].strip()),
                 ("heShortDesc", l[5].strip()),
                 ("compDate", l[6].strip()),
                 ("errorMargin", l[7].strip()),
                 ("compPlace", l[8].strip()),  # composition place
                 ("pubDate", l[9].strip()),
                 ("pubPlace", l[10].strip()),  # publication place
                 ("era", eras.get(l[11].strip()))]

        for aname, value in attrs:
            obj_val = getattr(i, aname, "")
            if (obj_val or value) and (obj_val != value):
                setattr(i, aname, value)
                updated = True
        if updated:
            response_texts.append("Updated - {}".format(l[0]))
            i.save(override_dependencies=True)
            
    return ["Errors:"] + error_texts + ["Updates:"] + response_texts 
```

### sefaria/helper/category.py

```
from sefaria.model import *
from sefaria.system.exceptions import BookNameError
from sefaria import tracker
def move_index_into(index, cat):
    """
    :param index: (String)  The primary name of the Index to move.
    :param cat:  (model.Category or List) Category to move into - either a Category object, or a List with the path leading to the Category
    :return: None
    """
    if not isinstance(index, Index):
        try:
            index = library.get_index(index)
        except BookNameError:
            print("Can not find: {}".format(index))
            return
    if not isinstance(cat, Category):
        cat = Category().load({"path": cat})

    index.categories = cat.path[:]
    print("Moving - " + index.get_title() + " to " + str(index.categories) + " (move_index_into)")
    index.save(override_dependencies=True)


def rename_category(cat, en, he=None):
    """
    :param cat: (model.Category or List) Either a Category object or a list of category keys defining a category
    :param en:  (String)    The new English name of the category.  If `en`` is a key for a Term, the Term will be used.
    Otherwise, the `he` is required, and the two will be used to create a new Term.
    :param he:  (String, optional)
    :return: model.Category - the newly renamed Category
    """
    if not isinstance(cat, Category):
        cat = Category().load({"path": cat})
    assert isinstance(cat, Category)

    if en is None:
        raise Exception("Need en name for category {} renaming.".format(cat.path[-1]))

    old_category_path = cat.path[:]
    path_length = len(old_category_path)

    if not Term().load({"name": en}):
        if he is None:
            raise Exception("Need Hebrew term names for {}".format(en))
        print("adding term for " + en)
        term = Term()
        term.name = en
        term.add_primary_titles(en, he)
        term.scheme = "toc_categories"
        term.save()
    cat.add_shared_term(en)
    cat.path[-1] = en
    cat.lastPath = en
    print("Renaming category to {}".format(en))
    cat.save(override_dependencies=True)

    # move all matching categories
    clauses = [{"path." + str(i): cname} for i, cname in enumerate(old_category_path)]
    query = {"$and": clauses}
    for c in CategorySet(query):
        # replace old_parent_path with new_parent_path
        c.path = cat.path + c.path[path_length:]
        print("Saving moved category - " + str(c.path) + " (rename_category)")
        c.save(override_dependencies=True)

    # move all matching Indexes
    clauses = [{"categories." + str(i): cname} for i, cname in enumerate(old_category_path)]
    query = {"$and": clauses}
    for ind in IndexSet(query):
        assert isinstance(ind, Index)
        ind.categories = cat.path + ind.categories[path_length:]
        print("Moving - " + ind.get_title() + " to " + str(ind.categories) + " (rename_category)")
        ind.save(override_dependencies=True)

    return cat


def move_category_into(cat, parent):
    """
    Move category `cat` to be a child of `parent`.  If `parent` is None, move `cat` to root.

    :param cat: (model.Category or List) either a Category object, or a list of keys for the path of a category
    :param parent: (model.Category or List) either a Category object, or a list of keys for the path of a category
    :return:

    >>> c = Category().load({'path': ["Tanaitic", "Minor Tractates"]})
    >>> p = Category().load({"path": ["Talmud", "Bavli"]})
    >>> move_category_into(c, p)

    """
    if not isinstance(cat, Category):
        cat = Category().load({"path": cat})
    assert isinstance(cat, Category)

    if not isinstance(parent, Category) and parent is not None:
        parent = Category().load({"path": parent})
    assert isinstance(parent, Category) or parent is None


    old_category_path = cat.path[:]
    old_parent_path = cat.path[:-1]
    new_parent_path = parent.path[:] if parent else []

    # move all matching categories
    clauses = [{"path." + str(i): cname} for i, cname in enumerate(old_category_path)]
    query = {"$and": clauses}
    old_parent_length = len(old_parent_path)
    for cat in CategorySet(query):
        # replace old_parent_path with new_parent_path
        cat.path = new_parent_path + cat.path[old_parent_length:]
        print("Saving moved category - " + str(cat.path))
        cat.save(override_dependencies=True)

    # move all matching Indexes
    clauses = [{"categories." + str(i): cname} for i, cname in enumerate(old_category_path)]
    query = {"$and": clauses}
    for ind in IndexSet(query):
        assert isinstance(ind, Index)
        ind.categories = new_parent_path + ind.categories[old_parent_length:]
        print("Moving - " + ind.get_title() + " to " + str(ind.categories) + " (move_category_into)")
        ind.save(override_dependencies=True)


def create_category(path, en=None, he=None, searchRoot=None, order=None):
    """
    Will create a new category at the location in the TOC indicated by `path`.
    If there is a term for `path[-1]`, then that term will be used for this category.
    Otherwise, a new Term will be created with titles `en` and `he`.

    :param path: (List) the full path of the category to create
    :param en: (String, optional)
    :param he: (String, optional)
    :param searchRoot: (String, optional) If this is present, then in the context of search filters, this category will appear under `searchRoot`.
    :param order: (int) the order of the category in the location (negative for the end)
    :return: (model.Category) the new category object
    """
    c = Category()
    if not Term().load({"name": path[-1]}):
        if en is None or he is None:
            raise Exception("Need term names for {}".format(path[-1]))
        print("adding term for " + en)
        term = Term()
        term.name = en
        term.add_primary_titles(en, he)
        term.scheme = "toc_categories"
        term.save()
    c.add_shared_term(path[-1])
    c.path = path
    c.lastPath = path[-1]
    if order:
        c.order = order
    if searchRoot is not None:
        c.searchRoot = searchRoot
    if order is not None:
        c.order = order
    print("Creating - {}".format(" / ".join(c.path)))
    c.save(override_dependencies=True)
    return c


def get_category_paths(path):
    """
    Returns a list of all of the category paths one level below `path`.
    Used for populating rows of the Categories spreadsheet, e.g. to add all the categories that
    appear as Tanakh Commentaries
    """
    from sefaria.model.category import TocCategory
    root = library.get_toc_tree().lookup(path)
    return [cat.full_path for cat in root.children if isinstance(cat, TocCategory)]


def update_order_of_category_children(cat, uid, subcategoriesAndBooks):
    """
    Used by ReorderEditor and CategoryEditor.  Reorders subcategories and books.
    :param cat: (model.Category or List) Either a Category object, a list of category keys defining a category, or None.
                If empty list or None, assumed to be at the root of the TOC tree.
    :param uid: (int) UID of user modifying categories and/or books
    :param subcategoriesAndBooks: (list) List of strings of titles of books and/or categories
    """
    if isinstance(cat, list):
        cat = Category().load({"path": cat})
    assert isinstance(cat, Category) or cat is None
    cat_path = cat.path if cat else []

    order = 0
    results = []
    for subcategoryOrBook in subcategoriesAndBooks:
        order += 5
        book = Index().load({'title': subcategoryOrBook, 'categories': cat_path})
        if book:
            book = book.contents(raw=True)
            book['order'] = [order]
            result = tracker.update(uid, Index, book)
        else:
            cat = Category().load({"path": cat_path+[subcategoryOrBook]}).contents()
            cat['order'] = order
            result = tracker.update(uid, Category, cat)
        results.append(result.contents())
    return results





def check_term(last_path, he_last_path):
    """
     if Category Editor is used, make sure English and Hebrew titles correspond to the same term.
     if neither of the titles correspond to a term, create the appropriate term
    :param last_path: (str) Corresponds to lastPath of Category and english title of Term
    :param he_last_path: (str) Corresponds to a hebrew title of Term
    """

    error_msg = ""
    en_term = Term().load_by_title(last_path)
    he_term = Term().load_by_title(he_last_path)

    if en_term == he_term:
        pass
    if (en_term and he_term != en_term) or (he_term and he_term != en_term):
        # they do not correspond, either because both terms exist but are not the same, or one term already
        # exists but the other one doesn't exist
        error_msg = f"English and Hebrew titles, {last_path} and {he_last_path}, do not correspond to the same term.  Please use the term editor."
    elif en_term is None and he_term is None:
        t = Term()
        t.name = last_path
        t.add_primary_titles(last_path, he_last_path)
        t.save()
    return error_msg

```

### sefaria/helper/schema.py

```
# -*- coding: utf-8 -*-

from sefaria.model import *
from sefaria.model.abstract import AbstractMongoRecord
from sefaria.model.schema import DictionaryNode
from sefaria.system.exceptions import InputError
from sefaria.system.database import db
from sefaria.sheets import save_sheet
from sefaria.utils.util import list_depth, traverse_dict_tree

import re

"""

To get the existing schema nodes to pass into these functions, easiest is likely:
Ref("...").index_node


Todo: (still?)
    Clean system from old refs:
        links to commentary
        transx reqs
        elastic search
        varnish
"""


def handle_dependant_indices(title):
    """
    A generic method for handling dependant commentaries for methods in this package
    :param title: Title of book being changed
    """
    dependant_indices = library.get_dependant_indices(title, dependence_type='commentary', structure_match=True,
                                                      full_records=True)
    if len(dependant_indices) == 0:
        return

    print("{}Warning! Commentary linking will be removed for {} texts{}".\
        format('\033[93m', len(dependant_indices), '\033[0m'))  # The text prints in yellow

    for record in dependant_indices:
        record.base_text_mapping = None
        record.save()


def insert_last_child(new_node, parent_node):
    return attach_branch(new_node, parent_node, len(parent_node.children))


def insert_first_child(new_node, parent_node):
    return attach_branch(new_node, parent_node, 0)


def attach_branch(new_node, parent_node, place=0):
    """
    :param new_node: A schema node tree to attach
    :param parent_node: The parent to attach it to
    :param place: The index of the child before which to insert, so place=0 inserts at the front of the list, and place=len(parent_node.children) inserts at the end
    :return:
    """
    assert isinstance(new_node, SchemaNode)
    assert isinstance(parent_node, SchemaNode)
    assert place <= len(parent_node.children)

    index = parent_node.index

    # Add node to versions & commentary versions
    vs = [v for v in index.versionSet()]
    for v in vs:
        pc = v.content_node(parent_node)
        pc[new_node.key] = new_node.create_skeleton()
        v.save(override_dependencies=True)

    # Update Index schema and save
    parent_node.children.insert(place, new_node)
    new_node.parent = parent_node
    new_node.index = parent_node.index

    index.save(override_dependencies=True)
    library.rebuild()
    refresh_version_state(index.title)

    handle_dependant_indices(index.title)


def remove_branch(node):
    """
    This will delete any text in `node`
    :param node: SchemaNode to remove
    :return:
    """
    assert isinstance(node, SchemaNode)
    parent = node.parent
    assert parent
    index = node.index

    node.ref().linkset().delete()
    # todo: commentary linkset

    vs = [v for v in index.versionSet()]
    for v in vs:
        assert isinstance(v, Version)
        pc = v.content_node(parent)
        del pc[node.key]
        v.save(override_dependencies=True)

    parent.children = [n for n in parent.children if n.key != node.key]

    index.save(override_dependencies=True)
    library.rebuild()
    refresh_version_state(index.title)

    handle_dependant_indices(index.title)


def reorder_children(parent_node, new_order):
    """
    :param parent_node:
    :param new_order: List of child keys, in their new order
    :return:
    """
    # With this one, we can get away with just an Index change
    assert isinstance(parent_node, SchemaNode)
    child_dict = {n.key: n for n in parent_node.children}
    assert set(child_dict.keys()) == set(new_order)
    parent_node.children = [child_dict[k] for k in new_order]
    parent_node.index.save()


def merge_default_into_parent(parent_node):
    """
    In a case where a parent has only one child - a default child - this merges the two together into one Jagged Array node.

    Example Usage:
    >>> r = Ref('Mei HaShiloach, Volume II, Prophets, Judges')
    >>> merge_default_into_parent(r.index_node)

    :param parent_node:
    :return:
    """
    assert isinstance(parent_node, SchemaNode)
    assert len(parent_node.children) == 1
    assert parent_node.has_default_child()
    default_node = parent_node.get_default_child()
    # assumption: there's a grandparent.  todo: handle the case where the parent is the root node of the schema
    is_root = True
    if parent_node.parent:
        is_root = False
        grandparent_node = parent_node.parent
    index = parent_node.index

    # Repair all versions
    vs = [v for v in index.versionSet()]
    for v in vs:
        assert isinstance(v, Version)
        if is_root:
            v.chapter = v.chapter["default"]
        else:
            grandparent_version_dict = v.sub_content(grandparent_node.version_address())
            grandparent_version_dict[parent_node.key] = grandparent_version_dict[parent_node.key]["default"]
        v.save(override_dependencies=True)

    # Rebuild Index
    new_node = JaggedArrayNode()
    new_node.key = parent_node.key
    new_node.title_group = parent_node.title_group
    new_node.sectionNames = default_node.sectionNames
    new_node.addressTypes = default_node.addressTypes
    new_node.depth = default_node.depth
    if is_root:
        index.nodes = new_node
    else:
        grandparent_node.children = [c if c.key != parent_node.key else new_node for c in grandparent_node.children]

    # Save index and rebuild library
    index.save(override_dependencies=True)
    library.rebuild()
    refresh_version_state(index.title)

    handle_dependant_indices(index.title)


# todo: Can we share code with this method and the next?
def convert_jagged_array_to_schema_with_default(ja_node):
    from sefaria.model.schema import TitleGroup

    assert isinstance(ja_node, JaggedArrayNode)
    assert len(ja_node.children) == 0
    parent = ja_node.parent
    assert parent, "Use convert_simple_index_to_complex instead."
    assert isinstance(parent, SchemaNode)
    index = ja_node.index

    vs = [v for v in index.versionSet()]
    for v in vs:
        assert isinstance(v, Version)
        old_parent_content = v.content_node(parent)
        content = old_parent_content.pop(ja_node.key) # Pop old JA content off
        old_parent_content[ja_node.key] = {"default": content} # Re-add it as a default node
        v.save(override_dependencies=True)

    # Find place of ja_node in parent's children
    index_of_ja_node = parent.children.index(ja_node)

    # Build new schema
    new_parent = SchemaNode()
    new_parent.title_group = ja_node.title_group
    new_parent.key = ja_node.key
    ja_node.title_group = TitleGroup()
    ja_node.key = "default"
    ja_node.default = True

    # Rework the family tree
    new_parent.append(ja_node)
    parent.children[index_of_ja_node] = new_parent
    new_parent.parent = parent

    index.save(override_dependencies=True)
    library.rebuild()
    refresh_version_state(index.title)
    handle_dependant_indices(index.title)


def convert_simple_index_to_complex(index):
    """
    The target complex text will have a 'default' node.
    All refs to this text should remain good.
    :param index:
    :return:
    """
    from sefaria.model.schema import TitleGroup

    assert isinstance(index, Index)

    ja_node = index.nodes
    assert isinstance(ja_node, JaggedArrayNode)

    # Repair all version
    vs = [v for v in index.versionSet()]
    for v in vs:
        assert isinstance(v, Version)
        v.chapter = {"default": v.chapter}
        v.save(override_dependencies=True)

    # Build new schema
    new_parent = SchemaNode()
    new_parent.title_group = ja_node.title_group
    new_parent.key = ja_node.key
    ja_node.title_group = TitleGroup()
    ja_node.key = "default"
    ja_node.default = True

    # attach to index record
    new_parent.append(ja_node)
    index.nodes = new_parent

    index.save(override_dependencies=True)
    library.rebuild()
    refresh_version_state(index.title)

    handle_dependant_indices(index.title)

def prepare_ja_for_children(ja):
    """
    JaggedArrayNodes can have children. However, when creating an empty JA and attaching it to a SchemaNode via attach_branch(),
    the content_node corresponding to the JA in each Version will be an empty array. We need this to a dict so we can add children.
    """
    assert isinstance(ja, JaggedArrayNode)
    vs = [v for v in ja.index.versionSet()]
    for v in vs:
        assert isinstance(v, Version)
        content_node = v.content_node(ja)
        if isinstance(content_node, dict):
            print("JA is already prepared for children")
            return

        assert isinstance(content_node, list) and len(content_node) == 0, "JA's content node must be a list and be empty in order to prepare for children"
        # convert content node to dict so it can have children (aka, IVF)
        v.sub_content(ja.version_address(), value={})
        v.save()

def change_parent(node, new_parent, place=0, exact_match=False):
    """
    :param node:
    :param new_parent:
    :param place: The index of the child before which to insert, so place=0 inserts at the front of the list, and place=len(parent_node.children) inserts at the end
    :param exact_match: if True, if there are two links, "X" and "Y on X", changing "X" will not also change "Y on X"
    :return:
    """
    assert isinstance(node, SchemaNode)
    assert isinstance(new_parent, SchemaNode)
    assert place <= len(new_parent.children)
    old_parent = node.parent
    index = new_parent.index

    old_normal_form = node.ref().normal()
    linkset = [l for l in node.ref().linkset()]

    vs = [v for v in index.versionSet()]
    for v in vs:
        assert isinstance(v, Version)
        old_parent_content = v.content_node(old_parent)
        content = old_parent_content.pop(node.key)
        new_parent_content = v.content_node(new_parent)
        new_parent_content[node.key] = content
        v.save(override_dependencies=True)

    old_parent.children = [n for n in old_parent.children if n.key != node.key]
    new_parent.children.insert(place, node)
    node.parent = new_parent
    new_normal_form = node.ref(force_update=True).normal()

    index.save(override_dependencies=True)
    library.rebuild()

    for link in linkset:
        if exact_match:
            link.refs = [ref.replace(old_normal_form, new_normal_form) if ref.startswith(old_normal_form) else ref for ref in link.refs]
        else:
            link.refs = [ref.replace(old_normal_form, new_normal_form) for ref in link.refs]
        link.save()
    # todo: commentary linkset

    refresh_version_state(index.title)

    handle_dependant_indices(index.title)


def refresh_version_state(title):
    """
    ** VersionState is *not* altered on Index save.  It is only created on Index creation.
    ^ It now seems that VersionState is referenced on Index save

    VersionState is *not* automatically updated on Version save.
    The VersionState update on version save happens in texts_api().
    VersionState.refresh() assumes the structure of content has not changed.
    To regenerate VersionState, we save the flags, delete the old one, and regenerate a new one.
    """

    vs = VersionState(title)
    flags = vs.flags
    vs.delete()
    VersionState(title, {"flags": flags})


def change_node_title(snode, old_title, lang, new_title, ignore_cascade=False):
    """
    Changes the title of snode specified by old_title and lang, to new_title.
    If the title changing is the primary english title, cascades to all of the impacted objects
    :param snode:
    :param old_title:
    :param lang:
    :param new_title:
    :param ignore_cascade:
    :return:
    """
    def rewriter(string):
        return string.replace(old_title, new_title)

    def needs_rewrite(string, *args):
        return string.find(old_title) >= 0 and snode.index.title in string

    if old_title == snode.primary_title(lang=lang):
        snode.add_title(new_title, lang, replace_primary=True, primary=True)
        snode.index.save()
        library.refresh_index_record_in_cache(snode.index)
        if lang == 'en' and not ignore_cascade:
            cascade(snode.index.title, rewriter=rewriter, needs_rewrite=needs_rewrite)
    else:
        snode.add_title(new_title, lang)

    snode.remove_title(old_title, lang)

    snode.index.save(override_dependencies=True)
    library.refresh_index_record_in_cache(snode.index)


"""
def change_char_node_titles(index_title, bad_char, good_char, lang):
    '''
     Replaces all instances of bad_char with good_char in all node titles in the book titled index_title.
    If the title changing is the primary english title, cascades to all of the impacted objects
    :param index_title:
    :param bad_char:
    :param good_char:
    :param lang:
    :return:
    '''


    def callback(node, **kwargs):
        titles = node.get_titles_object()
        for each_title in titles:
            if each_title['lang'] == lang and 'primary' in each_title and each_title['primary']:
                title = each_title['text']

        change_node_title(snode, old_title,lang)

    root = library.get_index(index_title).nodes
    root.traverse_tree(callback, False)



    def recurse(node):
        if 'nodes' in node:
            for each_one in node['nodes']:
                recurse(each_one)
        elif 'default' not in node:

            if 'title' in node:
                node['title'] = node['title'].replace(bad_char, good_char)
            if 'titles' in node:
                which_one = -1
                if node['titles'][0]['lang'] == lang:
                    which_one = 0
                elif len(node['titles']) > 1 and node['titles'][1]['lang'] == lang:
                    which_one = 1
                if which_one >= 0:
                    node['titles'][which_one]['text'] = node['titles'][which_one]['text'].replace(bad_char, good_char)

    data = library.get_index(title).nodes.serialize()
    recurse(data)
    return data
"""


def change_node_structure(ja_node, section_names, address_types=None, upsize_in_place=False):
    """
    Updates the structure of a JaggedArrayNode to the depth specified by the length of sectionNames.

    When increasing size, any existing text will become the first segment of the new level
    ["One", "Two", "Three"] -> [["One"], ["Two"], ["Three"]]

    When decreasing size, information is lost as any existing segments are concatenated with " "
    [["One1", "One2"], ["Two1", "Two2"], ["Three1", "Three2"]] - >["One1 One2", "Two1 Two2", "Three1 Three2"]

    A depth 0 text (i.e. a single string or an empty list) will be treated as if upsize_in_place was set to True

    :param ja_node: JaggedArrayNode to be edited. Must be instance of class: JaggedArrayNode

    :param section_names: sectionNames parameter of restructured node. This determines the depth
    :param address_types: address_type parameter of restructured node. Defaults to ['Integer'] * len(sectionNames)

    :param upsize_in_place: If True, existing text will stay in tact, but be wrapped in new depth:
    ["One", "Two", "Three"] -> [["One", "Two", "Three"]]
    """

    assert isinstance(ja_node, JaggedArrayNode)
    assert len(section_names) > 0

    if hasattr(ja_node, 'lengths'):
        print('WARNING: This node has predefined lengths!')
        del ja_node.lengths

    # `delta` is difference in depth.  If positive, we're adding depth.
    delta = len(section_names) - len(ja_node.sectionNames)
    if upsize_in_place:
        assert (delta > 0)

    if address_types is None:
        address_types = ['Integer'] * len(section_names)
    else:
        assert len(address_types) == len(section_names)

    def fix_ref(ref_string):
        """
        Takes a string from link.refs and updates to reflect the new structure.
        Uses the delta parameter from the main function to determine how to update the ref.
        `delta` is difference in depth.  If positive, we're adding depth.
        :param ref_string: A string which can be interpreted as a valid Ref
        :return: string
        """
        if delta == 0:
            return ref_string

        d = Ref(ref_string)._core_dict()

        if delta < 0:  # Making node shallower
            for i in range(-delta):
                if len(d["sections"]) == 0:
                    break
                d["sections"].pop()
                d["toSections"].pop()

                # else, making node deeper
        elif upsize_in_place:
            for i in range(delta):
                d["sections"].insert(0, 1)
                d["toSections"].insert(0, 1)
        else:
            for i in range(delta):
                d["sections"].append(1)
                d["toSections"].append(1)

        return Ref(_obj=d).normal()


    identifier = ja_node.ref().regex(anchored=False)

    def needs_fixing(ref_string, *args):
        if re.search(identifier, ref_string) is None:
            return False
        else:
            return True

    # For downsizing, refs will become invalidated in their current state, so changes must be made before the
    # structure change.
    if delta < 0:
        cascade(ja_node.ref(), rewriter=fix_ref, needs_rewrite=needs_fixing)
        # cascade updates the index record, ja_node index gets stale
        ja_node.index = library.get_index(ja_node.index.title)

    ja_node.sectionNames = section_names
    ja_node.addressTypes = address_types
    ja_node.depth = len(section_names)
    ja_node._regexes = {}
    ja_node._init_address_classes()
    index = ja_node.index
    index.save(override_dependencies=True)
    print('Index Saved')
    library.refresh_index_record_in_cache(index)
    # ensure the index on the ja_node object is updated with the library refresh
    ja_node.index = library.get_index(ja_node.index.title)

    vs = [v for v in index.versionSet()]
    print('Updating Versions')
    for v in vs:
        assert isinstance(v, Version)

        if v.get_index() == index:
            chunk = TextChunk(ja_node.ref(), lang=v.language, vtitle=v.versionTitle)
        else:
            library.refresh_index_record_in_cache(v.get_index())
            ref_name = ja_node.ref().normal()
            ref_name = ref_name.replace(index.title, v.get_index().title)
            chunk = TextChunk(Ref(ref_name), lang=v.language, vtitle=v.versionTitle)
        ja = chunk.ja()
        if ja.get_depth() == 0:
            continue

        if upsize_in_place:
            wrapper = chunk.text
            for i in range(delta):
                wrapper = [wrapper]
            chunk.text = wrapper
            chunk.save()

        else:
            # we're going to save directly on the version to avoid weird mid change Ref bugs
            new_text = ja.resize(delta).trim_ending_whitespace().array()
            if isinstance(v.chapter, dict):  # complex text
                version_address = ja_node.version_address()
                parent = traverse_dict_tree(v.chapter, version_address[:-1])
                parent[version_address[-1]] = new_text
            else:
                v.chapter = new_text
            v.save()

    # For upsizing, we are editing refs to a structure that would not be valid till after the change, therefore
    # cascading must be performed here
    if delta > 0:
        cascade(ja_node.ref(), rewriter=fix_ref, needs_rewrite=needs_fixing)

    library.rebuild()
    refresh_version_state(index.title)

    handle_dependant_indices(index.title)


def cascade(ref_identifier, rewriter=lambda x: x, needs_rewrite=lambda *args: True, skip_history=False):
    """
    Changes to indexes requires updating any and all data that reference that index. This routine will take a rewriter
     function and run it on every location that references the updated index.
    :param ref_identifier: Ref or String that can be used to implement a ref (an Index level Ref?  Or Deeper?)
    :param rewriter: f(String)->String. callback function used to update the field.
    :param needs_rewrite: f(String, Object)->Boolean. Criteria for which a save will be triggered. If not set, routine will trigger a save for
    every item within the set
    :param skip_history: Set to True to skip history updates
    """

    def generic_rewrite(model_set, attr_name='ref', sub_attr_name=None, ):
        """
        Generic routine to take any derivative of AbstractMongoSet and update the fields outlined by attr_name using
        the callback function rewriter.

        This routine is heavily inspired by SegmentSplicer._generic_set_rewrite
        :param model_set: Derivative of AbstractMongoSet
        :param attr_name: name of attribute to update
        :param sub_attr_name: Use to update nested attributes
        :return:
        """

        for record in model_set:
            assert isinstance(record, AbstractMongoRecord)
            if sub_attr_name is None:
                refs = getattr(record, attr_name)
            else:
                intermediate_obj = getattr(record, attr_name)
                refs = intermediate_obj[sub_attr_name]

            if isinstance(refs, list):
                needs_save = False
                for ref_num, ref in enumerate(refs):
                    if needs_rewrite(ref, record):
                        needs_save = True
                        refs[ref_num] = rewriter(ref)
                if needs_save:
                    try:
                        record.save()
                    except InputError as e:
                        print('Bad Data Found: {}'.format(refs))
                        print(e)
            else:
                if needs_rewrite(refs, record):
                    if sub_attr_name is None:
                        setattr(record, attr_name, rewriter(refs))
                    else:
                        intermediate_obj[sub_attr_name] = rewriter(refs)

                    try:
                        record.save()
                    except InputError as e:
                        print('Bad Data Found: {}'.format(refs))
                        print(e)

    def clean_sheets(sheets_to_update):

        def rewrite_source(source):
            requires_save = False
            if "ref" in source:
                original_tref = source["ref"]
                try:
                    rewrite = needs_rewrite(source["ref"])
                except (InputError, ValueError) as e:
                    print('needs_rewrite method threw exception:', source["ref"], e)
                    rewrite = False
                if rewrite:
                    requires_save = True
                    try:
                        source["ref"] = rewriter(source['ref'])
                    except (InputError, ValueError) as e:
                        print('rewriter threw exception:', source["ref"], e)
                    if source["ref"] != original_tref and not Ref.is_ref(source["ref"]):
                        print('rewiter created an invalid Ref:', source["ref"])
            if "subsources" in source:
                for subsource in source["subsources"]:
                    requires_save = rewrite_source(subsource) or requires_save
            return requires_save

        for sid in sheets_to_update:
            needs_save = False
            sheet = db.sheets.find_one({"id": sid})
            if not sheet:
                print("Likely error - can't load sheet {}".format(sid))
            for source in sheet["sources"]:
                if rewrite_source(source):
                    needs_save = True
            if needs_save:
                sheet["lastModified"] = sheet["dateModified"]
                save_sheet(sheet, sheet["owner"], search_override=True)

    def update_alt_structs(index):

        assert isinstance(index, Index)
        if not index.has_alt_structures():
            return
        needs_save = False

        for name, struct in index.get_alt_structures().items():
            for map_node in struct.get_leaf_nodes():
                assert map_node.depth <= 1, "Need to write some code to handle alt structs with depth > 1!"
                wr = map_node.wholeRef
                if needs_rewrite(wr):
                    needs_save = True
                    map_node.wholeRef = rewriter(wr)
                if hasattr(map_node, 'refs'):
                    for ref_num, ref in enumerate(map_node.refs):
                        if needs_rewrite(ref):
                            needs_save = True
                            map_node.refs[ref_num] = rewriter(ref)
        if needs_save:
            index.save()

    if isinstance(ref_identifier, str):
        ref_identifier = Ref(ref_identifier)
    assert isinstance(ref_identifier, Ref)

    identifier = ref_identifier.regex(anchored=False, as_list=True)

    # titles = re.compile(identifier)

    def construct_query(attribute, queries):

        query_list = [{attribute: {'$regex': '^' + query}} for query in queries]
        return {'$or': query_list}

    print('Updating Links')
    generic_rewrite(LinkSet(construct_query('refs', identifier)), attr_name='refs')
    print('Updating Notes')
    generic_rewrite(NoteSet(construct_query('ref', identifier)))
    print('Updating User History')
    generic_rewrite(UserHistorySet(construct_query('ref', identifier)))
    print('Updating Ref Data')
    generic_rewrite(RefDataSet(construct_query('ref', identifier)))
    print('Updating Topic Links')
    generic_rewrite(RefTopicLinkSet(construct_query('ref', identifier)))
    print('Updating Garden Stops')
    generic_rewrite(GardenStopSet(construct_query('ref', identifier)))
    print('Updating Sheets')
    clean_sheets([s['id'] for s in db.sheets.find(construct_query('sources.ref', identifier), {"id": 1})])
    print('Updating Alternate Structs')
    update_alt_structs(ref_identifier.index)
    if not skip_history:
        print('Updating History')
        generic_rewrite(HistorySet(construct_query('ref', identifier), sort=[('ref', 1)]))
        generic_rewrite(HistorySet(construct_query('new.ref', identifier), sort=[('new.ref', 1)]), attr_name='new', sub_attr_name='ref')
        generic_rewrite(HistorySet(construct_query('new.refs', identifier), sort=[('new.refs', 1)]), attr_name='new', sub_attr_name='refs')
        generic_rewrite(HistorySet(construct_query('old.ref', identifier), sort=[('old.ref', 1)]), attr_name='old', sub_attr_name='ref')
        generic_rewrite(HistorySet(construct_query('old.refs', identifier), sort=[('old.refs', 1)]), attr_name='old', sub_attr_name='refs')


def generate_segment_mapping(title, mapping, output_file=None, mapped_title=lambda x: "Complex {}".format(x)):
    '''
    :param title: title of Index record
    :param mapping: mapping is a dict where each key is a reference in the original simple Index and each value is a reference in the new complex Index
    such as mapping['Zohar 1:2a'] = 'Zohar, Genesis'
    :param output_file:
    :return segment_map: segment_map is the dict based on mapping

    The function takes each key/value pair in mapping and adds this key/value pair to the segment_map,
    and it also adds every possible key/value pair that are descendants of the key/value pairs in mapping to the segment_map.
    In the above example,
    segment_map['Zohar 1:2a'] = 'Zohar, Genesis'
    segment_map['Zohar 1:2a:1'] = 'Zohar, Genesis 1'
    segment_map['Zohar 1:2a:2'] = 'Zohar, Genesis 2'
    etc.

    :return segment_map:
    '''

    segment_map = {}
    for orig_ref in mapping:
        orig_ref_str = orig_ref
        orig_ref = Ref(orig_ref)
        refs = []

        #now create an array, refs that holds the orig_ref in addition to all of its children
        if orig_ref.is_range():
            depth = orig_ref.range_depth()
            if depth == 1:
                refs = orig_ref.range_list()
            elif depth == 2:
                top_level_refs = orig_ref.split_spanning_ref()
                segment_refs = orig_ref.range_list()
                refs = top_level_refs + segment_refs
            elif depth == 3:
                top_level_refs = orig_ref.split_spanning_ref()
                section_refs = orig_ref.range_list()
                segment_refs = orig_ref.as_ranged_segment_ref().range_list()
                refs = top_level_refs + section_refs + segment_refs
        else:
            refs = orig_ref.all_subrefs()
            if len(refs) > 0 and not refs[0].is_segment_level():
                len_refs = len(refs)
                segment_refs = []
                for i in range(len_refs):
                    segment_refs += refs[i].all_subrefs()
                assert segment_refs[0].is_segment_level()
                refs += segment_refs
            refs += [orig_ref]

        #segment_value is the value of the mapping that the user inputted
        segment_value = mapped_title(mapping[orig_ref_str])

        #now iterate over the refs and create the key/value pairs to put into segment_map
        for each_ref in refs:
            assert each_ref not in segment_map, "Invalid map ranges: Two overlap at reference {}".format(each_ref)
            if each_ref == orig_ref:
                segment_map[each_ref.normal()] = segment_value
            else:
                '''
                get in_terms_of() info to construct a string that represents the complex index's new reference.
                construct the new reference by appending the results of in_terms_of() onto
                segment_value -- where segment_value is the value that the parameter, mapping, returns for the key of orig_ref
                '''
                append_arr = each_ref.in_terms_of(orig_ref)
                assert append_arr, "{} cannot be computed to be in_terms_of() {}".format(each_ref, orig_ref)
                segment_ref = Ref(segment_value)
                core_dict = segment_ref._core_dict()
                core_dict['sections'] += append_arr
                core_dict['toSections'] += append_arr

                segment_map[each_ref.normal()] = Ref(_obj=core_dict).normal()

    # output results so that this map can be used again for other purposes
    if output_file:
        with open(output_file, 'w') as output_file:
            for key in segment_map:
                output_file.write("KEY: {}, VALUE: {}".format(key, segment_map[key])+"\n")
    return segment_map


def migrate_to_complex_structure(title, schema, mappings, validate_mapping=False):
    """
    Converts book that is simple structure to complex.
    :param title: title of book
    :param schema: the new complex structure schema, must be JSON
    :param mappings: a dictionary mapping references from simple structure to references in complex structure
                    For example:
        mappings = {"Midrash Tanchuma 1:1": "Midrash Tanchuma, Bereshit",
                    "Midrash Tanchuma 1:2": "Midrash Tanchuma, Noach",
                    ...
                    "Midrash Tanchuma 2:1": "Midrash Tanchuma, Shemot"}
    :return:
    """
    def needs_rewrite(ref, *args):
        try:
            return Ref(ref).index.title == title
        except InputError:
            return False


    def rewriter(ref):
        ref = Ref(ref)
        if ref.is_range():
            start = ref.starting_ref().normal()
            end = ref.ending_ref().normal()
            if start in segment_map and end in segment_map:
                return Ref(segment_map[start]).to(Ref(segment_map[end])).normal()
            elif start in segment_map:
                return segment_map[start]
            elif end in segment_map:
                return segment_map[end]
            else:
                return "Complex {}".format(ref.normal())
        elif ref.normal() not in segment_map:
            return "Complex {}".format(ref.normal())
        else:
            return segment_map[ref.normal()]


    print("begin conversion")
    #TODO: add method on model.Index to change all 3 (title, nodes.key and nodes.primary title)

    #create a new index with a temp file #make sure to later add all the alternate titles
    old_index = Index().load({"title": title})
    new_index_contents = {
        "title": title,
        "categories": old_index.categories,
        "schema": schema
    }
    for attr in Index.optional_attrs:
        if attr == 'schema':
            continue
        elif hasattr(old_index, attr):
            new_index_contents[attr] = getattr(old_index, attr)

    #TODO: these are ugly hacks to create a temp index
    temp_index = Index(new_index_contents)
    en_title = temp_index.get_title('en')
    temp_index.title = "Complex {}".format(en_title)
    he_title = temp_index.get_title('he')
    temp_index.set_title('{} '.format(he_title), 'he')
    temp_index.save()
    #the rest of the title variants need to be copied as well but it will create conflicts while the orig index exists, so we do it after removing the old index in completely_delete_index_and_related.py

    #create versions for the main text
    versions = VersionSet({'title': title})
    try:
        migrate_versions_of_text(versions, mappings, title, temp_index.title, temp_index)
    except InputError as e:
        temp_index.delete()
        print(str(e))
        raise e

    #are there commentaries? Need to move the text for them to conform to the new structure
    #basically a repeat process of the above, sans creating the index record
    #duplicate versionstate
    #TODO: untested
    vstate_old = VersionState().load({'title':title })
    vstate_new = VersionState(temp_index)
    vstate_new.flags = vstate_old.flags
    vstate_new.save()

    segment_map = generate_segment_mapping(title, mappings, "output_"+title+"_.txt")
    cascade(title, rewriter, needs_rewrite)

    handle_dependant_indices(title)

    Index().load({"title": title}).delete()

    #re-name the temporary index, "Complex ..." to the original title
    i = library.get_index("Complex {}".format(en_title))
    i.set_title(title)
    i.set_title(he_title, lang="he")
    i.save()


def migrate_versions_of_text(versions, mappings, orig_title, new_title, base_index):
    for i, version in enumerate(versions):
        print(version.versionTitle.encode('utf-8'))
        new_version_title = version.title.replace(orig_title, new_title)
        print(new_version_title)
        new_version = Version(
                {
                    "chapter": base_index.nodes.create_skeleton(),
                    "versionTitle": version.versionTitle,
                    "versionSource": version.versionSource,
                    "language": version.language,
                    "title": new_version_title
                }
            )
        for attr in ['status', 'license', 'method', 'versionNotes', 'priority', "digitizedBySefaria", "heversionSource"]:
            value = getattr(version, attr, None)
            if value:
                setattr(new_version, attr, value)
        new_version.save()
        for orig_ref in mappings:
            #this makes the mapping contain the correct text/commentary title
            orig_ref = orig_ref.replace(orig_title, version.title)
            print(orig_ref)
            orRef = Ref(orig_ref)
            tc = orRef.text(lang=version.language, vtitle=version.versionTitle)
            ref_text = tc.text

            #this makes the destination mapping contain both the correct text/commentary title
            # and have it changed to the temp index title
            dest_ref = mappings[orig_ref].replace(orig_title, version.title)
            dest_ref = dest_ref.replace(orig_title, new_title)
            print(dest_ref)

            dRef = Ref(dest_ref)
            ref_depth = dRef.range_index() if dRef.is_range() else len(dRef.sections)
            text_depth = 0 if isinstance(ref_text, str) else list_depth(ref_text) #length hack to fit the correct JA
            implied_depth = ref_depth + text_depth
            desired_depth = dRef.index_node.depth
            for i in range(implied_depth, desired_depth):
                ref_text = [ref_text]

            new_tc = dRef.text(lang=version.language, vtitle=version.versionTitle)
            new_tc.versionSource = version.versionSource
            new_tc.text = ref_text
            new_tc.save()
            VersionState(dRef.index.title).refresh()


def toc_opml():
    """Prints a simple representation of the TOC in OPML"""
    toc  = library.get_toc()

    def opml_node(node):
        if "category" in node:
            opml = '<outline text="%s">\n' % node["category"]
            for node in node["contents"]:
                opml += "    " + opml_node(node) + "\n"
            opml += '</outline>'
        else:
            opml = '<outline text="%s"></outline>\n' % node["title"]
        return opml

    opml = """
            <?xml version="1.0"?>
            <opml version="2.0">
              <body>
              %s
              </body>
            </opml>
            """ % "\n".join([opml_node(node) for node in toc])

    print(opml)


def toc_plaintext():
    """Prints a simple representation of the TOC in indented plaintext"""
    toc  = library.get_toc()

    def text_node(node, depth):
        if "category" in node:
            text = ("    " * depth) + node["category"] + "\n"
            for node in node["contents"]:
                text += text_node(node, depth+1)
        else:
            text = ("    " * depth) + node["title"] + "\n"
        return text

    text = "".join([text_node(node, 0) for node in toc])

    print(text)


def change_term_hebrew(en_primary, new_he):
    t = Term().load({"name": en_primary})
    assert t
    old_primary = t.get_primary_title("he")
    t.add_title(new_he, "he", True, True)
    t.remove_title(old_primary, "he")
    t.save()


def change_lexicon_headword(parent_lexicon, old_headword, new_headword):
    """
    Changes the headword of an entry.
    NOTICE: many lexicon has internal references, wrapped with an a tag within the data. This function won't change this.
    :param parent_lexicon: string
    :param old_headword: string
    :param new_headword: string
    :return: None

    Example: change_lexicon_headword('Jastrow Dictionary', '.1', ' 1')
    """

    def get_dictionary_node(node):
        if isinstance(node, DictionaryNode):
            return node
        else:
            for snode in node.children:
                result = get_dictionary_node(snode)
                if result:
                    return result

    def update_dictionary_node(index_node):
        dictionary_node = get_dictionary_node(index_node)
        parent = dictionary_node.parent
        dictionary_node_index = parent.children.index(dictionary_node)
        for attr in ('firstWord', 'lastWord'):
            if getattr(dictionary_node, attr) == old_headword:
                setattr(dictionary_node, attr, new_headword)
        parent.children[dictionary_node_index] = update_headwords_map(dictionary_node)
        return index_node

    def update_headwords_map(dictionary_node):
        hw_map = getattr(dictionary_node, 'headwordMap', None)
        node_ref = dictionary_node.ref().normal()
        if hw_map:
            for n, node in enumerate(hw_map):
                if node[1] == f'{node_ref}, {old_headword}':
                    node[1] = f'{node_ref}, {new_headword}'
            dictionary_node.headwordMap = hw_map
        return dictionary_node

    if LexiconEntry().load({'parent_lexicon': parent_lexicon, 'headword': new_headword}):
        raise ValueError(f'Entry of {parent_lexicon} with headword {new_headword} already exists')

    # change entry itself
    print('Updating entry')
    entry = LexiconEntry().load({'parent_lexicon': parent_lexicon, 'headword': old_headword})
    entry.headword = new_headword
    entry.save()

    # change prev and next
    print('Updating previous and next entries')
    adjacents = ['prev_hw', 'next_hw']
    for i in [1, -1]:
        adj_hw = getattr(entry, adjacents[::i][0], None)
        if adj_hw:
            adj_entry = LexiconEntry().load({'parent_lexicon': parent_lexicon, 'headword': adj_hw})
            setattr(adj_entry, adjacents[::i][1], new_headword)
            adj_entry.save()

    # change index
    index = Index().load({'lexiconName': parent_lexicon})
    if index:
        print('Updating index')
        index.nodes = update_dictionary_node(index.nodes)
        index.save()

        # cascade
        print('Cascading')
        node_ref = get_dictionary_node(index.nodes).ref().normal()
        ref = f'{node_ref}, {old_headword}'
        old_ref_reg = fr'^{re.escape(ref)} ?\d*$'
        rewriter = lambda x: x.replace(old_headword, new_headword)
        needs_rewrite = lambda x, *args: bool(re.search(old_ref_reg, x))
        cascade(index.title, rewriter, needs_rewrite, True)

    # word forms
    print('Updating word forms')
    db.word_form.update_many(
        {
            'lookups': {
                '$elemMatch': {
                    'parent_lexicon': parent_lexicon,
                    'headword': old_headword
                }
            }
        },
        {
            '$set': {
                'lookups.$[elem].headword': new_headword
            }
        },
        array_filters=[{
            'elem.parent_lexicon': parent_lexicon,
            'elem.headword': old_headword
        }]
    )

    library.rebuild()

    # other entries in the same dictionary that includes wrapped ref for the old headword
    # changing another entry is too complicated, for any lexicon has different entries structure, so it will be only printed
    if index:
        quoted = []
        for entry in LexiconEntrySet({'parent_lexicon': parent_lexicon}):
            oref = Ref(f'{index.title}, {entry.headword}')
            entry_text = ' '.join(oref.index_node.get_text())
            if ref in entry_text:
                quoted.append(f'"{entry.headword}"')
        if quoted:
            print(f'Other entries in this lexicon with this old headword as ref: {", ".join(quoted)}')
        print('Warning: old ref can appear as wrapped ref in other places in the library.')

```

### sefaria/local_settings_ci.py

```

from datetime import timedelta
import structlog
import os


DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'sefaria_auth',
        'USER': 'sefaria',
        'PASSWORD': '',
        'HOST': 'localhost',
        'PORT': '',
    }
}


# Map domain to an interface language that the domain should be pinned to.
# Leave as {} to prevent language pinning, in which case one domain can serve either Hebrew or English
DOMAIN_LANGUAGES = {
    "http://hebrew.example.org": "hebrew",
    "http://english.example.org": "english",
}


#SILENCED_SYSTEM_CHECKS = ['captcha.recaptcha_test_key_error']

ADMINS = (
     ('Your Name', 'you@example.com'),
)
MANAGERS = ADMINS

PINNED_IPCOUNTRY = "IL" #change if you want parashat hashavua to be diaspora.

CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.dummy.DummyCache',
    }
}

SITE_PACKAGE = "sites.sefaria"

DEBUG = True
ALLOWED_HOSTS = ['localhost', '127.0.0.1', "0.0.0.0", '[::1]']
OFFLINE = False
DOWN_FOR_MAINTENANCE = False
MAINTENANCE_MESSAGE = ""
GLOBAL_WARNING = False
GLOBAL_WARNING_MESSAGE = ""


SECRET_KEY = 'insert your long random secret key here !'


EMAIL_HOST = 'localhost'
EMAIL_PORT = 1025
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'

MONGO_HOST = "localhost"
MONGO_PORT = 27017

# Name of the MongoDB database to use.
SEFARIA_DB = os.getenv('MONGO_DB_NAME')

# Leave user and password blank if not using Mongo Auth
SEFARIA_DB_USER = ''
SEFARIA_DB_PASSWORD = ''
APSCHEDULER_NAME = "apscheduler"

# ElasticSearch server
SEARCH_URL = "http://localhost:9200"
SEARCH_INDEX_ON_SAVE = False  # Whether to send texts and source sheet to Search Host for indexing after save
SEARCH_INDEX_NAME_TEXT = 'text'  # name of the ElasticSearch index to use
SEARCH_INDEX_NAME_SHEET = 'sheet'

# Node Server
USE_NODE = False
NODE_HOST = "http://localhost:4040"
NODE_TIMEOUT = 10
# NODE_TIMEOUT_MONITOR = relative_to_abs_path("../log/forever/timeouts")

SEFARIA_DATA_PATH = '/path/to/your/Sefaria-Data' # used for Data
SEFARIA_EXPORT_PATH = '/path/to/your/Sefaria-Data/export' # used for exporting texts


GOOGLE_TAG_MANAGER_CODE = 'you tag manager code here'

# Determine which CRM connection implementations to use
CRM_TYPE = "NONE"  # "SALESFORCE" || "NATIONBUILDER" || "NONE"

# Integration with a NationBuilder list
NATIONBUILDER_SLUG = ""
NATIONBUILDER_TOKEN = ""
NATIONBUILDER_CLIENT_ID = ""
NATIONBUILDER_CLIENT_SECRET = ""

# Integration with Salesforce
SALESFORCE_BASE_URL = ""
SALESFORCE_CLIENT_ID = ""
SALESFORCE_CLIENT_SECRET = ""

# Issue bans to Varnish on update.
USE_VARNISH = False
FRONT_END_URL = "http://localhost:8000"      # This one wants the http://
VARNISH_ADM_ADDR = "localhost:6082"          # And this one doesn't
VARNISH_HOST = "localhost"
VARNISH_FRNT_PORT = 8040
VARNISH_SECRET = "/etc/varnish/secret"
# Use ESI for user box in header.
USE_VARNISH_ESI = False

# Prevent modification of Index records
DISABLE_INDEX_SAVE = False

# Caching with Cloudflare
CLOUDFLARE_ZONE = ""
CLOUDFLARE_EMAIL = ""
CLOUDFLARE_TOKEN = ""

# Multiserver
MULTISERVER_ENABLED = False
MULTISERVER_REDIS_SERVER = "127.0.0.1"
MULTISERVER_REDIS_PORT = 6379
MULTISERVER_REDIS_DB = 0
MULTISERVER_REDIS_EVENT_CHANNEL = "msync"   # Message queue on Redis
MULTISERVER_REDIS_CONFIRM_CHANNEL = "mconfirm"   # Message queue on Redis

# OAUTH these fields dont need to be filled in. they are only required for oauth2client to __init__ successfully
GOOGLE_OAUTH2_CLIENT_ID = ""
GOOGLE_OAUTH2_CLIENT_SECRET = ""
# This is the field that is actually used
GOOGLE_OAUTH2_CLIENT_SECRET_FILEPATH = ""

GOOGLE_APPLICATION_CREDENTIALS_FILEPATH = ""

GEOIP_DATABASE = 'data/geoip/GeoLiteCity.dat'
GEOIPV6_DATABASE = 'data/geoip/GeoLiteCityv6.dat'

PARTNER_GROUP_EMAIL_PATTERN_LOOKUP_FILE = None

# Simple JWT
SIMPLE_JWT = {
    'ACCESS_TOKEN_LIFETIME': timedelta(days=1),
    'REFRESH_TOKEN_LIFETIME': timedelta(days=90),
    'ROTATE_REFRESH_TOKENS': True,
    'SIGNING_KEY': 'a signing key: at least 256 bits',
}

# Celery
REDIS_PORT = 26379
REDIS_PASSWORD = None
CELERY_REDIS_BROKER_DB_NUM = 0
CELERY_REDIS_RESULT_BACKEND_DB_NUM = 1
CELERY_QUEUES = {}
# Either define SENTINEL_HEADLESS_URL if using sentinel or REDIS_URL for a simple redis instance
SENTINEL_HEADLESS_URL = None
SENTINEL_TRANSPORT_OPTS = {}
SENTINEL_PASSWORD = None
REDIS_URL = "redis://127.0.0.1"

# Key which identifies the Sefaria app as opposed to a user
# using our API outside of the app. Mainly for registration
MOBILE_APP_KEY = "MOBILE_APP_KEY"

ENABLE_LINKER = False
RAW_REF_MODEL_BY_LANG_FILEPATH = {
    "en": None,
    "he": None,
}

RAW_REF_PART_MODEL_BY_LANG_FILEPATH = {
    "en": None,
    "he": None,
}

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        "json_formatter": {
            "()": structlog.stdlib.ProcessorFormatter,
            "processor": structlog.processors.JSONRenderer(),
        },
    },
    'handlers': {
        'default': {
            "class": "logging.StreamHandler",
            "formatter": "json_formatter",
        },
    },
    'loggers': {
        '': {
            'handlers': ['default'],
            'propagate': False,
        },
        'django': {
            'handlers': ['default'],
            'propagate': False,
        },
        'django.request': {
            'handlers': ['default'],
            'propagate': False,
        },
    }
}

STRAPI_LOCATION = None
STRAPI_PORT = None

structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.ExceptionPrettyPrinter(),
        structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
    ],
    context_class=structlog.threadlocal.wrap_dict(dict),
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

WEBHOOK_USERNAME = os.getenv("WEBHOOK_USERNAME")
WEBHOOK_PASSWORD = os.getenv("WEBHOOK_PASSWORD")
```

### sefaria/client/wrapper.py

```
# -*- coding: utf-8 -*-
import re

import structlog
logger = structlog.get_logger(__name__)

from sefaria.model import *
from sefaria.datatype.jagged_array import JaggedTextArray
from sefaria.system.exceptions import InputError, NoVersionFoundError
from sefaria.model.user_profile import user_link, public_user_data
from sefaria.sheets import get_sheets_for_ref
from sefaria.utils.hebrew import hebrew_term


def format_link_object_for_client(link, with_text, ref, pos=None):
    """
    :param link: Link object
    :param ref: Ref object of the source of the link
    :param pos: Position of the Ref in the Link.  If not passed, it will be derived from the first two arguments.
    :return: Dict
    """
    com = {}

    # The text we're asked to get links to
    anchorTref = link.refs[pos]
    anchorRef  = Ref(anchorTref)
    anchorTrefExpanded = getattr(link, "expandedRefs{}".format(pos))

    # The link we found to anchorRef
    linkPos   = (pos + 1) % 2
    linkTref  = link.refs[linkPos]
    linkRef   = Ref(linkTref)
    langs     = getattr(link, "availableLangs", [[],[]])
    linkLangs = langs[linkPos]

    com["_id"]               = str(link._id)
    com['index_title']       = linkRef.index.title
    com["category"]          = linkRef.primary_category #usually the index's categories[0] or "Commentary".
    com["type"]              = link.type
    com["ref"]               = linkTref
    com["anchorRef"]         = anchorTref
    com["anchorRefExpanded"] = anchorTrefExpanded
    com["sourceRef"]         = linkTref
    com["sourceHeRef"]       = linkRef.he_normal()
    com["anchorVerse"]       = anchorRef.sections[-1] if len(anchorRef.sections) else 0
    com["sourceHasEn"]       = "en" in linkLangs
    # com["anchorText"]        = getattr(link, "anchorText", "") # not currently used
    if getattr(link, "inline_reference", None):
        com["inline_reference"]  = getattr(link, "inline_reference", None)
    if getattr(link, "highlightedWords", None):
        com["highlightedWords"] = getattr(link, "highlightedWords", None)
    if getattr(link, "versions", None) and link.type == "essay" and getattr(link, "displayedText", None):
        com["anchorVersion"] = {"title": link.versions[pos]["title"], "language": link.versions[pos].get("language", None)}
        com["sourceVersion"] = {"title": link.versions[linkPos]["title"], "language": link.versions[linkPos].get("language", None)}
        com["displayedText"] = link.displayedText[linkPos]  # we only want source displayedText

    compDate = getattr(linkRef.index, "compDate", None)  # default comp date to in the future
    if compDate:
        com["compDate"] = compDate

    # Pad out the sections list, so that comparison between comment numbers are apples-to-apples
    lsections = linkRef.sections[:] + [0] * (linkRef.index_node.depth - len(linkRef.sections))
    # Build a decimal comment number based on the last two digits of the section array
    com["commentaryNum"] = lsections[-1] if len(lsections) == 1 \
            else float('{0}.{1:04d}'.format(*lsections[-2:])) if len(lsections) > 1 else 0

    if with_text:
        text             = TextFamily(linkRef, context=0, commentary=False)
        com["text"]      = text.text if isinstance(text.text, str) else JaggedTextArray(text.text).flatten_to_array()
        com["he"]        = text.he if isinstance(text.he, str) else JaggedTextArray(text.he).flatten_to_array()

    # if the link is commentary, strip redundant info (e.g. "Rashi on Genesis 4:2" -> "Rashi")
    # this is now simpler, and there is explicit data on the index record for it.
    if com["type"] == "commentary":
        com["collectiveTitle"] = {
            'en': getattr(linkRef.index, 'collective_title', linkRef.index.title),
            'he': hebrew_term(getattr(linkRef.index, 'collective_title', linkRef.index.get_title("he")))
        }
    else:
        com["collectiveTitle"] = {'en': linkRef.index.title, 'he': linkRef.index.get_title("he")}

    if com["type"] == "essay":
        com["category"] = "Essay"
    elif com["type"] != "commentary" and com["category"] == "Commentary":
        com["category"] = "Quoting Commentary"

    if linkRef.index_node.primary_title("he"):
        com["heTitle"] = linkRef.index_node.primary_title("he")

    return com


def format_object_for_client(obj, with_text=True, ref=None, pos=None):
    """
    Assumption here is that if obj is a Link, and ref and pos are not specified, then position 0 is the root ref.
    :param obj:
    :param ref:
    :param pos:
    :return:
    """
    if isinstance(obj, Note):
        return format_note_object_for_client(obj)
    elif isinstance(obj, Link):
        if not ref and not pos:
            ref = obj.refs[0]
            pos = 0
        return format_link_object_for_client(obj, with_text, ref, pos)
    else:
        raise InputError("{} not valid for format_object_for_client".format(obj.__class__.__name__))


def format_note_object_for_client(note):
    """
    Returns an object that represents note in the format expected by the reader client,
    matching the format of links, which are currently handled together.
    """
    anchor_oref = Ref(note.ref).padded_ref()
    ownerData   = public_user_data(note.owner)

    com = {
        "category":        "Notes",
        "type":            "note",
        "owner":           note.owner,
        "_id":             str(note._id),
        "anchorRef":       note.ref,
        "anchorVerse":     anchor_oref.sections[-1],
        "anchorText":      getattr(note, "anchorText", ""),
        "public":          getattr(note, "public", False),
        "commentator":     user_link(note.owner),
        "text":            note.text,
        "title":           getattr(note, "title", ""),
        "ownerName":       ownerData["name"],
        "ownerProfileUrl": ownerData["profileUrl"],
        "ownerImageUrl":   ownerData["imageUrl"],
    }
    return com


def format_sheet_as_link(sheet):
    sheet["category"]        = sheet["collectionTOC"].get("dependence", sheet["collectionTOC"]["categories"][0])
    sheet["collectiveTitle"] = sheet["collectionTOC"]["collectiveTitle"] if "collectiveTitle" in sheet["collectionTOC"] else {"en": sheet["collectionTOC"]["title"], "he": sheet["collectionTOC"]["heTitle"]}
    sheet["index_title"]     = sheet["collectiveTitle"]["en"]
    sheet["sourceRef"]       = sheet["title"]
    sheet["sourceHeRef"]     = sheet["title"]
    sheet["isSheet"]         = True
    return sheet


def get_notes(oref, public=True, uid=None, context=1):
    """
    Returns a list of notes related to ref.
    If public, include any public note.
    If uid is set, return private notes of uid.
    """
    noteset = oref.noteset(public, uid)
    notes = [format_object_for_client(n) for n in noteset]

    return notes


def get_links(tref, with_text=True, with_sheet_links=False):
    """
    Return a list of links tied to 'ref' in client format.
    If `with_text`, retrieve texts for each link.
    If `with_sheet_links` include sheet results for sheets in collections which are listed in the TOC.
    """
    links = []
    oref = Ref(tref)
    nRef = oref.normal()
    lenRef = len(nRef)
    reRef = oref.regex() if oref.is_range() else None

    # for storing all the section level texts that need to be looked up
    texts = {}

    linkset = LinkSet(oref)
    # For all links that mention ref (in any position)
    for link in linkset:
        # each link contains 2 refs in a list
        # find the position (0 or 1) of "anchor", the one we're getting links for
        # If both sides of the ref are in the same section of a text, only one direction will be used.  bug? maybe not.
        if reRef:
            pos = 0 if any(re.match(reRef, tref) for tref in link.expandedRefs0) else 1
        else:
            pos = 0 if any(nRef == tref[:lenRef] for tref in link.expandedRefs0) else 1
        try:
            # Skip any anchor refs that aren't segment level.  Unrolling the call to is_segment_level() here, just to save the N function calls.
            anchor_ref = Ref(link.refs[pos])
            node_depth = getattr(anchor_ref.index_node, "depth", None)
            if node_depth is None or len(anchor_ref.sections) != node_depth:
                continue

            # Skip any related refs that are super section level
            source_ref = Ref(link.refs[0 if pos == 1 else 1])
            node_depth = getattr(source_ref.index_node, "depth", None)
            if node_depth is None or len(source_ref.sections) + 1 < node_depth:
                continue

            com = format_link_object_for_client(link, False, nRef, pos)
        except InputError:
            logger.warning("Bad link: {} - {}".format(link.refs[0], link.refs[1]))
            continue
        except AttributeError as e:
            logger.error("AttributeError in presenting link: {} - {} : {}".format(link.refs[0], link.refs[1], e))
            continue

        # Rather than getting text with each link, walk through all links here,
        # caching text so that redundant DB calls can be minimized
        # If link is spanning, split into section refs and rejoin
        try:
            if with_text:
                original_com_oref = Ref(com["ref"])
                com_orefs = original_com_oref.split_spanning_ref()
                for com_oref in com_orefs:
                    top_oref = com_oref.top_section_ref()
                    # Lookup and save top level text, only if we haven't already
                    top_nref = top_oref.normal()
                    if top_nref not in texts:
                        for lang in ("en", "he"):
                            top_nref_tc = TextChunk(top_oref, lang)
                            versionInfoMap = None if not top_nref_tc._versions else {
                                v.versionTitle: {
                                    'license': getattr(v, 'license', ''),
                                    'versionTitleInHebrew': getattr(v, 'versionTitleInHebrew', '')
                                } for v in top_nref_tc._versions
                            }
                            if top_nref_tc.is_merged:
                                version = top_nref_tc.sources
                                license = [versionInfoMap[vtitle]['license'] for vtitle in version]
                                versionTitleInHebrew = [versionInfoMap[vtitle]['versionTitleInHebrew'] for vtitle in version]
                            elif top_nref_tc._versions:
                                version_obj = top_nref_tc.version()
                                version = version_obj.versionTitle
                                license = versionInfoMap[version]['license']
                                versionTitleInHebrew = versionInfoMap[version]['versionTitleInHebrew']
                            else:
                                # version doesn't exist in this language
                                version = None
                                license = None
                                versionTitleInHebrew = None
                            version = top_nref_tc.sources if top_nref_tc.is_merged else (top_nref_tc.version().versionTitle if top_nref_tc._versions else None)
                            if top_nref not in texts:
                                texts[top_nref] = {}
                            texts[top_nref][lang] = {
                                'ja': top_nref_tc.ja(),
                                'version': version,
                                'license': license,
                                'versionTitleInHebrew': versionTitleInHebrew
                            }
                    com_sections = [i - 1 for i in com_oref.sections]
                    com_toSections = [i - 1 for i in com_oref.toSections]
                    for lang, (attr, versionAttr, licenseAttr, vtitleInHeAttr) in (
                            ("he", ("he","heVersionTitle","heLicense","heVersionTitleInHebrew")),
                            ("en", ("text", "versionTitle","license","versionTitleInHebrew"))):
                        temp_nref_data = texts[top_nref][lang]
                        # Because of how the jagged arrays work, res may be either a single line or a list of lines
                        res = temp_nref_data['ja'].subarray(com_sections[1:], com_toSections[1:]).array()
                        if attr not in com: # If this is the first com_oref, and so the object doesn't contain any data in this field,
                            com[attr] = res  # Set the text directly in the object
                        else:  # This is not the first oref (this was a spanning ref, e.g. "Ketubot 110b:25-111a:1".
                            # We'll want to connect the texts from all pages together. As mentioned, each can be either a string or a list.
                            if isinstance(com[attr], str):
                                com[attr] = [com[attr]]
                            if isinstance(res, str):
                                res = [res]
                            com[attr] += res  # Once they're both definitely lists, merge the lists together.
                        temp_version = temp_nref_data['version']
                        if isinstance(temp_version, str) or temp_version is None:
                            com[versionAttr] = temp_version
                            com[licenseAttr] = temp_nref_data['license']
                            com[vtitleInHeAttr] = temp_nref_data['versionTitleInHebrew']
                        else:
                            # merged. find exact version titles for each segment
                            start_sources = temp_nref_data['ja'].distance([], com_sections[1:])
                            if com_sections == com_toSections:
                                # simplify for the common case
                                versions = temp_version[start_sources] if start_sources < len(temp_version) - 1 else None
                                licenses = temp_nref_data['license'][start_sources] if start_sources < len(temp_nref_data['license']) - 1 else None
                                versionTitlesInHebrew = temp_nref_data['versionTitleInHebrew'][start_sources] if start_sources < len(temp_nref_data['versionTitleInHebrew']) - 1 else None
                            else:
                                end_sources = temp_nref_data['ja'].distance([], com_toSections[1:])
                                versions = temp_version[start_sources:end_sources + 1]
                                licenses = temp_nref_data['license'][start_sources:end_sources + 1]
                                versionTitlesInHebrew = temp_nref_data['versionTitleInHebrew'][start_sources:end_sources + 1]
                            com[versionAttr] = versions
                            com[licenseAttr] = licenses
                            com[vtitleInHeAttr] = versionTitlesInHebrew
            links.append(com)
        except NoVersionFoundError as e:
            logger.warning("Trying to get non existent text for ref '{}'. Link refs were: {}".format(top_nref, link.refs))
            continue

    # Hard-coding automatic display of links to an underlying text. bound_texts = ("Rashba on ",)
    # E.g., when requesting "Steinsaltz on X" also include links to "X" as though they were connected directly to Steinsaltz.
    bound_texts = ("Steinsaltz on ",)
    for prefix in bound_texts:
        if nRef.startswith(prefix):
            base_ref = nRef[len(prefix):]
            base_links = get_links(base_ref)
            def add_prefix(link):
                link["anchorRef"] = prefix + link["anchorRef"]
                link["anchorRefExpanded"] = [prefix + l for l in link["anchorRefExpanded"]]
                return link
            base_links = [add_prefix(link) for link in base_links]
            orig_links_refs = [(origlink['sourceRef'], origlink['anchorRef']) for origlink in links]
            base_links = [x for x in base_links if ((x['sourceRef'], x['anchorRef']) not in orig_links_refs) and (x["sourceRef"] != x["anchorRef"])]
            links += base_links

    collections = library.get_collections_in_library()
    if with_sheet_links and len(collections):
        sheet_links = get_sheets_for_ref(tref, in_collection=collections)
        formatted_sheet_links = [format_sheet_as_link(sheet) for sheet in sheet_links]
        links += formatted_sheet_links

    return links

```

### sefaria/client/util.py

```

import json
from datetime import datetime

from django.http import HttpResponse
from django.core.mail import EmailMultiAlternatives
from webpack_loader import utils as webpack_utils

from sefaria import settings as sls
# from sefaria.model.user_profile import UserProfile


def jsonResponse(data, callback=None, status=200):
    if callback:
        return jsonpResponse(data, callback, status)
    #these next few lines are a quick hack.  this needs thought.
    try: # Duck typing on AbstractMongoRecord's contents method
        data = data.contents()
    except AttributeError:
        pass

    if data is None:
        data = {"error": 'No data available'}

    if "_id" in data:
        data["_id"] = str(data["_id"])

    if isinstance(data, dict):
        for key in list(data.keys()):
            if isinstance(data[key], datetime):
                data[key] = data[key].isoformat()

    return HttpResponse(json.dumps(data, ensure_ascii=False), content_type="application/json; charset=utf-8", charset="utf-8", status=status)


def jsonpResponse(data, callback, status=200):
    if "_id" in data:
        data["_id"] = str(data["_id"])
    return HttpResponse("%s(%s)" % (callback, json.dumps(data, ensure_ascii=False)), content_type="application/javascript; charset=utf-8", charset="utf-8", status=status)


def celeryResponse(task_id: str, sub_task_ids: list[str] = None):
    data = {'task_id': task_id}
    if sub_task_ids:
        data['sub_task_ids'] = sub_task_ids
    return jsonResponse(data, status=202)

def send_email(subject, message_html, from_email, to_email):
    msg = EmailMultiAlternatives(subject, message_html, "Sefaria <hello@sefaria.org>", [to_email], reply_to=[from_email])
    msg.send()

    return True


def read_webpack_bundle(config_name):
    webpack_files = webpack_utils.get_files('main', config=config_name)
    bundle_path = sls.relative_to_abs_path('..' + webpack_files[0]["url"])
    with open(bundle_path, 'r') as file:
        return file.read()

```

### sefaria/client/__init__.py

```

```

### sefaria/gauth/__init__.py

```

```

### sefaria/gauth/tests.py

```
"""
This file demonstrates writing tests using the unittest module. These will pass
when you run "manage.py test".

Replace this with more appropriate tests for your application.
"""

from django.test import TestCase


class SimpleTest(TestCase):
    def test_basic_addition(self):
        """
        Tests that 1 + 1 always equals 2.
        """
        self.assertEqual(1 + 1, 2)

```

### sefaria/gauth/views.py

```
import os
import datetime
import json

from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import reverse
from django.http import HttpResponseBadRequest
from django.shortcuts import redirect

from sefaria.model.user_profile import UserProfile

import google.auth
import google.oauth2
import google_auth_oauthlib.flow

from sefaria import settings

# CLIENT_SECRETS, name of a file containing the OAuth 2.0 information for this
# application, including client_id and client_secret, which are found
# on the API Access tab on the Google APIs
# Console <http://code.google.com/apis/console>
# CLIENT_SECRETS = os.path.join(os.path.dirname(__file__), 'client_secrets.json')

@login_required
def index(request):
    """
    Step 1 of Google OAuth 2.0 flow.
    """
    # get authorization url

    flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(
        settings.GOOGLE_OAUTH2_CLIENT_SECRET_FILEPATH,
        scopes=request.session.get('gauth_scope', '')
    )

    redirect_url = request.build_absolute_uri(reverse('gauth_callback')).replace("http:", "https:")
    flow.redirect_uri = redirect_url

    authorization_url, _ = flow.authorization_url(
        access_type='offline',
        include_granted_scope='true',
    )

    try:
        request.session['next_view'] = request.GET['next']
    except KeyError:
        pass


    return redirect(authorization_url)

@login_required
def auth_return(request):
    """
    Step 2 of Google OAuth 2.0 flow.
    """
    state = request.GET.get('state', None)

    if not state:
        return redirect('gauth_index')

    flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(
        settings.GOOGLE_OAUTH2_CLIENT_SECRET_FILEPATH,
        scopes=request.session.get('gauth_scope', ''),
        state=state
    )

    redirect_url = request.build_absolute_uri(reverse('gauth_callback')).replace("http:", "https:")
    flow.redirect_uri = redirect_url

    # flow.redirect_uri = request.session.get('next_view', '/')

    authorization_response = request.build_absolute_uri().replace("http:", "https:")
    flow.fetch_token(authorization_response=authorization_response)
    credentials = flow.credentials

    credentials_dict = {
        'token': credentials.token,
        'refresh_token': credentials.refresh_token,
        'id_token':credentials.id_token,
        'token_uri': credentials.token_uri,
        'client_id': credentials.client_id,
        'client_secret': credentials.client_secret,
        'scopes': credentials.scopes,
        'expiry': datetime.datetime.strftime(credentials.expiry, '%Y-%m-%d %H:%M:%S')
    }

    profile = UserProfile(user_obj=request.user)

    if profile.gauth_token and profile.gauth_token["refresh_token"] and credentials_dict["refresh_token"] is None:
        credentials_dict["refresh_token"] = profile.gauth_token["refresh_token"]

    profile.update({"gauth_token": credentials_dict})
    profile.save()

    # return credentials

    return redirect(request.session.get('next_view', '/'))

```

### sefaria/gauth/decorators.py

```
from functools import wraps
import datetime

from django.contrib.auth.decorators import login_required
from django.http import HttpResponse
from django.shortcuts import redirect

from sefaria.model.user_profile import UserProfile

import google.auth
import google.oauth2
import google_auth_oauthlib.flow

import structlog
logger = structlog.get_logger(__name__)

def gauth_required(scope, ajax=False):
    """
    Decorator that requires the user to authenticate
    with Google and authorize Sefaria to act on their behalf.
    If the user has already authenticated, it will call the wrapped function
    with the kwarg `credential` set to the obtained credentials.
    If not, it will start the OAuth 2.0 flow.
    At the moment, only used for sheets.views.export_to_drive.
    """
    def decorator(func):
        @login_required
        @wraps(func)
        def inner(request, *args, **kwargs):
            # Try grabbing credential from storage
            profile = UserProfile(user_obj=request.user)
            credentials_dict = profile.gauth_token

            if credentials_dict is None or not set(scope).issubset(set(credentials_dict['scopes'])):
                request.session['next_view'] = request.path
                request.session['gauth_scope'] = scope
                return (HttpResponse('Unauthorized', status=401)
                    if ajax else redirect('gauth_index'))
           
            credentials = google.oauth2.credentials.Credentials(
                credentials_dict['token'],
                refresh_token=credentials_dict['refresh_token'],
                id_token=credentials_dict['id_token'],
                token_uri=credentials_dict['token_uri'],
                client_id=credentials_dict['client_id'],
                client_secret=credentials_dict['client_secret'],
                scopes=[credentials_dict['scopes']],
            )

            expiry = datetime.datetime.strptime(credentials_dict['expiry'], '%Y-%m-%d %H:%M:%S')
            credentials.expiry = expiry
            auth_request = google.auth.transport.requests.Request()
            if credentials.expired:
                try:
                    credentials.refresh(auth_request)
                except:
                    request.session['next_view'] = request.path
                    request.session['gauth_scope'] = scope
                    return (HttpResponse('Unauthorized', status=401)
                            if ajax else redirect('gauth_index'))

            # Everything went well, call wrapped view and give credential to it
            kwargs['credential'] = credentials
            return func(request, *args, **kwargs)
        return inner
    return decorator

```

### sefaria/profiling.py

```
"""
profiling.py - tools for profiling performance. 
"""

import cProfile
import pstats

def prof(cmd):
	"""
	Runs cmd and prints the profile sorted by cumulative time.
	"""
	cProfile.run(cmd, "stats")
	p = pstats.Stats("stats")
	p.strip_dirs().sort_stats("cumulative").print_stats()
```

### sefaria/views.py

```
# -*- coding: utf-8 -*-
import io
import os
import zipfile
import json
import re
import bleach
from datetime import datetime, timedelta
from urllib.parse import urlparse
from collections import defaultdict
from random import choice

from django.utils.translation import ugettext as _
from django.conf import settings
from django.http import HttpResponse, HttpResponseRedirect, Http404, HttpResponseBadRequest
from django.shortcuts import render, redirect
from django.template.loader import render_to_string
from django.template.response import TemplateResponse
from django.utils.http import is_safe_url
from django.utils.cache import patch_cache_control
from django.contrib.auth import authenticate
from django.contrib.auth import REDIRECT_FIELD_NAME, login as auth_login, logout as auth_logout
from django.contrib.auth.forms import UserCreationForm, AuthenticationForm
from django.contrib.auth.decorators import login_required
from django.contrib.sites.shortcuts import get_current_site
from django.contrib.admin.views.decorators import staff_member_required
from django.db import transaction
from django.views.decorators.debug import sensitive_post_parameters
from django.views.decorators.cache import never_cache
from django.views.decorators.csrf import csrf_protect, csrf_exempt
from django.urls import resolve
from django.urls.exceptions import Resolver404
from rest_framework.decorators import api_view
from rest_framework_simplejwt.serializers import TokenObtainPairSerializer

from sefaria.decorators import webhook_auth_or_staff_required
import sefaria.model as model
import sefaria.system.cache as scache
from sefaria.helper.crm.crm_mediator import CrmMediator
from sefaria.helper.crm.salesforce import SalesforceNewsletterListRetrievalError
from sefaria.system.cache import get_shared_cache_elem, in_memory_cache, set_shared_cache_elem
from sefaria.client.util import jsonResponse, send_email, read_webpack_bundle
from sefaria.forms import SefariaNewUserForm, SefariaNewUserFormAPI, SefariaDeleteUserForm, SefariaDeleteSheet
from sefaria.settings import MAINTENANCE_MESSAGE, USE_VARNISH, MULTISERVER_ENABLED
from sefaria.model.user_profile import UserProfile, user_link
from sefaria.model.collection import CollectionSet, process_sheet_deletion_in_collections
from sefaria.model.notification import process_sheet_deletion_in_notifications
from sefaria.export import export_all as start_export_all
from sefaria.datatype.jagged_array import JaggedTextArray
# noinspection PyUnresolvedReferences
from sefaria.system.exceptions import InputError, NoVersionFoundError
from api.api_errors import APIInvalidInputException
from sefaria.system.database import db
from sefaria.system.decorators import catch_error_as_http
from sefaria.utils.hebrew import has_hebrew, strip_nikkud
from sefaria.utils.util import strip_tags
from sefaria.helper.text import make_versions_csv, get_library_stats, get_core_link_stats, dual_text_diff
from sefaria.clean import remove_old_counts
from sefaria.search import index_sheets_by_timestamp as search_index_sheets_by_timestamp
from sefaria.model import *
from sefaria.model.webpage import *
from sefaria.system.multiserver.coordinator import server_coordinator
from sefaria.google_storage_manager import GoogleStorageManager
from sefaria.sheets import get_sheet_categorization_info
from reader.views import base_props, render_template
from sefaria.helper.link import add_links_from_csv, delete_links_from_text, get_csv_links_by_refs, remove_links_from_csv

if USE_VARNISH:
    from sefaria.system.varnish.wrapper import invalidate_index, invalidate_title, invalidate_ref, invalidate_counts, invalidate_all

import structlog
logger = structlog.get_logger(__name__)


def process_register_form(request, auth_method='session'):
    from sefaria.utils.util import epoch_time
    from sefaria.helper.file import get_resized_file
    import hashlib
    import urllib.parse, urllib.request
    from google.cloud.exceptions import GoogleCloudError
    from PIL import Image
    form = SefariaNewUserForm(request.POST) if auth_method == 'session' else SefariaNewUserFormAPI(request.POST)
    token_dict = None
    if form.is_valid():
        with transaction.atomic():
            new_user = form.save()
            user = authenticate(email=form.cleaned_data['email'],
                                password=form.cleaned_data['password1'])
            p = UserProfile(id=user.id, user_registration=True)
            p.assign_slug()
            p.join_invited_collections()
            if hasattr(request, "interfaceLang"):
                p.settings["interface_language"] = request.interfaceLang


            # auto-add profile pic from gravatar if exists
            email_hash = hashlib.md5(p.email.lower().encode('utf-8')).hexdigest()
            gravatar_url = "https://www.gravatar.com/avatar/" + email_hash + "?d=404&s=250"
            try:
                with urllib.request.urlopen(gravatar_url) as r:
                    bucket_name = GoogleStorageManager.PROFILES_BUCKET
                    with Image.open(r) as image:
                        now = epoch_time()
                        big_pic_url = GoogleStorageManager.upload_file(get_resized_file(image, (250, 250)), "{}-{}.png".format(p.slug, now), bucket_name, None)
                        small_pic_url = GoogleStorageManager.upload_file(get_resized_file(image, (80, 80)), "{}-{}-small.png".format(p.slug, now), bucket_name, None)
                        p.profile_pic_url = big_pic_url
                        p.profile_pic_url_small = small_pic_url
            except urllib.error.HTTPError as e:
                logger.info("The Gravatar server couldn't fulfill the request. Error Code {}".format(e.code))
            except urllib.error.URLError as e:
                logger.info("HTTP Error from Gravatar Server. Reason: {}".format(e.reason))
            except GoogleCloudError as e:
                logger.warning("Error communicating with Google Storage Manager. {}".format(e))
            p.save()

        if auth_method == 'session':
            auth_login(request, user)
        elif auth_method == 'jwt':
            token_dict = TokenObtainPairSerializer().validate({"username": form.cleaned_data['email'], "password": form.cleaned_data['password1']})
    return {
        k: v[0] if len(v) > 0 else str(v) for k, v in list(form.errors.items())
    }, token_dict, form


@api_view(["POST"])
def register_api(request):
    errors, token_dict, _ = process_register_form(request, auth_method='jwt')
    if len(errors) == 0:
        return jsonResponse(token_dict)

    return jsonResponse(errors)


def register(request):
    if request.user.is_authenticated:
        return redirect("login")

    next = request.GET.get('next', '')

    if request.method == 'POST':
        errors, _, form = process_register_form(request)
        if len(errors) == 0:
            if "noredirect" in request.POST:
                return HttpResponse("ok")
            elif "new?assignment=" in request.POST.get("next",""):
                next = request.POST.get("next", "")
                return HttpResponseRedirect(next)
            else:
                next = request.POST.get("next", "/")
                if "?" in next:
                    next += "&welcome=to-sefaria"
                else:
                    next += "?welcome=to-sefaria"
                return HttpResponseRedirect(next)
    else:
        if request.GET.get('educator', ''):
            form = SefariaNewUserForm(initial={'subscribe_educator': True})
        else:
            form = SefariaNewUserForm()

    return render_template(request, "registration/register.html", None, {'form': form, 'next': next})


def maintenance_message(request):
    resp = render_template(request,"static/maintenance.html", None, {"message": MAINTENANCE_MESSAGE}, status=503)
    return resp


def accounts(request):
    return render_template(request,"registration/accounts.html", None, {
        "createForm": UserCreationForm(),
        "loginForm": AuthenticationForm()
    })


def generic_subscribe_to_newsletter_api(request, org, email):
    """
    Generic view for subscribing a user to a newsletter
    """
    org_subscribe_fn_map = {
        "sefaria": subscribe_sefaria_newsletter,
        "steinsaltz": subscribe_steinsaltz,
    }
    body = json.loads(request.body)
    first_name = body.get("firstName")
    last_name = body.get("lastName")
    if not first_name or not last_name:
        return jsonResponse({"error": "You must provide first and last name."})
    try:
        subscribe = org_subscribe_fn_map.get(org)
        if not subscribe:
            return jsonResponse({"error": f"Organization '{org}' not recognized."})
        if subscribe(request, email, first_name, last_name):
            return jsonResponse({"status": "ok"})
        else:
            logger.error(f"Failed to subscribe to list")
            return jsonResponse({"error": _("Sorry, there was an error.")})
    except ValueError as e:
        logger.error(f"Failed to subscribe to list: {e}")
        return jsonResponse({"error": _("Sorry, there was an error.")})


def subscribe_sefaria_newsletter_view(request, email):
    return generic_subscribe_to_newsletter_api(request, 'sefaria', email)


def subscribe_sefaria_newsletter(request, email, first_name, last_name):
    """
    API for subscribing to mailing lists
    * By default, the user's email address is subscribed to the default lists: "Master," "General Updates" (or the one for Hebrew), and "Educator Updates" (if the user is an educator).
    * If the user's email address already exists, the email address to those lists is not resubscribed to those lists
    * When you provide additional newsletter mailing lists, the email address will be subscribed to those lists, along with the default ones, if the email address does not already exist.
    * However, if we pass the email address with any of those default newsletter lists as additional ones, Salesforce will resubscribe the email address to those lists.
    """
    body = json.loads(request.body)
    language = body.get("language", "")
    educator = body.get("educator", False)
    mailing_lists = body.get("lists", [])
    crm_mediator = CrmMediator()
    return crm_mediator.subscribe_to_lists(email, first_name, last_name, educator=educator, lang=language, mailing_lists=mailing_lists)

@csrf_exempt
def get_available_newsletter_mailing_lists(request):
    try:
        return jsonResponse({"newsletter_mailing_lists": CrmMediator().get_available_lists()})
    except SalesforceNewsletterListRetrievalError as e:
        return jsonResponse({"error": str(e)}, status=502)
    except:
        return jsonResponse({"error": "Unknown error occurred"}, status=500)

def subscribe_steinsaltz(request, email, first_name, last_name):
    """
    API for subscribing to Steinsaltz newsletter
    """
    import requests

    data = {
        "first_name": first_name,
        "last_name": last_name,
        "email": email,
    }
    headers = {'Content-Type': 'application/json'}
    response = requests.post('https://steinsaltz-center.org/api/mailer',
                             data=json.dumps(data), headers=headers)
    return response.ok


@login_required
def unlink_gauth(request):
    profile = UserProfile(id=request.user.id)
    try:
        profile.update({"gauth_token": None, "gauth_email": None})
        profile.save()
        redir = bool(int(request.GET.get("redirect", True)))
        if redir:
            return redirect(f"/profile/{profile.slug}")
        else:
            return jsonResponse({"status": "ok"})
    except:
        return jsonResponse({"error": "Failed to delete Google account"})


def generate_feedback(request):

    data = json.loads(request.POST.get('json', {}))

    fb_type = data.get('type', None)
    refs = data.get('refs', None)
    url = data.get('url', None)
    versions = data.get('currVersions', None)
    uid = data.get('uid', None)
    from_email = data.get('email', None)
    msg = data.get('msg', None)

    if not from_email:
        from_email = model.user_profile.UserProfile(id=uid).email

    if fb_type == "content_issue":
        to_email = "corrections@sefaria.org"
        subject = "Correction from website - " + ' / '.join(refs)
        message_html = msg + "\n\n" + "refs: " + ' / '.join(refs) + "\n" + "versions: " + str(versions) + "\n\n" + "URL: " + url
    elif fb_type == "user_testing":
        to_email = "gabriel@sefaria.org"
        subject = "User Testing Sign Up"
        message_html = "Hi! I want to sign up for user testing!"
    else:
        to_email = "hello@sefaria.org"
        subject = "Feedback from website - " + fb_type.replace("_"," ")
        message_html = msg + "\n\n" + "URL: " + url

    try:
        send_email(subject, message_html, from_email, to_email)
        return jsonResponse({"status": "ok"})
    except:
        return jsonResponse({"error": _("Sorry, there was an error.")})


def data_js(request):
    """
    Javascript populating dynamic data like book lists, toc.
    """
    response = render(request, "js/data.js", content_type="text/javascript; charset=utf-8")
    patch_cache_control(response, max_age=31536000, immutable=True)
    # equivalent to: response['Cache-Control'] = 'max-age=31536000, immutable'
    # cache for a year (cant cache indefinitely) and mark immutable so browser cache never revalidates.
    # This saves any roundtrip to the server untill the data.js url is changed upon update.
    return response


def sefaria_js(request):
    """
    Packaged Sefaria.js.
    """
    data_js = render_to_string("js/data.js", context={}, request=request)
    sefaria_js = read_webpack_bundle("SEFARIA_JS")
    attrs = {
        "data_js": data_js,
        "sefaria_js": sefaria_js,
    }

    return render(request, "js/sefaria.js", attrs, content_type= "text/javascript; charset=utf-8")


def linker_js(request, linker_version=None):
    """
    Javascript of Linker plugin.
    """
    CURRENT_LINKER_VERSION = "2"
    linker_version = linker_version or CURRENT_LINKER_VERSION

    if linker_version == "3":
        # linker.v3 is bundled using webpack as opposed to previous versions which are django templates
        return HttpResponse(read_webpack_bundle("LINKER"), content_type="text/javascript; charset=utf-8")

    linker_link = "js/linker.v" + linker_version + ".js"

    attrs = {
        "book_titles": json.dumps(model.library.citing_title_list("en")
                      + model.library.citing_title_list("he"), ensure_ascii=False)
    }

    return render(request, linker_link, attrs, content_type = "text/javascript; charset=utf-8")


@api_view(["POST"])
def find_refs_report_api(request):
    from sefaria.system.database import db
    post = json.loads(request.body)
    db.linker_feedback.insert_one(post)
    return jsonResponse({'ok': True})


@api_view(["POST"])
def find_refs_api(request):
    from sefaria.helper.linker.linker import make_find_refs_response
    try:
        return jsonResponse(make_find_refs_response(request))
    except APIInvalidInputException as e:
        return e.to_json_response()


@api_view(["GET"])
def websites_api(request, domain):
    cb = request.GET.get("callback", None)
    domain = WebPage.normalize_url(domain)
    website = WebSite().load({"domains": domain})
    if website is None:
        return jsonResponse({"error": f"no website found with domain: '{domain}'"})
    return jsonResponse(website.contents(), cb)


def linker_data_api(request, titles):
    if request.method == "GET":
        cb = request.GET.get("callback", None)
        res = {}
        title_regex = title_regex_api(request, titles, json_response=False)
        if "error" in title_regex:
            res["error"] = title_regex.pop("error")
        res["regexes"] = title_regex
        url = request.GET.get("url", "")
        domain = WebPage.domain_for_url(WebPage.normalize_url(url))

        website_match = WebSiteSet({"domains": domain})  # we know there can only be 0 or 1 matches found because of a constraint
                                                         # enforced in Sefaria-Data/sources/WebSites/populate_web_sites.py
        res["exclude_from_tracking"] = getattr(website_match[0], "exclude_from_tracking", "") if website_match.count() == 1 else ""
        resp = jsonResponse(res, cb)
        return resp
    else:
        return jsonResponse({"error": "Unsupported HTTP method."})


def title_regex_api(request, titles, json_response=True):
    if request.method == "GET":
        cb = request.GET.get("callback", None)
        parentheses = bool(int(request.GET.get("parentheses", False)))
        res = {}
        titles = set(titles.split("|"))
        errors = []
        # check request.domain and then look up in WebSites collection to get linker_params and return both resp and linker_params
        for title in titles:
            lang = "he" if has_hebrew(title) else "en"
            try:
                re_string = model.library.get_regex_string(title, lang, anchored=False, for_js=True, parentheses=parentheses)
                res[title] = re_string
            except (AttributeError, AssertionError) as e:
                # There are normal errors here, when a title matches a schema node, the chatter fills up the logs.
                # logger.warning(u"Library._build_ref_from_string() failed to create regex for: {}.  {}".format(title, e))
                errors.append("{} : {}".format(title, e))
        if len(errors):
            res["error"] = errors
        resp = jsonResponse(res, cb)
        return resp if json_response else res
    else:
        return jsonResponse({"error": "Unsupported HTTP method."}) if json_response else {"error": "Unsupported HTTP method."}


def bundle_many_texts(refs, use_text_family=False, as_sized_string=False, min_char=None, max_char=None, translation_language_preference=None, english_version=None, hebrew_version=None):
    res = {}
    for tref in refs:
        try:
            oref = model.Ref(tref)
            lang = "he" if has_hebrew(tref) else "en"
            if use_text_family:
                text_fam = model.TextFamily(oref, commentary=0, context=0, pad=False, translationLanguagePreference=translation_language_preference, stripItags=True,
                                            lang="he", version=hebrew_version,
                                            lang2="en", version2=english_version)
                he = text_fam.he
                en = text_fam.text
                res[tref] = {
                    'he': he,
                    'en': en,
                    'lang': lang,
                    'ref': oref.normal(),
                    'primary_category': text_fam.contents()['primary_category'],
                    'heRef': oref.he_normal(),
                    'url': oref.url()
                }
            else:
                he_tc = model.TextChunk(oref, "he", vtitle=hebrew_version)
                en_tc = model.TextChunk(oref, "en", actual_lang=translation_language_preference, vtitle=english_version)
                if hebrew_version and he_tc.is_empty():
                  raise NoVersionFoundError(f"{oref.normal()} does not have the Hebrew version: {hebrew_version}")
                if english_version and en_tc.is_empty():
                  raise NoVersionFoundError(f"{oref.normal()} does not have the English version: {english_version}")

                if as_sized_string:
                    kwargs = {}
                    if min_char:
                        kwargs['min_char'] = min_char
                    if max_char:
                        kwargs['max_char'] = max_char
                    he_text = he_tc.as_sized_string(**kwargs)
                    en_text = en_tc.as_sized_string(**kwargs)
                else:
                    he = he_tc.text
                    en = en_tc.text
                    # these could be flattened on the client, if need be.
                    he_text = he if isinstance(he, str) else JaggedTextArray(he).flatten_to_string()
                    en_text = en if isinstance(en, str) else JaggedTextArray(en).flatten_to_string()

                res[tref] = {
                    'he': he_text,
                    'en': en_text,
                    'lang': lang,
                    'ref': oref.normal(),
                    'heRef': oref.he_normal(),
                    'url': oref.url()
                }
        except (InputError, ValueError, AttributeError, KeyError) as e:
            # referer = request.META.get("HTTP_REFERER", "unknown page")
            # This chatter fills up the logs.  todo: put in it's own file
            # logger.warning(u"Linker failed to parse {} from {} : {}".format(tref, referer, e))
            res[tref] = {"error": 1}
    return res


def bulktext_api(request, refs):
    """
    Used by the linker.
    :param request:
    :param refs:
    :return:
    """
    if request.method == "GET":
        cb = request.GET.get("callback", None)
        refs = set(refs.split("|"))
        g = lambda x: request.GET.get(x, None)
        min_char = int(g("minChar")) if g("minChar") else None
        max_char = int(g("maxChar")) if g("maxChar") else None
        use_text_family = True if g("useTextFamily") == "1" else False
        res = bundle_many_texts(refs, use_text_family, g("asSizedString"), min_char, max_char, g("transLangPref"), g("ven"), g("vhe"))
        resp = jsonResponse(res, cb)
        return resp


@csrf_exempt
def linker_tracking_api(request):
    """
    API tracking hits on the linker and storing webpages from them.
    """
    if request.method != "POST":
        return jsonResponse({"error": "Method not implemented."})

    j = request.POST.get("json")
    if not j:
        return jsonResponse({"error": "Missing 'json' parameter in post data."})
    data = json.loads(j)

    status, webpage = WebPage.add_or_update_from_linker(data)

    return jsonResponse({"status": status})


def passages_api(request, refs):
    """
    Returns a dictionary, mapping the refs in the request to the sugya that they're a part of.

    :param request:
    :param refs:
    :return:
    """
    if request.method == "GET":
        response = {}
        cb = request.GET.get("callback", None)
        refs = set(refs.split("|"))

        # todo: Use PassageSet, so that it can be packaged as one query
        for tref in refs:
            try:
                oref = Ref(tref)
                p = Passage().load({"ref_list": oref.normal()})
                if p:
                    response[tref] = p.full_ref
                else:
                    response[tref] = oref.normal()
            except InputError:
                response[tref] = tref  # is this the best thing to do?  It passes junk along...

        resp = jsonResponse(response, cb)
        return resp


@login_required
def collections_image_upload(request, resize_image=True):
    from PIL import Image
    from tempfile import NamedTemporaryFile
    from sefaria.google_storage_manager import GoogleStorageManager
    from io import BytesIO
    import uuid
    if request.method == "POST":
        MAX_FILE_MB = 2
        MAX_FILE_SIZE = MAX_FILE_MB * 1024 * 1024
        MAX_FILE_DIMENSIONS = (1048, 1048)
        uploaded_file = request.FILES['file']
        if uploaded_file.size > MAX_FILE_SIZE:
            return jsonResponse({"error": "Uploaded files must be smaller than %dMB." % MAX_FILE_MB})
        name, extension = os.path.splitext(uploaded_file.name)
        with NamedTemporaryFile(suffix=extension) as temp_uploaded_file:
            temp_uploaded_file.write(uploaded_file.read())
            image = Image.open(temp_uploaded_file)
            resized_image_file = BytesIO()
            if resize_image:
                image.thumbnail(MAX_FILE_DIMENSIONS, Image.LANCZOS)
            image.save(resized_image_file, optimize=True, quality=70, format="PNG")
            resized_image_file.seek(0)
            bucket_name = GoogleStorageManager.COLLECTIONS_BUCKET
            unique_file_name = f"{request.user.id}-{uuid.uuid1()}.{uploaded_file.name[-3:].lower()}"
            try:
                url = GoogleStorageManager.upload_file(resized_image_file, unique_file_name, bucket_name)
                return jsonResponse({"status": "success", "url": url})
            except:
                return jsonResponse({"error": "There was an error uploading your file."})
    else:
        return jsonResponse({"error": "Unsupported HTTP method."})

@staff_member_required
def reset_cache(request):
    model.library.rebuild()

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "rebuild")

    if USE_VARNISH:
        invalidate_all()

    return HttpResponseRedirect("/?m=Cache-Reset")


@staff_member_required
def reset_websites_data(request):
    website_set = [w.contents() for w in WebSiteSet()]
    in_memory_cache.set("websites_data", website_set)
    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("in_memory_cache", "set", ["websites_data", website_set])
    return HttpResponseRedirect("/?m=Website-Data-Reset")


@staff_member_required
def reset_index_cache_for_text(request, title):

    index = model.library.get_index(title)
    model.library.refresh_index_record_in_cache(index)
    model.library.reset_text_titles_cache()

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "refresh_index_record_in_cache", [index.title])
    elif USE_VARNISH:
        invalidate_title(index.title)

    return HttpResponseRedirect("/%s?m=Cache-Reset" % model.Ref(title).url())


"""@staff_member_required
def view_cached_elem(request, title):
    return HttpResponse(get_template_cache('texts_list'), status=200)
"""

@staff_member_required
def reset_cached_api(request, apiurl):
    """
    This admin call gets the url of the original api that we wish to reset, backwards resolves that original function and gets its data back into cache
    :param request:
    :param apiurl:
    :return:
    """
    from undecorated import undecorated
    # from importlib import import_module
    try:
        match = resolve("/api/{}".format(apiurl))
        #mod = import_module(".".join(match.view_name.split(".")[:-1])) Dont actually need this, resolve gets us the func itself
        #func = mod.__getattribute__(match.func.func_name)

        if "django_cache" in match.func.__dict__:
            api_view = undecorated(match.func)
            redecorated_api_view = scache.django_cache(action="reset")(api_view)
            redecorated_api_view(request, *match.args, **match.kwargs)

            return HttpResponseRedirect("/api/{}".format(apiurl))
        else:
            raise Http404("API not in cache")

    except Resolver404 as re:
        logger.warn("Attempted to reset invalid url")
        raise Http404()
    except Exception as e:
        logger.warn("Unable to reset cache for {}".format(apiurl))
        raise Http404()


@staff_member_required
def reset_counts(request, title=None):
    if title:
        try:
            i = model.library.get_index(title)
        except:
            return HttpResponseRedirect("/dashboard?m=Unknown-Book")
        vs = model.VersionState(index=i)
        vs.refresh()

        return HttpResponseRedirect("/%s?m=Counts-Rebuilt" % model.Ref(i.title).url())
    else:
        model.refresh_all_states()

        if MULTISERVER_ENABLED:
            server_coordinator.publish_event("library", "rebuild_toc")

        return HttpResponseRedirect("/?m=Counts-Rebuilt")


@staff_member_required
def delete_orphaned_counts(request):
    remove_old_counts()
    scache.delete_template_cache("texts_dashboard")

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("scache", "delete_template_cache", ["texts_dashboard"])

    return HttpResponseRedirect("/dashboard?m=Orphaned-counts-deleted")


@staff_member_required
def rebuild_toc(request):
    model.library.rebuild_toc()

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "rebuild_toc")

    return HttpResponseRedirect("/?m=TOC-Rebuilt")


@staff_member_required
def rebuild_auto_completer(request):
    library.build_full_auto_completer()
    library.build_lexicon_auto_completers()
    library.build_cross_lexicon_auto_completer()

    if MULTISERVER_ENABLED:
        server_coordinator.publish_event("library", "build_full_auto_completer")
        server_coordinator.publish_event("library", "build_lexicon_auto_completers")
        server_coordinator.publish_event("library", "build_cross_lexicon_auto_completer")

    return HttpResponseRedirect("/?m=auto-completer-Rebuilt")


@staff_member_required
def reset_varnish(request, tref):
    if USE_VARNISH:
        oref = model.Ref(tref)
        if oref.is_book_level():
            invalidate_index(oref.index)
            invalidate_counts(oref.index)
        invalidate_ref(oref)
        return HttpResponseRedirect("/?m=Varnish-Reset-For-{}".format(oref.url()))
    return HttpResponseRedirect("/?m=Varnish-Not-Enabled")


@staff_member_required
def reset_ref(request, tref):
    """
    resets cache, versionstate, toc, varnish, & book TOC template
    :param tref:
    :return:
    """
    oref = model.Ref(tref)
    if oref.is_book_level():
        model.library.refresh_index_record_in_cache(oref.index)
        model.library.reset_text_titles_cache()
        index = model.library.get_index(tref)  # Get a fresh instance of the index
        vs = model.VersionState(index=index)
        vs.refresh()
        model.library.update_index_in_toc(index)

        if MULTISERVER_ENABLED:
            server_coordinator.publish_event("library", "refresh_index_record_in_cache", [oref.index.title])
            server_coordinator.publish_event("library", "update_index_in_toc", [oref.index.title])
        elif USE_VARNISH:
            invalidate_title(oref.index.title)

        return HttpResponseRedirect("/{}?m=Reset-Index".format(oref.url()))

    elif USE_VARNISH:
        invalidate_ref(oref)
        return HttpResponseRedirect("/{}?m=Reset-Ref".format(oref.url()))

    else:
        return HttpResponseRedirect("/?m=Nothing-to-Reset")


@staff_member_required
def rebuild_auto_links(request, title):
    from sefaria.helper.link import rebuild_links_for_title as rebuild
    rebuild(title, request.user.id)
    return HttpResponseRedirect("/?m=Automatic-Links-Rebuilt-on-%s" % title)


@staff_member_required
def rebuild_citation_links(request, title):
    from sefaria.helper.link import rebuild_links_from_text as rebuild
    rebuild(title, request.user.id)
    return HttpResponseRedirect("/?m=Citation-Links-Rebuilt-on-%s" % title)

@csrf_exempt
@webhook_auth_or_staff_required
def rebuild_shared_cache(request):
    regenerating = get_shared_cache_elem("regenerating")
    status = "build in progress" if regenerating else "start rebuilding"
    if not regenerating:
        set_shared_cache_elem("regenerating", True)
        library.init_shared_cache(rebuild=True)
    return jsonResponse({"status": status})

@staff_member_required
def delete_citation_links(request, title):
    delete_links_from_text(title, request.user.id)
    return HttpResponseRedirect("/?m=Citation-Links-Deleted-on-%s" % title)


@staff_member_required
def cache_stats(request):
    import resource
    from sefaria.utils.util import get_size
    from sefaria.model.user_profile import public_user_data_cache
    # from sefaria.sheets import last_updated
    resp = {
        'ref_cache_size': f'{model.Ref.cache_size():,}',
        # 'ref_cache_bytes': model.Ref.cache_size_bytes(), # This pretty expensive, not sure if it should run on prod.
        'public_user_data_size': f'{len(public_user_data_cache):,}',
        'public_user_data_bytes': f'{get_size(public_user_data_cache):,}',
        # 'sheets_last_updated_size': len(last_updated),
        # 'sheets_last_updated_bytes': get_size(last_updated),
        'memory usage': f'{resource.getrusage(resource.RUSAGE_SELF).ru_maxrss:,}'
    }
    return jsonResponse(resp)


@staff_member_required
def cache_dump(request):
    resp = {
        'ref_cache_dump': model.Ref.cache_dump()
    }
    return jsonResponse(resp)


@staff_member_required
def export_all(request):
    start = datetime.now()
    try:
        start_export_all()
        resp = {"status": "ok"}
    except Exception as e:
        resp = {"error": str(e)}
    resp["time"] = (datetime.now()-start).seconds
    return jsonResponse(resp)


@staff_member_required
def cause_error(request):
    resp = {}
    logger.error("This is a simple error")
    try:
        erorr = excepting
    except Exception as e:
        logger.exception('An Exception has ocurred in the code')
    erorr = error
    return jsonResponse(resp)

@staff_member_required
def account_stats(request):
    from django.contrib.auth.models import User
    from sefaria.stats import account_creation_stats

    html = account_creation_stats()
    html += "\n\nTotal Accounts: {}".format(User.objects.count())

    return HttpResponse("<pre>" + html + "<pre>")


@staff_member_required
def sheet_stats(request):
    from dateutil.relativedelta import relativedelta
    html  = ""

    html += "Total Sheets: %d\n" % len(list(db.sheets.find()))
    html += "Public Sheets: %d\n" % len(list(db.sheets.find({"status": "public"})))


    html += "\n\nYearly Totals Sheets / Public Sheets / Sheet Creators:\n\n"
    today = datetime.today()
    start = today.replace(year=today.year+1, month=1, day=1, hour=0, minute=0, second=0, microsecond=0)
    years = 5
    for i in range(years):
        end      = start
        start    = end - relativedelta(years=1)
        query    = {"dateCreated": {"$gt": start.isoformat(), "$lt": end.isoformat()}}
        cursor   = db.sheets.find(query)
        total    = len(list(cursor.clone()))
        creators = len(cursor.distinct("owner"))
        query    = {"dateCreated": {"$gt": start.isoformat(), "$lt": end.isoformat()}, "status": "public"}
        ptotal   = len(list(db.sheets.find(query)))
        html += "{}: {} / {} / {}\n".format(start.strftime("%Y"), total, ptotal, creators)

    html += "\n\nUnique Source Sheet creators per month:\n\n"
    start = datetime.today().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    months = 30
    for i in range(months):
        end   = start
        start = end - relativedelta(months=1)
        query = {"dateCreated": {"$gt": start.isoformat(), "$lt": end.isoformat()}}
        n = db.sheets.find(query).distinct("owner")
        html += "%s: %d\n" % (start.strftime("%b %y"), len(n))

    html += "\n\nUnique Source Sheet creators per year:\n\n"
    end   = datetime.today()
    start = datetime.today().replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)
    query = {"dateCreated": {"$gt": start.isoformat(), "$lt": end.isoformat()}}
    n = db.sheets.find(query).distinct("owner")
    html += "%s YTD: %d\n" % (start.strftime("%Y"), len(n))
    years = 3
    for i in range(years):
        end   = start
        start = end - relativedelta(years=1)
        query = {"dateCreated": {"$gt": start.isoformat(), "$lt": end.isoformat()}}
        n = db.sheets.find(query).distinct("owner")
        html += "%s: %d\n" % (start.strftime("%Y"), len(n))

    html += "\n\nAll time contributors:\n\n"
    all_sheet_makers = db.sheets.distinct("owner")
    public_sheet_makers = db.sheets.find({"status": "public"}).distinct("owner")
    public_contributors = set(db.history.distinct("user")+public_sheet_makers)
    all_contributors = set(db.history.distinct("user")+all_sheet_makers)

    html += "Public Sheet Makers: %d\n" % len(public_sheet_makers)
    html += "All Sheet Makers: %d\n" % len(all_sheet_makers)
    html += "Public Contributors: %d\n" % len(public_contributors)
    html += "Public Contributors and Source Sheet Makers: %d\n" % len(all_contributors)

    return HttpResponse("<pre>" + html + "<pre>")


@staff_member_required
def untagged_sheets(request):
    html = ""
    page = int(request.GET.get("page", 0))
    page_size = 100
    sheets = db.sheets.find({"status": "public", "tags": []}, {"id": 1, "title": 1}).limit(page_size).skip(page_size*page)

    for sheet in sheets:
        html += "<li><a href='/sheets/%d' target='_blank'>%s</a></li>" % (sheet["id"], strip_tags(sheet["title"]))
    html += "<br><a href='/admin/untagged-sheets?page=%d'>More </a>" % (page + 1)

    return HttpResponse("<html><h1>Untagged Public Sheets</h1><ul>" + html + "</ul></html>")

@staff_member_required
def categorize_sheets(request):
    props = base_props(request)
    categorize_props = get_sheet_categorization_info("categories")
    props.update(categorize_props)
    propsJSON = json.dumps(props, ensure_ascii=False)
    context = {
        "title": "Categorize Sheets",
        "description": "Retrieve the latest uncategorized, public sheet and allow user to tag",
        "propsJSON": propsJSON
    }
    return render(request, "static/categorize-sheets.html", context)

@staff_member_required
def sheet_spam_dashboard(request):

    from django.contrib.auth.models import User

    if request.method == 'POST':
        return jsonResponse({"error": "Unsupported Method: {}".format(request.method)})

    else:
        date = request.GET.get("date", None)

        if date:
            date = datetime.strptime(date, '%Y-%m-%d')

        else:
            date = request.GET.get("date", datetime.now() - timedelta(days=30))

        earliest_new_user_id = User.objects.filter(date_joined__gte=date).order_by('date_joined')[0].id

        regex = r'.*(?!href=[\'"](\/|http(s)?:\/\/(www\.)?sefaria).+[\'"])(href).*'
        sheets = db.sheets.find({"sources.ref": {"$exists": False}, "dateCreated": {"$gt": date.strftime("%Y-%m-%dT%H:%M:%S.%f")}, "owner": {"$gt": earliest_new_user_id}, "includedRefs": {"$size": 0}, "reviewed": {"$ne": True}, "$or": [{"sources.outsideText": {"$regex": regex}}, {"sources.comment": {"$regex": regex}}, {"sources.outsideBiText.en": {"$regex": regex}}, {"sources.outsideBiText.he": {"$regex": regex}}]})

        sheets_list = []

        for sheet in sheets:
            sheets_list.append({"id": sheet["id"], "title": strip_tags(sheet["title"]), "owner": user_link(sheet["owner"])})

        return render_template(request, 'spam_dashboard.html', None, {
            "title": "Potential Spam Sheets since %s" % date.strftime("%Y-%m-%d"),
            "sheets": sheets_list,
            "type": "sheet",
        })

@staff_member_required
def profile_spam_dashboard(request):

    from django.contrib.auth.models import User

    if request.method == 'POST':
        return jsonResponse({"error": "Unsupported Method: {}".format(request.method)})

    else:
        date = request.GET.get("date", None)

        if date:
            date = datetime.strptime(date, '%Y-%m-%d')

        else:
            date = request.GET.get("date", datetime.now() - timedelta(days=30))

        earliest_new_user_id = User.objects.filter(date_joined__gte=date).order_by('date_joined')[0].id

        regex = r'.*(?!href=[\'"](\/|http(s)?:\/\/(www\.)?sefaria).+[\'"])(href).*'

        spam_keywords_regex = r'(?i).*support.*|.*coin.*|.*helpline.*|.*base.*'

        users_to_check = db.profiles.find(
            {'$and': [
                {"id": {"$gt": earliest_new_user_id}, "reviewed": {"$ne": True}, "settings.reading_history": {"$ne": False}},
                {'$or': [
                    {'website': {"$ne": ""}},
                    {'facebook': {"$ne": ""}},
                    {'twitter': {"$ne": ""}},
                    {'youtube': {"$ne": ""}},
                    {'linkedin': {"$ne": ""}},
                    {'bio': {"$regex": regex}},
                    {'slug': {"$regex": spam_keywords_regex}}
            ]
        }]})



        profiles_list = []

        for user in users_to_check:
            history_count = len(list(db.user_history.find({'uid': user['id'], 'book': {'$ne': 'Sheet'}})))
            if history_count < 10:
                profile = model.user_profile.UserProfile(id=user["id"])

                profiles_list.append({"name": f"{profile.first_name} {profile.last_name}", "email": profile.email, "id": user["id"], "slug": user["slug"], "bio": strip_tags(user["bio"][0:250]), "website": user["website"][0:50]})

        return render_template(request, 'spam_dashboard.html', None, {
            "title": "Potential Spam Profiles since %s" % date.strftime("%Y-%m-%d"),
            "profiles": profiles_list,
            "type": "profile",
        })

@staff_member_required
def delete_user_by_email(request):
    from django.contrib.auth.models import User
    from sefaria.utils.user import delete_user_account
    if request.method == 'GET':
        form = SefariaDeleteUserForm()
        return render_template(request, "registration/delete_user_account.html", None, {'form': form, 'next': next})
    elif request.method == 'POST':
        user = User.objects.get(id=request.user.id)
        email = request.POST.get("email")
        password = request.POST.get("password")
        try:
            if not user.check_password(password):
                return jsonResponse({"failure": "incorrect password"})
        except:
            return jsonResponse({"failure": "incorrect password"})
        try:
            id_to_delete = UserProfile(email=email)
            if delete_user_account(id_to_delete.id, False):
                return jsonResponse({"success": f"deleted user {email}"})
            else:
                return jsonResponse({"failure": "user not deleted: try again or contact a developer"})
        except:
            return jsonResponse({"failure": "user not deleted: try again or contact a developer"})



@staff_member_required
def delete_sheet_by_id(request):

    from django.contrib.auth.models import User
    from sefaria.utils.user import delete_user_account
    if request.method == 'GET':
        form = SefariaDeleteSheet()
        return render_template(request, "delete-sheet.html", None, {'form': form, 'next': next})
    elif request.method == 'POST':
        user = User.objects.get(id=request.user.id)
        sheet_id = request.POST.get("sid")
        password = request.POST.get("password")
        try:
            if not user.check_password(password):
                return jsonResponse({"failure": "incorrect password"})
        except:
            return jsonResponse({"failure": "incorrect password"})
        try:

            import sefaria.search as search
            id = int(sheet_id)
            sheet = db.sheets.find_one({"id": id})
            if not sheet:
                return jsonResponse({"error": "Sheet %d not found." % id})

            db.sheets.delete_one({"id": id})
            process_sheet_deletion_in_collections(id)
            process_sheet_deletion_in_notifications(id)

            try:
                es_index_name = search.get_new_and_current_index_names("sheet")['current']
                search.delete_sheet(es_index_name, id)
            except NewConnectionError as e:
                logger.warn("Failed to connect to elastic search server on sheet delete.")
            except AuthorizationException as e:
                logger.warn("Failed to connect to elastic search server on sheet delete.")


            return jsonResponse({"success": f"deleted sheet {sheet_id}"})

        except:
            return jsonResponse({"failure": "sheet not deleted: try again or contact a developer"})









def purge_spammer_account_data(spammer_id, delete_from_crm=True):
    from django.contrib.auth.models import User
    # Delete from Nationbuilder
    profile = db.profiles.find_one({"id": spammer_id})
    if delete_from_crm:
        try:
            crm_connection_manager = CrmMediator().get_connection_manager()
            crm_connection_manager.mark_as_spam_in_crm(profile)
        except Exception as e:
            logger.error(f'Failed to mark user as spam: {e}')
    sheets = db.sheets.find({"owner": spammer_id})
    for sheet in sheets:
        sheet["spam_sheet_quarantine"] = datetime.now()
        sheet["datePublished"] = None
        sheet["status"] = "unlisted"
        sheet["displayedCollection"] = None
        db.sheets.replace_one({"_id":sheet["_id"]}, sheet, upsert=True)
    # Delete Notes
    db.notes.delete_many({"owner": spammer_id})
    # Delete Notifcations
    db.notifications.delete_many({"uid": spammer_id})
    # Delete Following Relationships
    db.following.delete_many({"follower": spammer_id})
    db.following.delete_many({"followee": spammer_id})
    # Delete Profile
    db.profiles.delete_one({"id": spammer_id})
    # Set account inactive
    spammer_account = User.objects.get(id=spammer_id)
    spammer_account.is_active = False
    spammer_account.save()


@staff_member_required
def spam_dashboard(request):
    if request.method == 'POST':
        req_type = request.POST.get("type")

        if req_type == "sheet":
            spam_sheet_ids = list(map(int, request.POST.getlist("spam_sheets[]", [])))
            reviewed_sheet_ids = list(map(int, request.POST.getlist("reviewed_sheets[]", [])))
            db.sheets.update_many({"id": {"$in": reviewed_sheet_ids}}, {"$set": {"reviewed": True}})
            spammers = db.sheets.find({"id": {"$in": spam_sheet_ids}}, {"owner": 1}).distinct("owner")
            db.sheets.delete_many({"id": {"$in": spam_sheet_ids}})

            for spammer in spammers:
                try:
                    purge_spammer_account_data(spammer)
                except:
                    continue

            return render_template(request, 'spam_dashboard.html', None, {
                "deleted": len(spam_sheet_ids),
                "ids": spam_sheet_ids,
                "reviewed": len(reviewed_sheet_ids),
                "spammers_deactivated": len(spammers)
            })

        elif req_type == "profile":
            spam_profile_ids = list(map(int, request.POST.getlist("spam_profiles[]", [])))
            reviewed_profile_ids = list(map(int, request.POST.getlist("reviewed_profiles[]", [])))
            db.profiles.update_many({"id": {"$in": reviewed_profile_ids}}, {"$set": {"reviewed": True}})

            for spammer in spam_profile_ids:
                try:
                    purge_spammer_account_data(spammer)
                except:
                    continue

            return render_template(request, 'spam_dashboard.html', None, {
                "deleted": len(spam_profile_ids),
                "ids": spam_profile_ids,
                "reviewed": len(reviewed_profile_ids),
                "spammers_deactivated": len(spam_profile_ids)
            })

        else:
            return jsonResponse({"error": "Unknown post type."})

    else:
        return jsonResponse({"error": "Unsupported Method: {}".format(request.method)})

@staff_member_required
def versions_csv(request):
    return HttpResponse(make_versions_csv(), content_type="text/csv")

@csrf_exempt
def index_sheets_by_timestamp(request):
    import dateutil.parser
    from django.contrib.auth.models import User

    key = request.POST.get("apikey")
    if not key:
        return jsonResponse({"error": "You must be logged in or use an API key to index sheets by timestamp."})
    apikey = db.apikeys.find_one({"key": key})
    if not apikey:
        return jsonResponse({"error": "Unrecognized API key."})
    user = User.objects.get(id=apikey["uid"])
    if not user.is_staff:
        return jsonResponse({"error": "Only Sefaria Moderators can add or edit terms."})

    timestamp = request.POST.get('timestamp')
    try:
        dateutil.parser.parse(timestamp)
    except ValueError:
        return jsonResponse({"error": "Timestamp {} not valid".format(timestamp)})
    response_str = search_index_sheets_by_timestamp(timestamp)
    return jsonResponse({"success": response_str})

def library_stats(request):
    return HttpResponse(get_library_stats(), content_type="text/csv")


def core_link_stats(request):
    return HttpResponse(get_core_link_stats(), content_type="text/csv")

@staff_member_required
def run_tests(request):
    # This was never fully developed, methinks
    from subprocess import call
    from .settings import DEBUG
    if not DEBUG:
        return
    call(["/var/bin/run_tests.sh"])


@catch_error_as_http
def text_download_api(request, format, title, lang, versionTitle):

    content = _get_text_version_file(format, title, lang, versionTitle)

    content_types = {
        "json": "application/json; charset=utf-8",
        "csv": "text/csv; charset=utf-8",
        "txt": "text/plain; charset=utf-8",
        "plain.txt": "text/plain; charset=utf-8"
    }
    response = HttpResponse(content, content_type=content_types[format])
    response["Content-Disposition"] = "attachment"
    return response


@staff_member_required
@catch_error_as_http
def bulk_download_versions_api(request):

    format = request.GET.get("format")
    title_pattern = request.GET.get("title_pattern")
    version_title_pattern = request.GET.get("version_title_pattern")
    language = request.GET.get("language")

    error = None
    if not format:
        error = "A value is required for 'format'"
    if not title_pattern and not version_title_pattern:
        error = "A value is required for either 'title_pattern' or 'version_title_pattern'"
    if error:
        return jsonResponse({"error": error})

    query = {}
    if title_pattern:
        query["title"] = {"$regex": title_pattern}
    if version_title_pattern:
        query["versionTitle"] = {"$regex": version_title_pattern}
    if language:
        query["language"] = language

    vs = VersionSet(query)

    if len(vs) == 0:
        return jsonResponse({"error": "No versions found to match query"})

    file_like_object = io.BytesIO()
    with zipfile.ZipFile(file_like_object, "a", zipfile.ZIP_DEFLATED) as zfile:
        for version in vs:
            filebytes = _get_text_version_file(format, version.title, version.language, version.versionTitle)
            name = '{} - {} - {}.{}'.format(version.title, version.language, version.versionTitle, format)
            zfile.writestr(name, filebytes)

    content = file_like_object.getvalue()
    response = HttpResponse(content, content_type="application/zip")
    filename = "{}-{}-{}-{}.zip".format(''.join(list(filter(str.isalnum, str(title_pattern)))), ''.join(list(filter(str.isalnum, str(version_title_pattern)))), language, format)
    response["Content-Disposition"] = 'attachment; filename="{}"'.format(filename)
    response["charset"] = 'utf-8'
    return response


def _get_text_version_file(format, title, lang, versionTitle):
    from sefaria.export import text_is_copyright, make_json, make_text, prepare_merged_text_for_export, prepare_text_for_export, export_merged_csv, export_version_csv

    assert lang in ["en", "he"]
    assert format in ["json", "csv", "txt", "plain.txt"]
    merged = versionTitle == "merged"
    index = library.get_index(title)

    if merged:
        if format == "csv" and merged:
            content = export_merged_csv(index, lang)

        elif format == "json" and merged:
            content = make_json(prepare_merged_text_for_export(title, lang=lang))

        elif format == "txt" and merged:
            content = make_text(prepare_merged_text_for_export(title, lang=lang))

        elif format == "plain.txt" and merged:
            content = make_text(prepare_merged_text_for_export(title, lang=lang), strip_html=True)

    else:
        version_query = {"title": title, "language": lang, "versionTitle": versionTitle}

        if format == "csv":
            version = Version().load(version_query)
            assert version, "Can not find version of {} in {}: {}".format(title, lang, versionTitle)
            assert not version.is_copyrighted(), "Cowardly refusing to export copyrighted text."
            content = export_version_csv(index, [version])
        else:
            version_object = db.texts.find_one(version_query)
            assert version_object, "Can not find version of {} in {}: {}".format(title, lang, versionTitle)
            assert not text_is_copyright(version_object), "Cowardly refusing to export copyrighted text."

            if format == "json":
                content = make_json(prepare_text_for_export(version_object))

            elif format == "txt":
                content = make_text(prepare_text_for_export(version_object))

            elif format == "plain.txt":
                content = make_text(prepare_text_for_export(version_object), strip_html=True)

    return content



@staff_member_required
def text_upload_api(request):
    if request.method != "POST":
        return jsonResponse({"error": "Unsupported Method: {}".format(request.method)})

    from sefaria.export import import_versions_from_stream
    message = ""
    files = request.FILES.getlist("texts[]")
    for f in files:
        try:
            import_versions_from_stream(f, [1], request.user.id)
            message += "Imported: {}.  ".format(f.name)
        except Exception as e:
            return jsonResponse({"error": str(e), "message": message})

    message = "Successfully imported {} versions".format(len(files))
    return jsonResponse({"status": "ok", "message": message})


@staff_member_required
def update_authors_from_sheet(request):
    from sefaria.helper.descriptions import update_authors_data
    res_text = update_authors_data()
    return HttpResponse("\n".join(res_text), content_type="text/plain")

@staff_member_required
def update_categories_from_sheet(request):
    from sefaria.helper.descriptions import update_categories_data
    res_text = update_categories_data()
    return HttpResponse("\n".join(res_text), content_type="text/plain")

@staff_member_required
def update_texts_from_sheet(request):
    from sefaria.helper.descriptions import update_texts_data
    res_text = update_texts_data()
    return HttpResponse("\n".join(res_text), content_type="text/plain")

@staff_member_required
def modtools_upload_workflowy(request):
    from sefaria.helper.text import WorkflowyParser
    if request.method != "POST":
        return jsonResponse({"error": "Unsupported Method: {}".format(request.method)})

    file = request.FILES['wf_file']
    c_index = request.POST.get("c_index", False)
    c_version = request.POST.get("c_version", False)
    delims = request.POST.get("delims", None) if len(request.POST.get("delims", None)) else None
    term_scheme = request.POST.get("term_scheme", None) if len(request.POST.get("term_scheme", None)) else None

    uid = request.user.id
    try:
        wfparser = WorkflowyParser(file, uid, term_scheme=term_scheme, c_index=c_index, c_version=c_version, delims=delims)
        res = wfparser.parse()
    except Exception as e:
        raise e #this will send the django error html down to the client... \_()_/ which is apparently what we want

    return jsonResponse({"status": "ok", "data": res})

@staff_member_required
def links_upload_api(request):
    if request.method != "POST":
        return jsonResponse({"error": "Unsupported Method: {}".format(request.method)})
    file = request.FILES['csv_file']
    uid = request.user.id
    if request.POST.get('action') == "DELETE":
        func = remove_links_from_csv
        args = (file, uid)
    else:
        linktype = request.POST.get("linkType")
        generated_by = request.POST.get("projectName") + ' csv upload'
        func = add_links_from_csv
        args = (file, linktype, generated_by, uid)
    try:
        return jsonResponse({"status": "ok", "data": func(*args)})
    except Exception as e:
        return HttpResponseBadRequest(e)

def get_csv_links_by_refs_api(request, tref1, tref2, by_segment=False):
    try:
        file = get_csv_links_by_refs([tref1, tref2], by_segment=by_segment, **{k: v for k, v in request.GET.items()})
    except Exception as e:
        return HttpResponseBadRequest(e)
    response = HttpResponse(file, content_type="text/csv; charset=utf-8")
    response['Content-Disposition'] = f'attachment; filename="{tref1}-{tref2} links.csv"'
    return response

def compare(request, comp_ref=None, lang=None, v1=None, v2=None):
    print(comp_ref)
    ref_array = None
    sec_ref = ""
    if comp_ref and Ref.is_ref(comp_ref):
        o_comp_ref = Ref(comp_ref)
        sec_ref = o_comp_ref.first_available_section_ref()
        if not sec_ref.is_section_level():
            sec_ref = sec_ref.section_ref()
        if o_comp_ref.is_book_level():
            o_comp_ref = sec_ref
        sec_ref = sec_ref.normal()
        if not o_comp_ref.is_section_level():
            ref_array = [r.normal() for r in o_comp_ref.all_subrefs()]
    if v1:
        v1 = v1.replace("_", " ")
    if v2:
        v2 = v2.replace("_", " ")

    return render_template(request,'compare.html', None, {
        "JSON_PROPS": json.dumps({
            'secRef': sec_ref,
            'v1': v1, 'v2': v2,
            'lang': lang,
            'refArray': ref_array,
        })
    })

```

### sefaria/image_generator.py

```
from PIL import Image, ImageDraw, ImageFont
import textwrap
from bidi.algorithm import get_display
import re
from django.http import HttpResponse
import io

palette = { # [(bg), (font)]
    "Commentary": [(75, 113, 183), (255, 255, 255)],
    "Tanakh": [(0, 78, 95), (255, 255, 255)],
    "Midrash":    [(93, 149, 111), (255, 255, 255)],
    "Mishnah": [(90, 153, 183), (0, 0, 0)],
    "Talmud":    [(204, 180, 121), (0, 0, 0)],
    "Halakhah":    [(128, 47, 62), (255, 255, 255)],
    "Kabbalah":    [(89, 65, 118), (255, 255, 255)],
    "Jewish Thought": [(127, 133, 169), (0, 0, 0)],
    "Liturgy":    [(171, 78, 102), (255, 255, 255)],
    "Tosefta":    [(0, 130, 127), (255, 255, 255)],
    "Chasidut":    [(151, 179, 134), (0, 0, 0)],
    "Musar":    [(124, 65, 111), (255, 255, 255)],
    "Responsa":    [(203, 97, 88), (255, 255, 255)],
    "Quoting Commentary": [(203, 97, 88), (255, 255, 255)],
    "Sheets":    [(24, 52, 93), (255, 255, 255)],
    "Sheet":    [(24, 52, 93), (255, 255, 255)],
    "Targum":    [(59, 88, 73), (255, 255, 255)],
    "Modern Commentary":    [(184, 212, 211), (255, 255, 255)],
    "Reference":    [(212, 137, 108), (255, 255, 255)],
    "System":    [(24, 52, 93), (255, 255, 255)]
}

platforms = {
    "facebook": {
        "width": 1200,
        "height": 630,
        "padding": 260,
        "font_size": 60,
        "ref_font_size": 24,
        "he_spacing": 5,
    },
    "twitter": {
        "width": 1200,
        "height": 600,
        "padding": 260,
        "font_size": 60,
        "ref_font_size": 24,
        "he_spacing": 5,
    }

}

def smart_truncate(content, length=180, suffix='...'):
    if len(content) <= length:
        return content
    else:
        return ' '.join(content[:length+1].split(' ')[0:-1]) + suffix

def calc_letters_per_line(text, font, img_width):
    avg_char_width = sum(font.getsize(char)[0] for char in text) / len(text)
    max_char_count = int(img_width / avg_char_width )
    return max_char_count

def cleanup_and_format_text(text, language):
    #removes html tags, nikkudot and taamim.
    text = text.replace('<br>', ' ')
    cleanr = re.compile('<.*?>')
    text = re.sub(cleanr, '', text)
    text = text.replace("", "-")
    text = text.replace(u"\u05BE", " ")  #replace hebrew dash with ascii

    strip_cantillation_vowel_regex = re.compile("[^\u05d0-\u05f4\s^\x00-\x7F\x80-\xFF\u0100-\u017F\u0180-\u024F\u1E00-\u1EFF\u2000-\u206f]")
    text = strip_cantillation_vowel_regex.sub('', text)
    text = smart_truncate(text)
    return text


def generate_image(text="", category="System", ref_str="", lang="he", platform="twitter"):
    text_color = palette[category][1]
    bg_color = palette[category][0]

    font = ImageFont.truetype(font='static/fonts/Amiri-Taamey-Frank-merged.ttf', size=platforms[platform]["font_size"])
    width = platforms[platform]["width"]
    height = platforms[platform]["height"]
    padding_x = platforms[platform]["padding"]
    padding_y = padding_x/2
    img = Image.new('RGBA', (width, height), color=bg_color)


    if lang == "en":
        align = "left"
        logo_url = "static/img/logo.png"
        spacing = 0
        ref_font = ImageFont.truetype(font='static/fonts/Roboto-Regular.ttf', size=platforms[platform]["ref_font_size"])
        cat_border_pos = (0, 0, 0, img.size[1])

    else:
        align = "right"
        logo_url = "static/img/logo-hebrew.png"
        spacing = platforms[platform]["he_spacing"]
        ref_font = ImageFont.truetype(font='static/fonts/Heebo-Regular.ttf', size=platforms[platform]["ref_font_size"])
        cat_border_pos = (img.size[0], 0, img.size[0], img.size[1])

    text = cleanup_and_format_text(text, lang)
    text = textwrap.fill(text=text, width= calc_letters_per_line(text, font, int(img.size[0]-padding_x)))
    text = get_display(text) # Applies BIDI algorithm to text so that letters aren't reversed in PIL.

    draw = ImageDraw.Draw(im=img)
    draw.text(xy=(img.size[0] / 2, img.size[1] / 2), text=text, font=font, spacing=spacing, align=align,
              fill=text_color, anchor='mm')


    #category line
    draw.line(cat_border_pos, fill=palette[category][0], width=int(width*.02))

    #header white
    draw.line((0, int(height*.05), img.size[0], int(height*.05)), fill=(255, 255, 255), width=int(height*.1))
    draw.line((0, int(height*.1), img.size[0], int(height*.1)), fill="#CCCCCC", width=int(height*.0025))

    #write ref
    draw.text(xy=(img.size[0] / 2, img.size[1]-padding_y/2), text=get_display(ref_str.upper()), font=ref_font, spacing=spacing, align=align, fill=text_color, anchor='mm')


    #border
    draw.line((0, 0, width, 0), fill="#666666", width=1)
    draw.line((0, 0, 0, height), fill="#666666", width=1)
    draw.line((width-1, 0, width-1, height), fill="#666666", width=1)
    draw.line((0, height-1, width, height-1), fill="#666666", width=1)


    #add sefaria logo
    logo = Image.open(logo_url)
    logo.thumbnail((width, int(height*.06)))
    logo_padded = Image.new('RGBA', (width, height))
    logo_padded.paste(logo, (int(width/2-logo.size[0]/2), int(height*.05-logo.size[1]/2)))

    img = Image.alpha_composite(img, logo_padded)


    return(img)

def make_img_http_response(text, category, ref_str, lang, platform):
    try:
        img = generate_image(text, category, ref_str, lang, platform)
    except Exception as e:
        print(e)
        height = platforms[platform]["height"]
        width = platforms[platform]["width"]
        img = Image.new('RGBA', (width, height), color="#18345D")
        logo = Image.open("static/img/logo-white.png")
        logo.thumbnail((400, 400))
        logo_padded = Image.new('RGBA', (width, height))
        logo_padded.paste(logo, (int(width/2-logo.size[0]/2), int(height/2-logo.size[1]/2)))
        img = Image.alpha_composite(img, logo_padded)

    buf = io.BytesIO()
    img.save(buf, format='png')

    res = HttpResponse(buf.getvalue(), content_type="image/png")
    return res

```

### sefaria/wsgi.py

```
"""
WSGI config for sefaria project.

This module contains the WSGI application used by Django's development server
and any production WSGI deployments. It should expose a module-level variable
named ``application``. Django's ``runserver`` and ``runfcgi`` commands discover
this application via the ``WSGI_APPLICATION`` setting.

Usually you will have the standard Django WSGI application here, but it also
might make sense to replace the whole Django WSGI application with a custom one
that later delegates to the Django one. For example, you could introduce WSGI
middleware here, or combine a Django application with an application of another
framework.

"""
import os

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "sefaria.settings")

# This application object is used by any WSGI server configured to use this
# file. This includes Django's development server, if the WSGI_APPLICATION
# setting points here.
from django.core.wsgi import get_wsgi_application
application = get_wsgi_application()


# Apply WSGI middleware here.
# from helloworld.wsgi import HelloWorldApplication
# application = HelloWorldApplication(application)

```

### sefaria/history.py

```
"""
history.py - managing the revision/activity history.

Writes to MongoDB collection: history
"""
from datetime import datetime
from diff_match_patch import diff_match_patch
from bson.code import Code

from sefaria.model import *
from sefaria.system.database import db

dmp = diff_match_patch()


def get_activity(query={}, page_size=100, page=1, filter_type=None, initial_skip=0):
    """
    Returns a list of activity items matching query,
    joins with user info on each item and sets urls.
    """
    query.update(filter_type_to_query(filter_type))
    skip = initial_skip + (page - 1) * page_size
    projection = { "revert_patch": 0 }
    activity = list(db.history.find(query, projection).sort([["date", -1]]).skip(skip).limit(page_size))

    for i in range(len(activity)):
        a = activity[i]
        if a["rev_type"].endswith("text") or a["rev_type"] == "review":
            try:
                a["history_url"] = "/activity/%s/%s/%s" % (Ref(a["ref"]).url(), a["language"], a["version"].replace(" ", "_"))
            except:
                a["history_url"] = "#"
    return activity


def text_history(oref, version, lang, filter_type=None, page=1):
    """
    Return a complete list of changes to a segment of text (identified by ref/version/lang)
    """
    regex_list = oref.regex(as_list=True)
    text_ref_clauses = [{"ref": {"$regex": r}, "version": version, "language": lang} for r in regex_list]
    link_ref_clauses = [{"new.refs": {"$regex": r}} for r in regex_list]
    query = {"$or": text_ref_clauses + link_ref_clauses}
    query.update(filter_type_to_query(filter_type))

    return get_activity(query, page_size=100, page=page, filter_type=filter_type)


def filter_type_to_query(filter_type):
    """
    Translates an activity filter string into a query that searches for it.
    Most strings search for filter_type in the rev_type field, but others may have different behavior:

    'translate' - version is SCT and type is 'add text'
    'flagged'   - type is review and score is less thatn 0.4
    """
    q = {}

    if filter_type == "translate":
        q = {"$and": [dict(list(q.items()) + list({"rev_type": "add text"}.items())), {"version": "Sefaria Community Translation"}]}
    elif filter_type == "index_change":
        q = {"rev_type": {"$in": ["add index", "edit index"]}}
    elif filter_type == "flagged":
        q = {"$and": [dict(list(q.items()) + list({"rev_type": "review"}.items())), {"score": {"$lte": 0.4}}]}
    elif filter_type:
        q["rev_type"] = filter_type.replace("_", " ")

    return q


def collapse_activity(activity):
    """
    Returns a list of activity items in which edits / additions to consecutive segments are collapsed
    into a single entry.
    """

    def continues_streak(a, streak):
        """Returns True if 'a' continues the streak in 'streak'"""
        if not len(streak):
            return False
        b = streak[-1]

        try:
            if a["user"] != b["user"] or \
                a["rev_type"] not in ("edit text", "add text") or \
                b["rev_type"] not in ("edit text", "add text") or \
                a["version"] != b["version"] or \
                Ref(a["ref"]).section_ref() != Ref(b["ref"]).section_ref():

                return False
        except:
            return False

        return True

    def collapse_streak(streak):
        """Returns a single summary activity item that collapses 'streak'"""
        if not len(streak):
            return None
        if len(streak) == 1:
            return streak[0]

        act = streak[0]
        act.update({
            "summary": True,
            #"contents": streak[1:],
            # add the update count form first item if it exists, in case that item was a sumamry itself
            "updates_count": len(streak) + act.get("updates_count", 1) -1,
            "history_url": "/activity/%s/%s/%s" % (Ref(act["ref"]).section_ref().url(),
                                                   act["language"],
                                                   act["version"].replace(" ", "_")),
        })
        return act

    collapsed = []
    current_streak = []

    for a in activity:
        if continues_streak(a, current_streak): # The current item continues
            current_streak.append(a)
        else:
            if len(current_streak):
                collapsed.append(collapse_streak(current_streak))
            current_streak = [a]

    if len(current_streak):
        collapsed.append(collapse_streak(current_streak))

    return collapsed


def get_maximal_collapsed_activity(query={}, page_size=100, page=1, filter_type=None):
    """
    Returns (activity, page) where
    activity is the collasped set of activity items, counting multiple consecutive actions as one
    page is the page number for the next page of queries to search, or None if there are no more results.

    Makes repeat DB calls to return more activity items so a full page_size of items cen returned.
    """
    activity = get_activity(query=query, page_size=page_size, page=page, filter_type=filter_type)
    enough = False
    if len(activity) < page_size:
        enough = True
        page = None

    activity = collapse_activity(activity)

    if len(activity) >= page_size:
        enough = True

    while not enough:
        new_activity = get_activity(query=query, page_size=page_size*5, page=page, filter_type=filter_type, initial_skip=page_size)
        if len(new_activity) < page_size:
            page = None
            enough = True
        else:
            page += 1
        activity = collapse_activity(activity + new_activity)
        enough = enough or len(activity) >= page_size # don't set enough to False if already set to True above

    return (activity, page)


def text_at_revision(tref, version, lang, revision):
    """
    Returns the state of a text (identified by ref/version/lang) at revision number 'revision'
    """
    changes = db.history.find({"ref": tref, "version": version, "language": lang}).sort([['revision', -1]])
    current = TextChunk(Ref(tref), lang, version)
    text = str(current.text)  # needed?

    for r in changes:
        if r["revision"] == revision: break
        patch = dmp.patch_fromText(r["revert_patch"])
        text = dmp.patch_apply(patch, text)[0]

    return text

'''
def next_revision_num():
    """
    Deprecated in favor of sefaria.model.history.next_revision_num()
    """
    last_rev = db.history.find().sort([['revision', -1]]).limit(1)
    revision = last_rev.next()["revision"] + 1 if last_rev.count() else 1
    return revision
'''

def record_index_deletion(title, uid):
    """
    Records the deletion of an index record.
    """
    log = {
        "user": uid,
        "title": title,
        "date": datetime.now(),
        "rev_type": "delete index",
    }
    db.history.insert_one(log)


def record_version_deletion(title, version, lang, uid):
    """
    Records the deletion of a text version.
    """
    log = {
        "user": uid,
        "title": title,
        "version": version,
        "language": lang,
        "date": datetime.now(),
        "rev_type": "delete text",
    }
    db.history.insert_one(log)


def record_sheet_publication(sheet_id, uid):
    """
    Records the publications of a new Source Sheet.
    """
    log = {
        "user": uid,
        "sheet": sheet_id,
        "date": datetime.now(),
        "rev_type": "publish sheet",
    }
    db.history.insert_one(log)


def delete_sheet_publication(sheet_id, user_id):
    """
    Deletes the activity feed item for a sheet publication
    (for when a user unpublishes a sheet)
    """
    db.history.delete_many({
            "user": user_id,
            "sheet": sheet_id,
            "rev_type": "publish sheet"
        })


def top_contributors(days=None):
    """
    Returns a list of users and their activity counts, either in the previous
    'days' if present or across all time.
    Assumes counts have been precalculated and stored in the DB.
    """
    if days:
        collection = "leaders_%d" % days
    else:
        collection = "leaders_alltime"

    leaders = db[collection].find().sort([["count", -1]])

    return [{"user": l["_id"], "count": l["count"]} for l in leaders]


def make_leaderboard_condition(start=None, end=None, ref_regex=None, version=None, actions=[], api=False):

    condition = {}

    # Time Conditions
    if start and end:
        condition["date"] = { "$gt": start, "$lt": end }
    elif start and not end:
        condition["date"] = { "$gt": start }
    elif end and not start:
        condition["date"] = { "$lt": end }

    # Regular Expression to search Ref
    if ref_regex:
        condition["ref"] = {"$regex": ref_regex}

    # Limit to a specific text version
    if version:
        condition["version"] = version

    # Count acitvity from API?
    if not api:
        condition["method"] = {"$ne": "API"}

    return condition


def make_leaderboard(condition):
    """
    Returns a list of user and activity counts for activity that
    matches the conditions of 'condition' - an object used to query
    the history collection.

    This function queries and calculates for all currently matching history.
    """

    reducer = Code("""
                    function(obj, prev) {

                        // Total Points
                        switch(obj.rev_type) {
                            case "add text":
                                if (obj.language !== 'he' && obj.version === "Sefaria Community Translation") {
                                    prev.count += Math.max(obj.revert_patch.length / 10, 10);
                                    prev.translateCount += 1
                                } else if(obj.language !== 'he') {
                                    prev.count += Math.max(obj.revert_patch.length / 400, 2);
                                    prev.addCount += 1
                                } else {
                                    prev.count += Math.max(obj.revert_patch.length / 800, 1);
                                    prev.addCount += 1
                                }
                                break;
                            case "edit text":
                                prev.count += Math.max(obj.revert_patch.length / 1200, 1);
                                prev.editCount += 1
                                break;
                            case "revert text":
                                prev.count += 1;
                                break;
                            case "review":
                                prev.count += 15;
                                prev.reviewCount += 1;
                                break;
                            case "add index":
                                prev.count += 5;
                                break;
                            case "edit index":
                                prev.count += 1;
                                prev.editCount += 1
                                break;
                            case "add link":
                                prev.count += 2;
                                prev.linkCount += 1;
                                break;
                            case "edit link":
                                prev.editCount += 1
                                prev.count += 1;
                                break;
                            case "delete link":
                                prev.count += 1;
                                break;
                            case "add note":
                                prev.count += 1;
                                prev.noteCount += 1;
                                break;
                            case "edit note":
                                prev.count += 1;
                                break;
                            case "delete note":
                                prev.count += 1;
                                break;
                        }

                        // Texts worked on
                        var refs = []
                        if ("ref" in obj && obj.ref) {
                            refs.push(obj.ref);
                        } else if ("refs" in obj && obj.refs[0] && obj.refs[1]) {
                            refs.push(obj.refs[0]);
                            refs.push(obj.refs[1]);
                        }
                        refs.forEach(function(ref) {
                            var text = ref;
                            var i = text.search(/\d/);
                            var text = text.slice(0,i).trim()

                            if (prev.texts[text]) {
                                prev.texts[text] += 1;
                            } else {
                                prev.texts[text] = 1;
                            }
                        });
                    }
                """)

    leaders = db.history.group(['user'],
                        condition,
                        {
                            'count': 0,
                            'translateCount': 0,
                            'addCount': 0,
                            'editCount': 0,
                            'linkCount': 0,
                            'noteCount': 0,
                            'reviewCount': 0,

                            'texts': {}
                        },
                        reducer)

    return sorted(leaders, key=lambda x: -x["count"])


```

### sefaria/decorators.py

```
import base64
from django.contrib.admin.views.decorators import staff_member_required
from functools import wraps
from django.conf import settings

from django.http import HttpResponse


def webhook_auth_or_staff_required(view_func):
    @wraps(view_func)
    def _wrapped_view(request, *args, **kwargs):
        auth_header = request.META.get("HTTP_AUTHORIZATION")

        if not auth_header or not auth_header.startswith("Basic "):
            return staff_member_required(view_func)(request, *args, **kwargs)

        try:
            encoded_credentials = auth_header.split(" ")[1]
            decoded_credentials = base64.b64decode(encoded_credentials).decode("utf-8")
            username, password = decoded_credentials.split(":", 1)
        except Exception:
            return HttpResponse("Invalid Authorization header", status=401)

        if username != settings.WEBHOOK_USERNAME or password != settings.WEBHOOK_PASSWORD:
            return HttpResponse("Invalid credentials", status=401)

        return view_func(request, *args, **kwargs)
    return _wrapped_view


```

